{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0zPe63S2LLc"
      },
      "source": [
        "# 1、**模型**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 论文提出的模型**"
      ],
      "metadata": {
        "id": "Yo-DAJ2xeXdb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V48NGbdax2VA",
        "outputId": "b09231e5-2557-474b-b888-727f2e714d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "!mkdir -p drive1\n",
        "!google-drive-ocamlfuse drive1 # 此时colab中出现drive的文件夹，里面就是你的google drive的根目录文件"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1.1 模块定义**"
      ],
      "metadata": {
        "id": "TpsB1A1P9EAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "#reload(sys)\n",
        "import imp\n",
        "imp.reload(sys)\n",
        "import importlib\n",
        "importlib.reload(sys)\n",
        "#import cPickle as pkl\n",
        "import _pickle as pkl\n",
        "import _pickle as cPickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import matplotlib\n",
        "import time, datetime\n",
        "import re\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "from sklearn import ensemble\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, concatenate, add\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate, Layer, Embedding, LSTM, Dense,Attention\n",
        "from keras import initializers\n",
        "from keras.layers import Input, Dense, merge\n",
        "from keras.models import *\n",
        "#np.random.seed(1337)  # for reproducibility\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "#from tsne import tsne\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "\n",
        "parser = argparse.ArgumentParser(description='embedconv training') #建立解析对象\n",
        "parser.add_argument('-batchsize', dest='batchsize', type=int, default=128, help='size of one batch')#超参数\n",
        "parser.add_argument('-init', dest='init', action='store_true', default=True, help='initialize vector')\n",
        "parser.add_argument('-noinit', dest='init', action='store_false', help='no initialize')\n",
        "parser.add_argument('-trainable', dest='trainable', action='store_true', default=True, help='embedding vectors trainable')\n",
        "parser.add_argument('-notrainable', dest='trainable', action='store_false', help='not trainable')\n",
        "parser.add_argument('-transform', dest='transform', action='store_true', default=True, help='transformation of the cost')\n",
        "parser.add_argument('-test', dest='test', action='store_true', default=False, help='only test step')\n",
        "parser.add_argument('-filter', dest='filter', action='store_true', default=True, help='filter rare codes')\n",
        "parser.add_argument('-isdays', dest='isdays', action='store_true', default=False, help='prediction of length of stay')\n",
        "parser.add_argument('-dropout', dest='dpt', type=float, default=0.01, help='drop out rate')#超参数\n",
        "parser.add_argument('-filtersize', dest='fz', type=int, default=3, help='filter region size')\n",
        "parser.add_argument('-filternumber', dest='fn', type=int, default=100, help='filter numbers')#\n",
        "parser.add_argument('-lr', dest='lr', type=float, default=0.001, help='learning rate' )#超参数\n",
        "parser.add_argument('-maxlen', dest='maxlen', type=int, default=17, help='max sequence length')\n",
        "parser.add_argument('-dim', dest='dim', type=int, default=900, help='embedding vector length')#\n",
        "parser.add_argument('-window', dest='window', type=int, default=10, help='word2vec window size')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "dropout = args.dpt\n",
        "filter_size = args.fz\n",
        "filter_number = args.fn\n",
        "lr = args.lr\n",
        "embedding_vector_length = args.dim\n",
        "print('embedding vector length %d'%(embedding_vector_length))\n",
        "\n",
        "if args.test:\n",
        "    MID = 29\n",
        "    SID = 48\n",
        "else:\n",
        "    i = datetime.datetime.now()\n",
        "    MID = i.minute\n",
        "    SID = i.second\n",
        "f = '/content/gdrive/MyDrive/15684_new_2.csv'\n",
        "p = r'^[A-Z]'\n",
        "pattern = re.compile(p) #将一个正则表达式编译成 Pattern 对象，可以利用 pattern 的一系列方法对文本进行匹配查找\n",
        "\n",
        "def DataClean ():\n",
        "    print('开始步骤1:数据的清洗和导入')\n",
        "    data = pd.read_csv('/content/gdrive/MyDrive/15684_new_2.csv', encoding='utf-8')\n",
        "    data = data[['target','Hospital_days', #预测目标值\n",
        "                'Historical_days','Age','Hospital_times','Historical_times','time_interval','Date','Year','Month','Week','Historical_diagnoses',#患者特征\n",
        "                'Diagnostic_code6']] #疾病特征\n",
        "    data = data.dropna(subset=['Historical_days','Age','Hospital_times','Diagnostic_code6',]) #该函数主要用于滤除缺失数据\n",
        "    cPickle.dump(data, open('/content/gdrive/MyDrive/dataclean.df', 'wb')) ##把data写入到后面链接文件中\n",
        "    return data\n",
        "\n",
        "def ToRawList(data):\n",
        "    print('开始步骤2:形成原始单词清单')\n",
        "    n_samples = len(data.index)\n",
        "    # demographics, P27=days\n",
        "    demographics = np.zeros((n_samples, 10))\n",
        "    demographics[:, 0:1] = data[['Historical_days']].values \n",
        "    demographics[:, 1:2] = data[['Age']].values \n",
        "    demographics[:, 2:3] = data[['Hospital_times']].values\n",
        "    demographics[:, 3:4] = data[['Historical_times']].values\n",
        "    demographics[:, 4:5] = data[['Date']].values\n",
        "    demographics[:, 5:6] = data[['Year']].values\n",
        "    demographics[:, 6:7] = data[['Month']].values\n",
        "    demographics[:, 7:8] = data[['Week']].values\n",
        "    demographics[:, 8:9] = data[['Historical_diagnoses']].values\n",
        "    demographics[:, 9:10] = data[['time_interval']].values\n",
        "    #diseases codes\n",
        "    disease = data[['Diagnostic_code6']]\n",
        "    disease = disease.fillna('')\n",
        "    disease = disease.values\n",
        "    disease = [[str(code).strip() for code in item if code != ''] for item in disease]\n",
        "    #print(disease[:5])\n",
        "    main_dis = data[['Diagnostic_code6']]\n",
        "    main_dis = main_dis.fillna('')\n",
        "    main_dis = main_dis.values\n",
        "    l_disease = [len(item) for item in disease]\n",
        "    l_disease = np.array(l_disease)\n",
        "    C_disease = Counter(l_disease)\n",
        "    #print(\"l_disease的最大长度:{}, 最小长度:{}\".format(np.max(l_disease),np.min(l_disease))) \n",
        "    #print(\"l_disease的the 25% quarter:{}, the 75% quarter:{}, the mean:{}(+-){}\".format(np.percentile(l_disease,25),np.percentile(l_disease,75),np.mean(l_disease),np.std(l_disease))) \n",
        "    l_disease = np.array(l_disease)\n",
        "\n",
        "    seqs = data[['Historical_days','Age','Hospital_times','Historical_times','time_interval','Date','Year','Month','Week','Historical_diagnoses','Diagnostic_code6']]\n",
        "    #seqs = data[['Diagnostic_code6']]\n",
        "    seqs = seqs.fillna('')\n",
        "    seqs = seqs.values\n",
        "    seqs = [['#'.join(str(code).strip().split(' ')) for code in item if code != ''] for item in seqs]#replace the space with '#''\n",
        "    seqs = [[str('0'+code) if len(code.split('.')[0])==1 else code for code in item] for item in seqs] #replace '3.90034' with the '03.90034'\n",
        "    #print(seqs[:5])\n",
        "\n",
        "    cost = data[['target']].values\n",
        "    cost = np.asarray(cost, dtype=np.float32)\n",
        "    days = data[['Hospital_days']].values\n",
        "    days = np.asarray(days, dtype=np.float32)\n",
        "\n",
        "    n_samples = len(seqs) #返回疾病编码的个数，包括相同值，即12548/15684\n",
        "    #print(f,'#samples(seqs的长度):%d'%(n_samples)) # %d：以十进制形式输出带符号整数(正数不输出符号)\n",
        "\n",
        "    main_dis = [[str(code) for code in item if code != ''] for item in main_dis] #输出每个单词\n",
        "    C_maincodes = Counter([code for seq in main_dis for code in seq]) #计数，求每个单词出现的次数\n",
        "    main_code = C_maincodes.keys() #形成单词字典\n",
        "    n_dim = len(main_code) #共有242个单词\n",
        "    #print(\"len(main_code):{}\".format(n_dim))\n",
        "\n",
        "    code2id = dict(zip(main_code, range(n_dim))) #zip：打包为元组的列表，相对于编号\n",
        "    maincodemat = np.zeros((n_samples, n_dim), dtype=np.float32) #形成12548*242的0矩阵\n",
        "    for i in range(n_samples):\n",
        "        for code in main_dis[i]:\n",
        "            if code in code2id:\n",
        "                index = code2id[code]\n",
        "                maincodemat[i,index] += 1\n",
        "    #print(\"maincodemat.shape:{}\".format(maincodemat.shape)) #形成单词矩阵maincodemat，出现，则该行为1\n",
        "    Data = (seqs, cost, days, demographics,disease)\n",
        "    pkl.dump(Data, open('/content/gdrive/MyDrive/Data.df','wb'))\n",
        "    return seqs, cost, days, demographics, disease, main_dis\n",
        "\n",
        "#步骤3:给单词编号\n",
        "def token_to_index(seqs):\n",
        "    print('开始步骤3:给单词编号,形成单词-编号的字典')\n",
        "    C_codes = Counter([code for seq in seqs for code in seq])\n",
        "    code_index = {}\n",
        "    for idx, item in enumerate(C_codes.keys()): #enumerate：同时列出数据、数据下标\n",
        "        code_index[item] = idx + 1\n",
        "    #print(\"the unique codes: {}\".format(len(C_codes.keys())))\n",
        "    return code_index   #输入code_index，输出：{'E10.901': 1,...}\n",
        "\n",
        "def get_index_embedding(code_index={}, level=0):\n",
        "    print(\"开始第4步\")\n",
        "    index_embedding = {}\n",
        "    cnt = 0\n",
        "    dim = 900\n",
        "    window = 10    #glove文件包含各种大小的文本编码向量:50维、100维、200维、300维\n",
        "    #model = KeyedVectors.load_word2vec_format(word_vector_path, binary=True)\n",
        "    model = KeyedVectors.load('/content/gdrive/MyDrive/newlevel%d_word2vec_dim%d_window%d.model' %(level, dim, window))\n",
        "   \n",
        "    for code, index in code_index.items():  # items(): 返回可遍历的(键, 值) 元组数组\n",
        "        #print(re.findall(pattern, code))\n",
        "        if len(re.findall(pattern, code))== 1:\n",
        "            if level == 0:\n",
        "                newcode = code\n",
        "            if level == 1:\n",
        "                newcode = code[0:3]\n",
        "            if level == 2:\n",
        "                newcode = code[0:5]\n",
        "            if level == 3:\n",
        "                newcode = code[0:6]\n",
        "        else:\n",
        "            if level == 0:\n",
        "                newcode = code\n",
        "            if level == 1:\n",
        "                newcode = code[0:3]\n",
        "            if level == 2:\n",
        "                newcode = code[0:4]\n",
        "            if level == 3:\n",
        "                newcode = code[0:5]\n",
        "\n",
        "        if newcode in model:\n",
        "            index_embedding[index] = model[newcode]\n",
        "        else:\n",
        "            cnt = cnt + 1\n",
        "            index_embedding[index] = np.random.uniform(-0.25,0.25,embedding_vector_length)\n",
        "    return index_embedding #即619个疾病代码，每个代码映射成了100维的向量\n",
        "\n",
        "def get_trained_embedding(index_embedding=None):\n",
        "    print(\"开始get_trained_embedding\")\n",
        "    index_sorted = sorted(index_embedding.items()) # sorted by index starting from 1 ;items(): 返回可遍历的(键, 值) 元组数组\n",
        "    trained_embedding = [t[1] for t in index_sorted] #len(trained_embedding):424 type:<class 'list'>\n",
        "    embedding_vector_length = args.dim\n",
        "    zeros = np.random.uniform(-0.25,0.25,embedding_vector_length) #一维的数组  zeros.shape:(424,)\n",
        "    trained_embedding = np.vstack((zeros, trained_embedding)) #它是垂直（按照行顺序）的把数组给堆叠起来。\n",
        "    trained_embedding = np.array(trained_embedding) #trained_embedding.shape:(101, 424);trained_embedding.维度:2\n",
        "    return trained_embedding\n",
        "\n",
        "#步骤5:单尺度编码 E10.901 C12.783\n",
        "def embedding_encoder(idseqs, index_embedding):\n",
        "    print('开始步骤51/3:单尺度编码')\n",
        "    n_samples = len(idseqs)\n",
        "    dim = args.dim\n",
        "    mat = np.zeros((n_samples, dim), np.float32)\n",
        "    for i in range(n_samples):\n",
        "        for codeid in idseqs[i]:\n",
        "            if codeid in index_embedding:\n",
        "                code_embedding = np.array(index_embedding[codeid])\n",
        "            else:\n",
        "                code_embedding = np.random.uniform(-0.25, 0.25, dim)\n",
        "            mat[i] += code_embedding\n",
        "    mat = np.array(mat)\n",
        "    return mat\n",
        "\n",
        "#多尺度编码 E10.901 C12.783     E10.90 C12.78  E10.9      6-5-4-3\n",
        "def multichannel_embedding_encoder(idseqs, index_embedding,index_embedding1,index_embedding2,index_embedding3):\n",
        "    print('开始步骤52/3:开始多尺度编码...')\n",
        "    n_samples = len(idseqs)\n",
        "    dim = args.dim\n",
        "    mat = np.zeros((n_samples, dim), np.float32)\n",
        "    for i in range(n_samples):\n",
        "        for codeid in idseqs[i]:\n",
        "            code_embedding = np.asarray([0]*dim, np.float32)\n",
        "            if codeid in index_embedding:\n",
        "                code_embedding += np.array(index_embedding[codeid])\n",
        "            if codeid in index_embedding1:\n",
        "                code_embedding += np.array(index_embedding1[codeid])\n",
        "            if codeid in index_embedding2:\n",
        "                code_embedding += np.array(index_embedding2[codeid])\n",
        "            if codeid in index_embedding3:\n",
        "                code_embedding += np.array(index_embedding3[codeid])\n",
        "            mat[i] += code_embedding\n",
        "    mat = np.array(mat) \n",
        "    return mat\n",
        "\n",
        "#独热码\n",
        "def one_hot_encoder(seqsind, nb_words):\n",
        "    print('开始步骤53/3:开始独热编码one_hot encoding...')\n",
        "    n_samples = len(seqsind)\n",
        "    n_dim = nb_words\n",
        "    mat = np.zeros((n_samples, n_dim), dtype=np.float32)\n",
        "    for i in range(n_samples):\n",
        "        for codeid in seqsind[i]:\n",
        "            if codeid !=0:\n",
        "                index = codeid - 1 # for the seqid starts from 1\n",
        "                mat[i,index] = mat[i,index] + 1\n",
        "    return mat\n",
        "\n",
        "#步骤6:\n",
        "def svd(seqsind, dim=args.dim): #600 奇异值分解(SVD)\n",
        "    print(\"开始61/3:开始svd\")\n",
        "    mat = one_hot_encoder(seqsind, nb_words)\n",
        "    #print(\"mat.shape:{}\".format(mat.shape))\n",
        "    from sklearn.decomposition import TruncatedSVD\n",
        "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
        "    print('SVD......') \n",
        "    svd.fit(mat)\n",
        "    res = svd.transform(mat)\n",
        "    print('SVD done!') \n",
        "    return res\n",
        "\n",
        "def glove(level, dim=args.dim, window=10): \n",
        "    print(\"开始62/3:开始glove\")\n",
        "    vectors = '/content/gdrive/MyDrive/weighted1_vectors%d'%(level)\n",
        "    model = KeyedVectors.load_word2vec_format(vectors, binary=False)\n",
        "    name = 'newlevel%d_weighted1_glove_dim%d_window%d.model'%(level, dim, window)\n",
        "    model.save('/content/gdrive/MyDrive/cnn_model/'+name)\n",
        "    return model\n",
        "\n",
        "def plot_embedding(data, label, title):\n",
        "    x_min, x_max = np.min(data, 0), np.max(data, 0)\n",
        "    data = (data - x_min) / (x_max - x_min)\n",
        " \n",
        "    fig = plt.figure()\n",
        "    ax = plt.subplot(111)\n",
        "    for i in range(data.shape[0]):\n",
        "        plt.text(data[i, 0], data[i, 1], str(label[i]),\n",
        "                 color=plt.cm.Set1(label[i]),\n",
        "                 fontdict={'weight': 'bold', 'size': 9})\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(title)\n",
        "    return fig\n",
        "\n",
        "def word2vec_model(disease, level, window=args.window, dim=args.dim, visualize=True): #seqs\n",
        "    print(\"开始63/3:开始word2vec\")\n",
        "    n_samples = len(disease)\n",
        "    sentences = word2vec.LineSentence('/content/gdrive/MyDrive/15684diagnosis.txt')\n",
        "    \n",
        "    sentences = []\n",
        "    for sentence in disease:\n",
        "        shuffle(sentence)\n",
        "        sentences.append(sentence)\n",
        "    \n",
        "    model = word2vec.Word2Vec(sentences, size=dim,sg=1,min_count=1,window=window)\n",
        "    model.wv.save_word2vec_format(\"/content/gdrive/MyDrive/word2vec.vector\", binary=True)\n",
        "    model.save('/content/gdrive/MyDrive/newlevel%d_word2vec_dim%d_window%d.model' %(level, dim, window))\n",
        "    print(\"model:\",model) \n",
        "    if visualize:\n",
        "        codes_vocabulary = Counter([code for seq in disease for code in seq]).keys()\n",
        "        #print(\"codes_vocabulary :\",codes_vocabulary ) #codes_vocabulary : dict_keys(['E10.901', 'O24.901', 返回值返回一个字典所有的键\n",
        "        X = np.vstack([model[item][np.newaxis,:] for item in codes_vocabulary])\n",
        "        #print(\"X:\",X,\"X.shape:\",X.shape) #(242,900)\n",
        "        Y = TSNE(n_components=2, init='pca', verbose=1).fit_transform(X)\n",
        "        #print(\"Y:\",Y,\"Y.shape:\",Y.shape) #(242,2)\n",
        "        codes_vocabulary11=list(codes_vocabulary)\n",
        "        #print(\"codes_vocabulary11:\",codes_vocabulary11) #codes_vocabulary11: ['E10.901', 'O24.901', 'I67.801', \n",
        "\n",
        "        pkl.dump(codes_vocabulary11, open('/content/gdrive/MyDrive/codes_vocabulary.pkl', 'wb')) #pickle.dump() 直接把对象序列化后，将对象obj保存到文件file(这里的file是文件句柄) 中去。\n",
        "        pkl.dump(X, open('/content/gdrive/MyDrive/word2vec_vector.pkl', 'wb'))\n",
        "        pkl.dump(Y, open('/content/gdrive/MyDrive/word2vec_2d_vector.pkl', 'wb'))\n",
        "        import pickle\n",
        "        f=open(r'/content/gdrive/MyDrive/word2vec_2d_vector.pkl', 'rb')\n",
        "        Y = pickle.load(f)\n",
        "        fig, ax = plt.subplots()\n",
        "        aa=[]\n",
        "        for a in codes_vocabulary11:aa.append(a[0])\n",
        "        label = np.array(aa)\n",
        "        #print(\"label:\",label,\"label.dtype:\",label.shape) #(242,)\n",
        "        #print(label[0],label[1])\n",
        "        tsne = TSNE(n_components=2, init='pca', verbose=1)\n",
        "        groups = pd.DataFrame(Y, columns=['x', 'y']).assign(category=label).groupby('category')\n",
        "        for name, points in groups:\n",
        "          plt.title('Visualization of medical concept vectors by t-SNE', size=10)\n",
        "          ax.scatter(points.x, points.y, label=name, marker='o')\n",
        "          ax.legend(loc=1,fontsize=5,frameon=True)\n",
        "        for i in range(len(codes_vocabulary11)):\n",
        "          ax.text(Y[i,0], Y[i,1], codes_vocabulary11[i], fontsize=4, \n",
        "          color = \"g\", style = 'normal', weight = \"light\", verticalalignment='center', \n",
        "          horizontalalignment='right',rotation=0) #给散点加标签\n",
        "        fig.savefig('/content/gdrive/MyDrive/%s_tsne.pdf'%(level))\n",
        "    #return model\n",
        "\n",
        "\n",
        "#步骤7:加载数据\n",
        "def load_data(X,demographics, y, onehot_mat):\n",
        "    print('开始步骤7:划分训练集、验证集、测试集...')\n",
        "    print(\"y:\",y)\n",
        "    #y=np.log(y)\n",
        "    indices = np.arange(n_seqs)\n",
        "    np.random.shuffle(indices) #打乱\n",
        "    X = X[indices]\n",
        "    demographics = demographics[indices]\n",
        "    y = y[indices]\n",
        "    #onehot_mat = onehot_mat[indices]\n",
        "\n",
        "    n_tr = int(n_seqs * 0.85)\n",
        "    n_va = int(n_seqs * 0.05)\n",
        "    n_te = n_seqs - n_tr - n_va\n",
        "    X_train = X[:n_tr]\n",
        "    demographics_train = demographics[:n_tr]\n",
        "    y_train = y[:n_tr]\n",
        "    #onehot_mat_train = onehot_mat[:n_tr]\n",
        "\n",
        "    X_valid = X[n_tr:n_tr+n_va]\n",
        "    demographics_valid = demographics[n_tr:n_tr+n_va]\n",
        "    y_valid = y[n_tr:n_tr+n_va]\n",
        "    #onehot_mat_valid = onehot_mat[n_tr:n_tr+n_va]\n",
        "\n",
        "    X_test = X[-n_te:]\n",
        "    demographics_test = demographics[-n_te:]\n",
        "    y_test = y[-n_te:]\n",
        "    #onehot_mat_test = onehot_mat[-n_te:]\n",
        "\n",
        "    #print np.max(y_test),np.min(y_test)\n",
        "    return X_train, X_test, X_valid, y_train, y_test, y_valid, demographics_train, demographics_test, demographics_valid#, onehot_mat_train,onehot_mat_test,onehot_mat_valid\n",
        "\n",
        "\n",
        "def filter_test(X_test, index_code, threshold=2):\n",
        "    print('开始步骤8:选择罕见集样本用于测试集...，这一步其实是将所有数据15684都用于了测试集')\n",
        "    comm_inds = []\n",
        "    for ind, seq in enumerate(X_test):\n",
        "        cnt = 0\n",
        "        for index in seq:\n",
        "            if index in index_code:\n",
        "                code = index_code[index]\n",
        "            else:\n",
        "                code = 'none'\n",
        "        if cnt < threshold:\n",
        "            comm_inds.append(int(ind))\n",
        "    print('comm_inds:%d'%(len(comm_inds)))\n",
        "    comm_inds = np.asarray(comm_inds)\n",
        "    return  comm_inds\n",
        "\n",
        "#步骤8:预测、可视化\n",
        "def cross_validation():\n",
        "    print(\"开始5折交叉验证\")\n",
        "    estimator = KerasRegressor(build_fn=cnn_model, epochs=100, batch_size=args.batchsize, verbose=0)\n",
        "    kfold = KFold(n_splits=5, random_state=seed)\n",
        "    res = cross_val_score(estimator, X, y, cv=kfold)\n",
        "    print( 'cross validation for cnn model ......')\n",
        "    print('cnn_model: MSE %.2f (+-)%.2f'%(res.mean(), res.std())) \n",
        "#cross_validation()\n",
        "\n",
        "def evaluation(y_test, y_pred):\n",
        "    print('y_test:{}'.format(y_test.shape))\n",
        "    print('y_pred:{}'.format(y_pred.shape))\n",
        "    print(y_pred.dtype) #float32\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print('MSE:{}, RMSE:{}, MAE:{}, R2:{}'.format(mse, rmse, mae,r2)) \n",
        "    return r2, rmse\n",
        "\n",
        "def daysplots(y_test, y_pred, r2, rmse, name):\n",
        "    print(\"开始步骤91/2:预测可视化-住院天数\")\n",
        "    fig, ax = plt.subplots()\n",
        "    #ax.set_xlim(0, 35)\n",
        "    #ax.set_ylim(0, 35)\n",
        "    #ax.plot(y_test.squeeze(), y_test.squeeze(), s=10, marker='.',c='r')\n",
        "    ax.scatter(y_test.squeeze(), y_pred, s=10,marker='.', c='b')\n",
        "    ax.text(28,30,'R2:{:.4f}'.format(r2))\n",
        "    ax.text(28,32,'RMSE:{:.2f}'.format(rmse))\n",
        "    ax.set_xlabel('True length of stay (days)')\n",
        "    ax.set_ylabel('predicted length of stay (days)')\n",
        "    fig.savefig('/content/gdrive/MyDrive/%s_days.pdf'%(name))\n",
        "\n",
        "def costplots(y_test, y_pred, r2, rmse, name):\n",
        "    print(\"开始步骤92/2:预测可视化-医疗费用\")\n",
        "    fig, ax = plt.subplots()\n",
        "    #ax.set_xlim(0, 12000)\n",
        "    #ax.set_ylim(0, 12000)\n",
        "    ax.scatter(y_test.squeeze(), y_pred, s=10,marker='o',c='b') #c='b'\n",
        "   # ax.text('R2:{:.4f}'.format(r2)) #8000,10000,\n",
        "    #ax.text('RMSE:{:.2f}'.format(rmse)) #8000,11000,\n",
        "    ax.set_xlabel('Series')\n",
        "    ax.set_ylabel('Medical cost')\n",
        "    fig.savefig('/content/gdrive/MyDrive/%s_cost.png'%(name),dpi=500,bbox_inches = 'tight')\n",
        "\n",
        "    #画散点图\n",
        "    plt.figure()#figsize=(15,5)\n",
        "    plt.plot(y_test,'rs',label='True value')\n",
        "    plt.plot(y_pred,'go',label='Predict value')\n",
        "    #plt.title('Predicted and true values of medical costs',fontsize=13)\n",
        "    plt.xticks(fontsize=13)\n",
        "    plt.yticks(fontsize=13)\n",
        "    plt.xlabel('Series',fontsize=13)\n",
        "    plt.ylabel('Medical Cost',fontsize=13)\n",
        "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
        "    plt.savefig('/content/gdrive/MyDrive/%s_散点图.png'%(name),dpi=500,bbox_inches = 'tight')\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(y_test, linewidth = 1,linestyle='-',label='True Value') #marker='*'\n",
        "    plt.plot(y_pred, linewidth = 1,linestyle='-',label='Predicted Value') #marker='^',\n",
        "    #ax.text('R2:{:.4f}'.format(r2)) #8000,10000,\n",
        "    #ax.text('RMSE:{:.2f}'.format(rmse)) #8000,11000,\n",
        "    plt.xlabel('Series',fontsize=13)\n",
        "    plt.ylabel('Medical Cost',fontsize=13)\n",
        "    plt.xlabel('Series',fontsize=13)\n",
        "    plt.ylabel('Medical Cost',fontsize=13)\n",
        "    fig.savefig('/content/gdrive/MyDrive/%s_曲线图.pdf'%(name))\n",
        "\n",
        "def extract_patientvec(model,modelpath, disease,  demographics):\n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),by_name=False)\n",
        "    sub_model1 = Model(inputs=model.inputs, outputs=model.get_layer('F1').output)#错误代码修改\n",
        "    sub_model2 = Model(inputs=model.inputs, outputs=model.get_layer('F2').output)#错误代码修改\n",
        "    sub_model3 = Model(inputs=model.inputs, outputs=model.get_layer('F3').output)#错误代码修改\n",
        "    sub_model4 = Model(inputs=model.inputs, outputs=model.get_layer('F4').output)#错误代码修改\n",
        "    sub_model5 = Model(inputs=model.inputs, outputs=model.get_layer('F5').output)#错误代码修改\n",
        "\n",
        "    patientvecs3 = sub_model3.predict([disease,  demographics], verbose=1)\n",
        "    modelname='sub_model3'\n",
        "    print(patientvecs3.shape)\n",
        "    pkl.dump(patientvecs3, open('/content/gdrive/MyDrive/cnn_model/patientvec_%s'%(modelname),'wb'))\n",
        "\n",
        "    patientvecs4 = sub_model4.predict([disease, demographics], verbose=1)\n",
        "    modelname='sub_model4'\n",
        "    print(patientvecs4.shape)\n",
        "    pkl.dump(patientvecs4, open('/content/gdrive/MyDrive/cnn_model/patientvec_%s'%(modelname),'wb'))\n",
        "\n",
        "    patientvecs5 = sub_model5.predict([disease,  demographics], verbose=1)\n",
        "    modelname='sub_model5'\n",
        "    print(patientvecs5.shape)\n",
        "    pkl.dump(patientvecs5, open('/content/gdrive/MyDrive/cnn_model/patientvec_%s'%(modelname),'wb'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqweJYLc8Umd",
        "outputId": "ed97936e-e34d-42c3-8eb4-fd0986acc581"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding vector length 900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1.2 对比模型的定义**"
      ],
      "metadata": {
        "id": "ujP1KsIf9NxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#单尺度模型\n",
        "####这里这里这里！！！！！！！！！！\n",
        "from tensorflow.keras import optimizers # 错误代码1的改正方案\n",
        "#模型1:单尺度融合模型\n",
        "def single_channel_merge_model(demgras_dim):\n",
        "    print('开始模型1:单尺度融合模型')\n",
        "    codes_in = Input(shape=(MAX_LEN, ), dtype='float32') #MAX_len 输入句子的最大长度17\n",
        "    if args.init:\n",
        "        print('initialize embedding layer with pre-training vectors')\n",
        "        print('依据预训练向量 初始化嵌入层')\n",
        "        print('embedding layers trainalble %s' % args.trainable) \n",
        "        embedding0level = Embedding(output_dim=embedding_vector_length,  #424\n",
        "                                input_dim=max_features, \n",
        "                                input_length=MAX_LEN,\n",
        "                                weights=[embedding_matrix0],\n",
        "                                trainable=args.trainable)(codes_in)\n",
        "    else:\n",
        "        print('one hot embedding with random initialization...') \n",
        "        embedding0level = Embedding(output_dim=embedding_vector_length,\n",
        "                                    input_dim=max_features,\n",
        "                                    embeddings_initializer='random_uniform',\n",
        "                                    input_length=MAX_LEN)(codes_in)\n",
        "    conv_result = []\n",
        "    for i in range(3):\n",
        "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
        "        conv0 = conv_layer(embedding0level)\n",
        "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
        "        conv_result.append(pooling0)\n",
        "\n",
        "    print('添加人口统计信息，融合')\n",
        "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
        "    dense_demgras = Dense(3, activation='sigmoid')(demgras_in)\n",
        "    conv_result.append(dense_demgras)#append the demographics information\n",
        "    merge_out = concatenate(conv_result)\n",
        "    dense_out = Dense(500, activation='relu')(merge_out)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(100, activation='relu')(dpt)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    mode_out = Dense(1)(dpt)\n",
        "    model = Model([codes_in, demgras_in], mode_out)\n",
        "    #rmsprop = optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) 出错代码1\n",
        "    #rmsprop = optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) #weight_decay的作用是正则化\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0)\n",
        "    model.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['mae']) #MSE均方误差：真实值-预测值 然后平方之后求和平均 #MAE：平均绝对误差\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "#模型2:单尺度划分模型\n",
        "def single_channel_split_model(demgras_dim):\n",
        "    dis_in = Input(shape=(DIS_MAX_LEN, ), dtype='float32')\n",
        "    if args.init:\n",
        "        print('initialize embedding layer with pre-training vectors')\n",
        "        print('依据预训练向量，初始化嵌入层...')\n",
        "        print('embedding layers trainalble %s' % args.trainable) \n",
        "        dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
        "                                        input_dim=max_features,\n",
        "                                        input_length=DIS_MAX_LEN,\n",
        "                                        weights=[embedding_matrix0],\n",
        "                                        trainable=args.trainable)(dis_in)\n",
        "\n",
        "    else:\n",
        "        print('one hot embedding with random initialization...')\n",
        "        print('随机初始化,独热嵌入层...')\n",
        "        dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                embeddings_initializer='random_uniform')(dis_in)\n",
        "\n",
        "    conv_result = []\n",
        "    for i in range(3):\n",
        "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
        "        conv0 = conv_layer(dis_embedding0level)\n",
        "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
        "        conv_result.append(pooling0)\n",
        "\n",
        "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
        "    dense_demgras = Dense(3, activation='sigmoid')(demgras_in)\n",
        "    conv_result.append(dense_demgras)#append the demographics information\n",
        "    merge_out = concatenate(conv_result)\n",
        "    dense_out = Dense(1000, activation='relu')(merge_out)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(500, activation='relu')(dpt)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(100, activation='relu')(dpt)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    mode_out = Dense(1)(dpt)\n",
        "    model = Model([dis_in,  demgras_in], mode_out) #sur_in,\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0)\n",
        "    model.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['mae'])\n",
        "    print(model.summary())\n",
        "    return model\n",
        "#第三个模型：多尺度划分模型\n",
        "def multi_channel_split_model(demgras_dim):\n",
        "    dis_in = Input(shape=(DIS_MAX_LEN, ), dtype='float32')\n",
        "    dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix0],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    dis_embedding1level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix1],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    dis_embedding2level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix2],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    dis_embedding3level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix3],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "\n",
        "    conv_result = []\n",
        "    for i in range(3):\n",
        "        channel_result = []\n",
        "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
        "        conv0 = conv_layer(dis_embedding0level)\n",
        "        conv1 = conv_layer(dis_embedding1level)\n",
        "        conv2 = conv_layer(dis_embedding2level)\n",
        "        conv3 = conv_layer(dis_embedding3level)\n",
        "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
        "        pooling1 = GlobalMaxPooling1D()(conv1)\n",
        "        pooling2 = GlobalMaxPooling1D()(conv2)\n",
        "        pooling3 = GlobalMaxPooling1D()(conv3)\n",
        "        channel_result.append(pooling0)\n",
        "        channel_result.append(pooling1)\n",
        "        channel_result.append(pooling2)\n",
        "        channel_result.append(pooling3)\n",
        "        allchannel = add(channel_result)\n",
        "        conv_result.append(allchannel)\n",
        "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
        "    print(\"demgras_dim:{}\".format(demgras_dim)) #demgras_dim:11\n",
        "    print(\"demgras_in.shape:{}\".format(demgras_in.shape))#(None, 11)\n",
        "    dense_demgras = Dense(1000, activation='sigmoid')(demgras_in)\n",
        "    conv_result.append(dense_demgras)#append the demographics information\n",
        "    merge_out = concatenate(conv_result) #numpy.concatenate函数 主要作用:沿现有的某个轴对一系列数组进行拼接。\n",
        "    dense_out = Dense(1000, activation='relu', name='F1')(merge_out)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(500, activation='relu', name='F2')(dpt) \n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(100, activation='relu', name='F3')(dpt)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(50, activation='relu', name='F4')(dpt)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(2, activation='relu', name='F5')(dpt)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    mode_out = Dense(1)(dpt)\n",
        "    model = Model([dis_in,demgras_in], mode_out) #sur_in, \n",
        "    #rmsprop = optimizers.rmsprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) 均方根传播(RMSProp) ;根据最近的权重梯度的平均值(例如变化的速度)来调整;这意味着该算法在线上和非平稳问题上表现良好(如:噪声)。\n",
        "    Adam=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
        "    model.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae']) #rmsprop\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def train_single_channel_merge_model(model, modelpath, X_train, demographics_train, y_train):\n",
        "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
        "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
        "    earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
        "\n",
        "    print('Training the merge model....')\n",
        "    model.fit([X_train, demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
        "              validation_split=0.1,\n",
        "              callbacks=[checkpointer,earlystopper],\n",
        "              verbose=1)\n",
        "\n",
        "def train_single_channel_split_model(model, modelpath, X1_train, demographics_train, y_train):\n",
        "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
        "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
        "    earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
        "\n",
        "    print('Training the merge model....') \n",
        "    model.fit([X1_train,  demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
        "              validation_split=0.1,\n",
        "              callbacks=[checkpointer,earlystopper],\n",
        "              verbose=1)\n",
        "\n",
        "def train_multi_channel_split_model(model, modelpath, X1_train,  demographics_train, y_train):\n",
        "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
        "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
        "    earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
        "\n",
        "    print('Training the merge model....') \n",
        "    model.fit([X1_train,  demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
        "              validation_split=0.1,\n",
        "              callbacks=[checkpointer,earlystopper],\n",
        "              verbose=1) \n",
        "\n",
        "def test_single_channel_merge_model(model, modelpath,X_test,demographics_test,y_test, index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    print(r2)\n",
        "    name = 'SG_merge%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "def test_single_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test,  demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    name = 'SG_split%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "def test_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    print(\"y_pred:{}\".format(y_pred.shape))\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    name = 'MG_split%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)"
      ],
      "metadata": {
        "id": "bdzN36hX8-ku"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1.3 我们的模型**"
      ],
      "metadata": {
        "id": "_9Y7b-aN9VMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#第四个模型：多尺度划分——注意力模型\n",
        "# se注意力机制 现有注意力模块的另一个重要影响因素：权值生成方法。现有注意力往往采用额外的子网络生成注意力权值，比如SE的GAP+FC+ReLU+FC+Sigmoid。\n",
        "def se_block(inputs, ratio=2):  # ratio代表第一个全连接层下降通道数的系数\n",
        "    in_channel = inputs.shape[-1]# 获取输入特征图的通道数\n",
        "    print('通道数:{}'.format(in_channel))\n",
        "    x = layers.GlobalAveragePooling1D()(inputs) # 全局平均池化[h,w,c]==>[None,c] #压缩：得到当前Feature Map的全局压缩特征量\n",
        "    x = layers.Reshape(target_shape=(1,1,in_channel))(x) # [None,c]==>[1,1,c]\n",
        "    x = layers.Dense(in_channel//ratio)(x)  # 全连接下降通道数 # [1,1,c]==>[1,1,c/4] #激发：通过两层全连接的bottleneck结构得到Feature Map中每个通道的权值\n",
        "    x = tf.nn.relu(x) # relu激活\n",
        "    x = layers.Dense(in_channel)(x)  # 全连接上升通道数 # [1,1,c/4]==>[1,1,c]\n",
        "    x = tf.nn.sigmoid(x) # sigmoid激活，权重归一化\n",
        "    print(\"sigmoid激活，权重归一化:\",x)\n",
        "    outputs = layers.multiply([inputs, x])  # 归一化权重和原输入特征图逐通道相乘 # [h,w,c]*[1,1,c]==>[h,w,c] #激发：并将加权后的Feature Map作为下一层网络的输入。\n",
        "    return outputs  \n",
        "#eca注意力机制：相比于se模块，实现了适当的跨通道交互而不是像全连接层一样全通道交互。（用一维卷积替换了全连接层）\n",
        "#通过执行卷积核大小为k的一维卷积来生成通道权重，其中k通过通道维度C的映射自适应地确定。\n",
        "import math\n",
        "def eca_block(inputs, b=1, gama=2):\n",
        "    in_channel = inputs.shape[-1] # 输入特征图的通道数\n",
        "    print('通道数:{}'.format(in_channel))\n",
        "    kernel_size = int(abs((math.log(in_channel, 2) + b) / gama)) # 根据公式计算自适应卷积核大小\n",
        "    if kernel_size % 2:              # 如果卷积核大小是偶数，就使用它\n",
        "        kernel_size = kernel_size\n",
        "    else:                            # 如果卷积核大小是奇数就变成偶数\n",
        "        kernel_size = kernel_size + 1\n",
        "    x = layers.GlobalAveragePooling1D()(inputs) # [h,w,c]==>[None,c] 全局平均池化\n",
        "    x = layers.Reshape(target_shape=(in_channel, 1))(x) # [None,c]==>[c,1]\n",
        "    x = layers.Conv1D(filters=1, kernel_size=kernel_size, padding='same', use_bias=False)(x) # [c,1]==>[c,1]\n",
        "    x = tf.nn.sigmoid(x) # sigmoid激活\n",
        "    x = layers.Reshape((1,1,in_channel))(x) # [c,1]==>[1,1,c]\n",
        "    outputs = layers.multiply([inputs, x]) # 结果和输入相乘\n",
        "    return outputs\n",
        "##\n",
        "def prelu(_x): #name=None\n",
        "    \"\"\"parametric ReLU activation\"\"\"\n",
        "    print(\"_x:\",_x.get_shape(),\"_x.get_shape()[-1]:\",_x.get_shape()[-1])\n",
        "    _alpha = tf.compat.v1.get_variable(\"prelu\", #name +\n",
        "                             shape=_x.get_shape()[-1], #get_shape():得到张量（数组）的维度\n",
        "                             dtype=_x.dtype,\n",
        "                             initializer=tf.constant_initializer(0.001))\n",
        "    pos = tf.nn.relu(_x)\n",
        "    neg = _alpha * (_x - tf.abs(_x)) * 0.5\n",
        "\n",
        "    return pos + neg\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "def ATTENTION_multi_channel_split_model(demgras_dim):\n",
        "    dis_in = Input(shape=(DIS_MAX_LEN, ), dtype='float32')\n",
        "    dis_embedding0level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix0],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    dis_embedding1level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix1],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    dis_embedding2level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix2],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    dis_embedding3level = Embedding(output_dim=embedding_vector_length,\n",
        "                                input_dim=max_features,\n",
        "                                input_length=DIS_MAX_LEN,\n",
        "                                weights=[embedding_matrix3],\n",
        "                                trainable=args.trainable)(dis_in)\n",
        "    #bn = BatchNormalization() #BN层就是为了让让每一层的值在一个有效范围内传递下去。1、加快网络的训练和收敛的速度 2、控制梯度爆炸防止梯度消失 3、防止过拟合\n",
        "    #fusion_vector = concatenate([dis_embedding0level, dis_embedding1level, dis_embedding2level, dis_embedding3level])\n",
        "    conv_result = []\n",
        "    for i in range(3):\n",
        "        channel_result = []\n",
        "        #lstm = LSTM(units=1000, recurrent_activation='leaky_relu', dropout=0.1) \n",
        "        #lstm_out = lstm(fusion_vector)\n",
        "        conv_layer = Conv1D(filter_number, filter_size, padding='same', activation='relu')\n",
        "        conv0 = conv_layer(dis_embedding0level)\n",
        "        conv1 = conv_layer(dis_embedding1level)\n",
        "        conv2 = conv_layer(dis_embedding2level)\n",
        "        conv3 = conv_layer(dis_embedding3level)\n",
        "        pooling0 = GlobalMaxPooling1D()(conv0)\n",
        "        pooling1 = GlobalMaxPooling1D()(conv1)\n",
        "        pooling2 = GlobalMaxPooling1D()(conv2)\n",
        "        pooling3 = GlobalMaxPooling1D()(conv3)\n",
        "        channel_result.append(pooling0)\n",
        "        channel_result.append(pooling1)\n",
        "        channel_result.append(pooling2)\n",
        "        channel_result.append(pooling3)\n",
        "        allchannel = add(channel_result)\n",
        "        conv_result.append(allchannel)\n",
        "        #bn = BatchNormalization()\n",
        "        #conv_result.append(lstm_out)\n",
        "\n",
        "    demgras_in = Input(shape=(demgras_dim,), dtype='float32')\n",
        "    print(\"demgras_dim:{}\".format(demgras_dim)) #demgras_dim:11\n",
        "    print(\"demgras_in.shape:{}\".format(demgras_in.shape))#(None, 11) sigmoid\n",
        "    dense_demgras = Dense(1000, activation='sigmoid')(demgras_in) #它将任意的值转换到 [0,1] 之间 BN层是将数据转换为均值为0，方差为1的正态分布\n",
        "    conv_result.append(dense_demgras)#append the demographics information\n",
        "    merge_out = concatenate(conv_result) #numpy.concatenate函数 主要作用:沿现有的某个轴对一系列数组进行拼接。\n",
        "    merge_out = tf.expand_dims(merge_out, axis=0)\n",
        "    x = se_block(merge_out)  # 接收SE返回值 #可行 但是平均误差突然间猛增 动荡 这个网络不错诶 虽然前期增加，但是后期整体趋势在下降，虽然偶尔动荡\n",
        "    #x = eca_block(merge_out)  # 接收ECA输出结果 这个网络类似于上面这个网络 等会可以试试 和SENet相比大大减少了参数量，参数量等于一维卷积的kernel_size的大小\n",
        "    print(x.shape)\n",
        "    x=tf.squeeze(x,[0, 1])\n",
        "    dense_out = Dense(1000, activation='relu', name='F1')(x)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(500, activation='relu', name='F2')(dpt) \n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(100, activation='relu', name='F3')(dpt) #RReLU中的aji是一个在一个给定的范围内随机抽取的值，这个值在测试环节就会固定下来。\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    dense_out = Dense(50, activation='relu', name='F4')(dpt) #Leaky ReLU中的ai是固定的\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    #activation_function = keras.layers.advanced_activations.PReLU(init='zero', weights=None) \n",
        "    dense_out = Dense(2, activation='leaky_relu', name='F5')(dpt) #leaky_relu elu PReLU #PReLU中的ai是根据数据变化的#\n",
        "    #dense_out = prelu(dense_out)\n",
        "    dpt = Dropout(dropout)(dense_out)\n",
        "    mode_out = Dense(1)(dpt)\n",
        "    model = Model([dis_in,demgras_in], mode_out) \n",
        "    #rmsprop = optimizers.rmsprop(lr=lr, rho=0.9, epsilon=10-8, decay=0.0) 均方根传播(RMSProp) ;根据最近的权重梯度的平均值(例如变化的速度)来调整;这意味着该算法在线上和非平稳问题上表现良好(如:噪声)。\n",
        "    Adam=tf.keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
        "    #rmsprop tf.keras.losses.Huber()\n",
        "    model.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae']) #tf.keras.losses.Huber()\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def train_ATTENTION_multi_channel_split_model(model, modelpath, X1_train,  demographics_train, y_train):\n",
        "    #train_mat,valid_mat, train_y, valid_y, demographics_train, demographics_valid = train_test_split(X_train, y_train, demographics_train, test_size=0.05, random_state=seed)\n",
        "    checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath),verbose=1, save_best_only=True)\n",
        "    earlystopper = EarlyStopping(monitor='val_loss', patience=30, verbose=1)#patience：能够容忍多少个epoch内都没有improvement。verbose：日志显示 verbose = 1 为输出进度条记录\n",
        "\n",
        "    print('Training the ATTENTION_merge model....') \n",
        "    history =model.fit([X1_train,  demographics_train], y_train, epochs=200, batch_size=args.batchsize, shuffle=True,\n",
        "              validation_split=0.1,\n",
        "              callbacks=[checkpointer,earlystopper], # 尝试关闭早停\n",
        "              verbose=1)\n",
        "    print(\"4、开始画图：损失\")\n",
        "    from matplotlib import rcParams\n",
        "    font2 = {'family': 'Times New Roman',\n",
        "             'weight': 'normal',\n",
        "               'size': 15,}\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss',fontsize=15)\n",
        "    plt.xlabel('Epoch',font2,verticalalignment='top') # fontsize=14, \n",
        "    plt.ylabel('Loss',font2, horizontalalignment='center') #fontsize=14, \n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
        "    plt.savefig('/content/gdrive/MyDrive/LOSS.png',dpi=500,bbox_inches = 'tight')\n",
        " \n",
        "def test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    print(\"y_pred:{}\".format(y_pred.shape))\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    print(\"r2:\",r2,\"rmse:\",rmse)\n",
        "    #画图1\n",
        "    name = 'ATTENTION_MG_split%d'%(index)\n",
        "    costplots(y_test, y_pred, r2, rmse, name)\n",
        "    #画图2\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "    #画图3\n",
        "    print(\"!!开始画图：预测值与真实值\")\n",
        "    print(\"y_test:\",y_test.shape)\n",
        "    print(\"y_pred:\",y_pred.shape)\n",
        "    print(\"y_pred.ndim:\",y_pred.ndim)\n",
        "    print(\"y_test.ndim:\",y_test.ndim)\n",
        "    from matplotlib import rcParams\n",
        "    font2 = {'family': 'Times New Roman',\n",
        "             'weight': 'normal',\n",
        "               'size': 15,}\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(list(range(1569)),y_test,label='True Value')\n",
        "    plt.plot(list(range(1569)),y_pred,label='Predicted Value')\n",
        "    b=np.squeeze(y_test)\n",
        "    c=np.squeeze(y_pred)\n",
        "    plt.fill_between(list(range(1569)),b,c,color='g',alpha=.25)\n",
        "    plt.title('Predicted and true values of medical costs',fontsize=13)\n",
        "    plt.xlabel('Series',fontsize=13, verticalalignment='top') #\n",
        "    plt.ylabel('Medical cost',fontsize=13,horizontalalignment='center') #\n",
        "    plt.xticks(fontsize=13)\n",
        "    plt.yticks(fontsize=13)\n",
        "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
        "    plt.savefig('/content/gdrive/MyDrive/Attention_曲线图.png',dpi=500,bbox_inches = 'tight')\n",
        "    #画散点图\n",
        "    plt.figure()#figsize=(15,5)\n",
        "    plt.plot(y_test,'rs',label='True value')\n",
        "    plt.plot(y_pred,'go',label='Predict value')\n",
        "    plt.title('Predicted and true values of medical costs',fontsize=13)\n",
        "    plt.xticks(fontsize=13)\n",
        "    plt.yticks(fontsize=13)\n",
        "    plt.xlabel('Series',fontsize=13,)\n",
        "    plt.ylabel('Medical Cost',fontsize=13)\n",
        "    plt.legend(loc=1,fontsize=10,frameon=True)\n",
        "    plt.savefig('/content/gdrive/MyDrive/Attention_散点图.png',dpi=500,bbox_inches = 'tight')\n",
        "    #画：测试集的预测损失\n",
        "    plt.figure()\n",
        "    a=y_test-y_pred\n",
        "    plt.plot(a, label='Loss')\n",
        "    plt.title('Predicted loss of the proposed model',fontsize=13)\n",
        "    plt.xlabel('Series',fontsize=10, verticalalignment='top') #\n",
        "    plt.ylabel('Loss value',fontsize=10,horizontalalignment='center') #\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.savefig('/content/gdrive/MyDrive/Predicted_LOSS.png',dpi=500,bbox_inches = 'tight')"
      ],
      "metadata": {
        "id": "qrLlSJs7-DL3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1.4 开始预测**"
      ],
      "metadata": {
        "id": "70KoYHye99cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tsne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKoL1_7JZ125",
        "outputId": "fcd98935-2d3f-4359-f496-859e855203aa",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tsne\n",
            "  Downloading tsne-0.3.1.tar.gz (547 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 71 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 184 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 194 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 215 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 225 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 235 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 245 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 256 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 266 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 276 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 286 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 296 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 307 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 317 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 327 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 337 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 348 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 358 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 368 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 378 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 389 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 399 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 409 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 419 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 430 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 440 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 450 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 460 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 471 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 481 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 491 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 501 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 512 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 522 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 532 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 542 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 547 kB 8.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tsne) (0.29.28)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from tsne) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from tsne) (1.4.1)\n",
            "Building wheels for collected packages: tsne\n",
            "  Building wheel for tsne (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tsne: filename=tsne-0.3.1-cp37-cp37m-linux_x86_64.whl size=260507 sha256=57d860cb9db7e6eaf880cc8661fe0b67adefb6b7bb9f3c637eedc8e45b233c56\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/a7/9d/d09b0eef66f28be09470e0e18629ae08aed772497b218f84f3\n",
            "Successfully built tsne\n",
            "Installing collected packages: tsne\n",
            "Successfully installed tsne-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据准备"
      ],
      "metadata": {
        "id": "pn5gYEFM9xwA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xqXuJHfPTcm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d54812a-9477-43a3-e350-749d927c44cb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤2:形成原始单词清单\n",
            "开始将疾病诊断代码转换为索引\n",
            "开始步骤3:给单词编号,形成单词-编号的字典\n",
            "开始形成词嵌入向量\n",
            "[['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.901'], ['I67.801'], ['E71.001'], ['H81.904'], ['E10.901'], ['E10.901'], ['I10.X02'], ['I25.101'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['H04.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['S37.001'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.502'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.203'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E14.203'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I50.911'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.302'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J30.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.002'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['K52.908'], ['E10.901'], ['K29'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.101'], ['E10.901'], ['E10.901'], ['I10.X01'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['I64'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['M89.391'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['J44.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D36.705'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M05'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['R04.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.002'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['I10.X01'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['K80.203'], ['E10.901'], ['E10.901'], ['I10.X01'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C22.751'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M47'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K63.501'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B45.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['D07.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K72'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['N19.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['PTJB'], ['PTJB'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J04.102'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['K62.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['M50.301'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['G45.001'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.203'], ['E10.901'], ['Q24.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S62.301'], ['E10.901'], ['E10.901'], ['I64'], ['I67.801'], ['B17.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M51.905'], ['E10.901'], ['E10.301'], ['E10.901'], ['E10.901'], ['J44.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J18.008'], ['W19.992'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['I63.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I67.801'], ['R42.X01'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K72.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['M54.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.501'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['RY'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['H25.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E14.203'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I67.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S09.905'], ['I25.101'], ['E10.901'], ['I67.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['N17.903'], ['E14.303'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I84.201'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N39.001'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['L08.907'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K92.208'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N20.901'], ['N18.905'], ['E10.901'], ['I25.101'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['H81.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['H04.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['C56.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H91.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G44.102'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.865'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['I25.101'], ['E10.901'], ['B21.253'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J20'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I61.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['B21.253'], ['E10.901'], ['B21.253'], ['E10.901'], ['E16.803'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I62.002'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X05'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.004'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['G43'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.900'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I67.801'], ['E10.901'], ['E10.901'], ['I10.X01'], ['J44.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J06.903'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['N13.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.501'], ['PTJB'], ['E10.901'], ['E71.001'], ['E10.901'], ['I25.101'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C22.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H52.301'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J42.X02'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['I63.902'], ['H81.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['C56.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['H26.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['T15.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X05'], ['E10.901'], ['M47.865'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I50.908'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.302'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['S09.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['N18.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['N45.902'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['PTJB'], ['E10.901'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['RY'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I63.905'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I66.903'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['R42.X01'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.103'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['PTJB'], ['J44.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['RY'], ['B21.253'], ['N20.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X02'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['K05.103'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['K29.502'], ['PTJB'], ['E10.901'], ['E10.901'], ['N15.101'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['PTJB'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I25.101'], ['K52.908'], ['PTJB'], ['M05'], ['E10.901'], ['E10.901'], ['E10.900'], ['E11.600'], ['E10.901'], ['I25.101'], ['M81'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['S82.801'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K25.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E71.001'], ['I67.801'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['J06.903'], ['G45.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['K92.208'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O99.506'], ['I67.801'], ['D64.900'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I63.902'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N20.051'], ['E10.901'], ['E10.901'], ['K61.002'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['PTJB'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I64'], ['J42.X02'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['M47'], ['I10.X01'], ['PTJB'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N39.001'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K92.208'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['D61.900'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['S32.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['I67.802'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X05'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['L08.907'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H26.201'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M19.966'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['PTJB'], ['A15.354'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I63.902'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I63.902'], ['J00'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N41'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['R73.003'], ['E10.901'], ['PTJB'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['M51.003'], ['PTJB'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R73.003'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['J11.101'], ['I10.X01'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['N18.905'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['C22.901'], ['E10.901'], ['E16.803'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['S42.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M17.961'], ['I25.101'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['I10.X02'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['I10.X01'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['C54.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I63.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.103'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R10.401'], ['E10.901'], ['I63.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.302'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['N18.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D69.602'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.802'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I61.902'], ['J00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X01'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['T01.951'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H93.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I25.101'], ['E10.901'], ['I67.801'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M25.561'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.901'], ['E10.901'], ['H18.702'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['N20.252'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B02.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I63.902'], ['I25.101'], ['I25.101'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.908'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['C54.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S82.203'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['N18.902'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['R10.401'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I66.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.865'], ['E10.901'], ['E10.900'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['N41.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['D64.900'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J03.903'], ['E10.901'], ['E10.901'], ['I25.101'], ['E14.303'], ['E10.901'], ['E10.901'], ['I49.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H16.203'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.901'], ['S60.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K81.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I67.801'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['Z47.951'], ['E10.901'], ['E10.901'], ['I67.801'], ['M05'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['I64'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.900'], ['S06.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N10.X01'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['I25.101'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M05'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I61.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['A15.354'], ['I25.101'], ['PTJB'], ['E10.901'], ['PTJB'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H40.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K30'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['A03.951'], ['I63.902'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['A15.354'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['J98.402'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K06.802'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['J44.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['J44.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['J98.454'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N41'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['I63.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C61.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S22.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['Z85.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.900'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['G20.X03'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.603'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['N01.700'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['R10.401'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I64'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.610'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.851'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K63.501'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['T01.951'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R73.003'], ['E10.901'], ['I25.101'], ['PTJB'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N34.202'], ['E10.901'], ['N40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['B21.253'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['C64.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['B21.253'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.900'], ['E11.600'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.908'], ['E10.901'], ['E10.901'], ['E10.901'], ['H49.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['I67.801'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.900'], ['J11.101'], ['J96.901'], ['E10.901'], ['E10.901'], ['K12.107'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I67.801'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['PTJB'], ['PTJB'], ['J18.003'], ['PTJB'], ['PTJB'], ['E10.901'], ['I25.101'], ['PTJB'], ['D64.900'], ['M51.905'], ['E10.901'], ['PTJB'], ['CX'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['K29.502'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M25.561'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['R22.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.702'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64.X03'], ['J98.454'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M84'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['G20.X03'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['A16.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['D61.903'], ['N19.X01'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S32.001'], ['E10.901'], ['I25.101'], ['N40'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['B21.253'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['E10.901'], ['K52.908'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['S22.001'], ['E10.901'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H11.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['G20.X03'], ['RY'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['I10.X01'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['PTJB'], ['I67.801'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['T01.951'], ['PTJB'], ['E10.901'], ['I49.902'], ['I25.101'], ['I25.101'], ['E10.901'], ['K52.909'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64.X03'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J20'], ['E71.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I64'], ['PTJB'], ['J42.X02'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X02'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['K80.203'], ['M47.802'], ['PTJB'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['RY'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C61.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['N39.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J18.003'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.802'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I63.801'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['L23.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E11.600'], ['E10.901'], ['B21.253'], ['I10.X03'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64.X03'], ['M51.202'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.908'], ['H40.204'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K81'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.802'], ['E10.901'], ['E10.901'], ['M05'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E11.600'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['I00.X03'], ['N50.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['M79.681'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['CX'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M25.561'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M48.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S83.400'], ['I00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E11.600'], ['I00.X03'], ['E10.901'], ['I25.101'], ['E11.600'], ['PTJB'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['H26.201'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I67.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.802'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['N20.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J96.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X03'], ['E10.901'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['K52.908'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['I67.802'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S82.001'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['J04.004'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M47.865'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['M51.202'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['I25.101'], ['H26.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['J98.402'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I63.801'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['N19.X01'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K73.201'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['F25.951'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['W64.952'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['L25'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H26.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.100'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['H81.904'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.610'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['PTJB'], ['I64'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['G45.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N84.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M25.561'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['QT'], ['E10.901'], ['K63.953'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['T91.153'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['K73.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H81.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M75.001'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['C22.901'], ['E10.901'], ['H43.100'], ['B21.253'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['N18.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X51'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['S96.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['M77.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['N93.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['J20'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E16.803'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M19.991'], ['I64'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N18.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['K80.403'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['M51.202'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.105'], ['I67.202'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I24.801'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['K56.603'], ['E10.901'], ['E10.901'], ['E11.600'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J06.900'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E11.600'], ['E10.901'], ['B21.253'], ['E11.600'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['J44.851'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I64'], ['PTJB'], ['J42.X02'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['S32.001'], ['E10.901'], ['I27.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I63.801'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.103'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I24.801'], ['E10.901'], ['I25.101'], ['I67.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I67.801'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['K52.908'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I63.902'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E11.600'], ['I25.101'], ['E10.901'], ['K52.914'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['PTJB'], ['E10.901'], ['E10.901'], ['QT'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['S06.001'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I27.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['G45.004'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['B21.253'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E11.600'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['M05'], ['PTJB'], ['E10.901'], ['E10.901'], ['E11.600'], ['I67.802'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['H43.100'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.103'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E11.600'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['I25.101'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['E10.901'], ['E10.901'], ['E10.901'], ['N40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['D61.903'], ['E10.901'], ['E10.901'], ['R42.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['I25.101'], ['E11.600'], ['PTJB'], ['E11.600'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.002'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I10.X01'], ['E10.901'], ['F48.000'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['W64.952'], ['I71.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['B21.253'], ['E10.901'], ['E10.901'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['G20.X03'], ['E10.901'], ['E10.900'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['E10.901'], ['I49.904'], ['E10.901'], ['I25.101'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X02'], ['PTJB'], ['N19.X01'], ['E10.901'], ['E11.600'], ['N19.X01'], ['I10.X02'], ['E11.600'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I00.X03'], ['E10.901'], ['E10.901'], ['I25.105'], ['I64'], ['PTJB'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['B21.253'], ['S42.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['QT'], ['A16.500'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['C44.051'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['K92.208'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['I25.101'], ['E10.901'], ['M87.951'], ['E10.901'], ['E10.901'], ['W64.952'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['H04.303'], ['B21.253'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['R94.501'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['R11.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['PTJB'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['I25.101'], ['E10.901'], ['I25.101'], ['E11.600'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N04.101'], ['E10.901'], ['I25.105'], ['M48.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['G43'], ['PTJB'], ['I25.101'], ['QT'], ['QT'], ['E10.901'], ['N20.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['K56.501'], ['E11.600'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I24.801'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['Z47.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H40.504'], ['E10.901'], ['S06.6051'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E11.600'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['Q85.152'], ['E10.901'], ['C22.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['K29.603'], ['I67.801'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K40'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J43'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J47.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N04.101'], ['I25.101'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.105'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X04'], ['E10.901'], ['E10.901'], ['E10.901'], ['C21.852'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E14.203'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['H02.003'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.900'], ['H91.201'], ['E11.600'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['PTJB'], ['PTJB'], ['I21.911'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['N19.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['N19.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253']]\n",
            "[['E10.901'], ['E10.901'], ['E10.901'], ['O24.901'], ['I67.801'], ['E71.001'], ['H81.904'], ['E10.901'], ['E10.901'], ['I10.X02'], ['I25.101'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['H04.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['S37.001'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.502'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.203'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E14.203'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I50.911'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.302'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J30.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.002'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['K52.908'], ['E10.901'], ['K29'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.101'], ['E10.901'], ['E10.901'], ['I10.X01'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['I64'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['M89.391'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['J44.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D36.705'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M05'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['R04.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.002'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['I10.X01'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['K80.203'], ['E10.901'], ['E10.901'], ['I10.X01'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C22.751'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M47'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K63.501'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B45.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['D07.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K72'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['N19.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['PTJB'], ['PTJB'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J04.102'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['K62.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['M50.301'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['G45.001'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.203'], ['E10.901'], ['Q24.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S62.301'], ['E10.901'], ['E10.901'], ['I64'], ['I67.801'], ['B17.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M51.905'], ['E10.901'], ['E10.301'], ['E10.901'], ['E10.901'], ['J44.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J18.008'], ['W19.992'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['I63.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I67.801'], ['R42.X01'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K72.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['M54.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.501'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['RY'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['H25.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E14.203'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I67.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S09.905'], ['I25.101'], ['E10.901'], ['I67.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['N17.903'], ['E14.303'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I84.201'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N39.001'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['L08.907'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K92.208'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N20.901'], ['N18.905'], ['E10.901'], ['I25.101'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['H81.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['H04.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['C56.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H91.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G44.102'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.865'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['I25.101'], ['E10.901'], ['B21.253'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J20'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I61.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['B21.253'], ['E10.901'], ['B21.253'], ['E10.901'], ['E16.803'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I62.002'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X05'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.004'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['G43'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.900'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I67.801'], ['E10.901'], ['E10.901'], ['I10.X01'], ['J44.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J06.903'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['N13.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.501'], ['PTJB'], ['E10.901'], ['E71.001'], ['E10.901'], ['I25.101'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C22.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H52.301'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J42.X02'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['I63.902'], ['H81.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['C56.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['H26.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['T15.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X05'], ['E10.901'], ['M47.865'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I50.908'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.302'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['S09.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['N18.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['N45.902'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['PTJB'], ['E10.901'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['RY'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I63.905'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I66.903'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['R42.X01'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.103'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['PTJB'], ['J44.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['RY'], ['B21.253'], ['N20.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X02'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['K05.103'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['K29.502'], ['PTJB'], ['E10.901'], ['E10.901'], ['N15.101'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['PTJB'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E11.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I25.101'], ['K52.908'], ['PTJB'], ['M05'], ['E10.901'], ['E10.901'], ['E10.900'], ['E11.600'], ['E10.901'], ['I25.101'], ['M81'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['S82.801'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K25.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E71.001'], ['I67.801'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['J06.903'], ['G45.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['K92.208'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O99.506'], ['I67.801'], ['D64.900'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I63.902'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N20.051'], ['E10.901'], ['E10.901'], ['K61.002'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['PTJB'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I64'], ['J42.X02'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['M47'], ['I10.X01'], ['PTJB'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N39.001'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K92.208'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['D61.900'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['S32.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['I67.802'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X05'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['L08.907'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H26.201'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M19.966'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['PTJB'], ['A15.354'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I63.902'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I63.902'], ['J00'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N41'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['R73.003'], ['E10.901'], ['PTJB'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['M51.003'], ['PTJB'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R73.003'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['J11.101'], ['I10.X01'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['N18.905'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29.502'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['C22.901'], ['E10.901'], ['E16.803'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['S42.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M17.961'], ['I25.101'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['I10.X02'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['I10.X01'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['C54.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I63.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.103'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R10.401'], ['E10.901'], ['I63.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S42.302'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['N18.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D69.602'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['K29.502'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.802'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I61.902'], ['J00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X01'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['T01.951'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H93.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I25.101'], ['E10.901'], ['I67.801'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M25.561'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.901'], ['E10.901'], ['H18.702'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['N20.252'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B02.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I63.902'], ['I25.101'], ['I25.101'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.908'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['C54.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S82.203'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['N18.902'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['R10.401'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I66.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.865'], ['E10.901'], ['E10.900'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['N41.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['D64.900'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J03.903'], ['E10.901'], ['E10.901'], ['I25.101'], ['E14.303'], ['E10.901'], ['E10.901'], ['I49.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H16.203'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.901'], ['S60.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K81.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I67.801'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['Z47.951'], ['E10.901'], ['E10.901'], ['I67.801'], ['M05'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['I64'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.900'], ['S06.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N10.X01'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['I25.101'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M05'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I61.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['A15.354'], ['I25.101'], ['PTJB'], ['E10.901'], ['PTJB'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H40.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K30'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['A03.951'], ['I63.902'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['A15.354'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['J98.402'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K06.802'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['J44.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['J44.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['J98.454'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N41'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['I63.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C61.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S22.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['Z85.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.900'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['G20.X03'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.603'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['N01.700'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['R10.401'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I64'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.610'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.851'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K63.501'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['T01.951'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R73.003'], ['E10.901'], ['I25.101'], ['PTJB'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N34.202'], ['E10.901'], ['N40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['B21.253'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['C64.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J11.101'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['B21.253'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.900'], ['E11.600'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D64.900'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.908'], ['E10.901'], ['E10.901'], ['E10.901'], ['H49.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['I67.801'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.900'], ['J11.101'], ['J96.901'], ['E10.901'], ['E10.901'], ['K12.107'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I67.801'], ['D64.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X01'], ['PTJB'], ['PTJB'], ['J18.003'], ['PTJB'], ['PTJB'], ['E10.901'], ['I25.101'], ['PTJB'], ['D64.900'], ['M51.905'], ['E10.901'], ['PTJB'], ['CX'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['K29.502'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M25.561'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['R22.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K56.702'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64.X03'], ['J98.454'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M84'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['G20.X03'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['A16.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['D61.903'], ['N19.X01'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S32.001'], ['E10.901'], ['I25.101'], ['N40'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['B21.253'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I63.902'], ['E10.901'], ['K52.908'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['S22.001'], ['E10.901'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H11.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['G20.X03'], ['RY'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.303'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['I10.X01'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['PTJB'], ['I67.801'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['T01.951'], ['PTJB'], ['E10.901'], ['I49.902'], ['I25.101'], ['I25.101'], ['E10.901'], ['K52.909'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64.X03'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J20'], ['E71.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['I64'], ['PTJB'], ['J42.X02'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X02'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['K80.203'], ['M47.802'], ['PTJB'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['RY'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['C61.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['N39.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J18.003'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.802'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I63.801'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['L23.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E11.600'], ['E10.901'], ['B21.253'], ['I10.X03'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64.X03'], ['M51.202'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.908'], ['H40.204'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K81'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.802'], ['E10.901'], ['E10.901'], ['M05'], ['I51.403'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E11.600'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['I00.X03'], ['N50.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['M79.681'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['CX'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M25.561'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['M48.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S83.400'], ['I00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E11.600'], ['I00.X03'], ['E10.901'], ['I25.101'], ['E11.600'], ['PTJB'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['H26.201'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I67.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['M47.802'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['N20.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J96.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X03'], ['E10.901'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['K52.908'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['PTJB'], ['PTJB'], ['E10.901'], ['PTJB'], ['I67.802'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['S82.001'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['J04.004'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M47.865'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['M51.202'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['I25.101'], ['H26.401'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['J98.402'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I63.801'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['N19.X01'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K73.201'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['F25.951'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['W64.952'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['L25'], ['PTJB'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H26.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.100'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['H81.904'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.610'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['PTJB'], ['I64'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['G45.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N84.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M25.561'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['QT'], ['E10.901'], ['K63.953'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['T91.153'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['K73.201'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H81.904'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['M75.001'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['C22.901'], ['E10.901'], ['H43.100'], ['B21.253'], ['E10.901'], ['PTJB'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['Z47.951'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['N18.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['R42.X51'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['PTJB'], ['J40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['S96.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['M77.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['B21.253'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['N93.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['J20'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E16.803'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['I10.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G45.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J06.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M19.991'], ['I64'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N18.001'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['K80.403'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['M51.202'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['M05'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.105'], ['I67.202'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I24.801'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I69.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['K56.603'], ['E10.901'], ['E10.901'], ['E11.600'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J06.900'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E11.600'], ['E10.901'], ['B21.253'], ['E11.600'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['J44.851'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I64'], ['PTJB'], ['J42.X02'], ['E10.901'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['S32.001'], ['E10.901'], ['I27.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I63.801'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.103'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['I25.101'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I24.801'], ['E10.901'], ['I25.101'], ['I67.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['I67.801'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['M51.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J44.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['D61.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['K52.908'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I63.902'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.402'], ['E11.600'], ['I25.101'], ['E10.901'], ['K52.914'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['PTJB'], ['E10.901'], ['E10.901'], ['QT'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H25.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['S06.001'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I27.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['G45.004'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['B21.253'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E11.600'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['M05'], ['PTJB'], ['E10.901'], ['E10.901'], ['E11.600'], ['I67.802'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['H43.100'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E14.103'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E11.600'], ['E10.901'], ['I67.801'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['I25.101'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['E10.901'], ['E10.901'], ['E10.901'], ['N40'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['D61.903'], ['E10.901'], ['E10.901'], ['R42.X01'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['I25.101'], ['E11.600'], ['PTJB'], ['E11.600'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['G51.002'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I10.X01'], ['E10.901'], ['F48.000'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['W64.952'], ['I71.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['RY'], ['B21.253'], ['E10.901'], ['E10.901'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E11.600'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['G20.X03'], ['E10.901'], ['E10.900'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['F25.951'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['E10.901'], ['I49.904'], ['E10.901'], ['I25.101'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['I10.X02'], ['PTJB'], ['N19.X01'], ['E10.901'], ['E11.600'], ['N19.X01'], ['I10.X02'], ['E11.600'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I00.X03'], ['E10.901'], ['E10.901'], ['I25.105'], ['I64'], ['PTJB'], ['B65.202'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['B21.253'], ['S42.301'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['QT'], ['A16.500'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['C44.051'], ['I25.101'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['B21.253'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I00.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.105'], ['K92.208'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['K29'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['I25.101'], ['E10.901'], ['M87.951'], ['E10.901'], ['E10.901'], ['W64.952'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['H04.303'], ['B21.253'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I67.801'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['R94.501'], ['E10.901'], ['PTJB'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['R11.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['PTJB'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['PTJB'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['I25.101'], ['I25.101'], ['E10.901'], ['I25.101'], ['E11.600'], ['E11.600'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N04.101'], ['E10.901'], ['I25.105'], ['M48.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['G43'], ['PTJB'], ['I25.101'], ['QT'], ['QT'], ['E10.901'], ['N20.901'], ['E10.901'], ['E11.600'], ['PTJB'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['F25.951'], ['E10.901'], ['J98.402'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['K56.501'], ['E11.600'], ['PTJB'], ['I25.101'], ['E10.901'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['M05'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I24.801'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['Z47.951'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['H40.504'], ['E10.901'], ['S06.6051'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.900'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K52.905'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J98.454'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['PTJB'], ['E10.901'], ['E11.600'], ['M47'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['Q85.152'], ['E10.901'], ['C22.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I63.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['K29.603'], ['I67.801'], ['I10.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K40'], ['E10.901'], ['E10.901'], ['I25.101'], ['I10.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['D61.903'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J43'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['PTJB'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['K73.201'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['QT'], ['E10.901'], ['E10.901'], ['E10.901'], ['J42.X02'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['I25.101'], ['I64'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E11.600'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['J47.X03'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N04.101'], ['I25.101'], ['N20.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E11.600'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.105'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['M51.202'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I49.902'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X04'], ['E10.901'], ['E10.901'], ['E10.901'], ['C21.852'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['I25.101'], ['I25.101'], ['E10.901'], ['G20.X03'], ['E10.901'], ['E10.901'], ['E14.203'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['H02.003'], ['I25.101'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['O24.900'], ['H91.201'], ['E11.600'], ['E10.901'], ['E10.901'], ['I10.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['I64'], ['E10.901'], ['PTJB'], ['PTJB'], ['I21.911'], ['E10.901'], ['I49.902'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I64'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['N19.X01'], ['E10.901'], ['PTJB'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['I10.X01'], ['I25.101'], ['E10.901'], ['B21.253'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['PTJB'], ['N19.X01'], ['E10.901'], ['E10.901'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['I25.101'], ['RY'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['PTJB'], ['N19.X01'], ['I25.101'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['E10.901'], ['B21.253']]\n",
            "开始63/3:开始word2vec\n",
            "model: Word2Vec(vocab=242, size=900, alpha=0.025)\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 242 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 242 samples in 0.037s...\n",
            "[t-SNE] Computed conditional probabilities for sample 242 / 242\n",
            "[t-SNE] Mean sigma: 0.001710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:327: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 76.328392\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.637868\n",
            "开始第4步\n",
            "开始get_trained_embedding\n",
            "开始63/3:开始word2vec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:207: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:208: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Word2Vec(vocab=242, size=900, alpha=0.025)\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 242 samples in 0.000s...\n",
            "[t-SNE] Computed neighbors for 242 samples in 0.010s...\n",
            "[t-SNE] Computed conditional probabilities for sample 242 / 242\n",
            "[t-SNE] Mean sigma: 0.001710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:327: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 73.181702\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.642782\n",
            "开始第4步\n",
            "开始get_trained_embedding\n",
            "开始63/3:开始word2vec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:207: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:208: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Word2Vec(vocab=242, size=900, alpha=0.025)\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 242 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 242 samples in 0.009s...\n",
            "[t-SNE] Computed conditional probabilities for sample 242 / 242\n",
            "[t-SNE] Mean sigma: 0.001710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:327: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 70.183693\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.636461\n",
            "开始第4步\n",
            "开始get_trained_embedding\n",
            "开始63/3:开始word2vec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:207: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:208: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: Word2Vec(vocab=242, size=900, alpha=0.025)\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 242 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 242 samples in 0.010s...\n",
            "[t-SNE] Computed conditional probabilities for sample 242 / 242\n",
            "[t-SNE] Mean sigma: 0.001710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:327: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 73.533821\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.639425\n",
            "开始第4步\n",
            "开始get_trained_embedding\n",
            "X(seqs): (15684, 17) X1(disease): (15684, 17)\n",
            "seqs: (15684, 17)\n",
            "np.log\n",
            "[[  0   0   0 ...   7   1   8]\n",
            " [  0   0   0 ...  11   1   8]\n",
            " [  0   0   0 ...  11   1   8]\n",
            " ...\n",
            " [  0   0   0 ...  11 112   8]\n",
            " [  0   0   0 ...  11 112   8]\n",
            " [  0   0   0 ...   3 111  49]] [[ 0. 63.  1. ...  5.  2.  0.]\n",
            " [ 0. 81.  1. ... 11.  7.  0.]\n",
            " [ 0. 43.  1. ... 11.  7.  0.]\n",
            " ...\n",
            " [ 0. 68.  0. ... 12.  7. 34.]\n",
            " [ 0. 62.  0. ... 12.  7. 34.]\n",
            " [ 0. 59.  0. ... 12.  1. 33.]]\n",
            "开始步骤8:选择罕见集样本用于测试集...，这一步其实是将所有数据15684都用于了测试集\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:207: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:208: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comm_inds:15684\n",
            "comm_X: [[  0   0   0 ...   7   1   8]\n",
            " [  0   0   0 ...  11   1   8]\n",
            " [  0   0   0 ...  11   1   8]\n",
            " ...\n",
            " [  0   0   0 ...  11 112   8]\n",
            " [  0   0   0 ...  11 112   8]\n",
            " [  0   0   0 ...   3 111  49]] comm_X1: [[ 0  0  0 ...  0  0  8]\n",
            " [ 0  0  0 ...  0  0  8]\n",
            " [ 0  0  0 ...  0  0  8]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  8]\n",
            " [ 0  0  0 ...  0  0  8]\n",
            " [ 0  0  0 ...  0  0 49]] comm_y: [[8.716206]\n",
            " [8.190104]\n",
            " [8.262434]\n",
            " ...\n",
            " [5.70374 ]\n",
            " [5.70374 ]\n",
            " [5.858118]]\n",
            "Spliting train, test parts...\n",
            "X_train:(14115, 17)\n",
            "X_test:(1569, 17)\n",
            "X1_train:(14115, 17)\n",
            "X1_test:(1569, 17)\n",
            "y_train:(14115, 1)\n",
            "y_test:(1569, 1)\n",
            "demographics_train:(14115, 10)\n",
            "demographics_test:(1569, 10)\n",
            "y_test: [[7.9070454]\n",
            " [8.753389 ]\n",
            " [5.70374  ]\n",
            " ...\n",
            " [8.566658 ]\n",
            " [5.70374  ]\n",
            " [5.70374  ]]\n",
            "demgras_dim:10\n"
          ]
        }
      ],
      "source": [
        "path = 'weighted_%s:%s_%sdays_%sinit_%strainable_%fdpt_%flr_%dfz_%dfn_%dmaxlen_%ddim_%sfilter_%stransform %sbatch_%swindow'%(MID, SID, args.isdays, args.init, args.trainable, args.dpt, args.lr, args.fz, args.fn, args.maxlen, args.dim, args.filter, args.transform,args.batchsize,args.window)\n",
        "\n",
        "if os.path.isfile('/content/gdrive/MyDrive/dataclean.df'):\n",
        "    data = pkl.load(open('/content/gdrive/MyDrive/dataclean.df','rb'))\n",
        "else:\n",
        "    data = DataClean()\n",
        "rawseqs, cost, days, demographics, disease, main_dis = ToRawList(data) #rawseqs就是seqs\n",
        "#print(\"rawseqs:\",rawseqs)\n",
        "\n",
        "seqs1levels = []\n",
        "seqs2levels = []\n",
        "seqs3levels = []\n",
        "for seqs in rawseqs:\n",
        "    levels1 = []\n",
        "    levels2 = []\n",
        "    levels3 = []\n",
        "    for code in seqs:\n",
        "        levels1.append(code[0:3])\n",
        "        if len(re.findall(pattern, code))== 1:\n",
        "            levels2.append(code[0:5])\n",
        "            levels3.append(code[0:6])\n",
        "        else:\n",
        "            levels2.append(code[0:4])\n",
        "            levels3.append(code[0:5])\n",
        "    seqs1levels.append(levels1)\n",
        "    seqs2levels.append(levels2)\n",
        "    seqs3levels.append(levels3)\n",
        "\n",
        "print('开始将疾病诊断代码转换为索引')\n",
        "code_index = token_to_index(rawseqs)# transform code into index 将代码转换为索引\n",
        "index_code = dict([(kv[1], kv[0]) for kv in code_index.items()])# validate the code_index 验证代码_索引\n",
        "#print [[index_code[index] for index in item ] for item in seqs[0:2]]\n",
        "\n",
        "idseqs = [[code_index[code] for code in item] for item in rawseqs]\n",
        "iddis = [[code_index[code] for code in item] for item in disease]\n",
        "idmain_dis = [[code_index[code] for code in item] for item in main_dis]\n",
        "#print(main_dis)\n",
        "#\"\"\"\n",
        "print('开始形成词嵌入向量')\n",
        "print(disease)\n",
        "print(disease[1:]) #list 15684\n",
        "word2vec_model(disease,0)\n",
        "index_embedding = get_index_embedding(code_index, 0)\n",
        "embedding_matrix0 = get_trained_embedding(index_embedding)\n",
        "word2vec_model(disease, 1)\n",
        "index_embedding1 = get_index_embedding(code_index, 1)\n",
        "embedding_matrix1 = get_trained_embedding(index_embedding1)\n",
        "word2vec_model(disease, 2)\n",
        "index_embedding2 = get_index_embedding(code_index, 2)\n",
        "embedding_matrix2 = get_trained_embedding(index_embedding2)\n",
        "word2vec_model(disease, 3)\n",
        "index_embedding3 = get_index_embedding(code_index, 3)\n",
        "embedding_matrix3 = get_trained_embedding(index_embedding3)\n",
        "\n",
        "main_code = []\n",
        "for item in idmain_dis:\n",
        "    for ind in item:\n",
        "        main_code.append(embedding_matrix0[1][ind])\n",
        "\n",
        "maincodemat = np.array(main_code)\n",
        "\n",
        "nb_words = len(index_code) # code_index starting from 1\n",
        "max_features = nb_words + 1 \n",
        "n_seqs = len(idseqs)\n",
        "\n",
        "MAX_LEN = args.maxlen      # the max len is 21 #这里没有治疗编码，所以最大长度为以记录为单位\n",
        "seqs = pad_sequences(idseqs, maxlen=MAX_LEN)\n",
        "DIS_MAX_LEN = 17  #其实这里需要改成以记录为单位才对 DIS_MAX_LEN = 11 SUR_MAX_LEN = 10 maxlen:21\n",
        "disease = pad_sequences(iddis, maxlen=DIS_MAX_LEN)\n",
        "\n",
        "\n",
        "X = np.array(seqs)\n",
        "X1 = np.array(disease)\n",
        "print(\"X(seqs):\",X.shape,\"X1(disease):\",X1.shape)\n",
        "print(\"seqs:\",seqs.shape)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "if args.test:\n",
        "    modelpath = 'MG_merge' + path\n",
        "    demgras_dim = 3\n",
        "    model= multi_channel_split_model(demgras_dim)\n",
        "    extract_patientvec(model, modelpath, disease, surgery, demographics)\n",
        "\"\"\"\n",
        "#demographics = np.hstack((demographics, maincodemat))\n",
        "if args.isdays:\n",
        "    y = np.asarray(days, dtype='float32')\n",
        "else:\n",
        "    y = np.asarray(cost,dtype='float32')\n",
        "\n",
        "if args.transform:\n",
        "    print(\"np.log\")\n",
        "    y = np.log(y)\n",
        "print(X, demographics)\n",
        "\n",
        "comm_inds= filter_test(X, index_code, 4)\n",
        "comm_X = X[comm_inds]\n",
        "comm_X1 = X1[comm_inds]\n",
        "comm_demographics = demographics[comm_inds]\n",
        "comm_y = y[comm_inds]\n",
        "print(\"comm_X:\",comm_X,\"comm_X1:\",comm_X1,\"comm_y:\",comm_y)\n",
        "\n",
        "print('Spliting train, test parts...')\n",
        "X_train, X_test, X1_train, X1_test, y_train, y_test, demographics_train, demographics_test = train_test_split(comm_X, comm_X1,comm_y, comm_demographics, test_size=0.1, random_state=90)\n",
        "print('X_train:{}'.format(X_train.shape))\n",
        "print('X_test:{}'.format(X_test.shape))\n",
        "print('X1_train:{}'.format(X1_train.shape))\n",
        "print('X1_test:{}'.format(X1_test.shape))\n",
        "print('y_train:{}'.format(y_train.shape))\n",
        "print('y_test:{}'.format(y_test.shape))\n",
        "print('demographics_train:{}'.format(demographics_train.shape))\n",
        "print('demographics_test:{}'.format(demographics_test.shape))\n",
        "\n",
        "#y_train=np.log(y_train)\n",
        "#y_test=np.log(y_test)\n",
        "print(\"y_test:\",y_test)\n",
        "  \n",
        "demgras_dim = demographics.shape[1] #图像的水平尺寸 \n",
        "print(\"demgras_dim:{}\".format(demgras_dim)) #demgras_dim:10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **有时间间隔**"
      ],
      "metadata": {
        "id": "-QY4roGT4Zk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=============================== 第四部分 开始预测 =====================================================\n",
        "r2_CNN_val=[]\n",
        "r2_CNN_test=[]\n",
        "for i in range(10):\n",
        "  print('第%s次训练'%i)\n",
        "  \n",
        "  modelpath = '有时间间隔_5月16日(下午)_'+'第%s次训练_'%i+'MG_merge' + path # %i\n",
        "  #定义模型\n",
        "  model= ATTENTION_multi_channel_split_model(demgras_dim) \n",
        "  #划分数据\n",
        "  X1_train_1,X1_val_1, demographics_train_1,demographics_val_1,y_train_1,y_val_1=train_test_split( X1_train, demographics_train, y_train, test_size=0.1, random_state=90) \n",
        "  if not args.test:\n",
        "      train_ATTENTION_multi_channel_split_model(model, modelpath, X1_train_1, demographics_train_1, y_train_1)\n",
        "  extract_patientvec(model, modelpath, disease, demographics)\n",
        "  #拟合、预测\n",
        "  y_CNN_hat = model.predict([X1_val_1,demographics_val_1])\n",
        "\n",
        "  r2,rmse = evaluation(y_val_1, y_CNN_hat) #验证集的拟合优度\n",
        "  R21=r2_score(y_val_1, y_CNN_hat)\n",
        "  print(i,R21)\n",
        "  r2_CNN_val.append(R21)\n",
        "  \n",
        "  test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
        "  model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "  y_CNN_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "  r22,rmse = evaluation(y_test, y_CNN_pred)\n",
        "  print(\"r2:\",r22,\"rmse:\",rmse)\n",
        "  r2_CNN_test.append(r22)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kDZAWFv9gZv",
        "outputId": "6e7384b5-2a66-448c-d149-d8dbb3efa97b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第0次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_13/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_13'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_73\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_52 (Embedding)       (None, 17, 900)      649800      ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_53 (Embedding)       (None, 17, 900)      649800      ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_54 (Embedding)       (None, 17, 900)      649800      ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_55 (Embedding)       (None, 17, 900)      649800      ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 17, 100)      270100      ['embedding_52[0][0]',           \n",
            "                                                                  'embedding_53[0][0]',           \n",
            "                                                                  'embedding_54[0][0]',           \n",
            "                                                                  'embedding_55[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 17, 100)      270100      ['embedding_52[0][0]',           \n",
            "                                                                  'embedding_53[0][0]',           \n",
            "                                                                  'embedding_54[0][0]',           \n",
            "                                                                  'embedding_55[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 17, 100)      270100      ['embedding_52[0][0]',           \n",
            "                                                                  'embedding_53[0][0]',           \n",
            "                                                                  'embedding_54[0][0]',           \n",
            "                                                                  'embedding_55[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_156 (Glob  (None, 100)         0           ['conv1d_39[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_157 (Glob  (None, 100)         0           ['conv1d_39[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_158 (Glob  (None, 100)         0           ['conv1d_39[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_159 (Glob  (None, 100)         0           ['conv1d_39[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_160 (Glob  (None, 100)         0           ['conv1d_40[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_161 (Glob  (None, 100)         0           ['conv1d_40[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_162 (Glob  (None, 100)         0           ['conv1d_40[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_163 (Glob  (None, 100)         0           ['conv1d_40[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_164 (Glob  (None, 100)         0           ['conv1d_41[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_165 (Glob  (None, 100)         0           ['conv1d_41[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_166 (Glob  (None, 100)         0           ['conv1d_41[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_167 (Glob  (None, 100)         0           ['conv1d_41[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_39 (Add)                   (None, 100)          0           ['global_max_pooling1d_156[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_157[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_158[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_159[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_40 (Add)                   (None, 100)          0           ['global_max_pooling1d_160[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_161[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_162[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_163[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_41 (Add)                   (None, 100)          0           ['global_max_pooling1d_164[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_165[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_166[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_167[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 1000)         11000       ['input_28[0][0]']               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 1300)         0           ['add_39[0][0]',                 \n",
            "                                                                  'add_40[0][0]',                 \n",
            "                                                                  'add_41[0][0]',                 \n",
            "                                                                  'dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_13 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_13[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_13 (G  (1, 1300)           0           ['tf.expand_dims_13[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_13 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (1, 1, 1, 650)       845650      ['reshape_13[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_13 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_53[0][0]']               \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_13[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_13 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_54[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_13[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_13[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_13 (TFOpL  (None, 1300)        0           ['multiply_13[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_13[0][0]']\n",
            "                                                                                                  \n",
            " dropout_65 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_65[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_66 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_66[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_67 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_67[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_68[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_69 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 1)            3           ['dropout_69[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.7399 - mae: 1.0438\n",
            "Epoch 1: val_loss improved from inf to 0.60347, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 12s 66ms/step - loss: 2.7321 - mae: 1.0417 - val_loss: 0.6035 - val_mae: 0.3471\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7623 - mae: 0.5374\n",
            "Epoch 2: val_loss improved from 0.60347 to 0.47465, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.7612 - mae: 0.5366 - val_loss: 0.4747 - val_mae: 0.2897\n",
            "Epoch 3/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.7106 - mae: 0.5203\n",
            "Epoch 3: val_loss improved from 0.47465 to 0.43398, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7106 - mae: 0.5203 - val_loss: 0.4340 - val_mae: 0.3074\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6624 - mae: 0.5066\n",
            "Epoch 4: val_loss improved from 0.43398 to 0.43289, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.6618 - mae: 0.5063 - val_loss: 0.4329 - val_mae: 0.3879\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6798 - mae: 0.5002\n",
            "Epoch 5: val_loss did not improve from 0.43289\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6793 - mae: 0.4999 - val_loss: 0.6266 - val_mae: 0.5953\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6886 - mae: 0.5088\n",
            "Epoch 6: val_loss improved from 0.43289 to 0.37292, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.6893 - mae: 0.5094 - val_loss: 0.3729 - val_mae: 0.3256\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5857 - mae: 0.4626\n",
            "Epoch 7: val_loss improved from 0.37292 to 0.36063, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.5868 - mae: 0.4632 - val_loss: 0.3606 - val_mae: 0.3122\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5814 - mae: 0.4569\n",
            "Epoch 8: val_loss improved from 0.36063 to 0.35731, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5825 - mae: 0.4571 - val_loss: 0.3573 - val_mae: 0.2848\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5840 - mae: 0.4527\n",
            "Epoch 9: val_loss improved from 0.35731 to 0.34084, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.5838 - mae: 0.4525 - val_loss: 0.3408 - val_mae: 0.2919\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5734 - mae: 0.4458\n",
            "Epoch 10: val_loss improved from 0.34084 to 0.33728, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.5748 - mae: 0.4462 - val_loss: 0.3373 - val_mae: 0.3126\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5750 - mae: 0.4632\n",
            "Epoch 11: val_loss did not improve from 0.33728\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5742 - mae: 0.4629 - val_loss: 0.3675 - val_mae: 0.3861\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5631 - mae: 0.4441\n",
            "Epoch 12: val_loss improved from 0.33728 to 0.30392, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.5621 - mae: 0.4437 - val_loss: 0.3039 - val_mae: 0.2962\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5503 - mae: 0.4418\n",
            "Epoch 13: val_loss did not improve from 0.30392\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5492 - mae: 0.4414 - val_loss: 0.3641 - val_mae: 0.4039\n",
            "Epoch 14/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.5002 - mae: 0.4172\n",
            "Epoch 14: val_loss improved from 0.30392 to 0.25388, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5002 - mae: 0.4172 - val_loss: 0.2539 - val_mae: 0.2734\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4710 - mae: 0.3966\n",
            "Epoch 15: val_loss improved from 0.25388 to 0.23207, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4699 - mae: 0.3958 - val_loss: 0.2321 - val_mae: 0.2480\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4532 - mae: 0.3931\n",
            "Epoch 16: val_loss improved from 0.23207 to 0.21392, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4540 - mae: 0.3939 - val_loss: 0.2139 - val_mae: 0.2650\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4693 - mae: 0.4057\n",
            "Epoch 17: val_loss did not improve from 0.21392\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4689 - mae: 0.4054 - val_loss: 0.2356 - val_mae: 0.3285\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4061 - mae: 0.3709\n",
            "Epoch 18: val_loss improved from 0.21392 to 0.18592, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.4070 - mae: 0.3711 - val_loss: 0.1859 - val_mae: 0.2594\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3982 - mae: 0.3650\n",
            "Epoch 19: val_loss improved from 0.18592 to 0.14455, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3973 - mae: 0.3646 - val_loss: 0.1446 - val_mae: 0.2183\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3250 - mae: 0.3127\n",
            "Epoch 20: val_loss did not improve from 0.14455\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3240 - mae: 0.3121 - val_loss: 0.1579 - val_mae: 0.2855\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3358 - mae: 0.3337\n",
            "Epoch 21: val_loss improved from 0.14455 to 0.12938, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3356 - mae: 0.3336 - val_loss: 0.1294 - val_mae: 0.1949\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3434 - mae: 0.3358\n",
            "Epoch 22: val_loss did not improve from 0.12938\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3425 - mae: 0.3352 - val_loss: 0.1405 - val_mae: 0.2550\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3732 - mae: 0.3458\n",
            "Epoch 23: val_loss did not improve from 0.12938\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3733 - mae: 0.3462 - val_loss: 0.2922 - val_mae: 0.4227\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3288 - mae: 0.3136\n",
            "Epoch 24: val_loss improved from 0.12938 to 0.10085, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3283 - mae: 0.3137 - val_loss: 0.1008 - val_mae: 0.1745\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3412 - mae: 0.3280\n",
            "Epoch 25: val_loss did not improve from 0.10085\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3431 - mae: 0.3283 - val_loss: 0.1520 - val_mae: 0.2875\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3080 - mae: 0.3061\n",
            "Epoch 26: val_loss did not improve from 0.10085\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3072 - mae: 0.3056 - val_loss: 0.1136 - val_mae: 0.2286\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3035 - mae: 0.3121\n",
            "Epoch 27: val_loss improved from 0.10085 to 0.09275, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.3046 - mae: 0.3131 - val_loss: 0.0927 - val_mae: 0.1409\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2985 - mae: 0.2943\n",
            "Epoch 28: val_loss did not improve from 0.09275\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2983 - mae: 0.2944 - val_loss: 0.1268 - val_mae: 0.2236\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3189 - mae: 0.3078\n",
            "Epoch 29: val_loss improved from 0.09275 to 0.09211, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3186 - mae: 0.3075 - val_loss: 0.0921 - val_mae: 0.2042\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2762 - mae: 0.2842\n",
            "Epoch 30: val_loss improved from 0.09211 to 0.06519, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2755 - mae: 0.2838 - val_loss: 0.0652 - val_mae: 0.1180\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2733 - mae: 0.2813\n",
            "Epoch 31: val_loss did not improve from 0.06519\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2739 - mae: 0.2815 - val_loss: 0.0788 - val_mae: 0.1332\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2784 - mae: 0.2788\n",
            "Epoch 32: val_loss did not improve from 0.06519\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2787 - mae: 0.2789 - val_loss: 0.1358 - val_mae: 0.2457\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3034 - mae: 0.3208\n",
            "Epoch 33: val_loss did not improve from 0.06519\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3031 - mae: 0.3204 - val_loss: 0.1139 - val_mae: 0.1718\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2699 - mae: 0.2825\n",
            "Epoch 34: val_loss did not improve from 0.06519\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2716 - mae: 0.2837 - val_loss: 0.1663 - val_mae: 0.3127\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2746 - mae: 0.2812\n",
            "Epoch 35: val_loss improved from 0.06519 to 0.06289, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2743 - mae: 0.2811 - val_loss: 0.0629 - val_mae: 0.1125\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2436 - mae: 0.2639\n",
            "Epoch 36: val_loss did not improve from 0.06289\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2433 - mae: 0.2639 - val_loss: 0.2051 - val_mae: 0.4044\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2608 - mae: 0.2759\n",
            "Epoch 37: val_loss improved from 0.06289 to 0.05750, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.2611 - mae: 0.2764 - val_loss: 0.0575 - val_mae: 0.1019\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3211 - mae: 0.3104\n",
            "Epoch 38: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3213 - mae: 0.3103 - val_loss: 0.1346 - val_mae: 0.2674\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3029 - mae: 0.3117\n",
            "Epoch 39: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3022 - mae: 0.3112 - val_loss: 0.0690 - val_mae: 0.1429\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2322 - mae: 0.2480\n",
            "Epoch 40: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2315 - mae: 0.2477 - val_loss: 0.0749 - val_mae: 0.1904\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2762 - mae: 0.2811\n",
            "Epoch 41: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2777 - mae: 0.2813 - val_loss: 0.1335 - val_mae: 0.2884\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2604 - mae: 0.2741\n",
            "Epoch 42: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2600 - mae: 0.2738 - val_loss: 0.0840 - val_mae: 0.2158\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3285 - mae: 0.3159\n",
            "Epoch 43: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3275 - mae: 0.3153 - val_loss: 0.1304 - val_mae: 0.3039\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2643 - mae: 0.2786\n",
            "Epoch 44: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2640 - mae: 0.2783 - val_loss: 0.0590 - val_mae: 0.1100\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2565 - mae: 0.2673\n",
            "Epoch 45: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2557 - mae: 0.2669 - val_loss: 0.0612 - val_mae: 0.1421\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2620 - mae: 0.2733\n",
            "Epoch 46: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2622 - mae: 0.2733 - val_loss: 0.0626 - val_mae: 0.1313\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2584 - mae: 0.2650\n",
            "Epoch 47: val_loss did not improve from 0.05750\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2584 - mae: 0.2653 - val_loss: 0.0703 - val_mae: 0.1889\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2417 - mae: 0.2503\n",
            "Epoch 48: val_loss improved from 0.05750 to 0.05304, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2441 - mae: 0.2511 - val_loss: 0.0530 - val_mae: 0.0956\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2656 - mae: 0.2734\n",
            "Epoch 49: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2655 - mae: 0.2735 - val_loss: 0.0673 - val_mae: 0.1320\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2487 - mae: 0.2680\n",
            "Epoch 50: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2486 - mae: 0.2681 - val_loss: 0.0591 - val_mae: 0.1308\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2374 - mae: 0.2555\n",
            "Epoch 51: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2368 - mae: 0.2552 - val_loss: 0.1178 - val_mae: 0.2480\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2605 - mae: 0.2803\n",
            "Epoch 52: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2602 - mae: 0.2803 - val_loss: 0.0749 - val_mae: 0.1705\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2403 - mae: 0.2459\n",
            "Epoch 53: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2399 - mae: 0.2459 - val_loss: 0.1265 - val_mae: 0.2931\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2436 - mae: 0.2541\n",
            "Epoch 54: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2438 - mae: 0.2544 - val_loss: 0.0913 - val_mae: 0.2179\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2601 - mae: 0.2966\n",
            "Epoch 55: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2596 - mae: 0.2963 - val_loss: 0.1118 - val_mae: 0.2682\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2496 - mae: 0.2548\n",
            "Epoch 56: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2497 - mae: 0.2548 - val_loss: 0.0562 - val_mae: 0.1303\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2584 - mae: 0.2699\n",
            "Epoch 57: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2600 - mae: 0.2700 - val_loss: 0.3465 - val_mae: 0.5293\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2596 - mae: 0.2845\n",
            "Epoch 58: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2598 - mae: 0.2847 - val_loss: 0.0655 - val_mae: 0.1630\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2297 - mae: 0.2424\n",
            "Epoch 59: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2293 - mae: 0.2422 - val_loss: 0.0946 - val_mae: 0.2356\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2325 - mae: 0.2552\n",
            "Epoch 60: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2326 - mae: 0.2554 - val_loss: 0.0930 - val_mae: 0.2371\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2550 - mae: 0.2724\n",
            "Epoch 61: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2546 - mae: 0.2721 - val_loss: 0.0806 - val_mae: 0.1961\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2350 - mae: 0.2453\n",
            "Epoch 62: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2350 - mae: 0.2454 - val_loss: 0.1810 - val_mae: 0.3607\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2751 - mae: 0.3020\n",
            "Epoch 63: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2750 - mae: 0.3020 - val_loss: 0.0583 - val_mae: 0.1123\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2212 - mae: 0.2452\n",
            "Epoch 64: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2211 - mae: 0.2453 - val_loss: 0.0597 - val_mae: 0.1180\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2291 - mae: 0.2420\n",
            "Epoch 65: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2292 - mae: 0.2421 - val_loss: 0.1324 - val_mae: 0.2728\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2608 - mae: 0.2714\n",
            "Epoch 66: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2601 - mae: 0.2712 - val_loss: 0.1164 - val_mae: 0.2702\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2338 - mae: 0.2425\n",
            "Epoch 67: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2333 - mae: 0.2424 - val_loss: 0.0546 - val_mae: 0.1216\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2266 - mae: 0.2526\n",
            "Epoch 68: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2261 - mae: 0.2523 - val_loss: 0.0697 - val_mae: 0.1170\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2412 - mae: 0.2662\n",
            "Epoch 69: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2415 - mae: 0.2662 - val_loss: 0.0795 - val_mae: 0.2035\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2464 - mae: 0.2724\n",
            "Epoch 70: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2462 - mae: 0.2722 - val_loss: 0.0977 - val_mae: 0.2047\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2286 - mae: 0.2407\n",
            "Epoch 71: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2291 - mae: 0.2408 - val_loss: 0.0713 - val_mae: 0.1830\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2182 - mae: 0.2465\n",
            "Epoch 72: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2182 - mae: 0.2465 - val_loss: 0.0674 - val_mae: 0.1635\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2401 - mae: 0.2614\n",
            "Epoch 73: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2402 - mae: 0.2615 - val_loss: 0.0859 - val_mae: 0.2078\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2068 - mae: 0.2316\n",
            "Epoch 74: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2065 - mae: 0.2319 - val_loss: 0.0547 - val_mae: 0.1106\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2101 - mae: 0.2354\n",
            "Epoch 75: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2102 - mae: 0.2357 - val_loss: 0.0579 - val_mae: 0.1253\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2231 - mae: 0.2545\n",
            "Epoch 76: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2228 - mae: 0.2542 - val_loss: 0.0641 - val_mae: 0.1082\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2282 - mae: 0.2584\n",
            "Epoch 77: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2280 - mae: 0.2585 - val_loss: 0.1058 - val_mae: 0.2328\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2297 - mae: 0.2589\n",
            "Epoch 78: val_loss did not improve from 0.05304\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2295 - mae: 0.2587 - val_loss: 0.0790 - val_mae: 0.1875\n",
            "Epoch 78: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07584987580776215, RMSE:0.2754085659980774, MAE:0.11753012984991074, R2:0.9170526773382071\n",
            "0 0.9170526773382071\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08113591372966766, RMSE:0.28484365344047546, MAE:0.11575396358966827, R2:0.9096758243606976\n",
            "r2: 0.9096758243606976 rmse: 0.28484365\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08113591372966766, RMSE:0.28484365344047546, MAE:0.11575396358966827, R2:0.9096758243606976\n",
            "r2: 0.9096758243606976 rmse: 0.28484365\n",
            "第1次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_14/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_14'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_79\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_56 (Embedding)       (None, 17, 900)      649800      ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_57 (Embedding)       (None, 17, 900)      649800      ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_58 (Embedding)       (None, 17, 900)      649800      ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_59 (Embedding)       (None, 17, 900)      649800      ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 17, 100)      270100      ['embedding_56[0][0]',           \n",
            "                                                                  'embedding_57[0][0]',           \n",
            "                                                                  'embedding_58[0][0]',           \n",
            "                                                                  'embedding_59[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 17, 100)      270100      ['embedding_56[0][0]',           \n",
            "                                                                  'embedding_57[0][0]',           \n",
            "                                                                  'embedding_58[0][0]',           \n",
            "                                                                  'embedding_59[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 17, 100)      270100      ['embedding_56[0][0]',           \n",
            "                                                                  'embedding_57[0][0]',           \n",
            "                                                                  'embedding_58[0][0]',           \n",
            "                                                                  'embedding_59[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_168 (Glob  (None, 100)         0           ['conv1d_42[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_169 (Glob  (None, 100)         0           ['conv1d_42[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_170 (Glob  (None, 100)         0           ['conv1d_42[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_171 (Glob  (None, 100)         0           ['conv1d_42[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_172 (Glob  (None, 100)         0           ['conv1d_43[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_173 (Glob  (None, 100)         0           ['conv1d_43[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_174 (Glob  (None, 100)         0           ['conv1d_43[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_175 (Glob  (None, 100)         0           ['conv1d_43[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_176 (Glob  (None, 100)         0           ['conv1d_44[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_177 (Glob  (None, 100)         0           ['conv1d_44[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_178 (Glob  (None, 100)         0           ['conv1d_44[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_179 (Glob  (None, 100)         0           ['conv1d_44[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_42 (Add)                   (None, 100)          0           ['global_max_pooling1d_168[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_169[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_170[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_171[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_43 (Add)                   (None, 100)          0           ['global_max_pooling1d_172[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_173[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_174[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_175[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_44 (Add)                   (None, 100)          0           ['global_max_pooling1d_176[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_177[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_178[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_179[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 1000)         11000       ['input_30[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 1300)         0           ['add_42[0][0]',                 \n",
            "                                                                  'add_43[0][0]',                 \n",
            "                                                                  'add_44[0][0]',                 \n",
            "                                                                  'dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_14 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_14[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_14 (G  (1, 1300)           0           ['tf.expand_dims_14[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_14 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_14[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (1, 1, 1, 650)       845650      ['reshape_14[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_14 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_57[0][0]']               \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_14[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_14 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_58[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_14[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_14[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_14 (TFOpL  (None, 1300)        0           ['multiply_14[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_14[0][0]']\n",
            "                                                                                                  \n",
            " dropout_70 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_70[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_71 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_71[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_72 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_72[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_73 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_73[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 1)            3           ['dropout_74[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.4562 - mae: 0.9829\n",
            "Epoch 1: val_loss improved from inf to 0.56230, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 43ms/step - loss: 2.4531 - mae: 0.9822 - val_loss: 0.5623 - val_mae: 0.4667\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7307 - mae: 0.5351\n",
            "Epoch 2: val_loss improved from 0.56230 to 0.43749, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.7303 - mae: 0.5348 - val_loss: 0.4375 - val_mae: 0.3304\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6628 - mae: 0.5028\n",
            "Epoch 3: val_loss did not improve from 0.43749\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6610 - mae: 0.5019 - val_loss: 0.5714 - val_mae: 0.5270\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7021 - mae: 0.5170\n",
            "Epoch 4: val_loss improved from 0.43749 to 0.40752, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.7017 - mae: 0.5170 - val_loss: 0.4075 - val_mae: 0.3499\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6826 - mae: 0.5144\n",
            "Epoch 5: val_loss improved from 0.40752 to 0.40137, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6814 - mae: 0.5139 - val_loss: 0.4014 - val_mae: 0.3297\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6504 - mae: 0.4973\n",
            "Epoch 6: val_loss improved from 0.40137 to 0.39890, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6502 - mae: 0.4974 - val_loss: 0.3989 - val_mae: 0.3724\n",
            "Epoch 7/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.6596 - mae: 0.5103\n",
            "Epoch 7: val_loss did not improve from 0.39890\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.6596 - mae: 0.5103 - val_loss: 0.4270 - val_mae: 0.4253\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6282 - mae: 0.4762\n",
            "Epoch 8: val_loss did not improve from 0.39890\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6282 - mae: 0.4762 - val_loss: 0.4161 - val_mae: 0.4259\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5985 - mae: 0.4673\n",
            "Epoch 9: val_loss improved from 0.39890 to 0.35375, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5978 - mae: 0.4669 - val_loss: 0.3538 - val_mae: 0.3032\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5879 - mae: 0.4560\n",
            "Epoch 10: val_loss improved from 0.35375 to 0.33432, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5878 - mae: 0.4559 - val_loss: 0.3343 - val_mae: 0.2933\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5821 - mae: 0.4653\n",
            "Epoch 11: val_loss did not improve from 0.33432\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.5823 - mae: 0.4655 - val_loss: 0.3880 - val_mae: 0.4350\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5477 - mae: 0.4306\n",
            "Epoch 12: val_loss did not improve from 0.33432\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5487 - mae: 0.4305 - val_loss: 0.4134 - val_mae: 0.4457\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5431 - mae: 0.4351\n",
            "Epoch 13: val_loss improved from 0.33432 to 0.30230, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 51ms/step - loss: 0.5429 - mae: 0.4349 - val_loss: 0.3023 - val_mae: 0.3520\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5062 - mae: 0.4202\n",
            "Epoch 14: val_loss improved from 0.30230 to 0.24533, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5050 - mae: 0.4197 - val_loss: 0.2453 - val_mae: 0.2712\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4846 - mae: 0.4065\n",
            "Epoch 15: val_loss improved from 0.24533 to 0.21478, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.4838 - mae: 0.4063 - val_loss: 0.2148 - val_mae: 0.2087\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4069 - mae: 0.3630\n",
            "Epoch 16: val_loss did not improve from 0.21478\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4064 - mae: 0.3627 - val_loss: 0.2259 - val_mae: 0.2968\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4363 - mae: 0.3897\n",
            "Epoch 17: val_loss improved from 0.21478 to 0.19170, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.4368 - mae: 0.3904 - val_loss: 0.1917 - val_mae: 0.3258\n",
            "Epoch 18/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4051 - mae: 0.3601\n",
            "Epoch 18: val_loss improved from 0.19170 to 0.16910, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4051 - mae: 0.3601 - val_loss: 0.1691 - val_mae: 0.2234\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3599 - mae: 0.3429\n",
            "Epoch 19: val_loss did not improve from 0.16910\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3598 - mae: 0.3429 - val_loss: 0.2103 - val_mae: 0.3420\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3664 - mae: 0.3449\n",
            "Epoch 20: val_loss improved from 0.16910 to 0.08299, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3661 - mae: 0.3448 - val_loss: 0.0830 - val_mae: 0.1552\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3347 - mae: 0.3352\n",
            "Epoch 21: val_loss did not improve from 0.08299\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3342 - mae: 0.3350 - val_loss: 0.2784 - val_mae: 0.4357\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3368 - mae: 0.3374\n",
            "Epoch 22: val_loss did not improve from 0.08299\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3359 - mae: 0.3369 - val_loss: 0.1146 - val_mae: 0.2373\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3142 - mae: 0.3054\n",
            "Epoch 23: val_loss did not improve from 0.08299\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3133 - mae: 0.3049 - val_loss: 0.0931 - val_mae: 0.1692\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3274 - mae: 0.3331\n",
            "Epoch 24: val_loss improved from 0.08299 to 0.08174, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3278 - mae: 0.3331 - val_loss: 0.0817 - val_mae: 0.1722\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2678 - mae: 0.2847\n",
            "Epoch 25: val_loss did not improve from 0.08174\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2681 - mae: 0.2851 - val_loss: 0.0824 - val_mae: 0.1707\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2916 - mae: 0.2911\n",
            "Epoch 26: val_loss did not improve from 0.08174\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2909 - mae: 0.2911 - val_loss: 0.1651 - val_mae: 0.3390\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3061 - mae: 0.3085\n",
            "Epoch 27: val_loss did not improve from 0.08174\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3053 - mae: 0.3080 - val_loss: 0.1069 - val_mae: 0.1949\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2703 - mae: 0.2818\n",
            "Epoch 28: val_loss improved from 0.08174 to 0.06715, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2708 - mae: 0.2826 - val_loss: 0.0672 - val_mae: 0.1194\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3460 - mae: 0.3505\n",
            "Epoch 29: val_loss did not improve from 0.06715\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3465 - mae: 0.3505 - val_loss: 0.0775 - val_mae: 0.1317\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2838 - mae: 0.2780\n",
            "Epoch 30: val_loss did not improve from 0.06715\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2830 - mae: 0.2777 - val_loss: 0.1248 - val_mae: 0.2724\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3132 - mae: 0.3116\n",
            "Epoch 31: val_loss did not improve from 0.06715\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3156 - mae: 0.3133 - val_loss: 0.6358 - val_mae: 0.7568\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3260 - mae: 0.3329\n",
            "Epoch 32: val_loss did not improve from 0.06715\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3250 - mae: 0.3324 - val_loss: 0.0920 - val_mae: 0.2277\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2696 - mae: 0.2683\n",
            "Epoch 33: val_loss did not improve from 0.06715\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2694 - mae: 0.2683 - val_loss: 0.0858 - val_mae: 0.1772\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2915 - mae: 0.3045\n",
            "Epoch 34: val_loss improved from 0.06715 to 0.06481, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2919 - mae: 0.3045 - val_loss: 0.0648 - val_mae: 0.1161\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3058 - mae: 0.2973\n",
            "Epoch 35: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3077 - mae: 0.2986 - val_loss: 0.4823 - val_mae: 0.3195\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3929 - mae: 0.3720\n",
            "Epoch 36: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3926 - mae: 0.3719 - val_loss: 0.0802 - val_mae: 0.1582\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2815 - mae: 0.2921\n",
            "Epoch 37: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2807 - mae: 0.2915 - val_loss: 0.2277 - val_mae: 0.4193\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2566 - mae: 0.2772\n",
            "Epoch 38: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2562 - mae: 0.2773 - val_loss: 0.2623 - val_mae: 0.4361\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2635 - mae: 0.2736\n",
            "Epoch 39: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2633 - mae: 0.2733 - val_loss: 0.0936 - val_mae: 0.2155\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2714 - mae: 0.2753\n",
            "Epoch 40: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2716 - mae: 0.2755 - val_loss: 0.2691 - val_mae: 0.4544\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3350 - mae: 0.3287\n",
            "Epoch 41: val_loss did not improve from 0.06481\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3340 - mae: 0.3282 - val_loss: 0.0859 - val_mae: 0.2112\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2572 - mae: 0.2597\n",
            "Epoch 42: val_loss improved from 0.06481 to 0.06303, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2586 - mae: 0.2606 - val_loss: 0.0630 - val_mae: 0.1357\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2766 - mae: 0.2847\n",
            "Epoch 43: val_loss improved from 0.06303 to 0.05546, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2758 - mae: 0.2843 - val_loss: 0.0555 - val_mae: 0.1077\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2560 - mae: 0.2656\n",
            "Epoch 44: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2556 - mae: 0.2657 - val_loss: 0.0733 - val_mae: 0.1793\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3022 - mae: 0.3094\n",
            "Epoch 45: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3020 - mae: 0.3093 - val_loss: 0.1883 - val_mae: 0.3905\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2429 - mae: 0.2558\n",
            "Epoch 46: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2434 - mae: 0.2563 - val_loss: 0.0664 - val_mae: 0.1476\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2756 - mae: 0.2680\n",
            "Epoch 47: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2753 - mae: 0.2683 - val_loss: 0.2698 - val_mae: 0.4612\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2595 - mae: 0.2750\n",
            "Epoch 48: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2610 - mae: 0.2760 - val_loss: 0.1375 - val_mae: 0.1982\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2823 - mae: 0.2898\n",
            "Epoch 49: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2820 - mae: 0.2897 - val_loss: 0.0661 - val_mae: 0.1429\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2722 - mae: 0.2672\n",
            "Epoch 50: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2716 - mae: 0.2668 - val_loss: 0.0940 - val_mae: 0.2251\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2609 - mae: 0.2718\n",
            "Epoch 51: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2604 - mae: 0.2715 - val_loss: 0.0619 - val_mae: 0.1337\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2615 - mae: 0.2639\n",
            "Epoch 52: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2610 - mae: 0.2640 - val_loss: 0.1107 - val_mae: 0.2619\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2561 - mae: 0.2747\n",
            "Epoch 53: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2566 - mae: 0.2751 - val_loss: 0.0602 - val_mae: 0.1007\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2884 - mae: 0.3076\n",
            "Epoch 54: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2915 - mae: 0.3080 - val_loss: 0.0752 - val_mae: 0.1822\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2346 - mae: 0.2475\n",
            "Epoch 55: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2340 - mae: 0.2472 - val_loss: 0.1070 - val_mae: 0.2586\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2350 - mae: 0.2513\n",
            "Epoch 56: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2350 - mae: 0.2513 - val_loss: 0.1868 - val_mae: 0.3857\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2437 - mae: 0.2752\n",
            "Epoch 57: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2438 - mae: 0.2752 - val_loss: 0.1339 - val_mae: 0.2938\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2466 - mae: 0.2597\n",
            "Epoch 58: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2463 - mae: 0.2595 - val_loss: 0.0765 - val_mae: 0.1453\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2480 - mae: 0.2634\n",
            "Epoch 59: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2473 - mae: 0.2630 - val_loss: 0.0697 - val_mae: 0.1751\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2270 - mae: 0.2521\n",
            "Epoch 60: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2267 - mae: 0.2520 - val_loss: 0.1373 - val_mae: 0.2773\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2311 - mae: 0.2534\n",
            "Epoch 61: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2310 - mae: 0.2533 - val_loss: 0.0563 - val_mae: 0.1058\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2665 - mae: 0.2935\n",
            "Epoch 62: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2669 - mae: 0.2939 - val_loss: 0.0873 - val_mae: 0.2251\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2145 - mae: 0.2422\n",
            "Epoch 63: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2140 - mae: 0.2421 - val_loss: 0.0816 - val_mae: 0.1929\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2243 - mae: 0.2422\n",
            "Epoch 64: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2240 - mae: 0.2421 - val_loss: 0.0645 - val_mae: 0.1394\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2351 - mae: 0.2492\n",
            "Epoch 65: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2354 - mae: 0.2493 - val_loss: 0.0931 - val_mae: 0.2294\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2185 - mae: 0.2384\n",
            "Epoch 66: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2184 - mae: 0.2383 - val_loss: 0.1257 - val_mae: 0.2700\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2357 - mae: 0.2587\n",
            "Epoch 67: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2357 - mae: 0.2589 - val_loss: 0.0763 - val_mae: 0.2002\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2367 - mae: 0.2465\n",
            "Epoch 68: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2364 - mae: 0.2464 - val_loss: 0.1934 - val_mae: 0.3691\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2386 - mae: 0.2696\n",
            "Epoch 69: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2385 - mae: 0.2695 - val_loss: 0.1194 - val_mae: 0.2737\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2285 - mae: 0.2403\n",
            "Epoch 70: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2282 - mae: 0.2404 - val_loss: 0.0661 - val_mae: 0.1591\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2141 - mae: 0.2464\n",
            "Epoch 71: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2139 - mae: 0.2462 - val_loss: 0.0742 - val_mae: 0.1804\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2338 - mae: 0.2684\n",
            "Epoch 72: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2334 - mae: 0.2683 - val_loss: 0.0665 - val_mae: 0.1572\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2754 - mae: 0.3020\n",
            "Epoch 73: val_loss did not improve from 0.05546\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2763 - mae: 0.3024 - val_loss: 0.5550 - val_mae: 0.3045\n",
            "Epoch 73: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.08142055571079254, RMSE:0.28534287214279175, MAE:0.1320306658744812, R2:0.910960730842986\n",
            "1 0.910960730842986\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08359091728925705, RMSE:0.28912094235420227, MAE:0.12723323702812195, R2:0.9069428181583941\n",
            "r2: 0.9069428181583941 rmse: 0.28912094\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 15ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08359091728925705, RMSE:0.28912094235420227, MAE:0.12723323702812195, R2:0.9069428181583941\n",
            "r2: 0.9069428181583941 rmse: 0.28912094\n",
            "第2次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_15/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_15'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_85\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_31 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_60 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_61 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_62 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_63 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 17, 100)      270100      ['embedding_60[0][0]',           \n",
            "                                                                  'embedding_61[0][0]',           \n",
            "                                                                  'embedding_62[0][0]',           \n",
            "                                                                  'embedding_63[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 17, 100)      270100      ['embedding_60[0][0]',           \n",
            "                                                                  'embedding_61[0][0]',           \n",
            "                                                                  'embedding_62[0][0]',           \n",
            "                                                                  'embedding_63[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 17, 100)      270100      ['embedding_60[0][0]',           \n",
            "                                                                  'embedding_61[0][0]',           \n",
            "                                                                  'embedding_62[0][0]',           \n",
            "                                                                  'embedding_63[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_180 (Glob  (None, 100)         0           ['conv1d_45[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_181 (Glob  (None, 100)         0           ['conv1d_45[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_182 (Glob  (None, 100)         0           ['conv1d_45[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_183 (Glob  (None, 100)         0           ['conv1d_45[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_184 (Glob  (None, 100)         0           ['conv1d_46[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_185 (Glob  (None, 100)         0           ['conv1d_46[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_186 (Glob  (None, 100)         0           ['conv1d_46[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_187 (Glob  (None, 100)         0           ['conv1d_46[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_188 (Glob  (None, 100)         0           ['conv1d_47[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_189 (Glob  (None, 100)         0           ['conv1d_47[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_190 (Glob  (None, 100)         0           ['conv1d_47[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_191 (Glob  (None, 100)         0           ['conv1d_47[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_45 (Add)                   (None, 100)          0           ['global_max_pooling1d_180[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_181[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_182[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_183[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_46 (Add)                   (None, 100)          0           ['global_max_pooling1d_184[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_185[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_186[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_187[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_47 (Add)                   (None, 100)          0           ['global_max_pooling1d_188[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_189[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_190[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_191[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 1000)         11000       ['input_32[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 1300)         0           ['add_45[0][0]',                 \n",
            "                                                                  'add_46[0][0]',                 \n",
            "                                                                  'add_47[0][0]',                 \n",
            "                                                                  'dense_60[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_15 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_15[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_15 (G  (1, 1300)           0           ['tf.expand_dims_15[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_15 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (1, 1, 1, 650)       845650      ['reshape_15[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_15 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_61[0][0]']               \n",
            "                                                                                                  \n",
            " dense_62 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_15[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_15 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_62[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_15[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_15[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_15 (TFOpL  (None, 1300)        0           ['multiply_15[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_15[0][0]']\n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_76[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_77[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_78[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_79 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 1)            3           ['dropout_79[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 2.2172 - mae: 0.9190\n",
            "Epoch 1: val_loss improved from inf to 0.58671, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 2.2172 - mae: 0.9190 - val_loss: 0.5867 - val_mae: 0.3711\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8271 - mae: 0.5593\n",
            "Epoch 2: val_loss improved from 0.58671 to 0.42421, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.8258 - mae: 0.5588 - val_loss: 0.4242 - val_mae: 0.3189\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6987 - mae: 0.5196\n",
            "Epoch 3: val_loss did not improve from 0.42421\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6978 - mae: 0.5196 - val_loss: 0.4761 - val_mae: 0.4360\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6678 - mae: 0.5014\n",
            "Epoch 4: val_loss did not improve from 0.42421\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6670 - mae: 0.5014 - val_loss: 0.4832 - val_mae: 0.4321\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7039 - mae: 0.5351\n",
            "Epoch 5: val_loss improved from 0.42421 to 0.41923, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.7034 - mae: 0.5351 - val_loss: 0.4192 - val_mae: 0.4284\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6407 - mae: 0.4803\n",
            "Epoch 6: val_loss did not improve from 0.41923\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6395 - mae: 0.4798 - val_loss: 0.4801 - val_mae: 0.4467\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5962 - mae: 0.4650\n",
            "Epoch 7: val_loss improved from 0.41923 to 0.40701, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5953 - mae: 0.4644 - val_loss: 0.4070 - val_mae: 0.3635\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6289 - mae: 0.4915\n",
            "Epoch 8: val_loss improved from 0.40701 to 0.40513, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6288 - mae: 0.4913 - val_loss: 0.4051 - val_mae: 0.4471\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6070 - mae: 0.4598\n",
            "Epoch 9: val_loss did not improve from 0.40513\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6057 - mae: 0.4594 - val_loss: 0.4116 - val_mae: 0.4257\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5731 - mae: 0.4672\n",
            "Epoch 10: val_loss improved from 0.40513 to 0.32742, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5739 - mae: 0.4674 - val_loss: 0.3274 - val_mae: 0.3252\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5493 - mae: 0.4344\n",
            "Epoch 11: val_loss did not improve from 0.32742\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5503 - mae: 0.4346 - val_loss: 0.3944 - val_mae: 0.4410\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5059 - mae: 0.4139\n",
            "Epoch 12: val_loss did not improve from 0.32742\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.5076 - mae: 0.4144 - val_loss: 0.3833 - val_mae: 0.4427\n",
            "Epoch 13/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4720 - mae: 0.3976\n",
            "Epoch 13: val_loss improved from 0.32742 to 0.26179, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4720 - mae: 0.3976 - val_loss: 0.2618 - val_mae: 0.2488\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4784 - mae: 0.4086\n",
            "Epoch 14: val_loss improved from 0.26179 to 0.24507, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4774 - mae: 0.4081 - val_loss: 0.2451 - val_mae: 0.3289\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4399 - mae: 0.3781\n",
            "Epoch 15: val_loss improved from 0.24507 to 0.20946, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.4392 - mae: 0.3779 - val_loss: 0.2095 - val_mae: 0.2715\n",
            "Epoch 16/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4199 - mae: 0.3698\n",
            "Epoch 16: val_loss improved from 0.20946 to 0.15028, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4199 - mae: 0.3698 - val_loss: 0.1503 - val_mae: 0.2035\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3698 - mae: 0.3411\n",
            "Epoch 17: val_loss improved from 0.15028 to 0.13628, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3695 - mae: 0.3411 - val_loss: 0.1363 - val_mae: 0.1907\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3686 - mae: 0.3470\n",
            "Epoch 18: val_loss improved from 0.13628 to 0.11716, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3681 - mae: 0.3468 - val_loss: 0.1172 - val_mae: 0.1916\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3925 - mae: 0.3458\n",
            "Epoch 19: val_loss did not improve from 0.11716\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3916 - mae: 0.3457 - val_loss: 0.1352 - val_mae: 0.2319\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3375 - mae: 0.3178\n",
            "Epoch 20: val_loss improved from 0.11716 to 0.10853, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3368 - mae: 0.3176 - val_loss: 0.1085 - val_mae: 0.1783\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3291 - mae: 0.3146\n",
            "Epoch 21: val_loss improved from 0.10853 to 0.10338, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.3292 - mae: 0.3148 - val_loss: 0.1034 - val_mae: 0.1602\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3481 - mae: 0.3517\n",
            "Epoch 22: val_loss did not improve from 0.10338\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3474 - mae: 0.3513 - val_loss: 0.1580 - val_mae: 0.2725\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2899 - mae: 0.2813\n",
            "Epoch 23: val_loss improved from 0.10338 to 0.09011, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2891 - mae: 0.2809 - val_loss: 0.0901 - val_mae: 0.1445\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3331 - mae: 0.3255\n",
            "Epoch 24: val_loss did not improve from 0.09011\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3322 - mae: 0.3249 - val_loss: 0.1198 - val_mae: 0.2143\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2985 - mae: 0.3040\n",
            "Epoch 25: val_loss improved from 0.09011 to 0.07842, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2978 - mae: 0.3037 - val_loss: 0.0784 - val_mae: 0.1671\n",
            "Epoch 26/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3563 - mae: 0.3426\n",
            "Epoch 26: val_loss did not improve from 0.07842\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3563 - mae: 0.3426 - val_loss: 0.0977 - val_mae: 0.2214\n",
            "Epoch 27/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2970 - mae: 0.3010\n",
            "Epoch 27: val_loss did not improve from 0.07842\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2970 - mae: 0.3010 - val_loss: 0.0873 - val_mae: 0.2070\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3196 - mae: 0.3401\n",
            "Epoch 28: val_loss did not improve from 0.07842\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3193 - mae: 0.3399 - val_loss: 0.1799 - val_mae: 0.2511\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2971 - mae: 0.2873\n",
            "Epoch 29: val_loss did not improve from 0.07842\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2978 - mae: 0.2878 - val_loss: 0.1154 - val_mae: 0.1572\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3018 - mae: 0.2826\n",
            "Epoch 30: val_loss improved from 0.07842 to 0.06377, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 0.3014 - mae: 0.2824 - val_loss: 0.0638 - val_mae: 0.1088\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2703 - mae: 0.2850\n",
            "Epoch 31: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2695 - mae: 0.2845 - val_loss: 0.1236 - val_mae: 0.2800\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2640 - mae: 0.2628\n",
            "Epoch 32: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2638 - mae: 0.2628 - val_loss: 0.0659 - val_mae: 0.1379\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2751 - mae: 0.2805\n",
            "Epoch 33: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2748 - mae: 0.2803 - val_loss: 0.2632 - val_mae: 0.4391\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3127 - mae: 0.3090\n",
            "Epoch 34: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3121 - mae: 0.3090 - val_loss: 0.0675 - val_mae: 0.1715\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2654 - mae: 0.2684\n",
            "Epoch 35: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2653 - mae: 0.2687 - val_loss: 0.0659 - val_mae: 0.1131\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2952 - mae: 0.3032\n",
            "Epoch 36: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2947 - mae: 0.3031 - val_loss: 0.0707 - val_mae: 0.1344\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2560 - mae: 0.2595\n",
            "Epoch 37: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2560 - mae: 0.2594 - val_loss: 0.1790 - val_mae: 0.3341\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2464 - mae: 0.2575\n",
            "Epoch 38: val_loss improved from 0.06377 to 0.05713, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2469 - mae: 0.2577 - val_loss: 0.0571 - val_mae: 0.0978\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5342 - mae: 0.4231\n",
            "Epoch 39: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5340 - mae: 0.4228 - val_loss: 0.4215 - val_mae: 0.3322\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4788 - mae: 0.3967\n",
            "Epoch 40: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4777 - mae: 0.3962 - val_loss: 0.1286 - val_mae: 0.2021\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2955 - mae: 0.2946\n",
            "Epoch 41: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2965 - mae: 0.2946 - val_loss: 0.1021 - val_mae: 0.2281\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2979 - mae: 0.2913\n",
            "Epoch 42: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3000 - mae: 0.2916 - val_loss: 0.0722 - val_mae: 0.1428\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2790 - mae: 0.2802\n",
            "Epoch 43: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2790 - mae: 0.2800 - val_loss: 0.0760 - val_mae: 0.1846\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2649 - mae: 0.2604\n",
            "Epoch 44: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2651 - mae: 0.2603 - val_loss: 0.0698 - val_mae: 0.1464\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2423 - mae: 0.2615\n",
            "Epoch 45: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2423 - mae: 0.2616 - val_loss: 0.0741 - val_mae: 0.1877\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2447 - mae: 0.2530\n",
            "Epoch 46: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2451 - mae: 0.2532 - val_loss: 0.0776 - val_mae: 0.1696\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2645 - mae: 0.2740\n",
            "Epoch 47: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2641 - mae: 0.2743 - val_loss: 0.0613 - val_mae: 0.1110\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2556 - mae: 0.2709\n",
            "Epoch 48: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2569 - mae: 0.2713 - val_loss: 0.0630 - val_mae: 0.1045\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2511 - mae: 0.2643\n",
            "Epoch 49: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2508 - mae: 0.2643 - val_loss: 0.0850 - val_mae: 0.2126\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2484 - mae: 0.2552\n",
            "Epoch 50: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2483 - mae: 0.2552 - val_loss: 0.1066 - val_mae: 0.2604\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2398 - mae: 0.2475\n",
            "Epoch 51: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2393 - mae: 0.2472 - val_loss: 0.0799 - val_mae: 0.1805\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2212 - mae: 0.2492\n",
            "Epoch 52: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2247 - mae: 0.2503 - val_loss: 0.1646 - val_mae: 0.3451\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2483 - mae: 0.2543\n",
            "Epoch 53: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2482 - mae: 0.2543 - val_loss: 0.0638 - val_mae: 0.1434\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2453 - mae: 0.2513\n",
            "Epoch 54: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2457 - mae: 0.2514 - val_loss: 0.1241 - val_mae: 0.2944\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2199 - mae: 0.2503\n",
            "Epoch 55: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2195 - mae: 0.2501 - val_loss: 0.0655 - val_mae: 0.1366\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2556 - mae: 0.2610\n",
            "Epoch 56: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2549 - mae: 0.2607 - val_loss: 0.0650 - val_mae: 0.1010\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2363 - mae: 0.2472\n",
            "Epoch 57: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2362 - mae: 0.2472 - val_loss: 0.1165 - val_mae: 0.2662\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2456 - mae: 0.2484\n",
            "Epoch 58: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2454 - mae: 0.2483 - val_loss: 0.0970 - val_mae: 0.2277\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2506 - mae: 0.2608\n",
            "Epoch 59: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2510 - mae: 0.2608 - val_loss: 0.0587 - val_mae: 0.1403\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2398 - mae: 0.2601\n",
            "Epoch 60: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2398 - mae: 0.2600 - val_loss: 0.1238 - val_mae: 0.2685\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2793 - mae: 0.3006\n",
            "Epoch 61: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2796 - mae: 0.3006 - val_loss: 0.0655 - val_mae: 0.1504\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2098 - mae: 0.2263\n",
            "Epoch 62: val_loss improved from 0.05713 to 0.05310, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2093 - mae: 0.2262 - val_loss: 0.0531 - val_mae: 0.0928\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2429 - mae: 0.2461\n",
            "Epoch 63: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2426 - mae: 0.2459 - val_loss: 0.0863 - val_mae: 0.2224\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2151 - mae: 0.2395\n",
            "Epoch 64: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2151 - mae: 0.2395 - val_loss: 0.0630 - val_mae: 0.1420\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2363 - mae: 0.2601\n",
            "Epoch 65: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2363 - mae: 0.2598 - val_loss: 0.0641 - val_mae: 0.1483\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2013 - mae: 0.2309\n",
            "Epoch 66: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2009 - mae: 0.2307 - val_loss: 0.0594 - val_mae: 0.1085\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2169 - mae: 0.2395\n",
            "Epoch 67: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2173 - mae: 0.2396 - val_loss: 0.1275 - val_mae: 0.2925\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2347 - mae: 0.2461\n",
            "Epoch 68: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2343 - mae: 0.2459 - val_loss: 0.0566 - val_mae: 0.0947\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2243 - mae: 0.2492\n",
            "Epoch 69: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2244 - mae: 0.2494 - val_loss: 0.0580 - val_mae: 0.1330\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2132 - mae: 0.2333\n",
            "Epoch 70: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2132 - mae: 0.2335 - val_loss: 0.1018 - val_mae: 0.2384\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2766 - mae: 0.3071\n",
            "Epoch 71: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2768 - mae: 0.3072 - val_loss: 0.0588 - val_mae: 0.1231\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1985 - mae: 0.2240\n",
            "Epoch 72: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1980 - mae: 0.2237 - val_loss: 0.0658 - val_mae: 0.1160\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2329 - mae: 0.2525\n",
            "Epoch 73: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2327 - mae: 0.2529 - val_loss: 0.0791 - val_mae: 0.1551\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2068 - mae: 0.2390\n",
            "Epoch 74: val_loss improved from 0.05310 to 0.05229, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2068 - mae: 0.2390 - val_loss: 0.0523 - val_mae: 0.0932\n",
            "Epoch 75/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2218 - mae: 0.2311\n",
            "Epoch 75: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2218 - mae: 0.2311 - val_loss: 0.0586 - val_mae: 0.1294\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2156 - mae: 0.2296\n",
            "Epoch 76: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2158 - mae: 0.2298 - val_loss: 0.1435 - val_mae: 0.3149\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2066 - mae: 0.2477\n",
            "Epoch 77: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2071 - mae: 0.2482 - val_loss: 0.1024 - val_mae: 0.1548\n",
            "Epoch 78/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2133 - mae: 0.2229\n",
            "Epoch 78: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2133 - mae: 0.2229 - val_loss: 0.0648 - val_mae: 0.1420\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2128 - mae: 0.2330\n",
            "Epoch 79: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2126 - mae: 0.2333 - val_loss: 0.0826 - val_mae: 0.1417\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1931 - mae: 0.2324\n",
            "Epoch 80: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1931 - mae: 0.2323 - val_loss: 0.1186 - val_mae: 0.2688\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2222 - mae: 0.2349\n",
            "Epoch 81: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2222 - mae: 0.2351 - val_loss: 0.0960 - val_mae: 0.2256\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2538 - mae: 0.2787\n",
            "Epoch 82: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2532 - mae: 0.2785 - val_loss: 0.0635 - val_mae: 0.1193\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2079 - mae: 0.2229\n",
            "Epoch 83: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2093 - mae: 0.2230 - val_loss: 0.0685 - val_mae: 0.1334\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2218 - mae: 0.2325\n",
            "Epoch 84: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2212 - mae: 0.2323 - val_loss: 0.0708 - val_mae: 0.1285\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2166 - mae: 0.2376\n",
            "Epoch 85: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2159 - mae: 0.2371 - val_loss: 0.0648 - val_mae: 0.1464\n",
            "Epoch 86/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2046 - mae: 0.2268\n",
            "Epoch 86: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2046 - mae: 0.2268 - val_loss: 0.0570 - val_mae: 0.1048\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4069 - mae: 0.3764\n",
            "Epoch 87: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4060 - mae: 0.3759 - val_loss: 0.0845 - val_mae: 0.1727\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1927 - mae: 0.2180\n",
            "Epoch 88: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1922 - mae: 0.2179 - val_loss: 0.0593 - val_mae: 0.1255\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1878 - mae: 0.2132\n",
            "Epoch 89: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1876 - mae: 0.2131 - val_loss: 0.0939 - val_mae: 0.2295\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1905 - mae: 0.2165\n",
            "Epoch 90: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1905 - mae: 0.2165 - val_loss: 0.1070 - val_mae: 0.2542\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1954 - mae: 0.2347\n",
            "Epoch 91: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1951 - mae: 0.2344 - val_loss: 0.0779 - val_mae: 0.1877\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1990 - mae: 0.2204\n",
            "Epoch 92: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1990 - mae: 0.2205 - val_loss: 0.0560 - val_mae: 0.1286\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1847 - mae: 0.2143\n",
            "Epoch 93: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1851 - mae: 0.2145 - val_loss: 0.0743 - val_mae: 0.1577\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1851 - mae: 0.2237\n",
            "Epoch 94: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1847 - mae: 0.2236 - val_loss: 0.0712 - val_mae: 0.1818\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1648 - mae: 0.2047\n",
            "Epoch 95: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1646 - mae: 0.2045 - val_loss: 0.0599 - val_mae: 0.1442\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1900 - mae: 0.2232\n",
            "Epoch 96: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1896 - mae: 0.2233 - val_loss: 0.0542 - val_mae: 0.0958\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2239 - mae: 0.2452\n",
            "Epoch 97: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2234 - mae: 0.2450 - val_loss: 0.0567 - val_mae: 0.1184\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1917 - mae: 0.2133\n",
            "Epoch 98: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1917 - mae: 0.2133 - val_loss: 0.0630 - val_mae: 0.1505\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2027 - mae: 0.2190\n",
            "Epoch 99: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2022 - mae: 0.2188 - val_loss: 0.0748 - val_mae: 0.1770\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2058 - mae: 0.2330\n",
            "Epoch 100: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2056 - mae: 0.2331 - val_loss: 0.0587 - val_mae: 0.1339\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1862 - mae: 0.2116\n",
            "Epoch 101: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1862 - mae: 0.2115 - val_loss: 0.0586 - val_mae: 0.0931\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1811 - mae: 0.2113\n",
            "Epoch 102: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1813 - mae: 0.2115 - val_loss: 0.0541 - val_mae: 0.1163\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.2071\n",
            "Epoch 103: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1759 - mae: 0.2071 - val_loss: 0.0552 - val_mae: 0.1068\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.2034\n",
            "Epoch 104: val_loss did not improve from 0.05229\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1726 - mae: 0.2033 - val_loss: 0.0565 - val_mae: 0.1215\n",
            "Epoch 104: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07069374620914459, RMSE:0.2658829689025879, MAE:0.11245600134134293, R2:0.922691272816625\n",
            "2 0.922691272816625\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07211609184741974, RMSE:0.26854440569877625, MAE:0.10999476909637451, R2:0.9197171100459803\n",
            "r2: 0.9197171100459803 rmse: 0.2685444\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 18ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07211609184741974, RMSE:0.26854440569877625, MAE:0.10999476909637451, R2:0.9197171100459803\n",
            "r2: 0.9197171100459803 rmse: 0.2685444\n",
            "第3次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_16/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_16'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_91\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_33 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_64 (Embedding)       (None, 17, 900)      649800      ['input_33[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_65 (Embedding)       (None, 17, 900)      649800      ['input_33[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_66 (Embedding)       (None, 17, 900)      649800      ['input_33[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_67 (Embedding)       (None, 17, 900)      649800      ['input_33[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)             (None, 17, 100)      270100      ['embedding_64[0][0]',           \n",
            "                                                                  'embedding_65[0][0]',           \n",
            "                                                                  'embedding_66[0][0]',           \n",
            "                                                                  'embedding_67[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)             (None, 17, 100)      270100      ['embedding_64[0][0]',           \n",
            "                                                                  'embedding_65[0][0]',           \n",
            "                                                                  'embedding_66[0][0]',           \n",
            "                                                                  'embedding_67[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 17, 100)      270100      ['embedding_64[0][0]',           \n",
            "                                                                  'embedding_65[0][0]',           \n",
            "                                                                  'embedding_66[0][0]',           \n",
            "                                                                  'embedding_67[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_192 (Glob  (None, 100)         0           ['conv1d_48[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_193 (Glob  (None, 100)         0           ['conv1d_48[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_194 (Glob  (None, 100)         0           ['conv1d_48[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_195 (Glob  (None, 100)         0           ['conv1d_48[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_196 (Glob  (None, 100)         0           ['conv1d_49[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_197 (Glob  (None, 100)         0           ['conv1d_49[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_198 (Glob  (None, 100)         0           ['conv1d_49[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_199 (Glob  (None, 100)         0           ['conv1d_49[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_200 (Glob  (None, 100)         0           ['conv1d_50[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_201 (Glob  (None, 100)         0           ['conv1d_50[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_202 (Glob  (None, 100)         0           ['conv1d_50[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_203 (Glob  (None, 100)         0           ['conv1d_50[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_34 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_48 (Add)                   (None, 100)          0           ['global_max_pooling1d_192[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_193[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_194[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_195[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_49 (Add)                   (None, 100)          0           ['global_max_pooling1d_196[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_197[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_198[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_199[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_50 (Add)                   (None, 100)          0           ['global_max_pooling1d_200[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_201[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_202[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_203[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_64 (Dense)               (None, 1000)         11000       ['input_34[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 1300)         0           ['add_48[0][0]',                 \n",
            "                                                                  'add_49[0][0]',                 \n",
            "                                                                  'add_50[0][0]',                 \n",
            "                                                                  'dense_64[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_16 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_16[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_16 (G  (1, 1300)           0           ['tf.expand_dims_16[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_16 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_16[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_65 (Dense)               (1, 1, 1, 650)       845650      ['reshape_16[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_16 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_65[0][0]']               \n",
            "                                                                                                  \n",
            " dense_66 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_16[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_16 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_66[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_16 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_16[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_16[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_16 (TFOpL  (None, 1300)        0           ['multiply_16[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_16[0][0]']\n",
            "                                                                                                  \n",
            " dropout_80 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_80[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_81 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_81[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_82 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_82[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_83 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_83[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_84 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_67 (Dense)               (None, 1)            3           ['dropout_84[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.8825 - mae: 0.8018\n",
            "Epoch 1: val_loss improved from inf to 0.81493, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 1.8825 - mae: 0.8018 - val_loss: 0.8149 - val_mae: 0.6195\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7834 - mae: 0.5681\n",
            "Epoch 2: val_loss improved from 0.81493 to 0.43478, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.7819 - mae: 0.5674 - val_loss: 0.4348 - val_mae: 0.3930\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6847 - mae: 0.5155\n",
            "Epoch 3: val_loss improved from 0.43478 to 0.37775, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6841 - mae: 0.5154 - val_loss: 0.3778 - val_mae: 0.3054\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6709 - mae: 0.5177\n",
            "Epoch 4: val_loss did not improve from 0.37775\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6723 - mae: 0.5181 - val_loss: 0.4754 - val_mae: 0.5161\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6530 - mae: 0.5059\n",
            "Epoch 5: val_loss did not improve from 0.37775\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6528 - mae: 0.5058 - val_loss: 0.3787 - val_mae: 0.3331\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5894 - mae: 0.4664\n",
            "Epoch 6: val_loss improved from 0.37775 to 0.36366, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.5909 - mae: 0.4670 - val_loss: 0.3637 - val_mae: 0.3468\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6009 - mae: 0.4653\n",
            "Epoch 7: val_loss did not improve from 0.36366\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5999 - mae: 0.4649 - val_loss: 0.3638 - val_mae: 0.3302\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5697 - mae: 0.4502\n",
            "Epoch 8: val_loss improved from 0.36366 to 0.31778, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5717 - mae: 0.4507 - val_loss: 0.3178 - val_mae: 0.3195\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5880 - mae: 0.4619\n",
            "Epoch 9: val_loss did not improve from 0.31778\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5868 - mae: 0.4616 - val_loss: 0.4677 - val_mae: 0.4130\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5625 - mae: 0.4426\n",
            "Epoch 10: val_loss improved from 0.31778 to 0.29529, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5614 - mae: 0.4422 - val_loss: 0.2953 - val_mae: 0.2916\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5183 - mae: 0.4205\n",
            "Epoch 11: val_loss improved from 0.29529 to 0.27917, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5190 - mae: 0.4202 - val_loss: 0.2792 - val_mae: 0.2512\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4733 - mae: 0.3964\n",
            "Epoch 12: val_loss did not improve from 0.27917\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4725 - mae: 0.3960 - val_loss: 0.4357 - val_mae: 0.4661\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5062 - mae: 0.4361\n",
            "Epoch 13: val_loss did not improve from 0.27917\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5052 - mae: 0.4354 - val_loss: 0.2887 - val_mae: 0.3565\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4361 - mae: 0.3769\n",
            "Epoch 14: val_loss improved from 0.27917 to 0.17888, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.4351 - mae: 0.3766 - val_loss: 0.1789 - val_mae: 0.2036\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4040 - mae: 0.3571\n",
            "Epoch 15: val_loss improved from 0.17888 to 0.15601, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4032 - mae: 0.3569 - val_loss: 0.1560 - val_mae: 0.2036\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4057 - mae: 0.3635\n",
            "Epoch 16: val_loss improved from 0.15601 to 0.11417, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4048 - mae: 0.3633 - val_loss: 0.1142 - val_mae: 0.1649\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3452 - mae: 0.3241\n",
            "Epoch 17: val_loss improved from 0.11417 to 0.10421, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3448 - mae: 0.3239 - val_loss: 0.1042 - val_mae: 0.1490\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3396 - mae: 0.3458\n",
            "Epoch 18: val_loss did not improve from 0.10421\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3405 - mae: 0.3458 - val_loss: 0.1566 - val_mae: 0.2718\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3520 - mae: 0.3386\n",
            "Epoch 19: val_loss did not improve from 0.10421\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3510 - mae: 0.3381 - val_loss: 0.1094 - val_mae: 0.1562\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3200 - mae: 0.3039\n",
            "Epoch 20: val_loss improved from 0.10421 to 0.07973, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3197 - mae: 0.3039 - val_loss: 0.0797 - val_mae: 0.1588\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2889 - mae: 0.2900\n",
            "Epoch 21: val_loss did not improve from 0.07973\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2885 - mae: 0.2902 - val_loss: 0.1780 - val_mae: 0.2973\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3369 - mae: 0.3393\n",
            "Epoch 22: val_loss did not improve from 0.07973\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3371 - mae: 0.3390 - val_loss: 0.1476 - val_mae: 0.1823\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3186 - mae: 0.3053\n",
            "Epoch 23: val_loss did not improve from 0.07973\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3189 - mae: 0.3055 - val_loss: 0.1585 - val_mae: 0.3053\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3876 - mae: 0.3814\n",
            "Epoch 24: val_loss did not improve from 0.07973\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3872 - mae: 0.3814 - val_loss: 0.1251 - val_mae: 0.2491\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3168 - mae: 0.2915\n",
            "Epoch 25: val_loss improved from 0.07973 to 0.06586, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.3159 - mae: 0.2912 - val_loss: 0.0659 - val_mae: 0.1051\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3413 - mae: 0.3477\n",
            "Epoch 26: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3410 - mae: 0.3479 - val_loss: 0.1267 - val_mae: 0.2884\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2825 - mae: 0.2768\n",
            "Epoch 27: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2827 - mae: 0.2769 - val_loss: 0.1239 - val_mae: 0.2767\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2859 - mae: 0.3016\n",
            "Epoch 28: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2858 - mae: 0.3014 - val_loss: 0.0700 - val_mae: 0.1583\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2853 - mae: 0.2882\n",
            "Epoch 29: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2846 - mae: 0.2879 - val_loss: 0.0849 - val_mae: 0.1893\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3187 - mae: 0.3158\n",
            "Epoch 30: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3189 - mae: 0.3157 - val_loss: 0.0707 - val_mae: 0.1655\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3748 - mae: 0.3621\n",
            "Epoch 31: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3741 - mae: 0.3618 - val_loss: 0.0792 - val_mae: 0.1554\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2719 - mae: 0.2800\n",
            "Epoch 32: val_loss did not improve from 0.06586\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2714 - mae: 0.2797 - val_loss: 0.0662 - val_mae: 0.1295\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2808 - mae: 0.2801\n",
            "Epoch 33: val_loss improved from 0.06586 to 0.06420, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2807 - mae: 0.2800 - val_loss: 0.0642 - val_mae: 0.1427\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2915 - mae: 0.2801\n",
            "Epoch 34: val_loss did not improve from 0.06420\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2916 - mae: 0.2800 - val_loss: 0.2024 - val_mae: 0.3826\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2951 - mae: 0.2951\n",
            "Epoch 35: val_loss did not improve from 0.06420\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2957 - mae: 0.2950 - val_loss: 0.0990 - val_mae: 0.1668\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2763 - mae: 0.2847\n",
            "Epoch 36: val_loss did not improve from 0.06420\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2763 - mae: 0.2848 - val_loss: 0.1031 - val_mae: 0.1892\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2790 - mae: 0.2829\n",
            "Epoch 37: val_loss did not improve from 0.06420\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2782 - mae: 0.2824 - val_loss: 0.0887 - val_mae: 0.1888\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2997 - mae: 0.2796\n",
            "Epoch 38: val_loss did not improve from 0.06420\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2991 - mae: 0.2795 - val_loss: 0.0796 - val_mae: 0.1780\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3080 - mae: 0.3101\n",
            "Epoch 39: val_loss improved from 0.06420 to 0.06258, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3078 - mae: 0.3098 - val_loss: 0.0626 - val_mae: 0.1112\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2574 - mae: 0.2571\n",
            "Epoch 40: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2577 - mae: 0.2572 - val_loss: 0.2548 - val_mae: 0.3181\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3328 - mae: 0.3287\n",
            "Epoch 41: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3325 - mae: 0.3284 - val_loss: 0.1599 - val_mae: 0.2479\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2726 - mae: 0.2952\n",
            "Epoch 42: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2725 - mae: 0.2951 - val_loss: 0.0729 - val_mae: 0.1752\n",
            "Epoch 43/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2825 - mae: 0.2836\n",
            "Epoch 43: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2825 - mae: 0.2836 - val_loss: 0.0677 - val_mae: 0.1588\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2965 - mae: 0.2919\n",
            "Epoch 44: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2958 - mae: 0.2915 - val_loss: 0.2128 - val_mae: 0.3387\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2794 - mae: 0.2852\n",
            "Epoch 45: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2786 - mae: 0.2848 - val_loss: 0.0798 - val_mae: 0.1934\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2406 - mae: 0.2558\n",
            "Epoch 46: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2399 - mae: 0.2554 - val_loss: 0.0693 - val_mae: 0.1425\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2554 - mae: 0.2645\n",
            "Epoch 47: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2555 - mae: 0.2647 - val_loss: 0.1866 - val_mae: 0.3703\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2588 - mae: 0.2671\n",
            "Epoch 48: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2583 - mae: 0.2668 - val_loss: 0.0775 - val_mae: 0.1814\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2394 - mae: 0.2569\n",
            "Epoch 49: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2388 - mae: 0.2566 - val_loss: 0.0670 - val_mae: 0.0995\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2420 - mae: 0.2633\n",
            "Epoch 50: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2415 - mae: 0.2630 - val_loss: 0.1053 - val_mae: 0.2177\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2627 - mae: 0.2753\n",
            "Epoch 51: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2623 - mae: 0.2752 - val_loss: 0.0674 - val_mae: 0.1329\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2326 - mae: 0.2332\n",
            "Epoch 52: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2321 - mae: 0.2332 - val_loss: 0.0770 - val_mae: 0.1651\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2565 - mae: 0.2566\n",
            "Epoch 53: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2560 - mae: 0.2566 - val_loss: 0.0652 - val_mae: 0.1460\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2640 - mae: 0.2639\n",
            "Epoch 54: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2653 - mae: 0.2638 - val_loss: 0.1263 - val_mae: 0.1509\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2303 - mae: 0.2449\n",
            "Epoch 55: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2312 - mae: 0.2452 - val_loss: 0.0722 - val_mae: 0.1577\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2343 - mae: 0.2433\n",
            "Epoch 56: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2344 - mae: 0.2436 - val_loss: 0.0925 - val_mae: 0.1769\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2368 - mae: 0.2655\n",
            "Epoch 57: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2368 - mae: 0.2653 - val_loss: 0.0876 - val_mae: 0.2188\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2174 - mae: 0.2409\n",
            "Epoch 58: val_loss did not improve from 0.06258\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2175 - mae: 0.2411 - val_loss: 0.0677 - val_mae: 0.1685\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2279 - mae: 0.2439\n",
            "Epoch 59: val_loss improved from 0.06258 to 0.05861, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2273 - mae: 0.2438 - val_loss: 0.0586 - val_mae: 0.0904\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3061 - mae: 0.3190\n",
            "Epoch 60: val_loss did not improve from 0.05861\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3061 - mae: 0.3187 - val_loss: 0.0705 - val_mae: 0.1429\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1947 - mae: 0.2224\n",
            "Epoch 61: val_loss did not improve from 0.05861\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1943 - mae: 0.2223 - val_loss: 0.1015 - val_mae: 0.2517\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2583 - mae: 0.2869\n",
            "Epoch 62: val_loss did not improve from 0.05861\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2580 - mae: 0.2871 - val_loss: 0.0971 - val_mae: 0.1950\n",
            "Epoch 63/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2330 - mae: 0.2458\n",
            "Epoch 63: val_loss improved from 0.05861 to 0.05549, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2330 - mae: 0.2458 - val_loss: 0.0555 - val_mae: 0.0967\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2330 - mae: 0.2374\n",
            "Epoch 64: val_loss did not improve from 0.05549\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2327 - mae: 0.2374 - val_loss: 0.0628 - val_mae: 0.1528\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2259 - mae: 0.2475\n",
            "Epoch 65: val_loss improved from 0.05549 to 0.05394, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2264 - mae: 0.2476 - val_loss: 0.0539 - val_mae: 0.1196\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2198 - mae: 0.2362\n",
            "Epoch 66: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2213 - mae: 0.2370 - val_loss: 0.0754 - val_mae: 0.1238\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3712 - mae: 0.3352\n",
            "Epoch 67: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3717 - mae: 0.3353 - val_loss: 0.0686 - val_mae: 0.1049\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2423 - mae: 0.2538\n",
            "Epoch 68: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2418 - mae: 0.2536 - val_loss: 0.1082 - val_mae: 0.2291\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2190 - mae: 0.2231\n",
            "Epoch 69: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2194 - mae: 0.2235 - val_loss: 0.1162 - val_mae: 0.2566\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2201 - mae: 0.2395\n",
            "Epoch 70: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2196 - mae: 0.2392 - val_loss: 0.0616 - val_mae: 0.1272\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2166 - mae: 0.2234\n",
            "Epoch 71: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2170 - mae: 0.2234 - val_loss: 0.0578 - val_mae: 0.1323\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2287 - mae: 0.2481\n",
            "Epoch 72: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2280 - mae: 0.2476 - val_loss: 0.0647 - val_mae: 0.1171\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2284 - mae: 0.2422\n",
            "Epoch 73: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2285 - mae: 0.2423 - val_loss: 0.0723 - val_mae: 0.1474\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2311 - mae: 0.2658\n",
            "Epoch 74: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2307 - mae: 0.2655 - val_loss: 0.0820 - val_mae: 0.1260\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2163 - mae: 0.2211\n",
            "Epoch 75: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2161 - mae: 0.2212 - val_loss: 0.0567 - val_mae: 0.0907\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2004 - mae: 0.2207\n",
            "Epoch 76: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1998 - mae: 0.2204 - val_loss: 0.0614 - val_mae: 0.1431\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2130 - mae: 0.2348\n",
            "Epoch 77: val_loss did not improve from 0.05394\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2130 - mae: 0.2348 - val_loss: 0.0632 - val_mae: 0.1052\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1998 - mae: 0.2198\n",
            "Epoch 78: val_loss improved from 0.05394 to 0.05218, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.1997 - mae: 0.2197 - val_loss: 0.0522 - val_mae: 0.1142\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2117 - mae: 0.2294\n",
            "Epoch 79: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2117 - mae: 0.2294 - val_loss: 0.0952 - val_mae: 0.1977\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2035 - mae: 0.2198\n",
            "Epoch 80: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2032 - mae: 0.2197 - val_loss: 0.0708 - val_mae: 0.1892\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2155 - mae: 0.2412\n",
            "Epoch 81: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2150 - mae: 0.2410 - val_loss: 0.6335 - val_mae: 0.5399\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2603 - mae: 0.2633\n",
            "Epoch 82: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2604 - mae: 0.2633 - val_loss: 0.0935 - val_mae: 0.2020\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1947 - mae: 0.2119\n",
            "Epoch 83: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1942 - mae: 0.2116 - val_loss: 0.0708 - val_mae: 0.1776\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2222 - mae: 0.2535\n",
            "Epoch 84: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2234 - mae: 0.2546 - val_loss: 0.1765 - val_mae: 0.3726\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1897 - mae: 0.2110\n",
            "Epoch 85: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1894 - mae: 0.2110 - val_loss: 0.0546 - val_mae: 0.0951\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1905 - mae: 0.2064\n",
            "Epoch 86: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1907 - mae: 0.2064 - val_loss: 0.0595 - val_mae: 0.1068\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1935 - mae: 0.2143\n",
            "Epoch 87: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1930 - mae: 0.2141 - val_loss: 0.0660 - val_mae: 0.1625\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2155 - mae: 0.2482\n",
            "Epoch 88: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2158 - mae: 0.2482 - val_loss: 0.0589 - val_mae: 0.1439\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1883 - mae: 0.2036\n",
            "Epoch 89: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1880 - mae: 0.2035 - val_loss: 0.0641 - val_mae: 0.1381\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1690 - mae: 0.1942\n",
            "Epoch 90: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1685 - mae: 0.1939 - val_loss: 0.0647 - val_mae: 0.1455\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1677 - mae: 0.1933\n",
            "Epoch 91: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1674 - mae: 0.1932 - val_loss: 0.0577 - val_mae: 0.1022\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1848 - mae: 0.2047\n",
            "Epoch 92: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1850 - mae: 0.2049 - val_loss: 0.0593 - val_mae: 0.1245\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3968 - mae: 0.3294\n",
            "Epoch 93: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3971 - mae: 0.3299 - val_loss: 0.5180 - val_mae: 0.4437\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6290 - mae: 0.4465\n",
            "Epoch 94: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6307 - mae: 0.4468 - val_loss: 0.4901 - val_mae: 0.3232\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5802 - mae: 0.4260\n",
            "Epoch 95: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5800 - mae: 0.4261 - val_loss: 0.4256 - val_mae: 0.3263\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5090 - mae: 0.3963\n",
            "Epoch 96: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5094 - mae: 0.3962 - val_loss: 0.3461 - val_mae: 0.3687\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4248 - mae: 0.3659\n",
            "Epoch 97: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4241 - mae: 0.3658 - val_loss: 0.3081 - val_mae: 0.3279\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3637 - mae: 0.3390\n",
            "Epoch 98: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3636 - mae: 0.3390 - val_loss: 0.1767 - val_mae: 0.2274\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2447 - mae: 0.2564\n",
            "Epoch 99: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2444 - mae: 0.2561 - val_loss: 0.0946 - val_mae: 0.1663\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2083 - mae: 0.2247\n",
            "Epoch 100: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2088 - mae: 0.2251 - val_loss: 0.3017 - val_mae: 0.3631\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2801 - mae: 0.2781\n",
            "Epoch 101: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2799 - mae: 0.2779 - val_loss: 0.0796 - val_mae: 0.1219\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1967 - mae: 0.2110\n",
            "Epoch 102: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1964 - mae: 0.2107 - val_loss: 0.0703 - val_mae: 0.1172\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2341 - mae: 0.2297\n",
            "Epoch 103: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2335 - mae: 0.2292 - val_loss: 0.1021 - val_mae: 0.1530\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.1992\n",
            "Epoch 104: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1769 - mae: 0.1994 - val_loss: 0.0647 - val_mae: 0.1518\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1788 - mae: 0.1973\n",
            "Epoch 105: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1789 - mae: 0.1973 - val_loss: 0.0666 - val_mae: 0.0987\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1891 - mae: 0.1975\n",
            "Epoch 106: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1887 - mae: 0.1975 - val_loss: 0.0571 - val_mae: 0.0971\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1824 - mae: 0.1955\n",
            "Epoch 107: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1823 - mae: 0.1955 - val_loss: 0.0618 - val_mae: 0.1081\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4834 - mae: 0.3848\n",
            "Epoch 108: val_loss did not improve from 0.05218\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4846 - mae: 0.3857 - val_loss: 0.3719 - val_mae: 0.4135\n",
            "Epoch 108: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.0733581930398941, RMSE:0.2708471715450287, MAE:0.13558891415596008, R2:0.9197774991282821\n",
            "3 0.9197774991282821\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0750998705625534, RMSE:0.27404356002807617, MAE:0.13307175040245056, R2:0.916395433172939\n",
            "r2: 0.916395433172939 rmse: 0.27404356\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0750998705625534, RMSE:0.27404356002807617, MAE:0.13307175040245056, R2:0.916395433172939\n",
            "r2: 0.916395433172939 rmse: 0.27404356\n",
            "第4次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_17/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_17'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_97\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_35 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_68 (Embedding)       (None, 17, 900)      649800      ['input_35[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_69 (Embedding)       (None, 17, 900)      649800      ['input_35[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_70 (Embedding)       (None, 17, 900)      649800      ['input_35[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_71 (Embedding)       (None, 17, 900)      649800      ['input_35[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 17, 100)      270100      ['embedding_68[0][0]',           \n",
            "                                                                  'embedding_69[0][0]',           \n",
            "                                                                  'embedding_70[0][0]',           \n",
            "                                                                  'embedding_71[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 17, 100)      270100      ['embedding_68[0][0]',           \n",
            "                                                                  'embedding_69[0][0]',           \n",
            "                                                                  'embedding_70[0][0]',           \n",
            "                                                                  'embedding_71[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 17, 100)      270100      ['embedding_68[0][0]',           \n",
            "                                                                  'embedding_69[0][0]',           \n",
            "                                                                  'embedding_70[0][0]',           \n",
            "                                                                  'embedding_71[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_204 (Glob  (None, 100)         0           ['conv1d_51[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_205 (Glob  (None, 100)         0           ['conv1d_51[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_206 (Glob  (None, 100)         0           ['conv1d_51[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_207 (Glob  (None, 100)         0           ['conv1d_51[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_208 (Glob  (None, 100)         0           ['conv1d_52[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_209 (Glob  (None, 100)         0           ['conv1d_52[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_210 (Glob  (None, 100)         0           ['conv1d_52[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_211 (Glob  (None, 100)         0           ['conv1d_52[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_212 (Glob  (None, 100)         0           ['conv1d_53[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_213 (Glob  (None, 100)         0           ['conv1d_53[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_214 (Glob  (None, 100)         0           ['conv1d_53[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_215 (Glob  (None, 100)         0           ['conv1d_53[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 100)          0           ['global_max_pooling1d_204[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_205[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_206[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_207[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_52 (Add)                   (None, 100)          0           ['global_max_pooling1d_208[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_209[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_210[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_211[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_53 (Add)                   (None, 100)          0           ['global_max_pooling1d_212[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_213[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_214[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_215[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_68 (Dense)               (None, 1000)         11000       ['input_36[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 1300)         0           ['add_51[0][0]',                 \n",
            "                                                                  'add_52[0][0]',                 \n",
            "                                                                  'add_53[0][0]',                 \n",
            "                                                                  'dense_68[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_17 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_17[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_17 (G  (1, 1300)           0           ['tf.expand_dims_17[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_17 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_17[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_69 (Dense)               (1, 1, 1, 650)       845650      ['reshape_17[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_17 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_69[0][0]']               \n",
            "                                                                                                  \n",
            " dense_70 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_17[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_17 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_70[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_17 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_17[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_17[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_17 (TFOpL  (None, 1300)        0           ['multiply_17[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_17[0][0]']\n",
            "                                                                                                  \n",
            " dropout_85 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_85[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_86 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_86[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_87 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_87[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_88 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_88[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_89 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_71 (Dense)               (None, 1)            3           ['dropout_89[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 2.9450 - mae: 1.1016\n",
            "Epoch 1: val_loss improved from inf to 0.65883, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 45ms/step - loss: 2.9450 - mae: 1.1016 - val_loss: 0.6588 - val_mae: 0.3731\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8177 - mae: 0.5512\n",
            "Epoch 2: val_loss improved from 0.65883 to 0.50503, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.8180 - mae: 0.5513 - val_loss: 0.5050 - val_mae: 0.4348\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7504 - mae: 0.5175\n",
            "Epoch 3: val_loss improved from 0.50503 to 0.49338, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7490 - mae: 0.5170 - val_loss: 0.4934 - val_mae: 0.3373\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7050 - mae: 0.5200\n",
            "Epoch 4: val_loss did not improve from 0.49338\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.7047 - mae: 0.5203 - val_loss: 0.5092 - val_mae: 0.5581\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6983 - mae: 0.5124\n",
            "Epoch 5: val_loss improved from 0.49338 to 0.40451, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6986 - mae: 0.5125 - val_loss: 0.4045 - val_mae: 0.4092\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6273 - mae: 0.4741\n",
            "Epoch 6: val_loss improved from 0.40451 to 0.38967, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6274 - mae: 0.4739 - val_loss: 0.3897 - val_mae: 0.3469\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5946 - mae: 0.4598\n",
            "Epoch 7: val_loss did not improve from 0.38967\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5960 - mae: 0.4607 - val_loss: 0.3906 - val_mae: 0.3042\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6209 - mae: 0.4616\n",
            "Epoch 8: val_loss improved from 0.38967 to 0.37495, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6205 - mae: 0.4615 - val_loss: 0.3749 - val_mae: 0.3162\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6035 - mae: 0.4555\n",
            "Epoch 9: val_loss improved from 0.37495 to 0.36719, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6028 - mae: 0.4554 - val_loss: 0.3672 - val_mae: 0.3407\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6215 - mae: 0.4712\n",
            "Epoch 10: val_loss did not improve from 0.36719\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6248 - mae: 0.4722 - val_loss: 0.3877 - val_mae: 0.3381\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5693 - mae: 0.4283\n",
            "Epoch 11: val_loss did not improve from 0.36719\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5692 - mae: 0.4287 - val_loss: 0.3927 - val_mae: 0.3885\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6020 - mae: 0.4658\n",
            "Epoch 12: val_loss improved from 0.36719 to 0.32264, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.6016 - mae: 0.4658 - val_loss: 0.3226 - val_mae: 0.2686\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5333 - mae: 0.4216\n",
            "Epoch 13: val_loss did not improve from 0.32264\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.5336 - mae: 0.4217 - val_loss: 0.4326 - val_mae: 0.4476\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5225 - mae: 0.4217\n",
            "Epoch 14: val_loss did not improve from 0.32264\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5231 - mae: 0.4216 - val_loss: 0.3369 - val_mae: 0.3869\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5250 - mae: 0.4231\n",
            "Epoch 15: val_loss improved from 0.32264 to 0.28923, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5254 - mae: 0.4236 - val_loss: 0.2892 - val_mae: 0.3069\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5109 - mae: 0.4125\n",
            "Epoch 16: val_loss improved from 0.28923 to 0.25737, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5107 - mae: 0.4124 - val_loss: 0.2574 - val_mae: 0.2583\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4761 - mae: 0.3918\n",
            "Epoch 17: val_loss improved from 0.25737 to 0.23035, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4750 - mae: 0.3913 - val_loss: 0.2304 - val_mae: 0.2509\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4667 - mae: 0.4057\n",
            "Epoch 18: val_loss did not improve from 0.23035\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4658 - mae: 0.4052 - val_loss: 0.4299 - val_mae: 0.4830\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4466 - mae: 0.3773\n",
            "Epoch 19: val_loss did not improve from 0.23035\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4460 - mae: 0.3767 - val_loss: 0.3588 - val_mae: 0.4341\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4235 - mae: 0.3741\n",
            "Epoch 20: val_loss improved from 0.23035 to 0.18301, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.4239 - mae: 0.3743 - val_loss: 0.1830 - val_mae: 0.2515\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4012 - mae: 0.3665\n",
            "Epoch 21: val_loss did not improve from 0.18301\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4010 - mae: 0.3666 - val_loss: 0.2047 - val_mae: 0.3365\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3861 - mae: 0.3581\n",
            "Epoch 22: val_loss improved from 0.18301 to 0.14033, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3858 - mae: 0.3580 - val_loss: 0.1403 - val_mae: 0.1906\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3677 - mae: 0.3426\n",
            "Epoch 23: val_loss improved from 0.14033 to 0.13697, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3677 - mae: 0.3426 - val_loss: 0.1370 - val_mae: 0.2083\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3638 - mae: 0.3368\n",
            "Epoch 24: val_loss improved from 0.13697 to 0.11630, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3631 - mae: 0.3368 - val_loss: 0.1163 - val_mae: 0.1717\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3346 - mae: 0.3236\n",
            "Epoch 25: val_loss improved from 0.11630 to 0.10043, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3345 - mae: 0.3234 - val_loss: 0.1004 - val_mae: 0.1598\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3532 - mae: 0.3260\n",
            "Epoch 26: val_loss did not improve from 0.10043\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3527 - mae: 0.3260 - val_loss: 0.1388 - val_mae: 0.2684\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3473 - mae: 0.3380\n",
            "Epoch 27: val_loss did not improve from 0.10043\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3463 - mae: 0.3374 - val_loss: 0.2743 - val_mae: 0.3715\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3426 - mae: 0.3577\n",
            "Epoch 28: val_loss did not improve from 0.10043\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3420 - mae: 0.3574 - val_loss: 0.1082 - val_mae: 0.2247\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2986 - mae: 0.2846\n",
            "Epoch 29: val_loss did not improve from 0.10043\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2988 - mae: 0.2850 - val_loss: 0.1134 - val_mae: 0.1941\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2866 - mae: 0.2936\n",
            "Epoch 30: val_loss improved from 0.10043 to 0.09842, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2864 - mae: 0.2935 - val_loss: 0.0984 - val_mae: 0.2184\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2965 - mae: 0.3070\n",
            "Epoch 31: val_loss improved from 0.09842 to 0.07770, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2960 - mae: 0.3067 - val_loss: 0.0777 - val_mae: 0.1426\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2849 - mae: 0.2870\n",
            "Epoch 32: val_loss did not improve from 0.07770\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2845 - mae: 0.2871 - val_loss: 0.1334 - val_mae: 0.2726\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3010 - mae: 0.2902\n",
            "Epoch 33: val_loss did not improve from 0.07770\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3004 - mae: 0.2899 - val_loss: 0.1771 - val_mae: 0.3691\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2899 - mae: 0.2935\n",
            "Epoch 34: val_loss improved from 0.07770 to 0.06714, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2895 - mae: 0.2933 - val_loss: 0.0671 - val_mae: 0.1272\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2702 - mae: 0.2838\n",
            "Epoch 35: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2695 - mae: 0.2835 - val_loss: 0.1654 - val_mae: 0.3067\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2908 - mae: 0.2921\n",
            "Epoch 36: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2902 - mae: 0.2921 - val_loss: 0.0675 - val_mae: 0.1524\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2877 - mae: 0.2907\n",
            "Epoch 37: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2875 - mae: 0.2905 - val_loss: 0.0749 - val_mae: 0.1516\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2616 - mae: 0.2794\n",
            "Epoch 38: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2616 - mae: 0.2794 - val_loss: 0.1930 - val_mae: 0.3452\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2557 - mae: 0.2670\n",
            "Epoch 39: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2552 - mae: 0.2667 - val_loss: 0.0809 - val_mae: 0.2146\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3107 - mae: 0.3280\n",
            "Epoch 40: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3114 - mae: 0.3281 - val_loss: 0.0996 - val_mae: 0.2595\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2477 - mae: 0.2541\n",
            "Epoch 41: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2479 - mae: 0.2542 - val_loss: 0.1222 - val_mae: 0.2751\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2528 - mae: 0.2682\n",
            "Epoch 42: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2523 - mae: 0.2680 - val_loss: 0.0880 - val_mae: 0.2190\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2592 - mae: 0.2601\n",
            "Epoch 43: val_loss did not improve from 0.06714\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2593 - mae: 0.2604 - val_loss: 0.0863 - val_mae: 0.1972\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2710 - mae: 0.2720\n",
            "Epoch 44: val_loss improved from 0.06714 to 0.06400, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2707 - mae: 0.2719 - val_loss: 0.0640 - val_mae: 0.1127\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2639 - mae: 0.2922\n",
            "Epoch 45: val_loss did not improve from 0.06400\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2654 - mae: 0.2927 - val_loss: 0.0678 - val_mae: 0.1748\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2377 - mae: 0.2548\n",
            "Epoch 46: val_loss did not improve from 0.06400\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2383 - mae: 0.2551 - val_loss: 0.1171 - val_mae: 0.2508\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2398 - mae: 0.2546\n",
            "Epoch 47: val_loss improved from 0.06400 to 0.06322, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2404 - mae: 0.2547 - val_loss: 0.0632 - val_mae: 0.1298\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3338 - mae: 0.3252\n",
            "Epoch 48: val_loss did not improve from 0.06322\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3354 - mae: 0.3263 - val_loss: 0.4739 - val_mae: 0.4093\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4282 - mae: 0.3656\n",
            "Epoch 49: val_loss did not improve from 0.06322\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4285 - mae: 0.3655 - val_loss: 0.1511 - val_mae: 0.3042\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2559 - mae: 0.2832\n",
            "Epoch 50: val_loss did not improve from 0.06322\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2561 - mae: 0.2836 - val_loss: 0.0788 - val_mae: 0.1391\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2786 - mae: 0.2748\n",
            "Epoch 51: val_loss did not improve from 0.06322\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2789 - mae: 0.2750 - val_loss: 0.1180 - val_mae: 0.2696\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2509 - mae: 0.2837\n",
            "Epoch 52: val_loss improved from 0.06322 to 0.05879, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2510 - mae: 0.2839 - val_loss: 0.0588 - val_mae: 0.1077\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2573 - mae: 0.2650\n",
            "Epoch 53: val_loss did not improve from 0.05879\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2577 - mae: 0.2650 - val_loss: 0.0633 - val_mae: 0.1385\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2477 - mae: 0.2677\n",
            "Epoch 54: val_loss did not improve from 0.05879\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2471 - mae: 0.2673 - val_loss: 0.0618 - val_mae: 0.1467\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2387 - mae: 0.2643\n",
            "Epoch 55: val_loss did not improve from 0.05879\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2390 - mae: 0.2645 - val_loss: 0.1125 - val_mae: 0.2459\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2415 - mae: 0.2616\n",
            "Epoch 56: val_loss did not improve from 0.05879\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2407 - mae: 0.2610 - val_loss: 0.0623 - val_mae: 0.1287\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2578 - mae: 0.2518\n",
            "Epoch 57: val_loss did not improve from 0.05879\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2577 - mae: 0.2517 - val_loss: 0.0831 - val_mae: 0.1912\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2280 - mae: 0.2498\n",
            "Epoch 58: val_loss improved from 0.05879 to 0.05054, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2284 - mae: 0.2502 - val_loss: 0.0505 - val_mae: 0.0964\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2417 - mae: 0.2618\n",
            "Epoch 59: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2413 - mae: 0.2618 - val_loss: 0.0604 - val_mae: 0.0982\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2317 - mae: 0.2495\n",
            "Epoch 60: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2315 - mae: 0.2498 - val_loss: 0.0538 - val_mae: 0.0996\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2378 - mae: 0.2522\n",
            "Epoch 61: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2385 - mae: 0.2524 - val_loss: 0.0541 - val_mae: 0.0970\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2190 - mae: 0.2438\n",
            "Epoch 62: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2189 - mae: 0.2437 - val_loss: 0.0943 - val_mae: 0.1692\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2477 - mae: 0.2653\n",
            "Epoch 63: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2470 - mae: 0.2649 - val_loss: 0.1276 - val_mae: 0.2902\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2337 - mae: 0.2549\n",
            "Epoch 64: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2337 - mae: 0.2547 - val_loss: 0.1071 - val_mae: 0.2565\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2197 - mae: 0.2458\n",
            "Epoch 65: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2192 - mae: 0.2457 - val_loss: 0.1060 - val_mae: 0.2445\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2386 - mae: 0.2359\n",
            "Epoch 66: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2381 - mae: 0.2359 - val_loss: 0.0550 - val_mae: 0.1007\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2233 - mae: 0.2640\n",
            "Epoch 67: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2234 - mae: 0.2642 - val_loss: 0.0849 - val_mae: 0.2252\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2224 - mae: 0.2417\n",
            "Epoch 68: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2218 - mae: 0.2415 - val_loss: 0.0917 - val_mae: 0.2340\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2669 - mae: 0.2882\n",
            "Epoch 69: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2666 - mae: 0.2881 - val_loss: 0.0690 - val_mae: 0.1698\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2165 - mae: 0.2320\n",
            "Epoch 70: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2161 - mae: 0.2318 - val_loss: 0.0581 - val_mae: 0.1318\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2273 - mae: 0.2323\n",
            "Epoch 71: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2267 - mae: 0.2321 - val_loss: 0.0780 - val_mae: 0.1968\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2391 - mae: 0.2558\n",
            "Epoch 72: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2390 - mae: 0.2557 - val_loss: 0.0531 - val_mae: 0.1084\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1997 - mae: 0.2316\n",
            "Epoch 73: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1997 - mae: 0.2316 - val_loss: 0.0516 - val_mae: 0.0968\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2323 - mae: 0.2625\n",
            "Epoch 74: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2317 - mae: 0.2621 - val_loss: 0.0822 - val_mae: 0.2165\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2511 - mae: 0.2671\n",
            "Epoch 75: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2509 - mae: 0.2670 - val_loss: 0.0582 - val_mae: 0.0976\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.2359\n",
            "Epoch 76: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2033 - mae: 0.2356 - val_loss: 0.0588 - val_mae: 0.1380\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2115 - mae: 0.2263\n",
            "Epoch 77: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2124 - mae: 0.2268 - val_loss: 0.0774 - val_mae: 0.2060\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2101 - mae: 0.2430\n",
            "Epoch 78: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2116 - mae: 0.2432 - val_loss: 0.0621 - val_mae: 0.1089\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2043 - mae: 0.2416\n",
            "Epoch 79: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2046 - mae: 0.2422 - val_loss: 0.1174 - val_mae: 0.2959\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1930 - mae: 0.2298\n",
            "Epoch 80: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1941 - mae: 0.2301 - val_loss: 0.0724 - val_mae: 0.1789\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1895 - mae: 0.2207\n",
            "Epoch 81: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1905 - mae: 0.2209 - val_loss: 0.0528 - val_mae: 0.1093\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2863 - mae: 0.3122\n",
            "Epoch 82: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2855 - mae: 0.3116 - val_loss: 0.0917 - val_mae: 0.2371\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2118 - mae: 0.2315\n",
            "Epoch 83: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2118 - mae: 0.2316 - val_loss: 0.0513 - val_mae: 0.0934\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1926 - mae: 0.2348\n",
            "Epoch 84: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1922 - mae: 0.2348 - val_loss: 0.0584 - val_mae: 0.1161\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2085 - mae: 0.2459\n",
            "Epoch 85: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2083 - mae: 0.2458 - val_loss: 0.0759 - val_mae: 0.1638\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2030 - mae: 0.2305\n",
            "Epoch 86: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2027 - mae: 0.2305 - val_loss: 0.0693 - val_mae: 0.1742\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2076 - mae: 0.2310\n",
            "Epoch 87: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2072 - mae: 0.2307 - val_loss: 0.1019 - val_mae: 0.2499\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1946 - mae: 0.2270\n",
            "Epoch 88: val_loss did not improve from 0.05054\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1941 - mae: 0.2268 - val_loss: 0.2177 - val_mae: 0.4015\n",
            "Epoch 88: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07688011974096298, RMSE:0.27727264165878296, MAE:0.12255561351776123, R2:0.9159260300487481\n",
            "4 0.9159260300487481\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0775270015001297, RMSE:0.27843672037124634, MAE:0.11736620217561722, R2:0.9136934311893576\n",
            "r2: 0.9136934311893576 rmse: 0.27843672\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0775270015001297, RMSE:0.27843672037124634, MAE:0.11736620217561722, R2:0.9136934311893576\n",
            "r2: 0.9136934311893576 rmse: 0.27843672\n",
            "第5次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_18/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_18'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_103\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_37 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_72 (Embedding)       (None, 17, 900)      649800      ['input_37[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_73 (Embedding)       (None, 17, 900)      649800      ['input_37[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_74 (Embedding)       (None, 17, 900)      649800      ['input_37[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_75 (Embedding)       (None, 17, 900)      649800      ['input_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 17, 100)      270100      ['embedding_72[0][0]',           \n",
            "                                                                  'embedding_73[0][0]',           \n",
            "                                                                  'embedding_74[0][0]',           \n",
            "                                                                  'embedding_75[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_55 (Conv1D)             (None, 17, 100)      270100      ['embedding_72[0][0]',           \n",
            "                                                                  'embedding_73[0][0]',           \n",
            "                                                                  'embedding_74[0][0]',           \n",
            "                                                                  'embedding_75[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_56 (Conv1D)             (None, 17, 100)      270100      ['embedding_72[0][0]',           \n",
            "                                                                  'embedding_73[0][0]',           \n",
            "                                                                  'embedding_74[0][0]',           \n",
            "                                                                  'embedding_75[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_216 (Glob  (None, 100)         0           ['conv1d_54[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_217 (Glob  (None, 100)         0           ['conv1d_54[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_218 (Glob  (None, 100)         0           ['conv1d_54[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_219 (Glob  (None, 100)         0           ['conv1d_54[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_220 (Glob  (None, 100)         0           ['conv1d_55[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_221 (Glob  (None, 100)         0           ['conv1d_55[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_222 (Glob  (None, 100)         0           ['conv1d_55[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_223 (Glob  (None, 100)         0           ['conv1d_55[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_224 (Glob  (None, 100)         0           ['conv1d_56[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_225 (Glob  (None, 100)         0           ['conv1d_56[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_226 (Glob  (None, 100)         0           ['conv1d_56[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_227 (Glob  (None, 100)         0           ['conv1d_56[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_38 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 100)          0           ['global_max_pooling1d_216[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_217[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_218[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_219[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 100)          0           ['global_max_pooling1d_220[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_221[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_222[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_223[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_56 (Add)                   (None, 100)          0           ['global_max_pooling1d_224[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_225[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_226[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_227[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_72 (Dense)               (None, 1000)         11000       ['input_38[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 1300)         0           ['add_54[0][0]',                 \n",
            "                                                                  'add_55[0][0]',                 \n",
            "                                                                  'add_56[0][0]',                 \n",
            "                                                                  'dense_72[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_18 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_18[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_18 (G  (1, 1300)           0           ['tf.expand_dims_18[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_18 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_73 (Dense)               (1, 1, 1, 650)       845650      ['reshape_18[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_18 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_73[0][0]']               \n",
            "                                                                                                  \n",
            " dense_74 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_18[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_18 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_74[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_18 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_18[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_18[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_18 (TFOpL  (None, 1300)        0           ['multiply_18[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_18[0][0]']\n",
            "                                                                                                  \n",
            " dropout_90 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_90[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_91[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_92[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_75 (Dense)               (None, 1)            3           ['dropout_94[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 1.8644 - mae: 0.7916\n",
            "Epoch 1: val_loss improved from inf to 0.47261, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 45ms/step - loss: 1.8591 - mae: 0.7900 - val_loss: 0.4726 - val_mae: 0.3332\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.9085 - mae: 0.5376\n",
            "Epoch 2: val_loss improved from 0.47261 to 0.45870, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.9072 - mae: 0.5372 - val_loss: 0.4587 - val_mae: 0.4272\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8451 - mae: 0.5182\n",
            "Epoch 3: val_loss improved from 0.45870 to 0.40796, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.8435 - mae: 0.5177 - val_loss: 0.4080 - val_mae: 0.3649\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8329 - mae: 0.4843\n",
            "Epoch 4: val_loss improved from 0.40796 to 0.40162, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.8363 - mae: 0.4850 - val_loss: 0.4016 - val_mae: 0.3523\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7606 - mae: 0.4832\n",
            "Epoch 5: val_loss did not improve from 0.40162\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.7598 - mae: 0.4835 - val_loss: 0.4063 - val_mae: 0.3512\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7195 - mae: 0.4837\n",
            "Epoch 6: val_loss did not improve from 0.40162\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.7175 - mae: 0.4829 - val_loss: 0.4528 - val_mae: 0.4680\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7581 - mae: 0.4828\n",
            "Epoch 7: val_loss improved from 0.40162 to 0.35975, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7571 - mae: 0.4824 - val_loss: 0.3597 - val_mae: 0.2842\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6721 - mae: 0.4542\n",
            "Epoch 8: val_loss improved from 0.35975 to 0.35793, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6711 - mae: 0.4542 - val_loss: 0.3579 - val_mae: 0.3375\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6872 - mae: 0.4730\n",
            "Epoch 9: val_loss improved from 0.35793 to 0.33787, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6865 - mae: 0.4730 - val_loss: 0.3379 - val_mae: 0.3092\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5737 - mae: 0.4217\n",
            "Epoch 10: val_loss did not improve from 0.33787\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5751 - mae: 0.4222 - val_loss: 0.4013 - val_mae: 0.5095\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6224 - mae: 0.4466\n",
            "Epoch 11: val_loss improved from 0.33787 to 0.30563, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6271 - mae: 0.4469 - val_loss: 0.3056 - val_mae: 0.3226\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5537 - mae: 0.4371\n",
            "Epoch 12: val_loss improved from 0.30563 to 0.29842, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5551 - mae: 0.4375 - val_loss: 0.2984 - val_mae: 0.3119\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5289 - mae: 0.4086\n",
            "Epoch 13: val_loss improved from 0.29842 to 0.24851, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5293 - mae: 0.4095 - val_loss: 0.2485 - val_mae: 0.3226\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5050 - mae: 0.4312\n",
            "Epoch 14: val_loss did not improve from 0.24851\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5040 - mae: 0.4310 - val_loss: 0.2546 - val_mae: 0.2787\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4814 - mae: 0.4007\n",
            "Epoch 15: val_loss did not improve from 0.24851\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4815 - mae: 0.4006 - val_loss: 0.2503 - val_mae: 0.3370\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3955 - mae: 0.3602\n",
            "Epoch 16: val_loss improved from 0.24851 to 0.11991, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.3956 - mae: 0.3607 - val_loss: 0.1199 - val_mae: 0.1529\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4250 - mae: 0.3782\n",
            "Epoch 17: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4254 - mae: 0.3782 - val_loss: 0.1602 - val_mae: 0.2478\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3538 - mae: 0.3299\n",
            "Epoch 18: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3542 - mae: 0.3302 - val_loss: 0.4204 - val_mae: 0.5383\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4233 - mae: 0.3934\n",
            "Epoch 19: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.4239 - mae: 0.3930 - val_loss: 0.1378 - val_mae: 0.2322\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3373 - mae: 0.3145\n",
            "Epoch 20: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3367 - mae: 0.3144 - val_loss: 0.1256 - val_mae: 0.2537\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3171 - mae: 0.3106\n",
            "Epoch 21: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3186 - mae: 0.3124 - val_loss: 0.2135 - val_mae: 0.3716\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3311 - mae: 0.3240\n",
            "Epoch 22: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3301 - mae: 0.3234 - val_loss: 0.1860 - val_mae: 0.3717\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3067 - mae: 0.3035\n",
            "Epoch 23: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3068 - mae: 0.3037 - val_loss: 0.1865 - val_mae: 0.3319\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3160 - mae: 0.3087\n",
            "Epoch 24: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3164 - mae: 0.3089 - val_loss: 0.1779 - val_mae: 0.3610\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6891 - mae: 0.4980\n",
            "Epoch 25: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6880 - mae: 0.4979 - val_loss: 0.4829 - val_mae: 0.4976\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5880 - mae: 0.4638\n",
            "Epoch 26: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5876 - mae: 0.4638 - val_loss: 0.3407 - val_mae: 0.3185\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5293 - mae: 0.4319\n",
            "Epoch 27: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5289 - mae: 0.4321 - val_loss: 0.2114 - val_mae: 0.2599\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4069 - mae: 0.3832\n",
            "Epoch 28: val_loss did not improve from 0.11991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4061 - mae: 0.3828 - val_loss: 0.1312 - val_mae: 0.1980\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3621 - mae: 0.3461\n",
            "Epoch 29: val_loss improved from 0.11991 to 0.09661, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.3614 - mae: 0.3461 - val_loss: 0.0966 - val_mae: 0.2064\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3078 - mae: 0.3136\n",
            "Epoch 30: val_loss improved from 0.09661 to 0.06919, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3087 - mae: 0.3140 - val_loss: 0.0692 - val_mae: 0.1312\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2970 - mae: 0.3054\n",
            "Epoch 31: val_loss did not improve from 0.06919\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2976 - mae: 0.3058 - val_loss: 0.1121 - val_mae: 0.2835\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2808 - mae: 0.2976\n",
            "Epoch 32: val_loss improved from 0.06919 to 0.05565, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.2810 - mae: 0.2979 - val_loss: 0.0557 - val_mae: 0.1042\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2713 - mae: 0.2952\n",
            "Epoch 33: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2717 - mae: 0.2952 - val_loss: 0.0582 - val_mae: 0.1039\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2742 - mae: 0.2901\n",
            "Epoch 34: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2736 - mae: 0.2901 - val_loss: 0.0609 - val_mae: 0.1305\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2838 - mae: 0.2954\n",
            "Epoch 35: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2840 - mae: 0.2954 - val_loss: 0.0718 - val_mae: 0.1490\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2909 - mae: 0.3068\n",
            "Epoch 36: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2901 - mae: 0.3063 - val_loss: 0.0660 - val_mae: 0.1507\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2602 - mae: 0.2748\n",
            "Epoch 37: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2613 - mae: 0.2755 - val_loss: 0.0757 - val_mae: 0.1897\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2758 - mae: 0.2875\n",
            "Epoch 38: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2761 - mae: 0.2877 - val_loss: 0.0654 - val_mae: 0.1172\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2406 - mae: 0.2653\n",
            "Epoch 39: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2401 - mae: 0.2650 - val_loss: 0.0946 - val_mae: 0.2258\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3666 - mae: 0.3497\n",
            "Epoch 40: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3659 - mae: 0.3495 - val_loss: 0.1176 - val_mae: 0.2699\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2684 - mae: 0.2870\n",
            "Epoch 41: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2682 - mae: 0.2870 - val_loss: 0.0597 - val_mae: 0.1339\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2705 - mae: 0.2783\n",
            "Epoch 42: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2696 - mae: 0.2778 - val_loss: 0.0585 - val_mae: 0.1013\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2745 - mae: 0.2792\n",
            "Epoch 43: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2744 - mae: 0.2791 - val_loss: 0.1011 - val_mae: 0.2524\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2393 - mae: 0.2610\n",
            "Epoch 44: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2393 - mae: 0.2609 - val_loss: 0.0624 - val_mae: 0.1523\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2532 - mae: 0.2759\n",
            "Epoch 45: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2531 - mae: 0.2760 - val_loss: 0.0676 - val_mae: 0.1101\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2609 - mae: 0.2702\n",
            "Epoch 46: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2615 - mae: 0.2704 - val_loss: 0.1086 - val_mae: 0.2754\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2755 - mae: 0.2867\n",
            "Epoch 47: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2757 - mae: 0.2866 - val_loss: 0.1619 - val_mae: 0.3310\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2640 - mae: 0.2593\n",
            "Epoch 48: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2644 - mae: 0.2597 - val_loss: 0.0865 - val_mae: 0.2228\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4622 - mae: 0.4169\n",
            "Epoch 49: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4619 - mae: 0.4165 - val_loss: 0.1154 - val_mae: 0.2752\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2477 - mae: 0.2552\n",
            "Epoch 50: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2472 - mae: 0.2548 - val_loss: 0.0875 - val_mae: 0.2298\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2461 - mae: 0.2559\n",
            "Epoch 51: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2455 - mae: 0.2557 - val_loss: 0.0694 - val_mae: 0.1777\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2474 - mae: 0.2572\n",
            "Epoch 52: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2496 - mae: 0.2579 - val_loss: 0.0598 - val_mae: 0.1469\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2421 - mae: 0.2589\n",
            "Epoch 53: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2415 - mae: 0.2587 - val_loss: 0.0694 - val_mae: 0.1697\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3708 - mae: 0.3602\n",
            "Epoch 54: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3716 - mae: 0.3605 - val_loss: 0.3206 - val_mae: 0.4639\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2968 - mae: 0.3031\n",
            "Epoch 55: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2959 - mae: 0.3025 - val_loss: 0.0770 - val_mae: 0.2008\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2424 - mae: 0.2710\n",
            "Epoch 56: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2417 - mae: 0.2706 - val_loss: 0.1183 - val_mae: 0.2184\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2578 - mae: 0.2658\n",
            "Epoch 57: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2571 - mae: 0.2655 - val_loss: 0.0846 - val_mae: 0.2065\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2320 - mae: 0.2477\n",
            "Epoch 58: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2316 - mae: 0.2476 - val_loss: 0.1046 - val_mae: 0.2565\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2541 - mae: 0.2848\n",
            "Epoch 59: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2554 - mae: 0.2851 - val_loss: 0.0756 - val_mae: 0.1662\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2345 - mae: 0.2541\n",
            "Epoch 60: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2347 - mae: 0.2542 - val_loss: 0.0626 - val_mae: 0.1166\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2323 - mae: 0.2532\n",
            "Epoch 61: val_loss did not improve from 0.05565\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2328 - mae: 0.2535 - val_loss: 0.0729 - val_mae: 0.1577\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2265 - mae: 0.2529\n",
            "Epoch 62: val_loss improved from 0.05565 to 0.05378, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2259 - mae: 0.2526 - val_loss: 0.0538 - val_mae: 0.1089\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2198 - mae: 0.2550\n",
            "Epoch 63: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2197 - mae: 0.2551 - val_loss: 0.0655 - val_mae: 0.1417\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2400 - mae: 0.2609\n",
            "Epoch 64: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2400 - mae: 0.2611 - val_loss: 0.1997 - val_mae: 0.4032\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2352 - mae: 0.2742\n",
            "Epoch 65: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2345 - mae: 0.2739 - val_loss: 0.0928 - val_mae: 0.2233\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2287 - mae: 0.2435\n",
            "Epoch 66: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2286 - mae: 0.2435 - val_loss: 0.0798 - val_mae: 0.1951\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2260 - mae: 0.2433\n",
            "Epoch 67: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2261 - mae: 0.2437 - val_loss: 0.0540 - val_mae: 0.1003\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2298 - mae: 0.2575\n",
            "Epoch 68: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2298 - mae: 0.2575 - val_loss: 0.0758 - val_mae: 0.1847\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2567 - mae: 0.3015\n",
            "Epoch 69: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2565 - mae: 0.3013 - val_loss: 0.0840 - val_mae: 0.2285\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2066 - mae: 0.2387\n",
            "Epoch 70: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2063 - mae: 0.2386 - val_loss: 0.0889 - val_mae: 0.2217\n",
            "Epoch 71/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3477 - mae: 0.3368\n",
            "Epoch 71: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3477 - mae: 0.3368 - val_loss: 0.0647 - val_mae: 0.1463\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2163 - mae: 0.2455\n",
            "Epoch 72: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2165 - mae: 0.2459 - val_loss: 0.0606 - val_mae: 0.1440\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2298 - mae: 0.2585\n",
            "Epoch 73: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2312 - mae: 0.2591 - val_loss: 0.0675 - val_mae: 0.1445\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2130 - mae: 0.2380\n",
            "Epoch 74: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2135 - mae: 0.2383 - val_loss: 0.0742 - val_mae: 0.1543\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2224 - mae: 0.2530\n",
            "Epoch 75: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2237 - mae: 0.2533 - val_loss: 0.0679 - val_mae: 0.1574\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2315 - mae: 0.2570\n",
            "Epoch 76: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2309 - mae: 0.2566 - val_loss: 0.1039 - val_mae: 0.2491\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2196 - mae: 0.2487\n",
            "Epoch 77: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2191 - mae: 0.2485 - val_loss: 0.0620 - val_mae: 0.1415\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2066 - mae: 0.2438\n",
            "Epoch 78: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2065 - mae: 0.2436 - val_loss: 0.1428 - val_mae: 0.2818\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2057 - mae: 0.2356\n",
            "Epoch 79: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2054 - mae: 0.2355 - val_loss: 0.0709 - val_mae: 0.1309\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2262 - mae: 0.2619\n",
            "Epoch 80: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2258 - mae: 0.2616 - val_loss: 0.0614 - val_mae: 0.1108\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2134 - mae: 0.2511\n",
            "Epoch 81: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2132 - mae: 0.2511 - val_loss: 0.1522 - val_mae: 0.3335\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5270 - mae: 0.4241\n",
            "Epoch 82: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5266 - mae: 0.4236 - val_loss: 0.0700 - val_mae: 0.1485\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1977 - mae: 0.2249\n",
            "Epoch 83: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1974 - mae: 0.2250 - val_loss: 0.0568 - val_mae: 0.1200\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2166 - mae: 0.2460\n",
            "Epoch 84: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2165 - mae: 0.2460 - val_loss: 0.0751 - val_mae: 0.1457\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1983 - mae: 0.2293\n",
            "Epoch 85: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1985 - mae: 0.2292 - val_loss: 0.0669 - val_mae: 0.1642\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1967 - mae: 0.2310\n",
            "Epoch 86: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1963 - mae: 0.2310 - val_loss: 0.0624 - val_mae: 0.1031\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1911 - mae: 0.2292\n",
            "Epoch 87: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1911 - mae: 0.2291 - val_loss: 0.0949 - val_mae: 0.2405\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2063 - mae: 0.2329\n",
            "Epoch 88: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2060 - mae: 0.2330 - val_loss: 0.2948 - val_mae: 0.3782\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2063 - mae: 0.2382\n",
            "Epoch 89: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2058 - mae: 0.2379 - val_loss: 0.0638 - val_mae: 0.1455\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2595 - mae: 0.2730\n",
            "Epoch 90: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2609 - mae: 0.2734 - val_loss: 0.5525 - val_mae: 0.3250\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6074 - mae: 0.4517\n",
            "Epoch 91: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6079 - mae: 0.4520 - val_loss: 0.4280 - val_mae: 0.3034\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5168 - mae: 0.4240\n",
            "Epoch 92: val_loss did not improve from 0.05378\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5163 - mae: 0.4235 - val_loss: 0.4086 - val_mae: 0.3307\n",
            "Epoch 92: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07402545213699341, RMSE:0.27207618951797485, MAE:0.1294078826904297, R2:0.9190478082128568\n",
            "5 0.9190478082128568\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07313095778226852, RMSE:0.2704273760318756, MAE:0.12480627745389938, R2:0.9185873098366197\n",
            "r2: 0.9185873098366197 rmse: 0.27042738\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07313095778226852, RMSE:0.2704273760318756, MAE:0.12480627745389938, R2:0.9185873098366197\n",
            "r2: 0.9185873098366197 rmse: 0.27042738\n",
            "第6次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_19/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_19'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_109\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_39 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_76 (Embedding)       (None, 17, 900)      649800      ['input_39[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_77 (Embedding)       (None, 17, 900)      649800      ['input_39[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_78 (Embedding)       (None, 17, 900)      649800      ['input_39[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_79 (Embedding)       (None, 17, 900)      649800      ['input_39[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_57 (Conv1D)             (None, 17, 100)      270100      ['embedding_76[0][0]',           \n",
            "                                                                  'embedding_77[0][0]',           \n",
            "                                                                  'embedding_78[0][0]',           \n",
            "                                                                  'embedding_79[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_58 (Conv1D)             (None, 17, 100)      270100      ['embedding_76[0][0]',           \n",
            "                                                                  'embedding_77[0][0]',           \n",
            "                                                                  'embedding_78[0][0]',           \n",
            "                                                                  'embedding_79[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_59 (Conv1D)             (None, 17, 100)      270100      ['embedding_76[0][0]',           \n",
            "                                                                  'embedding_77[0][0]',           \n",
            "                                                                  'embedding_78[0][0]',           \n",
            "                                                                  'embedding_79[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_228 (Glob  (None, 100)         0           ['conv1d_57[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_229 (Glob  (None, 100)         0           ['conv1d_57[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_230 (Glob  (None, 100)         0           ['conv1d_57[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_231 (Glob  (None, 100)         0           ['conv1d_57[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_232 (Glob  (None, 100)         0           ['conv1d_58[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_233 (Glob  (None, 100)         0           ['conv1d_58[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_234 (Glob  (None, 100)         0           ['conv1d_58[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_235 (Glob  (None, 100)         0           ['conv1d_58[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_236 (Glob  (None, 100)         0           ['conv1d_59[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_237 (Glob  (None, 100)         0           ['conv1d_59[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_238 (Glob  (None, 100)         0           ['conv1d_59[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_239 (Glob  (None, 100)         0           ['conv1d_59[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_40 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_57 (Add)                   (None, 100)          0           ['global_max_pooling1d_228[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_229[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_230[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_231[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_58 (Add)                   (None, 100)          0           ['global_max_pooling1d_232[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_233[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_234[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_235[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_59 (Add)                   (None, 100)          0           ['global_max_pooling1d_236[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_237[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_238[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_239[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_76 (Dense)               (None, 1000)         11000       ['input_40[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 1300)         0           ['add_57[0][0]',                 \n",
            "                                                                  'add_58[0][0]',                 \n",
            "                                                                  'add_59[0][0]',                 \n",
            "                                                                  'dense_76[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_19 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_19[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_19 (G  (1, 1300)           0           ['tf.expand_dims_19[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_19 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_77 (Dense)               (1, 1, 1, 650)       845650      ['reshape_19[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_19 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_77[0][0]']               \n",
            "                                                                                                  \n",
            " dense_78 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_19[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_19 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_78[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_19 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_19[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_19[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_19 (TFOpL  (None, 1300)        0           ['multiply_19[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_19[0][0]']\n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_95[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_96 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_96[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_97 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_97[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_98 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_98[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_99 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_79 (Dense)               (None, 1)            3           ['dropout_99[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 1.9742 - mae: 0.8586\n",
            "Epoch 1: val_loss improved from inf to 0.61708, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 45ms/step - loss: 1.9691 - mae: 0.8572 - val_loss: 0.6171 - val_mae: 0.4203\n",
            "Epoch 2/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.7713 - mae: 0.5450\n",
            "Epoch 2: val_loss improved from 0.61708 to 0.41133, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7713 - mae: 0.5450 - val_loss: 0.4113 - val_mae: 0.3622\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6593 - mae: 0.5061\n",
            "Epoch 3: val_loss improved from 0.41133 to 0.40421, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 43ms/step - loss: 0.6612 - mae: 0.5078 - val_loss: 0.4042 - val_mae: 0.3484\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6511 - mae: 0.5004\n",
            "Epoch 4: val_loss did not improve from 0.40421\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6525 - mae: 0.5009 - val_loss: 0.4092 - val_mae: 0.4015\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6377 - mae: 0.4857\n",
            "Epoch 5: val_loss improved from 0.40421 to 0.38356, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6373 - mae: 0.4857 - val_loss: 0.3836 - val_mae: 0.4092\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6081 - mae: 0.4799\n",
            "Epoch 6: val_loss improved from 0.38356 to 0.37810, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6064 - mae: 0.4790 - val_loss: 0.3781 - val_mae: 0.3414\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6451 - mae: 0.4912\n",
            "Epoch 7: val_loss did not improve from 0.37810\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6459 - mae: 0.4913 - val_loss: 0.5184 - val_mae: 0.5267\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5862 - mae: 0.4562\n",
            "Epoch 8: val_loss improved from 0.37810 to 0.34203, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.5871 - mae: 0.4567 - val_loss: 0.3420 - val_mae: 0.2885\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5779 - mae: 0.4446\n",
            "Epoch 9: val_loss did not improve from 0.34203\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5777 - mae: 0.4446 - val_loss: 0.3789 - val_mae: 0.3570\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5750 - mae: 0.4518\n",
            "Epoch 10: val_loss improved from 0.34203 to 0.30852, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5752 - mae: 0.4520 - val_loss: 0.3085 - val_mae: 0.2789\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5340 - mae: 0.4260\n",
            "Epoch 11: val_loss did not improve from 0.30852\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5343 - mae: 0.4262 - val_loss: 0.3356 - val_mae: 0.3629\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5593 - mae: 0.4511\n",
            "Epoch 12: val_loss improved from 0.30852 to 0.29661, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5583 - mae: 0.4503 - val_loss: 0.2966 - val_mae: 0.3033\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5071 - mae: 0.4244\n",
            "Epoch 13: val_loss improved from 0.29661 to 0.26339, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5063 - mae: 0.4239 - val_loss: 0.2634 - val_mae: 0.2661\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4718 - mae: 0.3924\n",
            "Epoch 14: val_loss improved from 0.26339 to 0.23681, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4720 - mae: 0.3922 - val_loss: 0.2368 - val_mae: 0.2264\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4557 - mae: 0.3828\n",
            "Epoch 15: val_loss improved from 0.23681 to 0.20679, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4556 - mae: 0.3828 - val_loss: 0.2068 - val_mae: 0.2747\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4177 - mae: 0.3680\n",
            "Epoch 16: val_loss did not improve from 0.20679\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4178 - mae: 0.3681 - val_loss: 0.2072 - val_mae: 0.3102\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3885 - mae: 0.3580\n",
            "Epoch 17: val_loss improved from 0.20679 to 0.13329, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.3885 - mae: 0.3580 - val_loss: 0.1333 - val_mae: 0.1842\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3861 - mae: 0.3573\n",
            "Epoch 18: val_loss improved from 0.13329 to 0.09951, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3858 - mae: 0.3571 - val_loss: 0.0995 - val_mae: 0.1642\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3313 - mae: 0.3314\n",
            "Epoch 19: val_loss did not improve from 0.09951\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3311 - mae: 0.3312 - val_loss: 0.1108 - val_mae: 0.1611\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3632 - mae: 0.3510\n",
            "Epoch 20: val_loss did not improve from 0.09951\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3637 - mae: 0.3512 - val_loss: 0.1066 - val_mae: 0.2084\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3200 - mae: 0.3222\n",
            "Epoch 21: val_loss improved from 0.09951 to 0.08824, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.3220 - mae: 0.3226 - val_loss: 0.0882 - val_mae: 0.1450\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2848 - mae: 0.2858\n",
            "Epoch 22: val_loss did not improve from 0.08824\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2860 - mae: 0.2863 - val_loss: 0.0926 - val_mae: 0.1852\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3342 - mae: 0.3311\n",
            "Epoch 23: val_loss did not improve from 0.08824\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3357 - mae: 0.3314 - val_loss: 0.1058 - val_mae: 0.2130\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3266 - mae: 0.3184\n",
            "Epoch 24: val_loss improved from 0.08824 to 0.08353, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3268 - mae: 0.3184 - val_loss: 0.0835 - val_mae: 0.1982\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3126 - mae: 0.3118\n",
            "Epoch 25: val_loss did not improve from 0.08353\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3124 - mae: 0.3118 - val_loss: 0.1162 - val_mae: 0.2208\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2942 - mae: 0.2992\n",
            "Epoch 26: val_loss did not improve from 0.08353\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2941 - mae: 0.2989 - val_loss: 0.1095 - val_mae: 0.2388\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2987 - mae: 0.2892\n",
            "Epoch 27: val_loss improved from 0.08353 to 0.06501, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2995 - mae: 0.2896 - val_loss: 0.0650 - val_mae: 0.1123\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3118 - mae: 0.3109\n",
            "Epoch 28: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3113 - mae: 0.3108 - val_loss: 0.1742 - val_mae: 0.3501\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2944 - mae: 0.2867\n",
            "Epoch 29: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2939 - mae: 0.2868 - val_loss: 0.0680 - val_mae: 0.1450\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2659 - mae: 0.2781\n",
            "Epoch 30: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2662 - mae: 0.2780 - val_loss: 0.1119 - val_mae: 0.2585\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2887 - mae: 0.2795\n",
            "Epoch 31: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2895 - mae: 0.2804 - val_loss: 0.0962 - val_mae: 0.2446\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2959 - mae: 0.2920\n",
            "Epoch 32: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2956 - mae: 0.2922 - val_loss: 0.1026 - val_mae: 0.1387\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2891 - mae: 0.2899\n",
            "Epoch 33: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2889 - mae: 0.2898 - val_loss: 0.0745 - val_mae: 0.1155\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3260 - mae: 0.3352\n",
            "Epoch 34: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3263 - mae: 0.3349 - val_loss: 0.0887 - val_mae: 0.1612\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2747 - mae: 0.2836\n",
            "Epoch 35: val_loss did not improve from 0.06501\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2745 - mae: 0.2841 - val_loss: 0.0763 - val_mae: 0.1643\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2527 - mae: 0.2721\n",
            "Epoch 36: val_loss improved from 0.06501 to 0.06202, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2521 - mae: 0.2719 - val_loss: 0.0620 - val_mae: 0.1260\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2360 - mae: 0.2524\n",
            "Epoch 37: val_loss did not improve from 0.06202\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2353 - mae: 0.2520 - val_loss: 0.0704 - val_mae: 0.1617\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2671 - mae: 0.2803\n",
            "Epoch 38: val_loss did not improve from 0.06202\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2679 - mae: 0.2805 - val_loss: 0.0630 - val_mae: 0.1194\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2702 - mae: 0.2712\n",
            "Epoch 39: val_loss did not improve from 0.06202\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2700 - mae: 0.2717 - val_loss: 0.1063 - val_mae: 0.2623\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2552 - mae: 0.2713\n",
            "Epoch 40: val_loss did not improve from 0.06202\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2551 - mae: 0.2713 - val_loss: 0.1143 - val_mae: 0.2703\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2630 - mae: 0.2813\n",
            "Epoch 41: val_loss improved from 0.06202 to 0.06071, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2646 - mae: 0.2818 - val_loss: 0.0607 - val_mae: 0.0980\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2644 - mae: 0.2690\n",
            "Epoch 42: val_loss improved from 0.06071 to 0.05406, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2641 - mae: 0.2691 - val_loss: 0.0541 - val_mae: 0.1120\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3726 - mae: 0.3437\n",
            "Epoch 43: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3720 - mae: 0.3434 - val_loss: 0.0583 - val_mae: 0.1288\n",
            "Epoch 44/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2469 - mae: 0.2590\n",
            "Epoch 44: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.2469 - mae: 0.2590 - val_loss: 0.1247 - val_mae: 0.2947\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2604 - mae: 0.2774\n",
            "Epoch 45: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2598 - mae: 0.2770 - val_loss: 0.1428 - val_mae: 0.3048\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2457 - mae: 0.2483\n",
            "Epoch 46: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2465 - mae: 0.2487 - val_loss: 0.0579 - val_mae: 0.0929\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2467 - mae: 0.2656\n",
            "Epoch 47: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2466 - mae: 0.2654 - val_loss: 0.1248 - val_mae: 0.2909\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2660 - mae: 0.2727\n",
            "Epoch 48: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2655 - mae: 0.2726 - val_loss: 0.1619 - val_mae: 0.3511\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2415 - mae: 0.2538\n",
            "Epoch 49: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2414 - mae: 0.2536 - val_loss: 0.0722 - val_mae: 0.1351\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2696 - mae: 0.2721\n",
            "Epoch 50: val_loss did not improve from 0.05406\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2688 - mae: 0.2718 - val_loss: 0.0945 - val_mae: 0.2396\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2213 - mae: 0.2427\n",
            "Epoch 51: val_loss improved from 0.05406 to 0.05390, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 49ms/step - loss: 0.2207 - mae: 0.2425 - val_loss: 0.0539 - val_mae: 0.1191\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2474 - mae: 0.2624\n",
            "Epoch 52: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2472 - mae: 0.2623 - val_loss: 0.0550 - val_mae: 0.0980\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2234 - mae: 0.2512\n",
            "Epoch 53: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2234 - mae: 0.2512 - val_loss: 0.0649 - val_mae: 0.1401\n",
            "Epoch 54/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3142 - mae: 0.3236\n",
            "Epoch 54: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3142 - mae: 0.3236 - val_loss: 0.0606 - val_mae: 0.1332\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2313 - mae: 0.2378\n",
            "Epoch 55: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2312 - mae: 0.2378 - val_loss: 0.0756 - val_mae: 0.1671\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2248 - mae: 0.2430\n",
            "Epoch 56: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2249 - mae: 0.2431 - val_loss: 0.0542 - val_mae: 0.1021\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2952 - mae: 0.3113\n",
            "Epoch 57: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2954 - mae: 0.3113 - val_loss: 0.1262 - val_mae: 0.2771\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2282 - mae: 0.2542\n",
            "Epoch 58: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2276 - mae: 0.2539 - val_loss: 0.0642 - val_mae: 0.1514\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2144 - mae: 0.2282\n",
            "Epoch 59: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2140 - mae: 0.2283 - val_loss: 0.0870 - val_mae: 0.1940\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2471 - mae: 0.2547\n",
            "Epoch 60: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2464 - mae: 0.2543 - val_loss: 0.0659 - val_mae: 0.1410\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2303 - mae: 0.2476\n",
            "Epoch 61: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2301 - mae: 0.2475 - val_loss: 0.1131 - val_mae: 0.2649\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3436 - mae: 0.3461\n",
            "Epoch 62: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3434 - mae: 0.3461 - val_loss: 0.0657 - val_mae: 0.1312\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2296 - mae: 0.2565\n",
            "Epoch 63: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2316 - mae: 0.2569 - val_loss: 0.1474 - val_mae: 0.3374\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2218 - mae: 0.2553\n",
            "Epoch 64: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2220 - mae: 0.2556 - val_loss: 0.0768 - val_mae: 0.1787\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2281 - mae: 0.2505\n",
            "Epoch 65: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2291 - mae: 0.2509 - val_loss: 0.0555 - val_mae: 0.0969\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2246 - mae: 0.2375\n",
            "Epoch 66: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2240 - mae: 0.2371 - val_loss: 0.0873 - val_mae: 0.2141\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2305 - mae: 0.2395\n",
            "Epoch 67: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2300 - mae: 0.2395 - val_loss: 0.0605 - val_mae: 0.1038\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2161 - mae: 0.2355\n",
            "Epoch 68: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2159 - mae: 0.2353 - val_loss: 0.0629 - val_mae: 0.1255\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2183 - mae: 0.2494\n",
            "Epoch 69: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2182 - mae: 0.2496 - val_loss: 0.0633 - val_mae: 0.1451\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2094 - mae: 0.2292\n",
            "Epoch 70: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2098 - mae: 0.2294 - val_loss: 0.0989 - val_mae: 0.2320\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2282 - mae: 0.2507\n",
            "Epoch 71: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2279 - mae: 0.2505 - val_loss: 0.0659 - val_mae: 0.1582\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4352 - mae: 0.3809\n",
            "Epoch 72: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4342 - mae: 0.3807 - val_loss: 0.1773 - val_mae: 0.2857\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2306 - mae: 0.2500\n",
            "Epoch 73: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2319 - mae: 0.2500 - val_loss: 0.0560 - val_mae: 0.1008\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2088 - mae: 0.2277\n",
            "Epoch 74: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2082 - mae: 0.2274 - val_loss: 0.0902 - val_mae: 0.2368\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2178 - mae: 0.2344\n",
            "Epoch 75: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2175 - mae: 0.2347 - val_loss: 0.0564 - val_mae: 0.1085\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2209 - mae: 0.2437\n",
            "Epoch 76: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2216 - mae: 0.2439 - val_loss: 0.1236 - val_mae: 0.2759\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2284 - mae: 0.2415\n",
            "Epoch 77: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2283 - mae: 0.2415 - val_loss: 0.0682 - val_mae: 0.1286\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2356 - mae: 0.2421\n",
            "Epoch 78: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2355 - mae: 0.2423 - val_loss: 0.0542 - val_mae: 0.0899\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2075 - mae: 0.2345\n",
            "Epoch 79: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2089 - mae: 0.2348 - val_loss: 0.0852 - val_mae: 0.2016\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2106 - mae: 0.2506\n",
            "Epoch 80: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2106 - mae: 0.2507 - val_loss: 0.0825 - val_mae: 0.2114\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2093 - mae: 0.2365\n",
            "Epoch 81: val_loss did not improve from 0.05390\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2088 - mae: 0.2363 - val_loss: 0.1487 - val_mae: 0.2961\n",
            "Epoch 81: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07307086139917374, RMSE:0.2703162133693695, MAE:0.1387256681919098, R2:0.9200917263247873\n",
            "6 0.9200917263247873\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07365051656961441, RMSE:0.2713862955570221, MAE:0.13688145577907562, R2:0.918008913017388\n",
            "r2: 0.918008913017388 rmse: 0.2713863\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07365051656961441, RMSE:0.2713862955570221, MAE:0.13688145577907562, R2:0.918008913017388\n",
            "r2: 0.918008913017388 rmse: 0.2713863\n",
            "第7次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_20/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_20'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_115\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_80 (Embedding)       (None, 17, 900)      649800      ['input_41[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_81 (Embedding)       (None, 17, 900)      649800      ['input_41[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_82 (Embedding)       (None, 17, 900)      649800      ['input_41[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_83 (Embedding)       (None, 17, 900)      649800      ['input_41[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_60 (Conv1D)             (None, 17, 100)      270100      ['embedding_80[0][0]',           \n",
            "                                                                  'embedding_81[0][0]',           \n",
            "                                                                  'embedding_82[0][0]',           \n",
            "                                                                  'embedding_83[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_61 (Conv1D)             (None, 17, 100)      270100      ['embedding_80[0][0]',           \n",
            "                                                                  'embedding_81[0][0]',           \n",
            "                                                                  'embedding_82[0][0]',           \n",
            "                                                                  'embedding_83[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_62 (Conv1D)             (None, 17, 100)      270100      ['embedding_80[0][0]',           \n",
            "                                                                  'embedding_81[0][0]',           \n",
            "                                                                  'embedding_82[0][0]',           \n",
            "                                                                  'embedding_83[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_240 (Glob  (None, 100)         0           ['conv1d_60[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_241 (Glob  (None, 100)         0           ['conv1d_60[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_242 (Glob  (None, 100)         0           ['conv1d_60[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_243 (Glob  (None, 100)         0           ['conv1d_60[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_244 (Glob  (None, 100)         0           ['conv1d_61[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_245 (Glob  (None, 100)         0           ['conv1d_61[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_246 (Glob  (None, 100)         0           ['conv1d_61[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_247 (Glob  (None, 100)         0           ['conv1d_61[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_248 (Glob  (None, 100)         0           ['conv1d_62[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_249 (Glob  (None, 100)         0           ['conv1d_62[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_250 (Glob  (None, 100)         0           ['conv1d_62[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_251 (Glob  (None, 100)         0           ['conv1d_62[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_42 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 100)          0           ['global_max_pooling1d_240[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_241[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_242[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_243[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 100)          0           ['global_max_pooling1d_244[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_245[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_246[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_247[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 100)          0           ['global_max_pooling1d_248[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_249[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_250[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_251[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_80 (Dense)               (None, 1000)         11000       ['input_42[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 1300)         0           ['add_60[0][0]',                 \n",
            "                                                                  'add_61[0][0]',                 \n",
            "                                                                  'add_62[0][0]',                 \n",
            "                                                                  'dense_80[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_20 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_20[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_20 (G  (1, 1300)           0           ['tf.expand_dims_20[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_20 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (1, 1, 1, 650)       845650      ['reshape_20[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_20 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_20[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_20 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_82[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_20 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_20[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_20[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_20 (TFOpL  (None, 1300)        0           ['multiply_20[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_20[0][0]']\n",
            "                                                                                                  \n",
            " dropout_100 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_100[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_101 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_101[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_102[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_103[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 1)            3           ['dropout_104[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.2339 - mae: 0.8672\n",
            "Epoch 1: val_loss improved from inf to 0.51505, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 45ms/step - loss: 2.2268 - mae: 0.8655 - val_loss: 0.5150 - val_mae: 0.3893\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8865 - mae: 0.5136\n",
            "Epoch 2: val_loss improved from 0.51505 to 0.47381, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.8873 - mae: 0.5136 - val_loss: 0.4738 - val_mae: 0.4551\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.9013 - mae: 0.5083\n",
            "Epoch 3: val_loss improved from 0.47381 to 0.40020, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.9024 - mae: 0.5083 - val_loss: 0.4002 - val_mae: 0.3056\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8140 - mae: 0.4960\n",
            "Epoch 4: val_loss improved from 0.40020 to 0.37171, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.8195 - mae: 0.4966 - val_loss: 0.3717 - val_mae: 0.3483\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8065 - mae: 0.4895\n",
            "Epoch 5: val_loss did not improve from 0.37171\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.8056 - mae: 0.4895 - val_loss: 0.5302 - val_mae: 0.6169\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7800 - mae: 0.4763\n",
            "Epoch 6: val_loss improved from 0.37171 to 0.36882, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7791 - mae: 0.4764 - val_loss: 0.3688 - val_mae: 0.3384\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8071 - mae: 0.4682\n",
            "Epoch 7: val_loss did not improve from 0.36882\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.8156 - mae: 0.4691 - val_loss: 0.4497 - val_mae: 0.4496\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8279 - mae: 0.4750\n",
            "Epoch 8: val_loss improved from 0.36882 to 0.32217, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.8262 - mae: 0.4748 - val_loss: 0.3222 - val_mae: 0.3129\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7612 - mae: 0.4647\n",
            "Epoch 9: val_loss improved from 0.32217 to 0.32032, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7623 - mae: 0.4646 - val_loss: 0.3203 - val_mae: 0.3774\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7030 - mae: 0.4324\n",
            "Epoch 10: val_loss improved from 0.32032 to 0.29412, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7017 - mae: 0.4318 - val_loss: 0.2941 - val_mae: 0.3257\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6275 - mae: 0.4049\n",
            "Epoch 11: val_loss did not improve from 0.29412\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6264 - mae: 0.4048 - val_loss: 0.3044 - val_mae: 0.3996\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5428 - mae: 0.3851\n",
            "Epoch 12: val_loss improved from 0.29412 to 0.22508, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5417 - mae: 0.3849 - val_loss: 0.2251 - val_mae: 0.3062\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5559 - mae: 0.3752\n",
            "Epoch 13: val_loss did not improve from 0.22508\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5553 - mae: 0.3757 - val_loss: 0.2318 - val_mae: 0.3576\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5368 - mae: 0.3940\n",
            "Epoch 14: val_loss improved from 0.22508 to 0.14165, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5382 - mae: 0.3941 - val_loss: 0.1417 - val_mae: 0.2380\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5021 - mae: 0.3660\n",
            "Epoch 15: val_loss improved from 0.14165 to 0.13051, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5035 - mae: 0.3663 - val_loss: 0.1305 - val_mae: 0.1730\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4601 - mae: 0.3327\n",
            "Epoch 16: val_loss improved from 0.13051 to 0.10991, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4587 - mae: 0.3320 - val_loss: 0.1099 - val_mae: 0.1650\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3876 - mae: 0.3167\n",
            "Epoch 17: val_loss did not improve from 0.10991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3866 - mae: 0.3163 - val_loss: 0.1677 - val_mae: 0.3222\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3858 - mae: 0.3230\n",
            "Epoch 18: val_loss did not improve from 0.10991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3866 - mae: 0.3232 - val_loss: 0.1545 - val_mae: 0.2924\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3511 - mae: 0.2944\n",
            "Epoch 19: val_loss did not improve from 0.10991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3520 - mae: 0.2947 - val_loss: 0.1276 - val_mae: 0.2690\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4248 - mae: 0.3641\n",
            "Epoch 20: val_loss did not improve from 0.10991\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.4254 - mae: 0.3643 - val_loss: 0.1494 - val_mae: 0.3112\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3511 - mae: 0.3127\n",
            "Epoch 21: val_loss did not improve from 0.10991\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3505 - mae: 0.3126 - val_loss: 0.1135 - val_mae: 0.2206\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3057 - mae: 0.3050\n",
            "Epoch 22: val_loss improved from 0.10991 to 0.07075, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.3048 - mae: 0.3047 - val_loss: 0.0707 - val_mae: 0.1416\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3255 - mae: 0.3031\n",
            "Epoch 23: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3250 - mae: 0.3030 - val_loss: 0.1254 - val_mae: 0.2135\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3084 - mae: 0.3175\n",
            "Epoch 24: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3082 - mae: 0.3174 - val_loss: 0.1548 - val_mae: 0.2995\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3008 - mae: 0.3033\n",
            "Epoch 25: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3000 - mae: 0.3029 - val_loss: 0.0793 - val_mae: 0.1859\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2832 - mae: 0.2977\n",
            "Epoch 26: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2834 - mae: 0.2978 - val_loss: 0.0821 - val_mae: 0.1834\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3443 - mae: 0.3330\n",
            "Epoch 27: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3437 - mae: 0.3327 - val_loss: 0.0769 - val_mae: 0.1360\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2701 - mae: 0.2812\n",
            "Epoch 28: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2693 - mae: 0.2808 - val_loss: 0.0784 - val_mae: 0.1858\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2939 - mae: 0.3133\n",
            "Epoch 29: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2943 - mae: 0.3140 - val_loss: 0.1650 - val_mae: 0.3580\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2690 - mae: 0.2748\n",
            "Epoch 30: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2712 - mae: 0.2755 - val_loss: 0.1342 - val_mae: 0.3200\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4273 - mae: 0.4123\n",
            "Epoch 31: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4261 - mae: 0.4115 - val_loss: 0.2120 - val_mae: 0.3546\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2676 - mae: 0.2792\n",
            "Epoch 32: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2672 - mae: 0.2790 - val_loss: 0.2261 - val_mae: 0.4071\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2597 - mae: 0.2838\n",
            "Epoch 33: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2597 - mae: 0.2836 - val_loss: 0.1773 - val_mae: 0.3420\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2802 - mae: 0.2892\n",
            "Epoch 34: val_loss did not improve from 0.07075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2798 - mae: 0.2894 - val_loss: 0.3114 - val_mae: 0.4854\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2854 - mae: 0.2921\n",
            "Epoch 35: val_loss improved from 0.07075 to 0.06450, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2847 - mae: 0.2917 - val_loss: 0.0645 - val_mae: 0.1457\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2722 - mae: 0.2702\n",
            "Epoch 36: val_loss did not improve from 0.06450\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2724 - mae: 0.2707 - val_loss: 0.0699 - val_mae: 0.1270\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2858 - mae: 0.2973\n",
            "Epoch 37: val_loss did not improve from 0.06450\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2871 - mae: 0.2979 - val_loss: 0.0990 - val_mae: 0.2364\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2803 - mae: 0.3026\n",
            "Epoch 38: val_loss improved from 0.06450 to 0.06333, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2801 - mae: 0.3024 - val_loss: 0.0633 - val_mae: 0.1165\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3359 - mae: 0.3430\n",
            "Epoch 39: val_loss did not improve from 0.06333\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3361 - mae: 0.3432 - val_loss: 0.0860 - val_mae: 0.1318\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2345 - mae: 0.2469\n",
            "Epoch 40: val_loss did not improve from 0.06333\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2339 - mae: 0.2467 - val_loss: 0.1429 - val_mae: 0.3079\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2342 - mae: 0.2526\n",
            "Epoch 41: val_loss did not improve from 0.06333\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2349 - mae: 0.2531 - val_loss: 0.1003 - val_mae: 0.1979\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2771 - mae: 0.2982\n",
            "Epoch 42: val_loss improved from 0.06333 to 0.05989, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2771 - mae: 0.2981 - val_loss: 0.0599 - val_mae: 0.1004\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2604 - mae: 0.2732\n",
            "Epoch 43: val_loss did not improve from 0.05989\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2607 - mae: 0.2731 - val_loss: 0.1232 - val_mae: 0.2357\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2926 - mae: 0.3066\n",
            "Epoch 44: val_loss did not improve from 0.05989\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2923 - mae: 0.3062 - val_loss: 0.3848 - val_mae: 0.5396\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3062 - mae: 0.3123\n",
            "Epoch 45: val_loss did not improve from 0.05989\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3054 - mae: 0.3118 - val_loss: 0.0630 - val_mae: 0.1303\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2355 - mae: 0.2487\n",
            "Epoch 46: val_loss did not improve from 0.05989\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2356 - mae: 0.2489 - val_loss: 0.0934 - val_mae: 0.2056\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2504 - mae: 0.2681\n",
            "Epoch 47: val_loss did not improve from 0.05989\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2497 - mae: 0.2677 - val_loss: 0.1432 - val_mae: 0.3207\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2411 - mae: 0.2689\n",
            "Epoch 48: val_loss improved from 0.05989 to 0.05517, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.2410 - mae: 0.2687 - val_loss: 0.0552 - val_mae: 0.1038\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2485 - mae: 0.2595\n",
            "Epoch 49: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2483 - mae: 0.2594 - val_loss: 0.0965 - val_mae: 0.2008\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2867 - mae: 0.3139\n",
            "Epoch 50: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2880 - mae: 0.3154 - val_loss: 0.6064 - val_mae: 0.6646\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3696 - mae: 0.3448\n",
            "Epoch 51: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3687 - mae: 0.3443 - val_loss: 0.0726 - val_mae: 0.1650\n",
            "Epoch 52/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2192 - mae: 0.2447\n",
            "Epoch 52: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2192 - mae: 0.2447 - val_loss: 0.0807 - val_mae: 0.2087\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2432 - mae: 0.2575\n",
            "Epoch 53: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2436 - mae: 0.2576 - val_loss: 0.0587 - val_mae: 0.1158\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2416 - mae: 0.2524\n",
            "Epoch 54: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2416 - mae: 0.2527 - val_loss: 0.0757 - val_mae: 0.1537\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2250 - mae: 0.2450\n",
            "Epoch 55: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2251 - mae: 0.2452 - val_loss: 0.0610 - val_mae: 0.1176\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3239 - mae: 0.3353\n",
            "Epoch 56: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3236 - mae: 0.3353 - val_loss: 0.1136 - val_mae: 0.2885\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2165 - mae: 0.2400\n",
            "Epoch 57: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2167 - mae: 0.2400 - val_loss: 0.0777 - val_mae: 0.1886\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2338 - mae: 0.2380\n",
            "Epoch 58: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2332 - mae: 0.2377 - val_loss: 0.0707 - val_mae: 0.1460\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3078 - mae: 0.3224\n",
            "Epoch 59: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3077 - mae: 0.3222 - val_loss: 0.1159 - val_mae: 0.1758\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2254 - mae: 0.2502\n",
            "Epoch 60: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2256 - mae: 0.2503 - val_loss: 0.0694 - val_mae: 0.1806\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2247 - mae: 0.2467\n",
            "Epoch 61: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2245 - mae: 0.2466 - val_loss: 0.0920 - val_mae: 0.2154\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2137 - mae: 0.2377\n",
            "Epoch 62: val_loss did not improve from 0.05517\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2134 - mae: 0.2374 - val_loss: 0.0805 - val_mae: 0.2102\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2505 - mae: 0.2811\n",
            "Epoch 63: val_loss improved from 0.05517 to 0.05416, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 51ms/step - loss: 0.2522 - mae: 0.2816 - val_loss: 0.0542 - val_mae: 0.0931\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2230 - mae: 0.2423\n",
            "Epoch 64: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2225 - mae: 0.2422 - val_loss: 0.1457 - val_mae: 0.3100\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2808 - mae: 0.3033\n",
            "Epoch 65: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2805 - mae: 0.3029 - val_loss: 0.0855 - val_mae: 0.2157\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2301 - mae: 0.2466\n",
            "Epoch 66: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2308 - mae: 0.2465 - val_loss: 0.0631 - val_mae: 0.1193\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2483 - mae: 0.2620\n",
            "Epoch 67: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2477 - mae: 0.2616 - val_loss: 0.0739 - val_mae: 0.1612\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2271 - mae: 0.2426\n",
            "Epoch 68: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2272 - mae: 0.2434 - val_loss: 0.2809 - val_mae: 0.4554\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3411 - mae: 0.3345\n",
            "Epoch 69: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3403 - mae: 0.3340 - val_loss: 0.0605 - val_mae: 0.1305\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2051 - mae: 0.2339\n",
            "Epoch 70: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2052 - mae: 0.2339 - val_loss: 0.0906 - val_mae: 0.2059\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2188 - mae: 0.2460\n",
            "Epoch 71: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2187 - mae: 0.2460 - val_loss: 0.1225 - val_mae: 0.2642\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2042 - mae: 0.2324\n",
            "Epoch 72: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2049 - mae: 0.2327 - val_loss: 0.0720 - val_mae: 0.1437\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2669 - mae: 0.2918\n",
            "Epoch 73: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2661 - mae: 0.2914 - val_loss: 0.0597 - val_mae: 0.1202\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1883 - mae: 0.2194\n",
            "Epoch 74: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1880 - mae: 0.2194 - val_loss: 0.0566 - val_mae: 0.0958\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2365 - mae: 0.2790\n",
            "Epoch 75: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2371 - mae: 0.2790 - val_loss: 0.0979 - val_mae: 0.2294\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1999 - mae: 0.2241\n",
            "Epoch 76: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2008 - mae: 0.2241 - val_loss: 0.0584 - val_mae: 0.1270\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2092 - mae: 0.2277\n",
            "Epoch 77: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2094 - mae: 0.2277 - val_loss: 0.0638 - val_mae: 0.1135\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2644 - mae: 0.3039\n",
            "Epoch 78: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2636 - mae: 0.3033 - val_loss: 0.1083 - val_mae: 0.2502\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1980 - mae: 0.2225\n",
            "Epoch 79: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1975 - mae: 0.2222 - val_loss: 0.0555 - val_mae: 0.1124\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2232 - mae: 0.2403\n",
            "Epoch 80: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2233 - mae: 0.2403 - val_loss: 0.0565 - val_mae: 0.1078\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2148 - mae: 0.2383\n",
            "Epoch 81: val_loss did not improve from 0.05416\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2146 - mae: 0.2382 - val_loss: 0.0682 - val_mae: 0.1189\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2020 - mae: 0.2291\n",
            "Epoch 82: val_loss improved from 0.05416 to 0.05387, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2019 - mae: 0.2290 - val_loss: 0.0539 - val_mae: 0.1044\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2313 - mae: 0.2631\n",
            "Epoch 83: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2306 - mae: 0.2626 - val_loss: 0.0617 - val_mae: 0.1248\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1854 - mae: 0.2221\n",
            "Epoch 84: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1849 - mae: 0.2219 - val_loss: 0.0721 - val_mae: 0.1503\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2014 - mae: 0.2219\n",
            "Epoch 85: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2009 - mae: 0.2219 - val_loss: 0.0839 - val_mae: 0.2231\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1841 - mae: 0.2236\n",
            "Epoch 86: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1841 - mae: 0.2236 - val_loss: 0.0602 - val_mae: 0.1082\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2001 - mae: 0.2289\n",
            "Epoch 87: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2006 - mae: 0.2294 - val_loss: 0.0564 - val_mae: 0.1049\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2023 - mae: 0.2281\n",
            "Epoch 88: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2026 - mae: 0.2282 - val_loss: 0.0632 - val_mae: 0.1421\n",
            "Epoch 89/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.1984 - mae: 0.2279\n",
            "Epoch 89: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1984 - mae: 0.2279 - val_loss: 0.0584 - val_mae: 0.0976\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2128 - mae: 0.2474\n",
            "Epoch 90: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2127 - mae: 0.2473 - val_loss: 0.0845 - val_mae: 0.1445\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1966 - mae: 0.2228\n",
            "Epoch 91: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1962 - mae: 0.2227 - val_loss: 0.0578 - val_mae: 0.1113\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2102 - mae: 0.2416\n",
            "Epoch 92: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2103 - mae: 0.2417 - val_loss: 0.0870 - val_mae: 0.2112\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1942 - mae: 0.2206\n",
            "Epoch 93: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1944 - mae: 0.2206 - val_loss: 0.0575 - val_mae: 0.1062\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2641 - mae: 0.3079\n",
            "Epoch 94: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2645 - mae: 0.3079 - val_loss: 0.0627 - val_mae: 0.1449\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1858 - mae: 0.2116\n",
            "Epoch 95: val_loss did not improve from 0.05387\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1857 - mae: 0.2117 - val_loss: 0.0627 - val_mae: 0.1583\n",
            "Epoch 96/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.1970 - mae: 0.2269\n",
            "Epoch 96: val_loss improved from 0.05387 to 0.05200, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.1970 - mae: 0.2269 - val_loss: 0.0520 - val_mae: 0.0952\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1830 - mae: 0.2163\n",
            "Epoch 97: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1826 - mae: 0.2161 - val_loss: 0.0590 - val_mae: 0.1193\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1955 - mae: 0.2210\n",
            "Epoch 98: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1959 - mae: 0.2218 - val_loss: 0.0702 - val_mae: 0.1368\n",
            "Epoch 99/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.1846 - mae: 0.2372\n",
            "Epoch 99: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.1846 - mae: 0.2372 - val_loss: 0.0610 - val_mae: 0.1305\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.2084\n",
            "Epoch 100: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1712 - mae: 0.2082 - val_loss: 0.0654 - val_mae: 0.1267\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2317 - mae: 0.2710\n",
            "Epoch 101: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2338 - mae: 0.2716 - val_loss: 0.0573 - val_mae: 0.1002\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1692 - mae: 0.2050\n",
            "Epoch 102: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1688 - mae: 0.2048 - val_loss: 0.0595 - val_mae: 0.1403\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.2055\n",
            "Epoch 103: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1744 - mae: 0.2057 - val_loss: 0.0615 - val_mae: 0.1431\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.2148\n",
            "Epoch 104: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1755 - mae: 0.2145 - val_loss: 0.0740 - val_mae: 0.1788\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2325 - mae: 0.2689\n",
            "Epoch 105: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2321 - mae: 0.2686 - val_loss: 0.0559 - val_mae: 0.1027\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.2060\n",
            "Epoch 106: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1745 - mae: 0.2059 - val_loss: 0.0698 - val_mae: 0.1643\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1814 - mae: 0.2160\n",
            "Epoch 107: val_loss did not improve from 0.05200\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1810 - mae: 0.2160 - val_loss: 0.1170 - val_mae: 0.2771\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.2100\n",
            "Epoch 108: val_loss improved from 0.05200 to 0.05158, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.1769 - mae: 0.2099 - val_loss: 0.0516 - val_mae: 0.0889\n",
            "Epoch 109/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1831 - mae: 0.2286\n",
            "Epoch 109: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1825 - mae: 0.2281 - val_loss: 0.0612 - val_mae: 0.1091\n",
            "Epoch 110/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1590 - mae: 0.2008\n",
            "Epoch 110: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1585 - mae: 0.2003 - val_loss: 0.0585 - val_mae: 0.1210\n",
            "Epoch 111/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.2108\n",
            "Epoch 111: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1762 - mae: 0.2105 - val_loss: 0.0995 - val_mae: 0.1717\n",
            "Epoch 112/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1822 - mae: 0.2156\n",
            "Epoch 112: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1837 - mae: 0.2163 - val_loss: 0.0838 - val_mae: 0.1868\n",
            "Epoch 113/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1857 - mae: 0.2243\n",
            "Epoch 113: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1851 - mae: 0.2240 - val_loss: 0.0676 - val_mae: 0.1567\n",
            "Epoch 114/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.2082\n",
            "Epoch 114: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1735 - mae: 0.2081 - val_loss: 0.0777 - val_mae: 0.2059\n",
            "Epoch 115/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1713 - mae: 0.2149\n",
            "Epoch 115: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1710 - mae: 0.2148 - val_loss: 0.0613 - val_mae: 0.1478\n",
            "Epoch 116/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1715 - mae: 0.2035\n",
            "Epoch 116: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1710 - mae: 0.2032 - val_loss: 0.0658 - val_mae: 0.1248\n",
            "Epoch 117/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.1688 - mae: 0.2045\n",
            "Epoch 117: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1688 - mae: 0.2045 - val_loss: 0.0572 - val_mae: 0.1146\n",
            "Epoch 118/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1520 - mae: 0.1975\n",
            "Epoch 118: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1519 - mae: 0.1975 - val_loss: 0.1673 - val_mae: 0.1796\n",
            "Epoch 119/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4949 - mae: 0.3968\n",
            "Epoch 119: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4943 - mae: 0.3968 - val_loss: 0.4888 - val_mae: 0.3173\n",
            "Epoch 120/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4943 - mae: 0.3953\n",
            "Epoch 120: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4932 - mae: 0.3951 - val_loss: 0.2098 - val_mae: 0.2645\n",
            "Epoch 121/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1846 - mae: 0.2336\n",
            "Epoch 121: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1844 - mae: 0.2333 - val_loss: 0.0677 - val_mae: 0.1187\n",
            "Epoch 122/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1654 - mae: 0.2086\n",
            "Epoch 122: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1651 - mae: 0.2085 - val_loss: 0.0566 - val_mae: 0.0977\n",
            "Epoch 123/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1669 - mae: 0.2134\n",
            "Epoch 123: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1665 - mae: 0.2131 - val_loss: 0.0648 - val_mae: 0.1295\n",
            "Epoch 124/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.2054\n",
            "Epoch 124: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1717 - mae: 0.2055 - val_loss: 0.0586 - val_mae: 0.1264\n",
            "Epoch 125/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1788 - mae: 0.2265\n",
            "Epoch 125: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1784 - mae: 0.2263 - val_loss: 0.0724 - val_mae: 0.1741\n",
            "Epoch 126/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1550 - mae: 0.2037\n",
            "Epoch 126: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1551 - mae: 0.2039 - val_loss: 0.1021 - val_mae: 0.1611\n",
            "Epoch 127/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1684 - mae: 0.2035\n",
            "Epoch 127: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1682 - mae: 0.2035 - val_loss: 0.0598 - val_mae: 0.1379\n",
            "Epoch 128/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1494 - mae: 0.2028\n",
            "Epoch 128: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1494 - mae: 0.2031 - val_loss: 0.0545 - val_mae: 0.1016\n",
            "Epoch 129/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1568 - mae: 0.2017\n",
            "Epoch 129: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1565 - mae: 0.2014 - val_loss: 0.0730 - val_mae: 0.1828\n",
            "Epoch 130/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1465 - mae: 0.1914\n",
            "Epoch 130: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1464 - mae: 0.1914 - val_loss: 0.0555 - val_mae: 0.1225\n",
            "Epoch 131/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1581 - mae: 0.2059\n",
            "Epoch 131: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1585 - mae: 0.2060 - val_loss: 0.0552 - val_mae: 0.1097\n",
            "Epoch 132/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1594 - mae: 0.2074\n",
            "Epoch 132: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1595 - mae: 0.2075 - val_loss: 0.0647 - val_mae: 0.1603\n",
            "Epoch 133/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1592 - mae: 0.2077\n",
            "Epoch 133: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1594 - mae: 0.2078 - val_loss: 0.0585 - val_mae: 0.1333\n",
            "Epoch 134/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1519 - mae: 0.1988\n",
            "Epoch 134: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1514 - mae: 0.1985 - val_loss: 0.0714 - val_mae: 0.1745\n",
            "Epoch 135/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1543 - mae: 0.1975\n",
            "Epoch 135: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1545 - mae: 0.1976 - val_loss: 0.0585 - val_mae: 0.1369\n",
            "Epoch 136/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1488 - mae: 0.1914\n",
            "Epoch 136: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1491 - mae: 0.1915 - val_loss: 0.0535 - val_mae: 0.1172\n",
            "Epoch 137/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1437 - mae: 0.1946\n",
            "Epoch 137: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1434 - mae: 0.1944 - val_loss: 0.0532 - val_mae: 0.0893\n",
            "Epoch 138/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1452 - mae: 0.1934\n",
            "Epoch 138: val_loss did not improve from 0.05158\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1458 - mae: 0.1937 - val_loss: 0.0594 - val_mae: 0.1429\n",
            "Epoch 138: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07195767760276794, RMSE:0.2682492733001709, MAE:0.11087792366743088, R2:0.921309073588352\n",
            "7 0.921309073588352\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07077091932296753, RMSE:0.2660280466079712, MAE:0.10452725738286972, R2:0.9212146105790974\n",
            "r2: 0.9212146105790974 rmse: 0.26602805\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07077091932296753, RMSE:0.2660280466079712, MAE:0.10452725738286972, R2:0.9212146105790974\n",
            "r2: 0.9212146105790974 rmse: 0.26602805\n",
            "第8次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_21/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_21'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_121\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_43 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_84 (Embedding)       (None, 17, 900)      649800      ['input_43[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_85 (Embedding)       (None, 17, 900)      649800      ['input_43[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_86 (Embedding)       (None, 17, 900)      649800      ['input_43[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_87 (Embedding)       (None, 17, 900)      649800      ['input_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_63 (Conv1D)             (None, 17, 100)      270100      ['embedding_84[0][0]',           \n",
            "                                                                  'embedding_85[0][0]',           \n",
            "                                                                  'embedding_86[0][0]',           \n",
            "                                                                  'embedding_87[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 17, 100)      270100      ['embedding_84[0][0]',           \n",
            "                                                                  'embedding_85[0][0]',           \n",
            "                                                                  'embedding_86[0][0]',           \n",
            "                                                                  'embedding_87[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 17, 100)      270100      ['embedding_84[0][0]',           \n",
            "                                                                  'embedding_85[0][0]',           \n",
            "                                                                  'embedding_86[0][0]',           \n",
            "                                                                  'embedding_87[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_252 (Glob  (None, 100)         0           ['conv1d_63[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_253 (Glob  (None, 100)         0           ['conv1d_63[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_254 (Glob  (None, 100)         0           ['conv1d_63[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_255 (Glob  (None, 100)         0           ['conv1d_63[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_256 (Glob  (None, 100)         0           ['conv1d_64[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_257 (Glob  (None, 100)         0           ['conv1d_64[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_258 (Glob  (None, 100)         0           ['conv1d_64[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_259 (Glob  (None, 100)         0           ['conv1d_64[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_260 (Glob  (None, 100)         0           ['conv1d_65[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_261 (Glob  (None, 100)         0           ['conv1d_65[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_262 (Glob  (None, 100)         0           ['conv1d_65[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_263 (Glob  (None, 100)         0           ['conv1d_65[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_44 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 100)          0           ['global_max_pooling1d_252[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_253[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_254[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_255[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 100)          0           ['global_max_pooling1d_256[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_257[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_258[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_259[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 100)          0           ['global_max_pooling1d_260[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_261[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_262[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_263[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 1000)         11000       ['input_44[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 1300)         0           ['add_63[0][0]',                 \n",
            "                                                                  'add_64[0][0]',                 \n",
            "                                                                  'add_65[0][0]',                 \n",
            "                                                                  'dense_84[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_21 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_21[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_21 (G  (1, 1300)           0           ['tf.expand_dims_21[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_21 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_21[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (1, 1, 1, 650)       845650      ['reshape_21[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_21 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_21[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_21 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_86[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_21 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_21[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_21[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_21 (TFOpL  (None, 1300)        0           ['multiply_21[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_21[0][0]']\n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_105[0][0]']            \n",
            "                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dropout_106 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_106[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_107[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_108 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_108[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_109 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 1)            3           ['dropout_109[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.8997 - mae: 0.8617\n",
            "Epoch 1: val_loss improved from inf to 0.65126, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 46ms/step - loss: 1.8997 - mae: 0.8617 - val_loss: 0.6513 - val_mae: 0.6609\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7416 - mae: 0.5319\n",
            "Epoch 2: val_loss improved from 0.65126 to 0.45150, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7432 - mae: 0.5326 - val_loss: 0.4515 - val_mae: 0.4677\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6746 - mae: 0.5029\n",
            "Epoch 3: val_loss improved from 0.45150 to 0.42514, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6748 - mae: 0.5032 - val_loss: 0.4251 - val_mae: 0.3820\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6518 - mae: 0.4861\n",
            "Epoch 4: val_loss did not improve from 0.42514\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6503 - mae: 0.4854 - val_loss: 0.6415 - val_mae: 0.5891\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6490 - mae: 0.4865\n",
            "Epoch 5: val_loss improved from 0.42514 to 0.37982, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6497 - mae: 0.4874 - val_loss: 0.3798 - val_mae: 0.3420\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6423 - mae: 0.4862\n",
            "Epoch 6: val_loss did not improve from 0.37982\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6443 - mae: 0.4874 - val_loss: 0.4425 - val_mae: 0.4934\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6088 - mae: 0.4744\n",
            "Epoch 7: val_loss improved from 0.37982 to 0.36778, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6082 - mae: 0.4742 - val_loss: 0.3678 - val_mae: 0.2838\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5875 - mae: 0.4401\n",
            "Epoch 8: val_loss improved from 0.36778 to 0.36016, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5862 - mae: 0.4397 - val_loss: 0.3602 - val_mae: 0.3446\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5905 - mae: 0.4590\n",
            "Epoch 9: val_loss did not improve from 0.36016\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5927 - mae: 0.4597 - val_loss: 0.4254 - val_mae: 0.4743\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5859 - mae: 0.4569\n",
            "Epoch 10: val_loss did not improve from 0.36016\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5844 - mae: 0.4561 - val_loss: 0.5861 - val_mae: 0.5643\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5554 - mae: 0.4357\n",
            "Epoch 11: val_loss did not improve from 0.36016\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5552 - mae: 0.4357 - val_loss: 0.4016 - val_mae: 0.4128\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5541 - mae: 0.4404\n",
            "Epoch 12: val_loss did not improve from 0.36016\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5540 - mae: 0.4404 - val_loss: 0.3656 - val_mae: 0.4169\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5363 - mae: 0.4233\n",
            "Epoch 13: val_loss improved from 0.36016 to 0.28830, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.5358 - mae: 0.4232 - val_loss: 0.2883 - val_mae: 0.2417\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4835 - mae: 0.4013\n",
            "Epoch 14: val_loss did not improve from 0.28830\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4856 - mae: 0.4018 - val_loss: 0.3028 - val_mae: 0.4168\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4441 - mae: 0.3868\n",
            "Epoch 15: val_loss improved from 0.28830 to 0.24512, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4434 - mae: 0.3866 - val_loss: 0.2451 - val_mae: 0.2833\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4575 - mae: 0.4054\n",
            "Epoch 16: val_loss did not improve from 0.24512\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4567 - mae: 0.4052 - val_loss: 0.3822 - val_mae: 0.4291\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4111 - mae: 0.3692\n",
            "Epoch 17: val_loss improved from 0.24512 to 0.19526, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4118 - mae: 0.3700 - val_loss: 0.1953 - val_mae: 0.1971\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3829 - mae: 0.3442\n",
            "Epoch 18: val_loss improved from 0.19526 to 0.18316, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3832 - mae: 0.3449 - val_loss: 0.1832 - val_mae: 0.2056\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3835 - mae: 0.3649\n",
            "Epoch 19: val_loss improved from 0.18316 to 0.13593, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3826 - mae: 0.3644 - val_loss: 0.1359 - val_mae: 0.1681\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3649 - mae: 0.3370\n",
            "Epoch 20: val_loss improved from 0.13593 to 0.11450, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3650 - mae: 0.3372 - val_loss: 0.1145 - val_mae: 0.1703\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3417 - mae: 0.3286\n",
            "Epoch 21: val_loss improved from 0.11450 to 0.11158, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3415 - mae: 0.3286 - val_loss: 0.1116 - val_mae: 0.1907\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3211 - mae: 0.3073\n",
            "Epoch 22: val_loss improved from 0.11158 to 0.11024, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3204 - mae: 0.3070 - val_loss: 0.1102 - val_mae: 0.1929\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3356 - mae: 0.3153\n",
            "Epoch 23: val_loss improved from 0.11024 to 0.09469, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3349 - mae: 0.3151 - val_loss: 0.0947 - val_mae: 0.1694\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3094 - mae: 0.3031\n",
            "Epoch 24: val_loss improved from 0.09469 to 0.09323, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 43ms/step - loss: 0.3096 - mae: 0.3032 - val_loss: 0.0932 - val_mae: 0.1609\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3156 - mae: 0.3003\n",
            "Epoch 25: val_loss improved from 0.09323 to 0.08558, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.3148 - mae: 0.3000 - val_loss: 0.0856 - val_mae: 0.1814\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2885 - mae: 0.2865\n",
            "Epoch 26: val_loss did not improve from 0.08558\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2888 - mae: 0.2867 - val_loss: 0.1895 - val_mae: 0.3806\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2825 - mae: 0.2870\n",
            "Epoch 27: val_loss improved from 0.08558 to 0.07333, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2824 - mae: 0.2869 - val_loss: 0.0733 - val_mae: 0.1812\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3025 - mae: 0.3101\n",
            "Epoch 28: val_loss did not improve from 0.07333\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3025 - mae: 0.3104 - val_loss: 0.1460 - val_mae: 0.2806\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2733 - mae: 0.2787\n",
            "Epoch 29: val_loss did not improve from 0.07333\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2794 - mae: 0.2791 - val_loss: 0.0765 - val_mae: 0.1693\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4356 - mae: 0.3703\n",
            "Epoch 30: val_loss did not improve from 0.07333\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4366 - mae: 0.3712 - val_loss: 0.4652 - val_mae: 0.6028\n",
            "Epoch 31/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4184 - mae: 0.3843\n",
            "Epoch 31: val_loss did not improve from 0.07333\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.4184 - mae: 0.3843 - val_loss: 0.1527 - val_mae: 0.2810\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2813 - mae: 0.3065\n",
            "Epoch 32: val_loss did not improve from 0.07333\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2818 - mae: 0.3068 - val_loss: 0.0872 - val_mae: 0.2004\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2851 - mae: 0.2846\n",
            "Epoch 33: val_loss improved from 0.07333 to 0.06648, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2860 - mae: 0.2847 - val_loss: 0.0665 - val_mae: 0.1201\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2631 - mae: 0.2733\n",
            "Epoch 34: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2623 - mae: 0.2730 - val_loss: 0.1416 - val_mae: 0.3204\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2922 - mae: 0.3015\n",
            "Epoch 35: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2921 - mae: 0.3015 - val_loss: 0.0751 - val_mae: 0.1647\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2544 - mae: 0.2629\n",
            "Epoch 36: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2549 - mae: 0.2630 - val_loss: 0.0906 - val_mae: 0.1937\n",
            "Epoch 37/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2851 - mae: 0.2725\n",
            "Epoch 37: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2851 - mae: 0.2725 - val_loss: 0.0937 - val_mae: 0.2280\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2307 - mae: 0.2583\n",
            "Epoch 38: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2302 - mae: 0.2580 - val_loss: 0.1030 - val_mae: 0.2520\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2684 - mae: 0.2949\n",
            "Epoch 39: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2677 - mae: 0.2947 - val_loss: 0.0860 - val_mae: 0.2203\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2643 - mae: 0.2742\n",
            "Epoch 40: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2635 - mae: 0.2739 - val_loss: 0.0897 - val_mae: 0.2186\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2447 - mae: 0.2575\n",
            "Epoch 41: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2441 - mae: 0.2571 - val_loss: 0.0680 - val_mae: 0.1137\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2797 - mae: 0.2765\n",
            "Epoch 42: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2796 - mae: 0.2764 - val_loss: 0.0924 - val_mae: 0.2065\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2741 - mae: 0.2910\n",
            "Epoch 43: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2739 - mae: 0.2907 - val_loss: 0.0808 - val_mae: 0.1953\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2576 - mae: 0.2764\n",
            "Epoch 44: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2569 - mae: 0.2760 - val_loss: 0.0827 - val_mae: 0.1980\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2463 - mae: 0.2595\n",
            "Epoch 45: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2459 - mae: 0.2594 - val_loss: 0.1457 - val_mae: 0.3178\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2711 - mae: 0.2814\n",
            "Epoch 46: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2705 - mae: 0.2810 - val_loss: 0.1662 - val_mae: 0.3511\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2404 - mae: 0.2424\n",
            "Epoch 47: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2403 - mae: 0.2428 - val_loss: 0.0812 - val_mae: 0.2267\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2589 - mae: 0.2764\n",
            "Epoch 48: val_loss did not improve from 0.06648\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2590 - mae: 0.2763 - val_loss: 0.0720 - val_mae: 0.1929\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2361 - mae: 0.2527\n",
            "Epoch 49: val_loss improved from 0.06648 to 0.05460, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2357 - mae: 0.2529 - val_loss: 0.0546 - val_mae: 0.1151\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2999 - mae: 0.2954\n",
            "Epoch 50: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2995 - mae: 0.2956 - val_loss: 0.1353 - val_mae: 0.2730\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2340 - mae: 0.2576\n",
            "Epoch 51: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2333 - mae: 0.2572 - val_loss: 0.0720 - val_mae: 0.1598\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2561 - mae: 0.2699\n",
            "Epoch 52: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2554 - mae: 0.2696 - val_loss: 0.0692 - val_mae: 0.1025\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2213 - mae: 0.2372\n",
            "Epoch 53: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2208 - mae: 0.2370 - val_loss: 0.0596 - val_mae: 0.1247\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2760 - mae: 0.2671\n",
            "Epoch 54: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2757 - mae: 0.2670 - val_loss: 0.0631 - val_mae: 0.1343\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2434 - mae: 0.2616\n",
            "Epoch 55: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2429 - mae: 0.2614 - val_loss: 0.0951 - val_mae: 0.1932\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2543 - mae: 0.2534\n",
            "Epoch 56: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2545 - mae: 0.2542 - val_loss: 0.0696 - val_mae: 0.1720\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2583 - mae: 0.2761\n",
            "Epoch 57: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2580 - mae: 0.2760 - val_loss: 0.1178 - val_mae: 0.2861\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2285 - mae: 0.2426\n",
            "Epoch 58: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2278 - mae: 0.2423 - val_loss: 0.0604 - val_mae: 0.1191\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2509 - mae: 0.2699\n",
            "Epoch 59: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2501 - mae: 0.2695 - val_loss: 0.0753 - val_mae: 0.1869\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4001 - mae: 0.3679\n",
            "Epoch 60: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4009 - mae: 0.3681 - val_loss: 0.3160 - val_mae: 0.4010\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2677 - mae: 0.2805\n",
            "Epoch 61: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2676 - mae: 0.2803 - val_loss: 0.0897 - val_mae: 0.2186\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2184 - mae: 0.2430\n",
            "Epoch 62: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2180 - mae: 0.2428 - val_loss: 0.0631 - val_mae: 0.1444\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2362 - mae: 0.2703\n",
            "Epoch 63: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2381 - mae: 0.2705 - val_loss: 0.0679 - val_mae: 0.1326\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2204 - mae: 0.2468\n",
            "Epoch 64: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2198 - mae: 0.2464 - val_loss: 0.0883 - val_mae: 0.1902\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2226 - mae: 0.2490\n",
            "Epoch 65: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2221 - mae: 0.2488 - val_loss: 0.0685 - val_mae: 0.1658\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2241 - mae: 0.2496\n",
            "Epoch 66: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2241 - mae: 0.2497 - val_loss: 0.1352 - val_mae: 0.3065\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2182 - mae: 0.2428\n",
            "Epoch 67: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2176 - mae: 0.2425 - val_loss: 0.0667 - val_mae: 0.1657\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2287 - mae: 0.2565\n",
            "Epoch 68: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2283 - mae: 0.2566 - val_loss: 0.0957 - val_mae: 0.2393\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2151 - mae: 0.2379\n",
            "Epoch 69: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2151 - mae: 0.2385 - val_loss: 0.1207 - val_mae: 0.2619\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2325 - mae: 0.2654\n",
            "Epoch 70: val_loss did not improve from 0.05460\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2324 - mae: 0.2653 - val_loss: 0.0722 - val_mae: 0.1736\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2266 - mae: 0.2483\n",
            "Epoch 71: val_loss improved from 0.05460 to 0.05272, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2265 - mae: 0.2483 - val_loss: 0.0527 - val_mae: 0.1138\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2171 - mae: 0.2369\n",
            "Epoch 72: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2168 - mae: 0.2368 - val_loss: 0.0609 - val_mae: 0.0922\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2160 - mae: 0.2377\n",
            "Epoch 73: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2160 - mae: 0.2376 - val_loss: 0.0829 - val_mae: 0.2002\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2121 - mae: 0.2328\n",
            "Epoch 74: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2114 - mae: 0.2324 - val_loss: 0.1032 - val_mae: 0.2517\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2001 - mae: 0.2280\n",
            "Epoch 75: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2004 - mae: 0.2281 - val_loss: 0.2254 - val_mae: 0.3913\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2333 - mae: 0.2712\n",
            "Epoch 76: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2335 - mae: 0.2712 - val_loss: 0.0657 - val_mae: 0.1725\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2124 - mae: 0.2397\n",
            "Epoch 77: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2122 - mae: 0.2397 - val_loss: 0.0780 - val_mae: 0.1754\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2058 - mae: 0.2287\n",
            "Epoch 78: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2062 - mae: 0.2287 - val_loss: 0.0914 - val_mae: 0.1944\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2236 - mae: 0.2605\n",
            "Epoch 79: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2232 - mae: 0.2606 - val_loss: 0.0592 - val_mae: 0.1371\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2152 - mae: 0.2326\n",
            "Epoch 80: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2149 - mae: 0.2327 - val_loss: 0.0597 - val_mae: 0.1460\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2195 - mae: 0.2428\n",
            "Epoch 81: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2204 - mae: 0.2430 - val_loss: 0.1768 - val_mae: 0.2729\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2532 - mae: 0.2767\n",
            "Epoch 82: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2530 - mae: 0.2766 - val_loss: 0.0541 - val_mae: 0.0998\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2148 - mae: 0.2276\n",
            "Epoch 83: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2142 - mae: 0.2273 - val_loss: 0.0585 - val_mae: 0.1090\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2166 - mae: 0.2490\n",
            "Epoch 84: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2161 - mae: 0.2487 - val_loss: 0.1107 - val_mae: 0.1991\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2075 - mae: 0.2322\n",
            "Epoch 85: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2080 - mae: 0.2326 - val_loss: 0.0568 - val_mae: 0.1029\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2141 - mae: 0.2362\n",
            "Epoch 86: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2145 - mae: 0.2366 - val_loss: 0.0797 - val_mae: 0.2213\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2186 - mae: 0.2515\n",
            "Epoch 87: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2186 - mae: 0.2516 - val_loss: 0.0850 - val_mae: 0.2232\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1859 - mae: 0.2164\n",
            "Epoch 88: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1854 - mae: 0.2161 - val_loss: 0.0550 - val_mae: 0.1191\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2216 - mae: 0.2549\n",
            "Epoch 89: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2219 - mae: 0.2551 - val_loss: 0.0806 - val_mae: 0.1960\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1936 - mae: 0.2195\n",
            "Epoch 90: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1936 - mae: 0.2197 - val_loss: 0.1251 - val_mae: 0.2729\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2234 - mae: 0.2461\n",
            "Epoch 91: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2228 - mae: 0.2458 - val_loss: 0.0913 - val_mae: 0.1856\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1897 - mae: 0.2208\n",
            "Epoch 92: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1895 - mae: 0.2207 - val_loss: 0.0577 - val_mae: 0.1374\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1951 - mae: 0.2374\n",
            "Epoch 93: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1950 - mae: 0.2373 - val_loss: 0.0893 - val_mae: 0.2267\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1992 - mae: 0.2343\n",
            "Epoch 94: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2006 - mae: 0.2346 - val_loss: 0.0610 - val_mae: 0.1232\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2173 - mae: 0.2495\n",
            "Epoch 95: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2172 - mae: 0.2493 - val_loss: 0.0624 - val_mae: 0.1372\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1878 - mae: 0.2158\n",
            "Epoch 96: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1873 - mae: 0.2157 - val_loss: 0.0631 - val_mae: 0.1519\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2137 - mae: 0.2470\n",
            "Epoch 97: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2131 - mae: 0.2466 - val_loss: 0.0661 - val_mae: 0.1676\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2024 - mae: 0.2390\n",
            "Epoch 98: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2023 - mae: 0.2388 - val_loss: 0.0743 - val_mae: 0.1595\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1893 - mae: 0.2245\n",
            "Epoch 99: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1891 - mae: 0.2245 - val_loss: 0.0531 - val_mae: 0.1134\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2477 - mae: 0.2843\n",
            "Epoch 100: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2475 - mae: 0.2840 - val_loss: 0.0639 - val_mae: 0.1588\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1891 - mae: 0.2207\n",
            "Epoch 101: val_loss did not improve from 0.05272\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1886 - mae: 0.2203 - val_loss: 0.1021 - val_mae: 0.2478\n",
            "Epoch 101: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07267321646213531, RMSE:0.2695797085762024, MAE:0.13323992490768433, R2:0.9205265706080363\n",
            "8 0.9205265706080363\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07426058501005173, RMSE:0.2725079655647278, MAE:0.1315876990556717, R2:0.9173297577883059\n",
            "r2: 0.9173297577883059 rmse: 0.27250797\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07426058501005173, RMSE:0.2725079655647278, MAE:0.1315876990556717, R2:0.9173297577883059\n",
            "r2: 0.9173297577883059 rmse: 0.27250797\n",
            "第9次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_22/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_22'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_127\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_45 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_88 (Embedding)       (None, 17, 900)      649800      ['input_45[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_89 (Embedding)       (None, 17, 900)      649800      ['input_45[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_90 (Embedding)       (None, 17, 900)      649800      ['input_45[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_91 (Embedding)       (None, 17, 900)      649800      ['input_45[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 17, 100)      270100      ['embedding_88[0][0]',           \n",
            "                                                                  'embedding_89[0][0]',           \n",
            "                                                                  'embedding_90[0][0]',           \n",
            "                                                                  'embedding_91[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_67 (Conv1D)             (None, 17, 100)      270100      ['embedding_88[0][0]',           \n",
            "                                                                  'embedding_89[0][0]',           \n",
            "                                                                  'embedding_90[0][0]',           \n",
            "                                                                  'embedding_91[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_68 (Conv1D)             (None, 17, 100)      270100      ['embedding_88[0][0]',           \n",
            "                                                                  'embedding_89[0][0]',           \n",
            "                                                                  'embedding_90[0][0]',           \n",
            "                                                                  'embedding_91[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_264 (Glob  (None, 100)         0           ['conv1d_66[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_265 (Glob  (None, 100)         0           ['conv1d_66[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_266 (Glob  (None, 100)         0           ['conv1d_66[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_267 (Glob  (None, 100)         0           ['conv1d_66[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_268 (Glob  (None, 100)         0           ['conv1d_67[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_269 (Glob  (None, 100)         0           ['conv1d_67[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_270 (Glob  (None, 100)         0           ['conv1d_67[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_271 (Glob  (None, 100)         0           ['conv1d_67[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_272 (Glob  (None, 100)         0           ['conv1d_68[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_273 (Glob  (None, 100)         0           ['conv1d_68[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_274 (Glob  (None, 100)         0           ['conv1d_68[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_275 (Glob  (None, 100)         0           ['conv1d_68[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_46 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 100)          0           ['global_max_pooling1d_264[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_265[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_266[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_267[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 100)          0           ['global_max_pooling1d_268[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_269[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_270[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_271[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 100)          0           ['global_max_pooling1d_272[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_273[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_274[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_275[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 1000)         11000       ['input_46[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 1300)         0           ['add_66[0][0]',                 \n",
            "                                                                  'add_67[0][0]',                 \n",
            "                                                                  'add_68[0][0]',                 \n",
            "                                                                  'dense_88[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_22 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_22[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_22 (G  (1, 1300)           0           ['tf.expand_dims_22[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_22 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_22[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (1, 1, 1, 650)       845650      ['reshape_22[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_22 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_22[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_22 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_90[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_22 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_22[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_22[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_22 (TFOpL  (None, 1300)        0           ['multiply_22[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_22[0][0]']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " dropout_110 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_110[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_111[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_112 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_112[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_113[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_114 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 1)            3           ['dropout_114[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.1571 - mae: 0.8797\n",
            "Epoch 1: val_loss improved from inf to 0.51425, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 45ms/step - loss: 2.1525 - mae: 0.8789 - val_loss: 0.5142 - val_mae: 0.4841\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7509 - mae: 0.5435\n",
            "Epoch 2: val_loss improved from 0.51425 to 0.44881, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7518 - mae: 0.5443 - val_loss: 0.4488 - val_mae: 0.4596\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7160 - mae: 0.5420\n",
            "Epoch 3: val_loss improved from 0.44881 to 0.41795, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7141 - mae: 0.5410 - val_loss: 0.4179 - val_mae: 0.3627\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6574 - mae: 0.4997\n",
            "Epoch 4: val_loss improved from 0.41795 to 0.40791, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6561 - mae: 0.4993 - val_loss: 0.4079 - val_mae: 0.3385\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6875 - mae: 0.5084\n",
            "Epoch 5: val_loss did not improve from 0.40791\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6873 - mae: 0.5083 - val_loss: 0.4260 - val_mae: 0.3989\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6493 - mae: 0.4935\n",
            "Epoch 6: val_loss did not improve from 0.40791\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6516 - mae: 0.4946 - val_loss: 0.5252 - val_mae: 0.5939\n",
            "Epoch 7/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.6274 - mae: 0.4794\n",
            "Epoch 7: val_loss improved from 0.40791 to 0.35029, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6274 - mae: 0.4794 - val_loss: 0.3503 - val_mae: 0.3021\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6113 - mae: 0.4775\n",
            "Epoch 8: val_loss improved from 0.35029 to 0.33688, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.6120 - mae: 0.4776 - val_loss: 0.3369 - val_mae: 0.3183\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6145 - mae: 0.4680\n",
            "Epoch 9: val_loss improved from 0.33688 to 0.32717, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6147 - mae: 0.4679 - val_loss: 0.3272 - val_mae: 0.3188\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6124 - mae: 0.4783\n",
            "Epoch 10: val_loss did not improve from 0.32717\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6124 - mae: 0.4782 - val_loss: 0.5334 - val_mae: 0.5756\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5500 - mae: 0.4422\n",
            "Epoch 11: val_loss did not improve from 0.32717\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5492 - mae: 0.4417 - val_loss: 0.3576 - val_mae: 0.3932\n",
            "Epoch 12/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.5571 - mae: 0.4424\n",
            "Epoch 12: val_loss did not improve from 0.32717\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5571 - mae: 0.4424 - val_loss: 0.3287 - val_mae: 0.3720\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5138 - mae: 0.4322\n",
            "Epoch 13: val_loss improved from 0.32717 to 0.26354, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.5134 - mae: 0.4321 - val_loss: 0.2635 - val_mae: 0.2932\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4870 - mae: 0.4174\n",
            "Epoch 14: val_loss improved from 0.26354 to 0.25799, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4867 - mae: 0.4173 - val_loss: 0.2580 - val_mae: 0.3184\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4800 - mae: 0.4138\n",
            "Epoch 15: val_loss improved from 0.25799 to 0.19726, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4802 - mae: 0.4138 - val_loss: 0.1973 - val_mae: 0.2298\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4220 - mae: 0.3771\n",
            "Epoch 16: val_loss improved from 0.19726 to 0.17392, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4209 - mae: 0.3766 - val_loss: 0.1739 - val_mae: 0.2164\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4055 - mae: 0.3685\n",
            "Epoch 17: val_loss did not improve from 0.17392\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4050 - mae: 0.3685 - val_loss: 0.2459 - val_mae: 0.3983\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3558 - mae: 0.3503\n",
            "Epoch 18: val_loss improved from 0.17392 to 0.09940, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3551 - mae: 0.3500 - val_loss: 0.0994 - val_mae: 0.1559\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4079 - mae: 0.3894\n",
            "Epoch 19: val_loss did not improve from 0.09940\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4075 - mae: 0.3892 - val_loss: 0.1941 - val_mae: 0.3480\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3593 - mae: 0.3454\n",
            "Epoch 20: val_loss did not improve from 0.09940\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3584 - mae: 0.3449 - val_loss: 0.3522 - val_mae: 0.5171\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3423 - mae: 0.3529\n",
            "Epoch 21: val_loss improved from 0.09940 to 0.07812, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.3419 - mae: 0.3527 - val_loss: 0.0781 - val_mae: 0.1393\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2905 - mae: 0.3007\n",
            "Epoch 22: val_loss did not improve from 0.07812\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2897 - mae: 0.3002 - val_loss: 0.1014 - val_mae: 0.2165\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3124 - mae: 0.3164\n",
            "Epoch 23: val_loss did not improve from 0.07812\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3129 - mae: 0.3170 - val_loss: 0.1131 - val_mae: 0.1823\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3073 - mae: 0.2989\n",
            "Epoch 24: val_loss did not improve from 0.07812\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3094 - mae: 0.2994 - val_loss: 0.0787 - val_mae: 0.1321\n",
            "Epoch 25/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3228 - mae: 0.3232\n",
            "Epoch 25: val_loss improved from 0.07812 to 0.07653, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 49ms/step - loss: 0.3228 - mae: 0.3232 - val_loss: 0.0765 - val_mae: 0.1628\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2810 - mae: 0.2798\n",
            "Epoch 26: val_loss improved from 0.07653 to 0.07544, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2810 - mae: 0.2797 - val_loss: 0.0754 - val_mae: 0.1417\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2982 - mae: 0.2920\n",
            "Epoch 27: val_loss did not improve from 0.07544\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2974 - mae: 0.2916 - val_loss: 0.1008 - val_mae: 0.2327\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3352 - mae: 0.3537\n",
            "Epoch 28: val_loss improved from 0.07544 to 0.07341, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3349 - mae: 0.3531 - val_loss: 0.0734 - val_mae: 0.1637\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2794 - mae: 0.2819\n",
            "Epoch 29: val_loss did not improve from 0.07341\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2789 - mae: 0.2817 - val_loss: 0.0754 - val_mae: 0.1812\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2657 - mae: 0.2702\n",
            "Epoch 30: val_loss improved from 0.07341 to 0.06327, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.2657 - mae: 0.2702 - val_loss: 0.0633 - val_mae: 0.1256\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2949 - mae: 0.3032\n",
            "Epoch 31: val_loss did not improve from 0.06327\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2940 - mae: 0.3027 - val_loss: 0.1400 - val_mae: 0.3116\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3050 - mae: 0.3129\n",
            "Epoch 32: val_loss did not improve from 0.06327\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3049 - mae: 0.3129 - val_loss: 0.1335 - val_mae: 0.2977\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2860 - mae: 0.2872\n",
            "Epoch 33: val_loss improved from 0.06327 to 0.05457, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 51ms/step - loss: 0.2863 - mae: 0.2874 - val_loss: 0.0546 - val_mae: 0.1001\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2797 - mae: 0.2954\n",
            "Epoch 34: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2791 - mae: 0.2950 - val_loss: 0.0909 - val_mae: 0.1655\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2767 - mae: 0.2768\n",
            "Epoch 35: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2768 - mae: 0.2768 - val_loss: 0.0793 - val_mae: 0.1884\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2967 - mae: 0.3019\n",
            "Epoch 36: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2963 - mae: 0.3023 - val_loss: 0.0595 - val_mae: 0.1122\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2622 - mae: 0.2674\n",
            "Epoch 37: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2616 - mae: 0.2674 - val_loss: 0.0624 - val_mae: 0.1495\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2548 - mae: 0.2659\n",
            "Epoch 38: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2548 - mae: 0.2657 - val_loss: 0.1088 - val_mae: 0.2571\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2597 - mae: 0.2687\n",
            "Epoch 39: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2590 - mae: 0.2684 - val_loss: 0.0735 - val_mae: 0.1825\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2641 - mae: 0.2671\n",
            "Epoch 40: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2645 - mae: 0.2672 - val_loss: 0.0651 - val_mae: 0.1177\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2728 - mae: 0.2715\n",
            "Epoch 41: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2736 - mae: 0.2715 - val_loss: 0.0643 - val_mae: 0.1548\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3634 - mae: 0.3587\n",
            "Epoch 42: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3628 - mae: 0.3583 - val_loss: 0.0792 - val_mae: 0.1444\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2419 - mae: 0.2667\n",
            "Epoch 43: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2415 - mae: 0.2667 - val_loss: 0.0731 - val_mae: 0.1625\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2653 - mae: 0.2718\n",
            "Epoch 44: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2647 - mae: 0.2719 - val_loss: 0.1292 - val_mae: 0.2716\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2528 - mae: 0.2691\n",
            "Epoch 45: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2521 - mae: 0.2687 - val_loss: 0.1115 - val_mae: 0.2708\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2954 - mae: 0.3098\n",
            "Epoch 46: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2946 - mae: 0.3092 - val_loss: 0.0717 - val_mae: 0.1195\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2545 - mae: 0.2602\n",
            "Epoch 47: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2538 - mae: 0.2598 - val_loss: 0.0738 - val_mae: 0.1698\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2737 - mae: 0.2735\n",
            "Epoch 48: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2729 - mae: 0.2731 - val_loss: 0.0658 - val_mae: 0.1151\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2388 - mae: 0.2581\n",
            "Epoch 49: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2388 - mae: 0.2580 - val_loss: 0.0724 - val_mae: 0.1927\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2515 - mae: 0.2615\n",
            "Epoch 50: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2516 - mae: 0.2621 - val_loss: 0.0847 - val_mae: 0.1678\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2821 - mae: 0.2757\n",
            "Epoch 51: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2820 - mae: 0.2756 - val_loss: 0.0967 - val_mae: 0.2340\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2904 - mae: 0.2924\n",
            "Epoch 52: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2900 - mae: 0.2925 - val_loss: 0.1298 - val_mae: 0.2685\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2310 - mae: 0.2483\n",
            "Epoch 53: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2311 - mae: 0.2482 - val_loss: 0.0610 - val_mae: 0.1158\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2374 - mae: 0.2476\n",
            "Epoch 54: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2382 - mae: 0.2476 - val_loss: 0.0728 - val_mae: 0.1647\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2699 - mae: 0.2636\n",
            "Epoch 55: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2691 - mae: 0.2632 - val_loss: 0.1084 - val_mae: 0.2656\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2496 - mae: 0.2740\n",
            "Epoch 56: val_loss did not improve from 0.05457\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2493 - mae: 0.2741 - val_loss: 0.0615 - val_mae: 0.1556\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2332 - mae: 0.2459\n",
            "Epoch 57: val_loss improved from 0.05457 to 0.05196, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2346 - mae: 0.2459 - val_loss: 0.0520 - val_mae: 0.0973\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2219 - mae: 0.2430\n",
            "Epoch 58: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2220 - mae: 0.2434 - val_loss: 0.0790 - val_mae: 0.1455\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2387 - mae: 0.2650\n",
            "Epoch 59: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2382 - mae: 0.2647 - val_loss: 0.0646 - val_mae: 0.1508\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2272 - mae: 0.2491\n",
            "Epoch 60: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2270 - mae: 0.2489 - val_loss: 0.0632 - val_mae: 0.1428\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2411 - mae: 0.2693\n",
            "Epoch 61: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2413 - mae: 0.2693 - val_loss: 0.0564 - val_mae: 0.1040\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2196 - mae: 0.2382\n",
            "Epoch 62: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2190 - mae: 0.2380 - val_loss: 0.0722 - val_mae: 0.1261\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2872 - mae: 0.3004\n",
            "Epoch 63: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2871 - mae: 0.3003 - val_loss: 0.0948 - val_mae: 0.2143\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2290 - mae: 0.2379\n",
            "Epoch 64: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2284 - mae: 0.2378 - val_loss: 0.0543 - val_mae: 0.1028\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2197 - mae: 0.2479\n",
            "Epoch 65: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2191 - mae: 0.2476 - val_loss: 0.0725 - val_mae: 0.1588\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2132 - mae: 0.2264\n",
            "Epoch 66: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2136 - mae: 0.2265 - val_loss: 0.0726 - val_mae: 0.1867\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2109 - mae: 0.2408\n",
            "Epoch 67: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2103 - mae: 0.2404 - val_loss: 0.0785 - val_mae: 0.1931\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2475 - mae: 0.2527\n",
            "Epoch 68: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2483 - mae: 0.2529 - val_loss: 0.0559 - val_mae: 0.1073\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2652 - mae: 0.2820\n",
            "Epoch 69: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2648 - mae: 0.2817 - val_loss: 0.0700 - val_mae: 0.1703\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2072 - mae: 0.2283\n",
            "Epoch 70: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2066 - mae: 0.2280 - val_loss: 0.0747 - val_mae: 0.1155\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2245 - mae: 0.2474\n",
            "Epoch 71: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2241 - mae: 0.2473 - val_loss: 0.1650 - val_mae: 0.3529\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2191 - mae: 0.2378\n",
            "Epoch 72: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2189 - mae: 0.2377 - val_loss: 0.0555 - val_mae: 0.0970\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2058 - mae: 0.2254\n",
            "Epoch 73: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2060 - mae: 0.2257 - val_loss: 0.5391 - val_mae: 0.5074\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2496 - mae: 0.2749\n",
            "Epoch 74: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2497 - mae: 0.2750 - val_loss: 0.0639 - val_mae: 0.1576\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2033 - mae: 0.2223\n",
            "Epoch 75: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2028 - mae: 0.2221 - val_loss: 0.1241 - val_mae: 0.2959\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2294 - mae: 0.2374\n",
            "Epoch 76: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2305 - mae: 0.2379 - val_loss: 0.0586 - val_mae: 0.1511\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2190 - mae: 0.2457\n",
            "Epoch 77: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2189 - mae: 0.2457 - val_loss: 0.0540 - val_mae: 0.1107\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2186 - mae: 0.2289\n",
            "Epoch 78: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2185 - mae: 0.2292 - val_loss: 0.0589 - val_mae: 0.1343\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2085 - mae: 0.2424\n",
            "Epoch 79: val_loss did not improve from 0.05196\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2094 - mae: 0.2427 - val_loss: 0.0582 - val_mae: 0.1336\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2146 - mae: 0.2336\n",
            "Epoch 80: val_loss improved from 0.05196 to 0.05089, saving model to /content/gdrive/MyDrive/cnn_model/有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 59ms/step - loss: 0.2143 - mae: 0.2336 - val_loss: 0.0509 - val_mae: 0.0953\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1837 - mae: 0.2229\n",
            "Epoch 81: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.1833 - mae: 0.2226 - val_loss: 0.0779 - val_mae: 0.2024\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3716 - mae: 0.3494\n",
            "Epoch 82: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3714 - mae: 0.3490 - val_loss: 0.0887 - val_mae: 0.2230\n",
            "Epoch 83/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2003 - mae: 0.2228\n",
            "Epoch 83: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2003 - mae: 0.2228 - val_loss: 0.0532 - val_mae: 0.1193\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1901 - mae: 0.2171\n",
            "Epoch 84: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1897 - mae: 0.2169 - val_loss: 0.0698 - val_mae: 0.1642\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2220 - mae: 0.2671\n",
            "Epoch 85: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2214 - mae: 0.2667 - val_loss: 0.0624 - val_mae: 0.1253\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1903 - mae: 0.2093\n",
            "Epoch 86: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1898 - mae: 0.2091 - val_loss: 0.0630 - val_mae: 0.1310\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2219 - mae: 0.2566\n",
            "Epoch 87: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2218 - mae: 0.2564 - val_loss: 0.1031 - val_mae: 0.2281\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2021 - mae: 0.2178\n",
            "Epoch 88: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2016 - mae: 0.2174 - val_loss: 0.0618 - val_mae: 0.1618\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1979 - mae: 0.2181\n",
            "Epoch 89: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1980 - mae: 0.2182 - val_loss: 0.1112 - val_mae: 0.2747\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.2119\n",
            "Epoch 90: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1745 - mae: 0.2115 - val_loss: 0.0641 - val_mae: 0.1410\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2247 - mae: 0.2493\n",
            "Epoch 91: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2255 - mae: 0.2498 - val_loss: 0.0570 - val_mae: 0.1296\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1905 - mae: 0.2219\n",
            "Epoch 92: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1909 - mae: 0.2220 - val_loss: 0.0564 - val_mae: 0.1059\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1855 - mae: 0.2133\n",
            "Epoch 93: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1855 - mae: 0.2133 - val_loss: 0.0787 - val_mae: 0.1378\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1823 - mae: 0.2120\n",
            "Epoch 94: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1826 - mae: 0.2121 - val_loss: 0.0728 - val_mae: 0.1638\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2023 - mae: 0.2489\n",
            "Epoch 95: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2022 - mae: 0.2489 - val_loss: 0.0958 - val_mae: 0.2246\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2588 - mae: 0.2816\n",
            "Epoch 96: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2586 - mae: 0.2814 - val_loss: 0.0532 - val_mae: 0.1134\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.2016\n",
            "Epoch 97: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1769 - mae: 0.2014 - val_loss: 0.0559 - val_mae: 0.1221\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.2052\n",
            "Epoch 98: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1758 - mae: 0.2050 - val_loss: 0.0668 - val_mae: 0.1058\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1830 - mae: 0.2135\n",
            "Epoch 99: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1826 - mae: 0.2132 - val_loss: 0.0648 - val_mae: 0.1456\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1880 - mae: 0.2187\n",
            "Epoch 100: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1885 - mae: 0.2189 - val_loss: 0.1070 - val_mae: 0.2451\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1945 - mae: 0.2206\n",
            "Epoch 101: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1946 - mae: 0.2205 - val_loss: 0.0800 - val_mae: 0.2102\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1957 - mae: 0.2244\n",
            "Epoch 102: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1961 - mae: 0.2246 - val_loss: 0.0656 - val_mae: 0.1290\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.1980\n",
            "Epoch 103: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1739 - mae: 0.1978 - val_loss: 0.0548 - val_mae: 0.1115\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1852 - mae: 0.2176\n",
            "Epoch 104: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1850 - mae: 0.2179 - val_loss: 0.0591 - val_mae: 0.1041\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1900 - mae: 0.2245\n",
            "Epoch 105: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1901 - mae: 0.2244 - val_loss: 0.0580 - val_mae: 0.1414\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.2010\n",
            "Epoch 106: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1723 - mae: 0.2009 - val_loss: 0.0624 - val_mae: 0.1341\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.2027\n",
            "Epoch 107: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1750 - mae: 0.2025 - val_loss: 0.0641 - val_mae: 0.1227\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1696 - mae: 0.2057\n",
            "Epoch 108: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1701 - mae: 0.2064 - val_loss: 0.0608 - val_mae: 0.1613\n",
            "Epoch 109/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1663 - mae: 0.1995\n",
            "Epoch 109: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1658 - mae: 0.1992 - val_loss: 0.0533 - val_mae: 0.1048\n",
            "Epoch 110/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3618 - mae: 0.3384\n",
            "Epoch 110: val_loss did not improve from 0.05089\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3609 - mae: 0.3380 - val_loss: 0.0688 - val_mae: 0.1214\n",
            "Epoch 110: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 4s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07227037847042084, RMSE:0.2688314914703369, MAE:0.11494307219982147, R2:0.9209671109789732\n",
            "9 0.9209671109789732\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07523752748966217, RMSE:0.27429458498954773, MAE:0.11420423537492752, R2:0.9162421799026989\n",
            "r2: 0.9162421799026989 rmse: 0.27429458\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07523752748966217, RMSE:0.27429458498954773, MAE:0.11420423537492752, R2:0.9162421799026989\n",
            "r2: 0.9162421799026989 rmse: 0.27429458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_CNN_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB3Bjdck0V_B",
        "outputId": "760055b1-fcad-488c-dc17-64414d5ab81d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9170526773382071,\n",
              " 0.910960730842986,\n",
              " 0.922691272816625,\n",
              " 0.9197774991282821,\n",
              " 0.9159260300487481,\n",
              " 0.9190478082128568,\n",
              " 0.9200917263247873,\n",
              " 0.921309073588352,\n",
              " 0.9205265706080363,\n",
              " 0.9209671109789732]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_CNN_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEnavUz9g96E",
        "outputId": "3634a394-1563-49a8-f0f0-3fc3be528273"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9096758243606976,\n",
              " 0.9069428181583941,\n",
              " 0.9197171100459803,\n",
              " 0.916395433172939,\n",
              " 0.9136934311893576,\n",
              " 0.9185873098366197,\n",
              " 0.918008913017388,\n",
              " 0.9212146105790974,\n",
              " 0.9173297577883059,\n",
              " 0.9162421799026989]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **没有时间间隔**"
      ],
      "metadata": {
        "id": "MpfYayx6mTFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=============================== 第四部分 没有时间间隔——开始预测 =====================================================\n",
        "r2_CNN_val=[]\n",
        "r2_CNN_test=[]\n",
        "for i in range(10):\n",
        "  print('第%s次训练'%i)\n",
        "  \n",
        "  modelpath = '没有时间间隔_5月16日(下午)_'+'第%s次训练_'%i+'MG_merge' + path # %i\n",
        "  #定义模型\n",
        "  model= ATTENTION_multi_channel_split_model(demgras_dim) \n",
        "  #划分数据\n",
        "  X1_train_1,X1_val_1, demographics_train_1,demographics_val_1,y_train_1,y_val_1=train_test_split( X1_train, demographics_train, y_train, test_size=0.1, random_state=90) \n",
        "  if not args.test:\n",
        "      train_ATTENTION_multi_channel_split_model(model, modelpath, X1_train_1, demographics_train_1, y_train_1)\n",
        "  extract_patientvec(model, modelpath, disease, demographics)\n",
        "  #拟合、预测\n",
        "  y_CNN_hat = model.predict([X1_val_1,demographics_val_1])\n",
        "\n",
        "  r2,rmse = evaluation(y_val_1, y_CNN_hat) #验证集的拟合优度\n",
        "  R21=r2_score(y_val_1, y_CNN_hat)\n",
        "  print(i,R21)\n",
        "  r2_CNN_val.append(R21)\n",
        "  \n",
        "  test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
        "  model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "  y_CNN_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "  r22,rmse = evaluation(y_test, y_CNN_pred)\n",
        "  print(\"r2:\",r22,\"rmse:\",rmse)\n",
        "  r2_CNN_test.append(r22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbl84-EhmYcl",
        "outputId": "173252a5-d586-4bda-c072-bb6983e9ce03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第0次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_3/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_3'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_12 (Embedding)       (None, 17, 900)      649800      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_13 (Embedding)       (None, 17, 900)      649800      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_14 (Embedding)       (None, 17, 900)      649800      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_15 (Embedding)       (None, 17, 900)      649800      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 17, 100)      270100      ['embedding_12[0][0]',           \n",
            "                                                                  'embedding_13[0][0]',           \n",
            "                                                                  'embedding_14[0][0]',           \n",
            "                                                                  'embedding_15[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 17, 100)      270100      ['embedding_12[0][0]',           \n",
            "                                                                  'embedding_13[0][0]',           \n",
            "                                                                  'embedding_14[0][0]',           \n",
            "                                                                  'embedding_15[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 17, 100)      270100      ['embedding_12[0][0]',           \n",
            "                                                                  'embedding_13[0][0]',           \n",
            "                                                                  'embedding_14[0][0]',           \n",
            "                                                                  'embedding_15[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_36 (Globa  (None, 100)         0           ['conv1d_9[0][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_37 (Globa  (None, 100)         0           ['conv1d_9[1][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_38 (Globa  (None, 100)         0           ['conv1d_9[2][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_39 (Globa  (None, 100)         0           ['conv1d_9[3][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_40 (Globa  (None, 100)         0           ['conv1d_10[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_41 (Globa  (None, 100)         0           ['conv1d_10[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_42 (Globa  (None, 100)         0           ['conv1d_10[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_43 (Globa  (None, 100)         0           ['conv1d_10[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_44 (Globa  (None, 100)         0           ['conv1d_11[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_45 (Globa  (None, 100)         0           ['conv1d_11[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_46 (Globa  (None, 100)         0           ['conv1d_11[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_47 (Globa  (None, 100)         0           ['conv1d_11[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 100)          0           ['global_max_pooling1d_36[0][0]',\n",
            "                                                                  'global_max_pooling1d_37[0][0]',\n",
            "                                                                  'global_max_pooling1d_38[0][0]',\n",
            "                                                                  'global_max_pooling1d_39[0][0]']\n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 100)          0           ['global_max_pooling1d_40[0][0]',\n",
            "                                                                  'global_max_pooling1d_41[0][0]',\n",
            "                                                                  'global_max_pooling1d_42[0][0]',\n",
            "                                                                  'global_max_pooling1d_43[0][0]']\n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 100)          0           ['global_max_pooling1d_44[0][0]',\n",
            "                                                                  'global_max_pooling1d_45[0][0]',\n",
            "                                                                  'global_max_pooling1d_46[0][0]',\n",
            "                                                                  'global_max_pooling1d_47[0][0]']\n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 1000)         11000       ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 1300)         0           ['add_9[0][0]',                  \n",
            "                                                                  'add_10[0][0]',                 \n",
            "                                                                  'add_11[0][0]',                 \n",
            "                                                                  'dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_3 (Gl  (1, 1300)           0           ['tf.expand_dims_3[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (1, 1, 1, 650)       845650      ['reshape_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_3 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_3[0][0]']           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " tf.math.sigmoid_3 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_3[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_3[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_3 (TFOpLa  (None, 1300)        0           ['multiply_3[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_3[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 1)            3           ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.3237 - mae: 0.9572\n",
            "Epoch 1: val_loss improved from inf to 0.59240, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 9s 53ms/step - loss: 2.3175 - mae: 0.9550 - val_loss: 0.5924 - val_mae: 0.3535\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7485 - mae: 0.5289\n",
            "Epoch 2: val_loss improved from 0.59240 to 0.53079, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 48ms/step - loss: 0.7483 - mae: 0.5283 - val_loss: 0.5308 - val_mae: 0.4220\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7072 - mae: 0.5242\n",
            "Epoch 3: val_loss improved from 0.53079 to 0.41912, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.7072 - mae: 0.5242 - val_loss: 0.4191 - val_mae: 0.3661\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6431 - mae: 0.4828\n",
            "Epoch 4: val_loss did not improve from 0.41912\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6419 - mae: 0.4823 - val_loss: 0.4593 - val_mae: 0.3852\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6415 - mae: 0.4936\n",
            "Epoch 5: val_loss improved from 0.41912 to 0.37819, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.6410 - mae: 0.4932 - val_loss: 0.3782 - val_mae: 0.3302\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6116 - mae: 0.4770\n",
            "Epoch 6: val_loss did not improve from 0.37819\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6126 - mae: 0.4777 - val_loss: 1.0646 - val_mae: 0.9621\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6378 - mae: 0.4856\n",
            "Epoch 7: val_loss did not improve from 0.37819\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6364 - mae: 0.4848 - val_loss: 0.5439 - val_mae: 0.4897\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5832 - mae: 0.4644\n",
            "Epoch 8: val_loss improved from 0.37819 to 0.34761, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.5826 - mae: 0.4642 - val_loss: 0.3476 - val_mae: 0.3157\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5873 - mae: 0.4517\n",
            "Epoch 9: val_loss improved from 0.34761 to 0.34414, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5867 - mae: 0.4513 - val_loss: 0.3441 - val_mae: 0.2972\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5697 - mae: 0.4549\n",
            "Epoch 10: val_loss improved from 0.34414 to 0.32077, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5703 - mae: 0.4555 - val_loss: 0.3208 - val_mae: 0.3191\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5464 - mae: 0.4324\n",
            "Epoch 11: val_loss improved from 0.32077 to 0.31587, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5462 - mae: 0.4322 - val_loss: 0.3159 - val_mae: 0.2954\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5092 - mae: 0.4186\n",
            "Epoch 12: val_loss improved from 0.31587 to 0.27806, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5088 - mae: 0.4188 - val_loss: 0.2781 - val_mae: 0.3154\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4702 - mae: 0.4111\n",
            "Epoch 13: val_loss improved from 0.27806 to 0.22433, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4704 - mae: 0.4111 - val_loss: 0.2243 - val_mae: 0.2587\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4319 - mae: 0.3886\n",
            "Epoch 14: val_loss did not improve from 0.22433\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4315 - mae: 0.3884 - val_loss: 0.2347 - val_mae: 0.2980\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3687 - mae: 0.3428\n",
            "Epoch 15: val_loss did not improve from 0.22433\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3694 - mae: 0.3429 - val_loss: 0.3215 - val_mae: 0.4632\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3914 - mae: 0.3621\n",
            "Epoch 16: val_loss improved from 0.22433 to 0.18842, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.3909 - mae: 0.3618 - val_loss: 0.1884 - val_mae: 0.3087\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3332 - mae: 0.3253\n",
            "Epoch 17: val_loss improved from 0.18842 to 0.11074, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3328 - mae: 0.3251 - val_loss: 0.1107 - val_mae: 0.1747\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3404 - mae: 0.3323\n",
            "Epoch 18: val_loss did not improve from 0.11074\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3401 - mae: 0.3321 - val_loss: 0.2043 - val_mae: 0.3674\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3199 - mae: 0.3196\n",
            "Epoch 19: val_loss improved from 0.11074 to 0.09465, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3210 - mae: 0.3197 - val_loss: 0.0947 - val_mae: 0.1551\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3007 - mae: 0.3046\n",
            "Epoch 20: val_loss did not improve from 0.09465\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2999 - mae: 0.3040 - val_loss: 0.1213 - val_mae: 0.2195\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3379 - mae: 0.3300\n",
            "Epoch 21: val_loss did not improve from 0.09465\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3399 - mae: 0.3305 - val_loss: 0.1273 - val_mae: 0.2639\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3541 - mae: 0.3433\n",
            "Epoch 22: val_loss did not improve from 0.09465\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3532 - mae: 0.3427 - val_loss: 0.1991 - val_mae: 0.3907\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2885 - mae: 0.2779\n",
            "Epoch 23: val_loss improved from 0.09465 to 0.06362, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2909 - mae: 0.2784 - val_loss: 0.0636 - val_mae: 0.1103\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3202 - mae: 0.3224\n",
            "Epoch 24: val_loss did not improve from 0.06362\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3196 - mae: 0.3225 - val_loss: 0.0806 - val_mae: 0.1774\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2788 - mae: 0.2848\n",
            "Epoch 25: val_loss did not improve from 0.06362\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2793 - mae: 0.2849 - val_loss: 0.0747 - val_mae: 0.1367\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3235 - mae: 0.2983\n",
            "Epoch 26: val_loss did not improve from 0.06362\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3231 - mae: 0.2982 - val_loss: 0.0759 - val_mae: 0.1593\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2678 - mae: 0.2809\n",
            "Epoch 27: val_loss did not improve from 0.06362\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2672 - mae: 0.2807 - val_loss: 0.0675 - val_mae: 0.1361\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2998 - mae: 0.2925\n",
            "Epoch 28: val_loss improved from 0.06362 to 0.06295, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3005 - mae: 0.2927 - val_loss: 0.0629 - val_mae: 0.1188\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3540 - mae: 0.3467\n",
            "Epoch 29: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3538 - mae: 0.3464 - val_loss: 0.1506 - val_mae: 0.2775\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3017 - mae: 0.2971\n",
            "Epoch 30: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3045 - mae: 0.2974 - val_loss: 0.0689 - val_mae: 0.1148\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3000 - mae: 0.3029\n",
            "Epoch 31: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3004 - mae: 0.3029 - val_loss: 0.1034 - val_mae: 0.2191\n",
            "Epoch 32/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2721 - mae: 0.2742\n",
            "Epoch 32: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2721 - mae: 0.2742 - val_loss: 0.1074 - val_mae: 0.2534\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4804 - mae: 0.4374\n",
            "Epoch 33: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4795 - mae: 0.4372 - val_loss: 0.1168 - val_mae: 0.2118\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2647 - mae: 0.2663\n",
            "Epoch 34: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2642 - mae: 0.2661 - val_loss: 0.1186 - val_mae: 0.2699\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2795 - mae: 0.2845\n",
            "Epoch 35: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2802 - mae: 0.2855 - val_loss: 0.0942 - val_mae: 0.2494\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2800 - mae: 0.2779\n",
            "Epoch 36: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2795 - mae: 0.2778 - val_loss: 0.0756 - val_mae: 0.2120\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2957 - mae: 0.3045\n",
            "Epoch 37: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2951 - mae: 0.3044 - val_loss: 0.1382 - val_mae: 0.2628\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2686 - mae: 0.2762\n",
            "Epoch 38: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2677 - mae: 0.2756 - val_loss: 0.0680 - val_mae: 0.1268\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2737 - mae: 0.2818\n",
            "Epoch 39: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2736 - mae: 0.2817 - val_loss: 0.1277 - val_mae: 0.2744\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3092 - mae: 0.3135\n",
            "Epoch 40: val_loss did not improve from 0.06295\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3083 - mae: 0.3130 - val_loss: 0.0778 - val_mae: 0.1270\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2414 - mae: 0.2578\n",
            "Epoch 41: val_loss improved from 0.06295 to 0.06106, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2419 - mae: 0.2580 - val_loss: 0.0611 - val_mae: 0.1282\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2752 - mae: 0.2765\n",
            "Epoch 42: val_loss did not improve from 0.06106\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2753 - mae: 0.2765 - val_loss: 0.0684 - val_mae: 0.1466\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2584 - mae: 0.2677\n",
            "Epoch 43: val_loss improved from 0.06106 to 0.05827, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2587 - mae: 0.2680 - val_loss: 0.0583 - val_mae: 0.1096\n",
            "Epoch 44/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2657 - mae: 0.2779\n",
            "Epoch 44: val_loss did not improve from 0.05827\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2657 - mae: 0.2779 - val_loss: 0.0689 - val_mae: 0.1772\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2476 - mae: 0.2567\n",
            "Epoch 45: val_loss improved from 0.05827 to 0.05666, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2469 - mae: 0.2565 - val_loss: 0.0567 - val_mae: 0.1057\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2860 - mae: 0.3101\n",
            "Epoch 46: val_loss did not improve from 0.05666\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2866 - mae: 0.3105 - val_loss: 0.0951 - val_mae: 0.1574\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2583 - mae: 0.2684\n",
            "Epoch 47: val_loss did not improve from 0.05666\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2580 - mae: 0.2683 - val_loss: 0.0746 - val_mae: 0.1759\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2682 - mae: 0.2792\n",
            "Epoch 48: val_loss did not improve from 0.05666\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2693 - mae: 0.2794 - val_loss: 0.0966 - val_mae: 0.2228\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2276 - mae: 0.2451\n",
            "Epoch 49: val_loss did not improve from 0.05666\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2277 - mae: 0.2451 - val_loss: 0.0682 - val_mae: 0.1509\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2550 - mae: 0.2769\n",
            "Epoch 50: val_loss did not improve from 0.05666\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2543 - mae: 0.2764 - val_loss: 0.0569 - val_mae: 0.1162\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2515 - mae: 0.2609\n",
            "Epoch 51: val_loss improved from 0.05666 to 0.05595, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第0次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2529 - mae: 0.2614 - val_loss: 0.0559 - val_mae: 0.0961\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2347 - mae: 0.2554\n",
            "Epoch 52: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2340 - mae: 0.2549 - val_loss: 0.0769 - val_mae: 0.1763\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2507 - mae: 0.2600\n",
            "Epoch 53: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2500 - mae: 0.2597 - val_loss: 0.1315 - val_mae: 0.3077\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2520 - mae: 0.2859\n",
            "Epoch 54: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2529 - mae: 0.2859 - val_loss: 0.1462 - val_mae: 0.3177\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2474 - mae: 0.2588\n",
            "Epoch 55: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2474 - mae: 0.2587 - val_loss: 0.0796 - val_mae: 0.1836\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3161 - mae: 0.3221\n",
            "Epoch 56: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3153 - mae: 0.3220 - val_loss: 0.1098 - val_mae: 0.2744\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2299 - mae: 0.2408\n",
            "Epoch 57: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2292 - mae: 0.2405 - val_loss: 0.0595 - val_mae: 0.1109\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2394 - mae: 0.2459\n",
            "Epoch 58: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2390 - mae: 0.2457 - val_loss: 0.0895 - val_mae: 0.2152\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2418 - mae: 0.2472\n",
            "Epoch 59: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2417 - mae: 0.2472 - val_loss: 0.0649 - val_mae: 0.1394\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2259 - mae: 0.2416\n",
            "Epoch 60: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2259 - mae: 0.2415 - val_loss: 0.1020 - val_mae: 0.2553\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2433 - mae: 0.2543\n",
            "Epoch 61: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2441 - mae: 0.2549 - val_loss: 0.0634 - val_mae: 0.1182\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2869 - mae: 0.3048\n",
            "Epoch 62: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2867 - mae: 0.3048 - val_loss: 0.0931 - val_mae: 0.2408\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3031 - mae: 0.2928\n",
            "Epoch 63: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3024 - mae: 0.2925 - val_loss: 0.1193 - val_mae: 0.2844\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2341 - mae: 0.2426\n",
            "Epoch 64: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2336 - mae: 0.2426 - val_loss: 0.0803 - val_mae: 0.2047\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2375 - mae: 0.2412\n",
            "Epoch 65: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2375 - mae: 0.2413 - val_loss: 0.0829 - val_mae: 0.1978\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2306 - mae: 0.2409\n",
            "Epoch 66: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2300 - mae: 0.2406 - val_loss: 0.0584 - val_mae: 0.1125\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2913 - mae: 0.3246\n",
            "Epoch 67: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2905 - mae: 0.3240 - val_loss: 0.0983 - val_mae: 0.2127\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2022 - mae: 0.2263\n",
            "Epoch 68: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2018 - mae: 0.2261 - val_loss: 0.1136 - val_mae: 0.2719\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2128 - mae: 0.2319\n",
            "Epoch 69: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2129 - mae: 0.2320 - val_loss: 0.0658 - val_mae: 0.0952\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2356 - mae: 0.2412\n",
            "Epoch 70: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2359 - mae: 0.2414 - val_loss: 0.0653 - val_mae: 0.1130\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2086 - mae: 0.2299\n",
            "Epoch 71: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2080 - mae: 0.2296 - val_loss: 0.0684 - val_mae: 0.1131\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2146 - mae: 0.2400\n",
            "Epoch 72: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2146 - mae: 0.2401 - val_loss: 0.0828 - val_mae: 0.1825\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2506 - mae: 0.2857\n",
            "Epoch 73: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2499 - mae: 0.2853 - val_loss: 0.0795 - val_mae: 0.1922\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2306 - mae: 0.2394\n",
            "Epoch 74: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2311 - mae: 0.2396 - val_loss: 0.0572 - val_mae: 0.1373\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2111 - mae: 0.2341\n",
            "Epoch 75: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2115 - mae: 0.2343 - val_loss: 0.0764 - val_mae: 0.1794\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6644 - mae: 0.4865\n",
            "Epoch 76: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6634 - mae: 0.4861 - val_loss: 0.4739 - val_mae: 0.3003\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5350 - mae: 0.4113\n",
            "Epoch 77: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.5339 - mae: 0.4112 - val_loss: 0.2815 - val_mae: 0.3427\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3275 - mae: 0.3256\n",
            "Epoch 78: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3272 - mae: 0.3256 - val_loss: 0.0930 - val_mae: 0.1615\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2329 - mae: 0.2678\n",
            "Epoch 79: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2350 - mae: 0.2684 - val_loss: 0.0756 - val_mae: 0.1777\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2241 - mae: 0.2489\n",
            "Epoch 80: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2237 - mae: 0.2487 - val_loss: 0.0611 - val_mae: 0.1091\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2109 - mae: 0.2365\n",
            "Epoch 81: val_loss did not improve from 0.05595\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2108 - mae: 0.2364 - val_loss: 0.0686 - val_mae: 0.1550\n",
            "Epoch 81: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07909499108791351, RMSE:0.281238317489624, MAE:0.11872976273298264, R2:0.9135039079983848\n",
            "0 0.9135039079983848\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08110012114048004, RMSE:0.2847808301448822, MAE:0.1184941902756691, R2:0.9097156799067575\n",
            "r2: 0.9097156799067575 rmse: 0.28478083\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08110012114048004, RMSE:0.2847808301448822, MAE:0.1184941902756691, R2:0.9097156799067575\n",
            "r2: 0.9097156799067575 rmse: 0.28478083\n",
            "第1次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_4/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_4'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_16 (Embedding)       (None, 17, 900)      649800      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_17 (Embedding)       (None, 17, 900)      649800      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_18 (Embedding)       (None, 17, 900)      649800      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_19 (Embedding)       (None, 17, 900)      649800      ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 17, 100)      270100      ['embedding_16[0][0]',           \n",
            "                                                                  'embedding_17[0][0]',           \n",
            "                                                                  'embedding_18[0][0]',           \n",
            "                                                                  'embedding_19[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 17, 100)      270100      ['embedding_16[0][0]',           \n",
            "                                                                  'embedding_17[0][0]',           \n",
            "                                                                  'embedding_18[0][0]',           \n",
            "                                                                  'embedding_19[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 17, 100)      270100      ['embedding_16[0][0]',           \n",
            "                                                                  'embedding_17[0][0]',           \n",
            "                                                                  'embedding_18[0][0]',           \n",
            "                                                                  'embedding_19[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_48 (Globa  (None, 100)         0           ['conv1d_12[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_49 (Globa  (None, 100)         0           ['conv1d_12[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_50 (Globa  (None, 100)         0           ['conv1d_12[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_51 (Globa  (None, 100)         0           ['conv1d_12[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_52 (Globa  (None, 100)         0           ['conv1d_13[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_53 (Globa  (None, 100)         0           ['conv1d_13[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_54 (Globa  (None, 100)         0           ['conv1d_13[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_55 (Globa  (None, 100)         0           ['conv1d_13[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_56 (Globa  (None, 100)         0           ['conv1d_14[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_57 (Globa  (None, 100)         0           ['conv1d_14[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_58 (Globa  (None, 100)         0           ['conv1d_14[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_59 (Globa  (None, 100)         0           ['conv1d_14[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 100)          0           ['global_max_pooling1d_48[0][0]',\n",
            "                                                                  'global_max_pooling1d_49[0][0]',\n",
            "                                                                  'global_max_pooling1d_50[0][0]',\n",
            "                                                                  'global_max_pooling1d_51[0][0]']\n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 100)          0           ['global_max_pooling1d_52[0][0]',\n",
            "                                                                  'global_max_pooling1d_53[0][0]',\n",
            "                                                                  'global_max_pooling1d_54[0][0]',\n",
            "                                                                  'global_max_pooling1d_55[0][0]']\n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 100)          0           ['global_max_pooling1d_56[0][0]',\n",
            "                                                                  'global_max_pooling1d_57[0][0]',\n",
            "                                                                  'global_max_pooling1d_58[0][0]',\n",
            "                                                                  'global_max_pooling1d_59[0][0]']\n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 1000)         11000       ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 1300)         0           ['add_12[0][0]',                 \n",
            "                                                                  'add_13[0][0]',                 \n",
            "                                                                  'add_14[0][0]',                 \n",
            "                                                                  'dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_4 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_4 (Gl  (1, 1300)           0           ['tf.expand_dims_4[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (1, 1, 1, 650)       845650      ['reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_4 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_4[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_4 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_4[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_4[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_4 (TFOpLa  (None, 1300)        0           ['multiply_4[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_4[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_20[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_22[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 1)            3           ['dropout_24[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 2.2358 - mae: 0.9007\n",
            "Epoch 1: val_loss improved from inf to 0.54128, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 2.2358 - mae: 0.9007 - val_loss: 0.5413 - val_mae: 0.4268\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7531 - mae: 0.5227\n",
            "Epoch 2: val_loss improved from 0.54128 to 0.47191, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.7527 - mae: 0.5225 - val_loss: 0.4719 - val_mae: 0.5090\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6702 - mae: 0.4883\n",
            "Epoch 3: val_loss improved from 0.47191 to 0.46704, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6692 - mae: 0.4877 - val_loss: 0.4670 - val_mae: 0.4826\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6459 - mae: 0.4877\n",
            "Epoch 4: val_loss improved from 0.46704 to 0.45545, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6468 - mae: 0.4878 - val_loss: 0.4555 - val_mae: 0.4558\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6573 - mae: 0.5173\n",
            "Epoch 5: val_loss improved from 0.45545 to 0.39433, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6570 - mae: 0.5170 - val_loss: 0.3943 - val_mae: 0.3633\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6404 - mae: 0.4774\n",
            "Epoch 6: val_loss improved from 0.39433 to 0.38743, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6395 - mae: 0.4772 - val_loss: 0.3874 - val_mae: 0.3376\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6382 - mae: 0.4878\n",
            "Epoch 7: val_loss improved from 0.38743 to 0.38447, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6367 - mae: 0.4873 - val_loss: 0.3845 - val_mae: 0.3226\n",
            "Epoch 8/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.5785 - mae: 0.4408\n",
            "Epoch 8: val_loss did not improve from 0.38447\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5785 - mae: 0.4408 - val_loss: 0.4074 - val_mae: 0.3776\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5520 - mae: 0.4372\n",
            "Epoch 9: val_loss did not improve from 0.38447\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5514 - mae: 0.4373 - val_loss: 0.3924 - val_mae: 0.4369\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5815 - mae: 0.4586\n",
            "Epoch 10: val_loss did not improve from 0.38447\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5824 - mae: 0.4596 - val_loss: 0.4834 - val_mae: 0.5448\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5664 - mae: 0.4512\n",
            "Epoch 11: val_loss did not improve from 0.38447\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.5661 - mae: 0.4512 - val_loss: 0.3882 - val_mae: 0.4304\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5362 - mae: 0.4461\n",
            "Epoch 12: val_loss improved from 0.38447 to 0.29734, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.5357 - mae: 0.4459 - val_loss: 0.2973 - val_mae: 0.2897\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4855 - mae: 0.4080\n",
            "Epoch 13: val_loss improved from 0.29734 to 0.22610, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4861 - mae: 0.4083 - val_loss: 0.2261 - val_mae: 0.2228\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4253 - mae: 0.3776\n",
            "Epoch 14: val_loss did not improve from 0.22610\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4270 - mae: 0.3781 - val_loss: 0.2693 - val_mae: 0.3543\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3927 - mae: 0.3711\n",
            "Epoch 15: val_loss improved from 0.22610 to 0.20553, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3923 - mae: 0.3708 - val_loss: 0.2055 - val_mae: 0.2342\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3808 - mae: 0.3517\n",
            "Epoch 16: val_loss did not improve from 0.20553\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3804 - mae: 0.3514 - val_loss: 0.2285 - val_mae: 0.3353\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3745 - mae: 0.3476\n",
            "Epoch 17: val_loss improved from 0.20553 to 0.12590, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.3745 - mae: 0.3473 - val_loss: 0.1259 - val_mae: 0.2423\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3519 - mae: 0.3299\n",
            "Epoch 18: val_loss did not improve from 0.12590\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3514 - mae: 0.3300 - val_loss: 0.3053 - val_mae: 0.4794\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3216 - mae: 0.3217\n",
            "Epoch 19: val_loss did not improve from 0.12590\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3223 - mae: 0.3220 - val_loss: 0.1638 - val_mae: 0.2888\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2892 - mae: 0.2907\n",
            "Epoch 20: val_loss improved from 0.12590 to 0.10435, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2901 - mae: 0.2909 - val_loss: 0.1044 - val_mae: 0.2530\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3233 - mae: 0.3065\n",
            "Epoch 21: val_loss improved from 0.10435 to 0.07565, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.3228 - mae: 0.3061 - val_loss: 0.0757 - val_mae: 0.1269\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3120 - mae: 0.3068\n",
            "Epoch 22: val_loss did not improve from 0.07565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3123 - mae: 0.3066 - val_loss: 0.0909 - val_mae: 0.1467\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3382 - mae: 0.3214\n",
            "Epoch 23: val_loss did not improve from 0.07565\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3372 - mae: 0.3208 - val_loss: 0.0958 - val_mae: 0.1779\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2863 - mae: 0.2958\n",
            "Epoch 24: val_loss did not improve from 0.07565\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2862 - mae: 0.2957 - val_loss: 0.1154 - val_mae: 0.2457\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3314 - mae: 0.3151\n",
            "Epoch 25: val_loss improved from 0.07565 to 0.07275, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3316 - mae: 0.3151 - val_loss: 0.0727 - val_mae: 0.1688\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2721 - mae: 0.2686\n",
            "Epoch 26: val_loss did not improve from 0.07275\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2715 - mae: 0.2683 - val_loss: 0.0933 - val_mae: 0.2154\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4912 - mae: 0.4157\n",
            "Epoch 27: val_loss did not improve from 0.07275\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4909 - mae: 0.4158 - val_loss: 0.2174 - val_mae: 0.3506\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3080 - mae: 0.3002\n",
            "Epoch 28: val_loss improved from 0.07275 to 0.07113, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.3074 - mae: 0.2999 - val_loss: 0.0711 - val_mae: 0.1296\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3028 - mae: 0.2989\n",
            "Epoch 29: val_loss improved from 0.07113 to 0.06478, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.3032 - mae: 0.2993 - val_loss: 0.0648 - val_mae: 0.1357\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2891 - mae: 0.3103\n",
            "Epoch 30: val_loss did not improve from 0.06478\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2890 - mae: 0.3101 - val_loss: 0.1000 - val_mae: 0.2376\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3201 - mae: 0.3138\n",
            "Epoch 31: val_loss improved from 0.06478 to 0.06130, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3204 - mae: 0.3139 - val_loss: 0.0613 - val_mae: 0.1193\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2641 - mae: 0.2670\n",
            "Epoch 32: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2640 - mae: 0.2669 - val_loss: 0.0673 - val_mae: 0.1217\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2652 - mae: 0.2635\n",
            "Epoch 33: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2645 - mae: 0.2632 - val_loss: 0.0761 - val_mae: 0.1710\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3373 - mae: 0.3382\n",
            "Epoch 34: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3367 - mae: 0.3381 - val_loss: 0.2029 - val_mae: 0.2751\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3102 - mae: 0.3018\n",
            "Epoch 35: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3105 - mae: 0.3018 - val_loss: 0.0681 - val_mae: 0.1370\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2624 - mae: 0.2553\n",
            "Epoch 36: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2620 - mae: 0.2552 - val_loss: 0.0833 - val_mae: 0.1918\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2814 - mae: 0.2974\n",
            "Epoch 37: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2806 - mae: 0.2969 - val_loss: 0.0905 - val_mae: 0.2192\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2885 - mae: 0.2824\n",
            "Epoch 38: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2892 - mae: 0.2827 - val_loss: 0.1165 - val_mae: 0.2384\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2648 - mae: 0.2641\n",
            "Epoch 39: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2657 - mae: 0.2644 - val_loss: 0.0753 - val_mae: 0.1874\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2744 - mae: 0.2632\n",
            "Epoch 40: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2735 - mae: 0.2626 - val_loss: 0.1135 - val_mae: 0.2491\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2626 - mae: 0.2610\n",
            "Epoch 41: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2623 - mae: 0.2610 - val_loss: 0.0955 - val_mae: 0.1565\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2626 - mae: 0.2695\n",
            "Epoch 42: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2630 - mae: 0.2695 - val_loss: 0.0858 - val_mae: 0.1828\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2578 - mae: 0.2591\n",
            "Epoch 43: val_loss did not improve from 0.06130\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2571 - mae: 0.2587 - val_loss: 0.0649 - val_mae: 0.1413\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2372 - mae: 0.2563\n",
            "Epoch 44: val_loss improved from 0.06130 to 0.05799, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2367 - mae: 0.2564 - val_loss: 0.0580 - val_mae: 0.1118\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2236 - mae: 0.2467\n",
            "Epoch 45: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2229 - mae: 0.2463 - val_loss: 0.0655 - val_mae: 0.1420\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2381 - mae: 0.2606\n",
            "Epoch 46: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2376 - mae: 0.2602 - val_loss: 0.0593 - val_mae: 0.0978\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2501 - mae: 0.2564\n",
            "Epoch 47: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2504 - mae: 0.2565 - val_loss: 0.0939 - val_mae: 0.2323\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2447 - mae: 0.2660\n",
            "Epoch 48: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2456 - mae: 0.2662 - val_loss: 0.0658 - val_mae: 0.1268\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2460 - mae: 0.2516\n",
            "Epoch 49: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2452 - mae: 0.2512 - val_loss: 0.0908 - val_mae: 0.1607\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2354 - mae: 0.2565\n",
            "Epoch 50: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2354 - mae: 0.2563 - val_loss: 0.0830 - val_mae: 0.2146\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3248 - mae: 0.3224\n",
            "Epoch 51: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3268 - mae: 0.3232 - val_loss: 0.2183 - val_mae: 0.1953\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2586 - mae: 0.2622\n",
            "Epoch 52: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2589 - mae: 0.2622 - val_loss: 0.0648 - val_mae: 0.1141\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2258 - mae: 0.2356\n",
            "Epoch 53: val_loss did not improve from 0.05799\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2264 - mae: 0.2358 - val_loss: 0.0624 - val_mae: 0.1579\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2219 - mae: 0.2366\n",
            "Epoch 54: val_loss improved from 0.05799 to 0.05727, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2224 - mae: 0.2369 - val_loss: 0.0573 - val_mae: 0.0955\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2794 - mae: 0.2944\n",
            "Epoch 55: val_loss improved from 0.05727 to 0.05523, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第1次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2786 - mae: 0.2939 - val_loss: 0.0552 - val_mae: 0.1024\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2575 - mae: 0.2426\n",
            "Epoch 56: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2582 - mae: 0.2429 - val_loss: 0.0604 - val_mae: 0.1082\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2202 - mae: 0.2397\n",
            "Epoch 57: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2201 - mae: 0.2398 - val_loss: 0.0955 - val_mae: 0.2290\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2191 - mae: 0.2310\n",
            "Epoch 58: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2189 - mae: 0.2314 - val_loss: 0.0578 - val_mae: 0.1333\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2771 - mae: 0.3049\n",
            "Epoch 59: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2782 - mae: 0.3049 - val_loss: 0.0750 - val_mae: 0.1818\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2260 - mae: 0.2283\n",
            "Epoch 60: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2261 - mae: 0.2288 - val_loss: 0.0600 - val_mae: 0.1240\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2386 - mae: 0.2463\n",
            "Epoch 61: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2380 - mae: 0.2461 - val_loss: 0.1994 - val_mae: 0.3653\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2650 - mae: 0.2826\n",
            "Epoch 62: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2647 - mae: 0.2828 - val_loss: 0.0868 - val_mae: 0.1319\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2372 - mae: 0.2651\n",
            "Epoch 63: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2365 - mae: 0.2647 - val_loss: 0.0736 - val_mae: 0.1811\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2258 - mae: 0.2438\n",
            "Epoch 64: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2253 - mae: 0.2438 - val_loss: 0.0704 - val_mae: 0.1192\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2150 - mae: 0.2338\n",
            "Epoch 65: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2152 - mae: 0.2338 - val_loss: 0.1094 - val_mae: 0.2372\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2874 - mae: 0.2905\n",
            "Epoch 66: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2876 - mae: 0.2912 - val_loss: 0.8107 - val_mae: 0.8274\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6209 - mae: 0.4638\n",
            "Epoch 67: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.6194 - mae: 0.4631 - val_loss: 0.3365 - val_mae: 0.2838\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2683 - mae: 0.2769\n",
            "Epoch 68: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2687 - mae: 0.2769 - val_loss: 0.0603 - val_mae: 0.1293\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2200 - mae: 0.2580\n",
            "Epoch 69: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2200 - mae: 0.2581 - val_loss: 0.0609 - val_mae: 0.1338\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2007 - mae: 0.2265\n",
            "Epoch 70: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2011 - mae: 0.2267 - val_loss: 0.0766 - val_mae: 0.1690\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2266 - mae: 0.2487\n",
            "Epoch 71: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2261 - mae: 0.2485 - val_loss: 0.0746 - val_mae: 0.1800\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2117 - mae: 0.2315\n",
            "Epoch 72: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2119 - mae: 0.2315 - val_loss: 0.0697 - val_mae: 0.1539\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2143 - mae: 0.2382\n",
            "Epoch 73: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2160 - mae: 0.2387 - val_loss: 0.0954 - val_mae: 0.2505\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2274 - mae: 0.2470\n",
            "Epoch 74: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2268 - mae: 0.2466 - val_loss: 0.1304 - val_mae: 0.2866\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2283 - mae: 0.2393\n",
            "Epoch 75: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2278 - mae: 0.2392 - val_loss: 0.0779 - val_mae: 0.1867\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2326 - mae: 0.2508\n",
            "Epoch 76: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2321 - mae: 0.2507 - val_loss: 0.0555 - val_mae: 0.1078\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2180 - mae: 0.2345\n",
            "Epoch 77: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2184 - mae: 0.2347 - val_loss: 0.0569 - val_mae: 0.1101\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2071 - mae: 0.2268\n",
            "Epoch 78: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2070 - mae: 0.2268 - val_loss: 0.0777 - val_mae: 0.1877\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2121 - mae: 0.2249\n",
            "Epoch 79: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2119 - mae: 0.2246 - val_loss: 0.0685 - val_mae: 0.1785\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2145 - mae: 0.2467\n",
            "Epoch 80: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2140 - mae: 0.2465 - val_loss: 0.0561 - val_mae: 0.1277\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2015 - mae: 0.2251\n",
            "Epoch 81: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2014 - mae: 0.2251 - val_loss: 0.0629 - val_mae: 0.1600\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2000 - mae: 0.2224\n",
            "Epoch 82: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1995 - mae: 0.2222 - val_loss: 0.0577 - val_mae: 0.1200\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2089 - mae: 0.2378\n",
            "Epoch 83: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2086 - mae: 0.2377 - val_loss: 0.0599 - val_mae: 0.1365\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2130 - mae: 0.2345\n",
            "Epoch 84: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2136 - mae: 0.2348 - val_loss: 0.0778 - val_mae: 0.1929\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1895 - mae: 0.2118\n",
            "Epoch 85: val_loss did not improve from 0.05523\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1903 - mae: 0.2123 - val_loss: 0.0659 - val_mae: 0.1738\n",
            "Epoch 85: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07575326412916183, RMSE:0.27523311972618103, MAE:0.12152895331382751, R2:0.9171583225677296\n",
            "1 0.9171583225677296\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07783698290586472, RMSE:0.27899280190467834, MAE:0.11970201134681702, R2:0.9133483565999173\n",
            "r2: 0.9133483565999173 rmse: 0.2789928\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07783698290586472, RMSE:0.27899280190467834, MAE:0.11970201134681702, R2:0.9133483565999173\n",
            "r2: 0.9133483565999173 rmse: 0.2789928\n",
            "第2次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_5/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_5'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_20 (Embedding)       (None, 17, 900)      649800      ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_21 (Embedding)       (None, 17, 900)      649800      ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_22 (Embedding)       (None, 17, 900)      649800      ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_23 (Embedding)       (None, 17, 900)      649800      ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 17, 100)      270100      ['embedding_20[0][0]',           \n",
            "                                                                  'embedding_21[0][0]',           \n",
            "                                                                  'embedding_22[0][0]',           \n",
            "                                                                  'embedding_23[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 17, 100)      270100      ['embedding_20[0][0]',           \n",
            "                                                                  'embedding_21[0][0]',           \n",
            "                                                                  'embedding_22[0][0]',           \n",
            "                                                                  'embedding_23[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 17, 100)      270100      ['embedding_20[0][0]',           \n",
            "                                                                  'embedding_21[0][0]',           \n",
            "                                                                  'embedding_22[0][0]',           \n",
            "                                                                  'embedding_23[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_60 (Globa  (None, 100)         0           ['conv1d_15[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_61 (Globa  (None, 100)         0           ['conv1d_15[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_62 (Globa  (None, 100)         0           ['conv1d_15[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_63 (Globa  (None, 100)         0           ['conv1d_15[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_64 (Globa  (None, 100)         0           ['conv1d_16[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_65 (Globa  (None, 100)         0           ['conv1d_16[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_66 (Globa  (None, 100)         0           ['conv1d_16[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_67 (Globa  (None, 100)         0           ['conv1d_16[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_68 (Globa  (None, 100)         0           ['conv1d_17[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_69 (Globa  (None, 100)         0           ['conv1d_17[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_70 (Globa  (None, 100)         0           ['conv1d_17[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_71 (Globa  (None, 100)         0           ['conv1d_17[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 100)          0           ['global_max_pooling1d_60[0][0]',\n",
            "                                                                  'global_max_pooling1d_61[0][0]',\n",
            "                                                                  'global_max_pooling1d_62[0][0]',\n",
            "                                                                  'global_max_pooling1d_63[0][0]']\n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 100)          0           ['global_max_pooling1d_64[0][0]',\n",
            "                                                                  'global_max_pooling1d_65[0][0]',\n",
            "                                                                  'global_max_pooling1d_66[0][0]',\n",
            "                                                                  'global_max_pooling1d_67[0][0]']\n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 100)          0           ['global_max_pooling1d_68[0][0]',\n",
            "                                                                  'global_max_pooling1d_69[0][0]',\n",
            "                                                                  'global_max_pooling1d_70[0][0]',\n",
            "                                                                  'global_max_pooling1d_71[0][0]']\n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 1000)         11000       ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 1300)         0           ['add_15[0][0]',                 \n",
            "                                                                  'add_16[0][0]',                 \n",
            "                                                                  'add_17[0][0]',                 \n",
            "                                                                  'dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_5 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_5 (Gl  (1, 1300)           0           ['tf.expand_dims_5[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (1, 1, 1, 650)       845650      ['reshape_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_5 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_5[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_5 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_5[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_5[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_5 (TFOpLa  (None, 1300)        0           ['multiply_5[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_5[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_26[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_27[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_28[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 1)            3           ['dropout_29[0][0]']             \n",
            "                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.6626 - mae: 0.7707\n",
            "Epoch 1: val_loss improved from inf to 0.60055, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 1.6626 - mae: 0.7707 - val_loss: 0.6006 - val_mae: 0.6609\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7636 - mae: 0.5641\n",
            "Epoch 2: val_loss did not improve from 0.60055\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.7640 - mae: 0.5642 - val_loss: 0.6591 - val_mae: 0.5854\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7021 - mae: 0.5259\n",
            "Epoch 3: val_loss improved from 0.60055 to 0.44394, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.7020 - mae: 0.5257 - val_loss: 0.4439 - val_mae: 0.4319\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6638 - mae: 0.5237\n",
            "Epoch 4: val_loss improved from 0.44394 to 0.40737, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6643 - mae: 0.5238 - val_loss: 0.4074 - val_mae: 0.3828\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6370 - mae: 0.4923\n",
            "Epoch 5: val_loss improved from 0.40737 to 0.40214, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6384 - mae: 0.4924 - val_loss: 0.4021 - val_mae: 0.4127\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6179 - mae: 0.4750\n",
            "Epoch 6: val_loss improved from 0.40214 to 0.36482, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6169 - mae: 0.4746 - val_loss: 0.3648 - val_mae: 0.2958\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5781 - mae: 0.4535\n",
            "Epoch 7: val_loss improved from 0.36482 to 0.35897, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5798 - mae: 0.4545 - val_loss: 0.3590 - val_mae: 0.2740\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6088 - mae: 0.4738\n",
            "Epoch 8: val_loss did not improve from 0.35897\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6095 - mae: 0.4738 - val_loss: 0.4872 - val_mae: 0.4818\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5596 - mae: 0.4441\n",
            "Epoch 9: val_loss improved from 0.35897 to 0.31690, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5591 - mae: 0.4439 - val_loss: 0.3169 - val_mae: 0.3111\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5514 - mae: 0.4421\n",
            "Epoch 10: val_loss improved from 0.31690 to 0.30178, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5505 - mae: 0.4416 - val_loss: 0.3018 - val_mae: 0.3627\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5173 - mae: 0.4293\n",
            "Epoch 11: val_loss did not improve from 0.30178\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5182 - mae: 0.4297 - val_loss: 0.8726 - val_mae: 0.8183\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5336 - mae: 0.4329\n",
            "Epoch 12: val_loss improved from 0.30178 to 0.23266, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5336 - mae: 0.4329 - val_loss: 0.2327 - val_mae: 0.3035\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4619 - mae: 0.3970\n",
            "Epoch 13: val_loss improved from 0.23266 to 0.19444, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.4608 - mae: 0.3964 - val_loss: 0.1944 - val_mae: 0.2963\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4152 - mae: 0.3789\n",
            "Epoch 14: val_loss improved from 0.19444 to 0.16078, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.4146 - mae: 0.3786 - val_loss: 0.1608 - val_mae: 0.2216\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4036 - mae: 0.3667\n",
            "Epoch 15: val_loss did not improve from 0.16078\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4044 - mae: 0.3672 - val_loss: 0.1813 - val_mae: 0.3218\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4738 - mae: 0.4170\n",
            "Epoch 16: val_loss did not improve from 0.16078\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4733 - mae: 0.4166 - val_loss: 0.2072 - val_mae: 0.2873\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3388 - mae: 0.3255\n",
            "Epoch 17: val_loss improved from 0.16078 to 0.12650, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3381 - mae: 0.3252 - val_loss: 0.1265 - val_mae: 0.1868\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3250 - mae: 0.3103\n",
            "Epoch 18: val_loss did not improve from 0.12650\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3241 - mae: 0.3098 - val_loss: 0.4584 - val_mae: 0.5511\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3510 - mae: 0.3393\n",
            "Epoch 19: val_loss did not improve from 0.12650\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3513 - mae: 0.3393 - val_loss: 0.2856 - val_mae: 0.4598\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3182 - mae: 0.3179\n",
            "Epoch 20: val_loss did not improve from 0.12650\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3184 - mae: 0.3182 - val_loss: 0.2268 - val_mae: 0.4147\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3473 - mae: 0.3303\n",
            "Epoch 21: val_loss did not improve from 0.12650\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3462 - mae: 0.3298 - val_loss: 0.2317 - val_mae: 0.3649\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3218 - mae: 0.3314\n",
            "Epoch 22: val_loss improved from 0.12650 to 0.10447, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.3210 - mae: 0.3311 - val_loss: 0.1045 - val_mae: 0.2424\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3618 - mae: 0.3416\n",
            "Epoch 23: val_loss did not improve from 0.10447\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3623 - mae: 0.3420 - val_loss: 0.2120 - val_mae: 0.3725\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3187 - mae: 0.3152\n",
            "Epoch 24: val_loss improved from 0.10447 to 0.10001, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3188 - mae: 0.3153 - val_loss: 0.1000 - val_mae: 0.1989\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3308 - mae: 0.3243\n",
            "Epoch 25: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3301 - mae: 0.3238 - val_loss: 0.2808 - val_mae: 0.3779\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3007 - mae: 0.3025\n",
            "Epoch 26: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3011 - mae: 0.3025 - val_loss: 0.1030 - val_mae: 0.2323\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2812 - mae: 0.2768\n",
            "Epoch 27: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2804 - mae: 0.2765 - val_loss: 0.1332 - val_mae: 0.2962\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3042 - mae: 0.3020\n",
            "Epoch 28: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3037 - mae: 0.3017 - val_loss: 0.2227 - val_mae: 0.3704\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3010 - mae: 0.3026\n",
            "Epoch 29: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3022 - mae: 0.3028 - val_loss: 0.2710 - val_mae: 0.4121\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2755 - mae: 0.2753\n",
            "Epoch 30: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2748 - mae: 0.2752 - val_loss: 0.1194 - val_mae: 0.2831\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3027 - mae: 0.3110\n",
            "Epoch 31: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3026 - mae: 0.3109 - val_loss: 0.1056 - val_mae: 0.1597\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2967 - mae: 0.2913\n",
            "Epoch 32: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2967 - mae: 0.2911 - val_loss: 0.1162 - val_mae: 0.2518\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4261 - mae: 0.3872\n",
            "Epoch 33: val_loss did not improve from 0.10001\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.4268 - mae: 0.3880 - val_loss: 0.3909 - val_mae: 0.4090\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3139 - mae: 0.3096\n",
            "Epoch 34: val_loss improved from 0.10001 to 0.09935, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.3137 - mae: 0.3097 - val_loss: 0.0994 - val_mae: 0.2188\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2624 - mae: 0.2776\n",
            "Epoch 35: val_loss did not improve from 0.09935\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2622 - mae: 0.2774 - val_loss: 0.1492 - val_mae: 0.3065\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2703 - mae: 0.2828\n",
            "Epoch 36: val_loss did not improve from 0.09935\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2697 - mae: 0.2826 - val_loss: 0.1037 - val_mae: 0.2440\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2600 - mae: 0.2771\n",
            "Epoch 37: val_loss improved from 0.09935 to 0.09724, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2605 - mae: 0.2771 - val_loss: 0.0972 - val_mae: 0.1868\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2682 - mae: 0.2731\n",
            "Epoch 38: val_loss did not improve from 0.09724\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2697 - mae: 0.2734 - val_loss: 0.1064 - val_mae: 0.2704\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2816 - mae: 0.2923\n",
            "Epoch 39: val_loss improved from 0.09724 to 0.06058, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2813 - mae: 0.2923 - val_loss: 0.0606 - val_mae: 0.1039\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2597 - mae: 0.2727\n",
            "Epoch 40: val_loss did not improve from 0.06058\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2599 - mae: 0.2733 - val_loss: 0.1430 - val_mae: 0.2699\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2786 - mae: 0.2780\n",
            "Epoch 41: val_loss improved from 0.06058 to 0.05621, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2786 - mae: 0.2784 - val_loss: 0.0562 - val_mae: 0.1098\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2746 - mae: 0.2801\n",
            "Epoch 42: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2746 - mae: 0.2802 - val_loss: 0.1820 - val_mae: 0.3315\n",
            "Epoch 43/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2759 - mae: 0.2736\n",
            "Epoch 43: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2759 - mae: 0.2736 - val_loss: 0.0862 - val_mae: 0.2013\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2460 - mae: 0.2738\n",
            "Epoch 44: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2456 - mae: 0.2739 - val_loss: 0.1141 - val_mae: 0.2635\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2755 - mae: 0.2755\n",
            "Epoch 45: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2759 - mae: 0.2757 - val_loss: 0.0646 - val_mae: 0.1630\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2526 - mae: 0.2723\n",
            "Epoch 46: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2529 - mae: 0.2725 - val_loss: 0.1927 - val_mae: 0.3839\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2904 - mae: 0.2807\n",
            "Epoch 47: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2909 - mae: 0.2808 - val_loss: 0.0841 - val_mae: 0.2277\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2503 - mae: 0.2655\n",
            "Epoch 48: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2506 - mae: 0.2655 - val_loss: 0.0832 - val_mae: 0.2031\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2482 - mae: 0.2593\n",
            "Epoch 49: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2483 - mae: 0.2593 - val_loss: 0.0709 - val_mae: 0.1518\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4554 - mae: 0.3825\n",
            "Epoch 50: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4575 - mae: 0.3832 - val_loss: 0.5624 - val_mae: 0.3448\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6178 - mae: 0.4541\n",
            "Epoch 51: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6182 - mae: 0.4541 - val_loss: 0.3758 - val_mae: 0.3640\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4913 - mae: 0.4017\n",
            "Epoch 52: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4909 - mae: 0.4021 - val_loss: 0.2742 - val_mae: 0.2738\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3924 - mae: 0.3662\n",
            "Epoch 53: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3930 - mae: 0.3668 - val_loss: 0.4990 - val_mae: 0.6262\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3235 - mae: 0.3321\n",
            "Epoch 54: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3267 - mae: 0.3329 - val_loss: 0.1474 - val_mae: 0.2649\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2441 - mae: 0.2822\n",
            "Epoch 55: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2446 - mae: 0.2822 - val_loss: 0.1010 - val_mae: 0.2344\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2517 - mae: 0.2809\n",
            "Epoch 56: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2516 - mae: 0.2807 - val_loss: 0.1044 - val_mae: 0.2334\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2534 - mae: 0.2617\n",
            "Epoch 57: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2527 - mae: 0.2613 - val_loss: 0.1560 - val_mae: 0.3341\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2479 - mae: 0.2752\n",
            "Epoch 58: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2479 - mae: 0.2750 - val_loss: 0.1062 - val_mae: 0.2363\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2489 - mae: 0.2666\n",
            "Epoch 59: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2486 - mae: 0.2667 - val_loss: 0.0712 - val_mae: 0.1606\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2557 - mae: 0.2779\n",
            "Epoch 60: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2551 - mae: 0.2775 - val_loss: 0.1220 - val_mae: 0.2765\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2506 - mae: 0.2690\n",
            "Epoch 61: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2500 - mae: 0.2687 - val_loss: 0.0603 - val_mae: 0.1075\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2367 - mae: 0.2628\n",
            "Epoch 62: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2378 - mae: 0.2632 - val_loss: 0.0597 - val_mae: 0.1001\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2327 - mae: 0.2507\n",
            "Epoch 63: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2325 - mae: 0.2507 - val_loss: 0.0705 - val_mae: 0.1685\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2162 - mae: 0.2427\n",
            "Epoch 64: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2167 - mae: 0.2430 - val_loss: 0.2172 - val_mae: 0.4094\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2424 - mae: 0.2536\n",
            "Epoch 65: val_loss did not improve from 0.05621\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2424 - mae: 0.2539 - val_loss: 0.0576 - val_mae: 0.1034\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2249 - mae: 0.2690\n",
            "Epoch 66: val_loss improved from 0.05621 to 0.05472, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 51ms/step - loss: 0.2248 - mae: 0.2689 - val_loss: 0.0547 - val_mae: 0.0965\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2435 - mae: 0.2507\n",
            "Epoch 67: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2430 - mae: 0.2506 - val_loss: 0.1266 - val_mae: 0.2603\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2314 - mae: 0.2445\n",
            "Epoch 68: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2309 - mae: 0.2446 - val_loss: 0.0635 - val_mae: 0.1262\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2357 - mae: 0.2545\n",
            "Epoch 69: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2354 - mae: 0.2545 - val_loss: 0.0744 - val_mae: 0.1704\n",
            "Epoch 70/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2475 - mae: 0.2632\n",
            "Epoch 70: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2475 - mae: 0.2632 - val_loss: 0.0643 - val_mae: 0.1251\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2260 - mae: 0.2533\n",
            "Epoch 71: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2262 - mae: 0.2534 - val_loss: 0.1145 - val_mae: 0.2625\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2758 - mae: 0.2757\n",
            "Epoch 72: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2752 - mae: 0.2758 - val_loss: 0.0914 - val_mae: 0.2473\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2156 - mae: 0.2377\n",
            "Epoch 73: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2156 - mae: 0.2377 - val_loss: 0.0552 - val_mae: 0.1000\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2242 - mae: 0.2422\n",
            "Epoch 74: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2242 - mae: 0.2422 - val_loss: 0.0734 - val_mae: 0.1793\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2145 - mae: 0.2354\n",
            "Epoch 75: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2149 - mae: 0.2357 - val_loss: 0.0788 - val_mae: 0.1825\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2174 - mae: 0.2408\n",
            "Epoch 76: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2180 - mae: 0.2412 - val_loss: 0.0767 - val_mae: 0.1877\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2202 - mae: 0.2577\n",
            "Epoch 77: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2200 - mae: 0.2576 - val_loss: 0.0566 - val_mae: 0.1216\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2276 - mae: 0.2507\n",
            "Epoch 78: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2277 - mae: 0.2506 - val_loss: 0.0561 - val_mae: 0.1017\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2093 - mae: 0.2340\n",
            "Epoch 79: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2090 - mae: 0.2337 - val_loss: 0.0657 - val_mae: 0.1615\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2140 - mae: 0.2320\n",
            "Epoch 80: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2141 - mae: 0.2325 - val_loss: 0.0607 - val_mae: 0.0981\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2334 - mae: 0.2441\n",
            "Epoch 81: val_loss did not improve from 0.05472\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2329 - mae: 0.2440 - val_loss: 0.0598 - val_mae: 0.1099\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2362 - mae: 0.2714\n",
            "Epoch 82: val_loss improved from 0.05472 to 0.05417, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2356 - mae: 0.2711 - val_loss: 0.0542 - val_mae: 0.0979\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2062 - mae: 0.2267\n",
            "Epoch 83: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2058 - mae: 0.2265 - val_loss: 0.1073 - val_mae: 0.2670\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2270 - mae: 0.2619\n",
            "Epoch 84: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2278 - mae: 0.2621 - val_loss: 0.0638 - val_mae: 0.1427\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2080 - mae: 0.2202\n",
            "Epoch 85: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2091 - mae: 0.2205 - val_loss: 0.0594 - val_mae: 0.1382\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2222 - mae: 0.2488\n",
            "Epoch 86: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2219 - mae: 0.2486 - val_loss: 0.0638 - val_mae: 0.1309\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2491 - mae: 0.2825\n",
            "Epoch 87: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2495 - mae: 0.2825 - val_loss: 0.1185 - val_mae: 0.1866\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1985 - mae: 0.2224\n",
            "Epoch 88: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1985 - mae: 0.2225 - val_loss: 0.0561 - val_mae: 0.1241\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2187 - mae: 0.2280\n",
            "Epoch 89: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2188 - mae: 0.2281 - val_loss: 0.0598 - val_mae: 0.1367\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1981 - mae: 0.2236\n",
            "Epoch 90: val_loss did not improve from 0.05417\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1981 - mae: 0.2237 - val_loss: 0.0542 - val_mae: 0.0995\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2102 - mae: 0.2349\n",
            "Epoch 91: val_loss improved from 0.05417 to 0.05344, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2115 - mae: 0.2352 - val_loss: 0.0534 - val_mae: 0.0931\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2154 - mae: 0.2406\n",
            "Epoch 92: val_loss did not improve from 0.05344\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2169 - mae: 0.2409 - val_loss: 0.0664 - val_mae: 0.1549\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2011 - mae: 0.2346\n",
            "Epoch 93: val_loss did not improve from 0.05344\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2007 - mae: 0.2344 - val_loss: 0.0600 - val_mae: 0.1044\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2024 - mae: 0.2302\n",
            "Epoch 94: val_loss improved from 0.05344 to 0.05294, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第2次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2027 - mae: 0.2305 - val_loss: 0.0529 - val_mae: 0.1041\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1967 - mae: 0.2336\n",
            "Epoch 95: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1973 - mae: 0.2340 - val_loss: 0.0544 - val_mae: 0.0952\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1829 - mae: 0.2207\n",
            "Epoch 96: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1831 - mae: 0.2209 - val_loss: 0.0915 - val_mae: 0.2161\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2103 - mae: 0.2497\n",
            "Epoch 97: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2100 - mae: 0.2497 - val_loss: 0.0914 - val_mae: 0.2235\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1921 - mae: 0.2202\n",
            "Epoch 98: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1918 - mae: 0.2201 - val_loss: 0.0864 - val_mae: 0.2191\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1979 - mae: 0.2404\n",
            "Epoch 99: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1980 - mae: 0.2404 - val_loss: 0.0762 - val_mae: 0.2043\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.2114\n",
            "Epoch 100: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1760 - mae: 0.2115 - val_loss: 0.0607 - val_mae: 0.1369\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1902 - mae: 0.2241\n",
            "Epoch 101: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1905 - mae: 0.2243 - val_loss: 0.0600 - val_mae: 0.1427\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1883 - mae: 0.2250\n",
            "Epoch 102: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1883 - mae: 0.2250 - val_loss: 0.0559 - val_mae: 0.0990\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2004 - mae: 0.2230\n",
            "Epoch 103: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2010 - mae: 0.2230 - val_loss: 0.0748 - val_mae: 0.2019\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2015 - mae: 0.2344\n",
            "Epoch 104: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2016 - mae: 0.2345 - val_loss: 0.0725 - val_mae: 0.1617\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1887 - mae: 0.2150\n",
            "Epoch 105: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1888 - mae: 0.2151 - val_loss: 0.1266 - val_mae: 0.2853\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1916 - mae: 0.2379\n",
            "Epoch 106: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1919 - mae: 0.2381 - val_loss: 0.0631 - val_mae: 0.1458\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1885 - mae: 0.2146\n",
            "Epoch 107: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1883 - mae: 0.2147 - val_loss: 0.0687 - val_mae: 0.1758\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.2189\n",
            "Epoch 108: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1734 - mae: 0.2193 - val_loss: 0.0675 - val_mae: 0.1663\n",
            "Epoch 109/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1850 - mae: 0.2137\n",
            "Epoch 109: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1862 - mae: 0.2143 - val_loss: 0.1428 - val_mae: 0.3205\n",
            "Epoch 110/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.2230\n",
            "Epoch 110: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1721 - mae: 0.2228 - val_loss: 0.0556 - val_mae: 0.1237\n",
            "Epoch 111/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.2075\n",
            "Epoch 111: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1711 - mae: 0.2074 - val_loss: 0.0550 - val_mae: 0.1019\n",
            "Epoch 112/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.2110\n",
            "Epoch 112: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1757 - mae: 0.2113 - val_loss: 0.0610 - val_mae: 0.1341\n",
            "Epoch 113/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.2049\n",
            "Epoch 113: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1751 - mae: 0.2050 - val_loss: 0.0587 - val_mae: 0.0950\n",
            "Epoch 114/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1853 - mae: 0.2179\n",
            "Epoch 114: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1850 - mae: 0.2178 - val_loss: 0.0562 - val_mae: 0.1192\n",
            "Epoch 115/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1795 - mae: 0.2113\n",
            "Epoch 115: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1793 - mae: 0.2114 - val_loss: 0.0610 - val_mae: 0.0964\n",
            "Epoch 116/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.2100\n",
            "Epoch 116: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1756 - mae: 0.2101 - val_loss: 0.0534 - val_mae: 0.1148\n",
            "Epoch 117/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1793 - mae: 0.2187\n",
            "Epoch 117: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1799 - mae: 0.2187 - val_loss: 0.0740 - val_mae: 0.1863\n",
            "Epoch 118/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.2068\n",
            "Epoch 118: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1751 - mae: 0.2069 - val_loss: 0.0619 - val_mae: 0.1364\n",
            "Epoch 119/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1696 - mae: 0.2234\n",
            "Epoch 119: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1691 - mae: 0.2230 - val_loss: 0.0639 - val_mae: 0.1615\n",
            "Epoch 120/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1703 - mae: 0.2070\n",
            "Epoch 120: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1702 - mae: 0.2068 - val_loss: 0.0596 - val_mae: 0.1339\n",
            "Epoch 121/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1789 - mae: 0.2107\n",
            "Epoch 121: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1790 - mae: 0.2108 - val_loss: 0.0544 - val_mae: 0.1025\n",
            "Epoch 122/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.2167\n",
            "Epoch 122: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1750 - mae: 0.2165 - val_loss: 0.1009 - val_mae: 0.1750\n",
            "Epoch 123/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1697 - mae: 0.2114\n",
            "Epoch 123: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1701 - mae: 0.2116 - val_loss: 0.0623 - val_mae: 0.1108\n",
            "Epoch 124/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1867 - mae: 0.2119\n",
            "Epoch 124: val_loss did not improve from 0.05294\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1878 - mae: 0.2124 - val_loss: 0.0611 - val_mae: 0.1496\n",
            "Epoch 124: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07014626264572144, RMSE:0.2648513913154602, MAE:0.12366954237222672, R2:0.9232899830262921\n",
            "2 0.9232899830262921\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07188301533460617, RMSE:0.268110066652298, MAE:0.1207733228802681, R2:0.9199765740763938\n",
            "r2: 0.9199765740763938 rmse: 0.26811007\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 18ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07188301533460617, RMSE:0.268110066652298, MAE:0.1207733228802681, R2:0.9199765740763938\n",
            "r2: 0.9199765740763938 rmse: 0.26811007\n",
            "第3次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_6/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_6'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_31\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_24 (Embedding)       (None, 17, 900)      649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_25 (Embedding)       (None, 17, 900)      649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_26 (Embedding)       (None, 17, 900)      649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_27 (Embedding)       (None, 17, 900)      649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 17, 100)      270100      ['embedding_24[0][0]',           \n",
            "                                                                  'embedding_25[0][0]',           \n",
            "                                                                  'embedding_26[0][0]',           \n",
            "                                                                  'embedding_27[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 17, 100)      270100      ['embedding_24[0][0]',           \n",
            "                                                                  'embedding_25[0][0]',           \n",
            "                                                                  'embedding_26[0][0]',           \n",
            "                                                                  'embedding_27[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 17, 100)      270100      ['embedding_24[0][0]',           \n",
            "                                                                  'embedding_25[0][0]',           \n",
            "                                                                  'embedding_26[0][0]',           \n",
            "                                                                  'embedding_27[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_72 (Globa  (None, 100)         0           ['conv1d_18[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_73 (Globa  (None, 100)         0           ['conv1d_18[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_74 (Globa  (None, 100)         0           ['conv1d_18[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_75 (Globa  (None, 100)         0           ['conv1d_18[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_76 (Globa  (None, 100)         0           ['conv1d_19[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_77 (Globa  (None, 100)         0           ['conv1d_19[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_78 (Globa  (None, 100)         0           ['conv1d_19[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_79 (Globa  (None, 100)         0           ['conv1d_19[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_80 (Globa  (None, 100)         0           ['conv1d_20[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_81 (Globa  (None, 100)         0           ['conv1d_20[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_82 (Globa  (None, 100)         0           ['conv1d_20[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_83 (Globa  (None, 100)         0           ['conv1d_20[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 100)          0           ['global_max_pooling1d_72[0][0]',\n",
            "                                                                  'global_max_pooling1d_73[0][0]',\n",
            "                                                                  'global_max_pooling1d_74[0][0]',\n",
            "                                                                  'global_max_pooling1d_75[0][0]']\n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 100)          0           ['global_max_pooling1d_76[0][0]',\n",
            "                                                                  'global_max_pooling1d_77[0][0]',\n",
            "                                                                  'global_max_pooling1d_78[0][0]',\n",
            "                                                                  'global_max_pooling1d_79[0][0]']\n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 100)          0           ['global_max_pooling1d_80[0][0]',\n",
            "                                                                  'global_max_pooling1d_81[0][0]',\n",
            "                                                                  'global_max_pooling1d_82[0][0]',\n",
            "                                                                  'global_max_pooling1d_83[0][0]']\n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 1000)         11000       ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 1300)         0           ['add_18[0][0]',                 \n",
            "                                                                  'add_19[0][0]',                 \n",
            "                                                                  'add_20[0][0]',                 \n",
            "                                                                  'dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_6 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (1, 1300)           0           ['tf.expand_dims_6[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (1, 1, 1, 650)       845650      ['reshape_6[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_6 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_25[0][0]']               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_6[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_6 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_6[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_6[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_6 (TFOpLa  (None, 1300)        0           ['multiply_6[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_6[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_30[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_31[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_32[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_33[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 1)            3           ['dropout_34[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.0133 - mae: 0.8678\n",
            "Epoch 1: val_loss improved from inf to 0.49762, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 2.0105 - mae: 0.8670 - val_loss: 0.4976 - val_mae: 0.4288\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7406 - mae: 0.5423\n",
            "Epoch 2: val_loss improved from 0.49762 to 0.45226, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7396 - mae: 0.5417 - val_loss: 0.4523 - val_mae: 0.3569\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6535 - mae: 0.4832\n",
            "Epoch 3: val_loss improved from 0.45226 to 0.38878, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6551 - mae: 0.4837 - val_loss: 0.3888 - val_mae: 0.3404\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7441 - mae: 0.5595\n",
            "Epoch 4: val_loss did not improve from 0.38878\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.7438 - mae: 0.5595 - val_loss: 0.4281 - val_mae: 0.4743\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6513 - mae: 0.4924\n",
            "Epoch 5: val_loss did not improve from 0.38878\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6533 - mae: 0.4925 - val_loss: 0.4955 - val_mae: 0.4993\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6408 - mae: 0.4893\n",
            "Epoch 6: val_loss did not improve from 0.38878\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6411 - mae: 0.4894 - val_loss: 0.5010 - val_mae: 0.4943\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6364 - mae: 0.4874\n",
            "Epoch 7: val_loss improved from 0.38878 to 0.37232, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.6363 - mae: 0.4872 - val_loss: 0.3723 - val_mae: 0.2771\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6496 - mae: 0.4971\n",
            "Epoch 8: val_loss improved from 0.37232 to 0.36164, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6490 - mae: 0.4968 - val_loss: 0.3616 - val_mae: 0.3144\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5766 - mae: 0.4619\n",
            "Epoch 9: val_loss did not improve from 0.36164\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5754 - mae: 0.4614 - val_loss: 0.3938 - val_mae: 0.3784\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5867 - mae: 0.4591\n",
            "Epoch 10: val_loss did not improve from 0.36164\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5854 - mae: 0.4584 - val_loss: 0.3654 - val_mae: 0.3414\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5300 - mae: 0.4279\n",
            "Epoch 11: val_loss improved from 0.36164 to 0.30891, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.5307 - mae: 0.4281 - val_loss: 0.3089 - val_mae: 0.2762\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5225 - mae: 0.4296\n",
            "Epoch 12: val_loss did not improve from 0.30891\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5218 - mae: 0.4295 - val_loss: 0.3355 - val_mae: 0.3876\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5598 - mae: 0.4370\n",
            "Epoch 13: val_loss improved from 0.30891 to 0.30081, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.5588 - mae: 0.4366 - val_loss: 0.3008 - val_mae: 0.3279\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5121 - mae: 0.4148\n",
            "Epoch 14: val_loss did not improve from 0.30081\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.5107 - mae: 0.4144 - val_loss: 0.4453 - val_mae: 0.5020\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4617 - mae: 0.3900\n",
            "Epoch 15: val_loss did not improve from 0.30081\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4608 - mae: 0.3898 - val_loss: 0.3394 - val_mae: 0.4295\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4508 - mae: 0.3958\n",
            "Epoch 16: val_loss improved from 0.30081 to 0.22160, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4514 - mae: 0.3959 - val_loss: 0.2216 - val_mae: 0.2754\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3907 - mae: 0.3548\n",
            "Epoch 17: val_loss improved from 0.22160 to 0.15548, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 3s 39ms/step - loss: 0.3901 - mae: 0.3546 - val_loss: 0.1555 - val_mae: 0.2495\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4078 - mae: 0.3820\n",
            "Epoch 18: val_loss did not improve from 0.15548\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4074 - mae: 0.3821 - val_loss: 0.2249 - val_mae: 0.3537\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3556 - mae: 0.3495\n",
            "Epoch 19: val_loss did not improve from 0.15548\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3549 - mae: 0.3493 - val_loss: 0.2022 - val_mae: 0.3278\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3614 - mae: 0.3324\n",
            "Epoch 20: val_loss did not improve from 0.15548\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3608 - mae: 0.3324 - val_loss: 0.2324 - val_mae: 0.3636\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3382 - mae: 0.3210\n",
            "Epoch 21: val_loss improved from 0.15548 to 0.09616, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3389 - mae: 0.3211 - val_loss: 0.0962 - val_mae: 0.1955\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3273 - mae: 0.3200\n",
            "Epoch 22: val_loss did not improve from 0.09616\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3275 - mae: 0.3201 - val_loss: 0.1266 - val_mae: 0.1872\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3135 - mae: 0.3068\n",
            "Epoch 23: val_loss improved from 0.09616 to 0.09264, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3146 - mae: 0.3069 - val_loss: 0.0926 - val_mae: 0.1923\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2858 - mae: 0.2824\n",
            "Epoch 24: val_loss improved from 0.09264 to 0.09149, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2854 - mae: 0.2823 - val_loss: 0.0915 - val_mae: 0.1676\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2993 - mae: 0.3045\n",
            "Epoch 25: val_loss did not improve from 0.09149\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2984 - mae: 0.3040 - val_loss: 0.1105 - val_mae: 0.2662\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3107 - mae: 0.3029\n",
            "Epoch 26: val_loss did not improve from 0.09149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3112 - mae: 0.3033 - val_loss: 0.3937 - val_mae: 0.4918\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3147 - mae: 0.3108\n",
            "Epoch 27: val_loss did not improve from 0.09149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3141 - mae: 0.3107 - val_loss: 0.1099 - val_mae: 0.2521\n",
            "Epoch 28/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2752 - mae: 0.2785\n",
            "Epoch 28: val_loss did not improve from 0.09149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2752 - mae: 0.2785 - val_loss: 0.0976 - val_mae: 0.2377\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2928 - mae: 0.2838\n",
            "Epoch 29: val_loss did not improve from 0.09149\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2926 - mae: 0.2839 - val_loss: 0.5078 - val_mae: 0.5915\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2984 - mae: 0.3067\n",
            "Epoch 30: val_loss did not improve from 0.09149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2995 - mae: 0.3070 - val_loss: 0.0983 - val_mae: 0.2162\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2931 - mae: 0.2917\n",
            "Epoch 31: val_loss improved from 0.09149 to 0.07230, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 49ms/step - loss: 0.2931 - mae: 0.2917 - val_loss: 0.0723 - val_mae: 0.1685\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2634 - mae: 0.2669\n",
            "Epoch 32: val_loss did not improve from 0.07230\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2636 - mae: 0.2674 - val_loss: 0.0847 - val_mae: 0.1967\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2636 - mae: 0.2623\n",
            "Epoch 33: val_loss improved from 0.07230 to 0.06721, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2630 - mae: 0.2620 - val_loss: 0.0672 - val_mae: 0.1555\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2984 - mae: 0.3150\n",
            "Epoch 34: val_loss did not improve from 0.06721\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2984 - mae: 0.3149 - val_loss: 0.1430 - val_mae: 0.3142\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2791 - mae: 0.2893\n",
            "Epoch 35: val_loss did not improve from 0.06721\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2795 - mae: 0.2901 - val_loss: 0.1125 - val_mae: 0.2763\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2530 - mae: 0.2601\n",
            "Epoch 36: val_loss did not improve from 0.06721\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2526 - mae: 0.2601 - val_loss: 0.0706 - val_mae: 0.1211\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2792 - mae: 0.2935\n",
            "Epoch 37: val_loss improved from 0.06721 to 0.06525, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.2783 - mae: 0.2930 - val_loss: 0.0653 - val_mae: 0.1304\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2231 - mae: 0.2400\n",
            "Epoch 38: val_loss improved from 0.06525 to 0.05667, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2227 - mae: 0.2398 - val_loss: 0.0567 - val_mae: 0.0990\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3176 - mae: 0.3332\n",
            "Epoch 39: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3168 - mae: 0.3327 - val_loss: 0.0595 - val_mae: 0.1405\n",
            "Epoch 40/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2499 - mae: 0.2569\n",
            "Epoch 40: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2499 - mae: 0.2569 - val_loss: 0.1324 - val_mae: 0.2834\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2712 - mae: 0.2763\n",
            "Epoch 41: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2730 - mae: 0.2768 - val_loss: 0.0682 - val_mae: 0.1307\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2695 - mae: 0.2759\n",
            "Epoch 42: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2710 - mae: 0.2763 - val_loss: 0.1016 - val_mae: 0.2460\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2912 - mae: 0.2961\n",
            "Epoch 43: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2906 - mae: 0.2957 - val_loss: 0.0713 - val_mae: 0.1700\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2527 - mae: 0.2593\n",
            "Epoch 44: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2522 - mae: 0.2592 - val_loss: 0.0988 - val_mae: 0.2452\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2588 - mae: 0.2617\n",
            "Epoch 45: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2580 - mae: 0.2614 - val_loss: 0.1046 - val_mae: 0.2459\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2732 - mae: 0.2884\n",
            "Epoch 46: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2727 - mae: 0.2882 - val_loss: 0.0580 - val_mae: 0.1250\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2588 - mae: 0.2563\n",
            "Epoch 47: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2589 - mae: 0.2565 - val_loss: 0.3207 - val_mae: 0.4641\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2612 - mae: 0.2769\n",
            "Epoch 48: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2612 - mae: 0.2767 - val_loss: 0.0764 - val_mae: 0.1736\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2499 - mae: 0.2577\n",
            "Epoch 49: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2497 - mae: 0.2574 - val_loss: 0.0634 - val_mae: 0.1062\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2496 - mae: 0.2627\n",
            "Epoch 50: val_loss did not improve from 0.05667\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2493 - mae: 0.2627 - val_loss: 0.1943 - val_mae: 0.3155\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2550 - mae: 0.2618\n",
            "Epoch 51: val_loss improved from 0.05667 to 0.05591, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2546 - mae: 0.2617 - val_loss: 0.0559 - val_mae: 0.0996\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2644 - mae: 0.2687\n",
            "Epoch 52: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2637 - mae: 0.2685 - val_loss: 0.0675 - val_mae: 0.1392\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2483 - mae: 0.2634\n",
            "Epoch 53: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2477 - mae: 0.2632 - val_loss: 0.1172 - val_mae: 0.2630\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2483 - mae: 0.2602\n",
            "Epoch 54: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2478 - mae: 0.2602 - val_loss: 0.0830 - val_mae: 0.2070\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2494 - mae: 0.2751\n",
            "Epoch 55: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2494 - mae: 0.2753 - val_loss: 0.1132 - val_mae: 0.2375\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2586 - mae: 0.2686\n",
            "Epoch 56: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2584 - mae: 0.2684 - val_loss: 0.2220 - val_mae: 0.4101\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2424 - mae: 0.2585\n",
            "Epoch 57: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2431 - mae: 0.2589 - val_loss: 0.0561 - val_mae: 0.1080\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2208 - mae: 0.2495\n",
            "Epoch 58: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2203 - mae: 0.2494 - val_loss: 0.0642 - val_mae: 0.1615\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2134 - mae: 0.2398\n",
            "Epoch 59: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2127 - mae: 0.2394 - val_loss: 0.0774 - val_mae: 0.1866\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2448 - mae: 0.2575\n",
            "Epoch 60: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2442 - mae: 0.2572 - val_loss: 0.0631 - val_mae: 0.1555\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2381 - mae: 0.2472\n",
            "Epoch 61: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2375 - mae: 0.2470 - val_loss: 0.0561 - val_mae: 0.1036\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2183 - mae: 0.2476\n",
            "Epoch 62: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2180 - mae: 0.2475 - val_loss: 0.0596 - val_mae: 0.1472\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2188 - mae: 0.2453\n",
            "Epoch 63: val_loss did not improve from 0.05591\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2200 - mae: 0.2456 - val_loss: 0.0617 - val_mae: 0.1444\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2243 - mae: 0.2513\n",
            "Epoch 64: val_loss improved from 0.05591 to 0.05337, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2249 - mae: 0.2516 - val_loss: 0.0534 - val_mae: 0.0931\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2489 - mae: 0.2749\n",
            "Epoch 65: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2489 - mae: 0.2747 - val_loss: 0.0835 - val_mae: 0.1875\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2407 - mae: 0.2547\n",
            "Epoch 66: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2402 - mae: 0.2548 - val_loss: 0.1491 - val_mae: 0.3336\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2198 - mae: 0.2416\n",
            "Epoch 67: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2202 - mae: 0.2417 - val_loss: 0.0820 - val_mae: 0.2011\n",
            "Epoch 68/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2148 - mae: 0.2362\n",
            "Epoch 68: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2148 - mae: 0.2362 - val_loss: 0.0547 - val_mae: 0.0902\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2955 - mae: 0.3212\n",
            "Epoch 69: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2956 - mae: 0.3209 - val_loss: 0.1798 - val_mae: 0.3647\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2209 - mae: 0.2358\n",
            "Epoch 70: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2205 - mae: 0.2359 - val_loss: 0.0551 - val_mae: 0.0986\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1971 - mae: 0.2297\n",
            "Epoch 71: val_loss did not improve from 0.05337\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1966 - mae: 0.2296 - val_loss: 0.0718 - val_mae: 0.1800\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2045 - mae: 0.2310\n",
            "Epoch 72: val_loss improved from 0.05337 to 0.05236, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第3次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2058 - mae: 0.2312 - val_loss: 0.0524 - val_mae: 0.0978\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2118 - mae: 0.2369\n",
            "Epoch 73: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2122 - mae: 0.2371 - val_loss: 0.0676 - val_mae: 0.1625\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2139 - mae: 0.2524\n",
            "Epoch 74: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2149 - mae: 0.2527 - val_loss: 0.0585 - val_mae: 0.1344\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2128 - mae: 0.2362\n",
            "Epoch 75: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2132 - mae: 0.2364 - val_loss: 0.1061 - val_mae: 0.2696\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1882 - mae: 0.2282\n",
            "Epoch 76: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1879 - mae: 0.2281 - val_loss: 0.1096 - val_mae: 0.2607\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2081 - mae: 0.2348\n",
            "Epoch 77: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2083 - mae: 0.2348 - val_loss: 0.0669 - val_mae: 0.1621\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1929 - mae: 0.2231\n",
            "Epoch 78: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1926 - mae: 0.2229 - val_loss: 0.0543 - val_mae: 0.0933\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1977 - mae: 0.2308\n",
            "Epoch 79: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1979 - mae: 0.2309 - val_loss: 0.0557 - val_mae: 0.1098\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2094 - mae: 0.2352\n",
            "Epoch 80: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2098 - mae: 0.2353 - val_loss: 0.0565 - val_mae: 0.1181\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2216 - mae: 0.2386\n",
            "Epoch 81: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2211 - mae: 0.2383 - val_loss: 0.1338 - val_mae: 0.2166\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2004 - mae: 0.2236\n",
            "Epoch 82: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1999 - mae: 0.2234 - val_loss: 0.0801 - val_mae: 0.1745\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2041 - mae: 0.2226\n",
            "Epoch 83: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2043 - mae: 0.2228 - val_loss: 0.0562 - val_mae: 0.1169\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1959 - mae: 0.2247\n",
            "Epoch 84: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1956 - mae: 0.2246 - val_loss: 0.0612 - val_mae: 0.1529\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2130 - mae: 0.2295\n",
            "Epoch 85: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2134 - mae: 0.2298 - val_loss: 0.0722 - val_mae: 0.1785\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2504 - mae: 0.2805\n",
            "Epoch 86: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2505 - mae: 0.2803 - val_loss: 0.0594 - val_mae: 0.1045\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2133 - mae: 0.2330\n",
            "Epoch 87: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2130 - mae: 0.2329 - val_loss: 0.0637 - val_mae: 0.1032\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2029 - mae: 0.2244\n",
            "Epoch 88: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2026 - mae: 0.2247 - val_loss: 0.0688 - val_mae: 0.1052\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2124 - mae: 0.2347\n",
            "Epoch 89: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2127 - mae: 0.2348 - val_loss: 0.0789 - val_mae: 0.1994\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1918 - mae: 0.2135\n",
            "Epoch 90: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1913 - mae: 0.2134 - val_loss: 0.2020 - val_mae: 0.3529\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2025 - mae: 0.2428\n",
            "Epoch 91: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2024 - mae: 0.2427 - val_loss: 0.0700 - val_mae: 0.1638\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.2230\n",
            "Epoch 92: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1975 - mae: 0.2233 - val_loss: 0.0899 - val_mae: 0.2340\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1919 - mae: 0.2130\n",
            "Epoch 93: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1919 - mae: 0.2133 - val_loss: 0.0716 - val_mae: 0.1634\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3799 - mae: 0.3545\n",
            "Epoch 94: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3786 - mae: 0.3537 - val_loss: 0.0750 - val_mae: 0.1866\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2000 - mae: 0.2281\n",
            "Epoch 95: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2003 - mae: 0.2284 - val_loss: 0.0550 - val_mae: 0.1000\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1836 - mae: 0.2128\n",
            "Epoch 96: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1836 - mae: 0.2129 - val_loss: 0.0645 - val_mae: 0.1533\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1835 - mae: 0.2253\n",
            "Epoch 97: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1831 - mae: 0.2250 - val_loss: 0.0557 - val_mae: 0.1207\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1939 - mae: 0.2233\n",
            "Epoch 98: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.1938 - mae: 0.2235 - val_loss: 0.0604 - val_mae: 0.1502\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.2157\n",
            "Epoch 99: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1774 - mae: 0.2158 - val_loss: 0.0946 - val_mae: 0.2081\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1894 - mae: 0.2225\n",
            "Epoch 100: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1890 - mae: 0.2223 - val_loss: 0.0663 - val_mae: 0.1387\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1797 - mae: 0.2107\n",
            "Epoch 101: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1797 - mae: 0.2110 - val_loss: 0.0614 - val_mae: 0.1253\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1885 - mae: 0.2164\n",
            "Epoch 102: val_loss did not improve from 0.05236\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1890 - mae: 0.2167 - val_loss: 0.0542 - val_mae: 0.1400\n",
            "Epoch 102: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.08235641568899155, RMSE:0.2869780659675598, MAE:0.1259322315454483, R2:0.9099373038593238\n",
            "3 0.9099373038593238\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0828009769320488, RMSE:0.28775158524513245, MAE:0.12230132520198822, R2:0.9078222057681373\n",
            "r2: 0.9078222057681373 rmse: 0.2877516\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0828009769320488, RMSE:0.28775158524513245, MAE:0.12230132520198822, R2:0.9078222057681373\n",
            "r2: 0.9078222057681373 rmse: 0.2877516\n",
            "第4次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_7/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_7'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_37\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_28 (Embedding)       (None, 17, 900)      649800      ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_29 (Embedding)       (None, 17, 900)      649800      ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_30 (Embedding)       (None, 17, 900)      649800      ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_31 (Embedding)       (None, 17, 900)      649800      ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 17, 100)      270100      ['embedding_28[0][0]',           \n",
            "                                                                  'embedding_29[0][0]',           \n",
            "                                                                  'embedding_30[0][0]',           \n",
            "                                                                  'embedding_31[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 17, 100)      270100      ['embedding_28[0][0]',           \n",
            "                                                                  'embedding_29[0][0]',           \n",
            "                                                                  'embedding_30[0][0]',           \n",
            "                                                                  'embedding_31[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 17, 100)      270100      ['embedding_28[0][0]',           \n",
            "                                                                  'embedding_29[0][0]',           \n",
            "                                                                  'embedding_30[0][0]',           \n",
            "                                                                  'embedding_31[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_84 (Globa  (None, 100)         0           ['conv1d_21[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_85 (Globa  (None, 100)         0           ['conv1d_21[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_86 (Globa  (None, 100)         0           ['conv1d_21[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_87 (Globa  (None, 100)         0           ['conv1d_21[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_88 (Globa  (None, 100)         0           ['conv1d_22[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_89 (Globa  (None, 100)         0           ['conv1d_22[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_90 (Globa  (None, 100)         0           ['conv1d_22[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_91 (Globa  (None, 100)         0           ['conv1d_22[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_92 (Globa  (None, 100)         0           ['conv1d_23[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_93 (Globa  (None, 100)         0           ['conv1d_23[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_94 (Globa  (None, 100)         0           ['conv1d_23[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_95 (Globa  (None, 100)         0           ['conv1d_23[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 100)          0           ['global_max_pooling1d_84[0][0]',\n",
            "                                                                  'global_max_pooling1d_85[0][0]',\n",
            "                                                                  'global_max_pooling1d_86[0][0]',\n",
            "                                                                  'global_max_pooling1d_87[0][0]']\n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 100)          0           ['global_max_pooling1d_88[0][0]',\n",
            "                                                                  'global_max_pooling1d_89[0][0]',\n",
            "                                                                  'global_max_pooling1d_90[0][0]',\n",
            "                                                                  'global_max_pooling1d_91[0][0]']\n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 100)          0           ['global_max_pooling1d_92[0][0]',\n",
            "                                                                  'global_max_pooling1d_93[0][0]',\n",
            "                                                                  'global_max_pooling1d_94[0][0]',\n",
            "                                                                  'global_max_pooling1d_95[0][0]']\n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 1000)         11000       ['input_16[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 1300)         0           ['add_21[0][0]',                 \n",
            "                                                                  'add_22[0][0]',                 \n",
            "                                                                  'add_23[0][0]',                 \n",
            "                                                                  'dense_28[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_7 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_7 (Gl  (1, 1300)           0           ['tf.expand_dims_7[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (1, 1, 1, 650)       845650      ['reshape_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_7 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_29[0][0]']               \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_7[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_7 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_30[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_7[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_7[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_7 (TFOpLa  (None, 1300)        0           ['multiply_7[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_7[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_35[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_36 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_36[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 1)            3           ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.9475 - mae: 0.8160\n",
            "Epoch 1: val_loss improved from inf to 0.48867, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 1.9475 - mae: 0.8160 - val_loss: 0.4887 - val_mae: 0.4157\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7825 - mae: 0.5193\n",
            "Epoch 2: val_loss improved from 0.48867 to 0.41278, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.7812 - mae: 0.5192 - val_loss: 0.4128 - val_mae: 0.3463\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6910 - mae: 0.5057\n",
            "Epoch 3: val_loss did not improve from 0.41278\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6898 - mae: 0.5055 - val_loss: 0.5449 - val_mae: 0.5009\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6908 - mae: 0.4981\n",
            "Epoch 4: val_loss did not improve from 0.41278\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6914 - mae: 0.4984 - val_loss: 0.6234 - val_mae: 0.6806\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6932 - mae: 0.5165\n",
            "Epoch 5: val_loss improved from 0.41278 to 0.38006, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.6918 - mae: 0.5159 - val_loss: 0.3801 - val_mae: 0.3142\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6115 - mae: 0.4652\n",
            "Epoch 6: val_loss did not improve from 0.38006\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6118 - mae: 0.4658 - val_loss: 0.5127 - val_mae: 0.5940\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6241 - mae: 0.4673\n",
            "Epoch 7: val_loss did not improve from 0.38006\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6251 - mae: 0.4677 - val_loss: 0.4131 - val_mae: 0.3795\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6033 - mae: 0.4739\n",
            "Epoch 8: val_loss improved from 0.38006 to 0.33458, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6043 - mae: 0.4745 - val_loss: 0.3346 - val_mae: 0.3399\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5291 - mae: 0.4292\n",
            "Epoch 9: val_loss improved from 0.33458 to 0.29845, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5284 - mae: 0.4291 - val_loss: 0.2984 - val_mae: 0.3242\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5412 - mae: 0.4313\n",
            "Epoch 10: val_loss improved from 0.29845 to 0.28627, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5413 - mae: 0.4311 - val_loss: 0.2863 - val_mae: 0.2624\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5286 - mae: 0.4235\n",
            "Epoch 11: val_loss improved from 0.28627 to 0.24978, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5278 - mae: 0.4234 - val_loss: 0.2498 - val_mae: 0.2905\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4681 - mae: 0.3896\n",
            "Epoch 12: val_loss did not improve from 0.24978\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4681 - mae: 0.3897 - val_loss: 0.7803 - val_mae: 0.7253\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4711 - mae: 0.3896\n",
            "Epoch 13: val_loss improved from 0.24978 to 0.16041, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4710 - mae: 0.3895 - val_loss: 0.1604 - val_mae: 0.2366\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4142 - mae: 0.3727\n",
            "Epoch 14: val_loss improved from 0.16041 to 0.15042, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4135 - mae: 0.3726 - val_loss: 0.1504 - val_mae: 0.2045\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4161 - mae: 0.3699\n",
            "Epoch 15: val_loss improved from 0.15042 to 0.10721, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4149 - mae: 0.3692 - val_loss: 0.1072 - val_mae: 0.1532\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3288 - mae: 0.3253\n",
            "Epoch 16: val_loss did not improve from 0.10721\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3298 - mae: 0.3252 - val_loss: 0.1083 - val_mae: 0.1737\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3710 - mae: 0.3558\n",
            "Epoch 17: val_loss did not improve from 0.10721\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3705 - mae: 0.3553 - val_loss: 0.2320 - val_mae: 0.3748\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3321 - mae: 0.3146\n",
            "Epoch 18: val_loss did not improve from 0.10721\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3312 - mae: 0.3142 - val_loss: 0.3706 - val_mae: 0.4642\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3511 - mae: 0.3273\n",
            "Epoch 19: val_loss did not improve from 0.10721\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3504 - mae: 0.3270 - val_loss: 0.1781 - val_mae: 0.3260\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3158 - mae: 0.3167\n",
            "Epoch 20: val_loss improved from 0.10721 to 0.07003, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 49ms/step - loss: 0.3157 - mae: 0.3166 - val_loss: 0.0700 - val_mae: 0.1424\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3272 - mae: 0.3131\n",
            "Epoch 21: val_loss did not improve from 0.07003\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3268 - mae: 0.3132 - val_loss: 0.1162 - val_mae: 0.2577\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3604 - mae: 0.3463\n",
            "Epoch 22: val_loss did not improve from 0.07003\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3599 - mae: 0.3465 - val_loss: 0.0844 - val_mae: 0.1971\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3139 - mae: 0.2931\n",
            "Epoch 23: val_loss did not improve from 0.07003\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3147 - mae: 0.2934 - val_loss: 0.0926 - val_mae: 0.1330\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3083 - mae: 0.3080\n",
            "Epoch 24: val_loss improved from 0.07003 to 0.06665, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.3078 - mae: 0.3081 - val_loss: 0.0666 - val_mae: 0.1292\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2801 - mae: 0.2838\n",
            "Epoch 25: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2797 - mae: 0.2838 - val_loss: 0.4558 - val_mae: 0.5438\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2950 - mae: 0.3070\n",
            "Epoch 26: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2948 - mae: 0.3072 - val_loss: 0.1762 - val_mae: 0.3046\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2958 - mae: 0.2962\n",
            "Epoch 27: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2955 - mae: 0.2960 - val_loss: 0.0678 - val_mae: 0.1244\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4610 - mae: 0.4017\n",
            "Epoch 28: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4657 - mae: 0.4030 - val_loss: 0.3825 - val_mae: 0.4669\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4103 - mae: 0.3681\n",
            "Epoch 29: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4109 - mae: 0.3684 - val_loss: 0.5965 - val_mae: 0.7148\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3602 - mae: 0.3600\n",
            "Epoch 30: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3595 - mae: 0.3597 - val_loss: 0.0825 - val_mae: 0.1610\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2882 - mae: 0.2871\n",
            "Epoch 31: val_loss did not improve from 0.06665\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2874 - mae: 0.2867 - val_loss: 0.0816 - val_mae: 0.1707\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2837 - mae: 0.2816\n",
            "Epoch 32: val_loss improved from 0.06665 to 0.06033, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2836 - mae: 0.2816 - val_loss: 0.0603 - val_mae: 0.1087\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3085 - mae: 0.3119\n",
            "Epoch 33: val_loss did not improve from 0.06033\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3083 - mae: 0.3121 - val_loss: 0.1653 - val_mae: 0.3513\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3014 - mae: 0.3130\n",
            "Epoch 34: val_loss did not improve from 0.06033\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3007 - mae: 0.3126 - val_loss: 0.0862 - val_mae: 0.1927\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2742 - mae: 0.2757\n",
            "Epoch 35: val_loss improved from 0.06033 to 0.06022, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2745 - mae: 0.2756 - val_loss: 0.0602 - val_mae: 0.1189\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2924 - mae: 0.2892\n",
            "Epoch 36: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2928 - mae: 0.2893 - val_loss: 0.0717 - val_mae: 0.1244\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2735 - mae: 0.2802\n",
            "Epoch 37: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2740 - mae: 0.2804 - val_loss: 0.1226 - val_mae: 0.2167\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2571 - mae: 0.2565\n",
            "Epoch 38: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2571 - mae: 0.2565 - val_loss: 0.0659 - val_mae: 0.1374\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2672 - mae: 0.2709\n",
            "Epoch 39: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2667 - mae: 0.2707 - val_loss: 0.1402 - val_mae: 0.3264\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2709 - mae: 0.2802\n",
            "Epoch 40: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2702 - mae: 0.2798 - val_loss: 0.0662 - val_mae: 0.1508\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3093 - mae: 0.3100\n",
            "Epoch 41: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3091 - mae: 0.3100 - val_loss: 0.0731 - val_mae: 0.1696\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2582 - mae: 0.2652\n",
            "Epoch 42: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2585 - mae: 0.2659 - val_loss: 0.0689 - val_mae: 0.1160\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4010 - mae: 0.3727\n",
            "Epoch 43: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4005 - mae: 0.3723 - val_loss: 0.1208 - val_mae: 0.2540\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2599 - mae: 0.2705\n",
            "Epoch 44: val_loss did not improve from 0.06022\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2592 - mae: 0.2703 - val_loss: 0.0621 - val_mae: 0.1388\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2604 - mae: 0.2531\n",
            "Epoch 45: val_loss improved from 0.06022 to 0.05713, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2616 - mae: 0.2534 - val_loss: 0.0571 - val_mae: 0.1361\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2666 - mae: 0.2792\n",
            "Epoch 46: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2663 - mae: 0.2792 - val_loss: 0.0580 - val_mae: 0.1267\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2801 - mae: 0.2818\n",
            "Epoch 47: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2812 - mae: 0.2828 - val_loss: 0.1077 - val_mae: 0.2038\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6781 - mae: 0.4805\n",
            "Epoch 48: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6792 - mae: 0.4808 - val_loss: 0.4721 - val_mae: 0.2853\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5424 - mae: 0.4258\n",
            "Epoch 49: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5414 - mae: 0.4253 - val_loss: 0.2964 - val_mae: 0.2914\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3368 - mae: 0.3289\n",
            "Epoch 50: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3378 - mae: 0.3297 - val_loss: 0.0799 - val_mae: 0.1922\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2527 - mae: 0.2686\n",
            "Epoch 51: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2523 - mae: 0.2683 - val_loss: 0.0646 - val_mae: 0.1258\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2472 - mae: 0.2508\n",
            "Epoch 52: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2468 - mae: 0.2509 - val_loss: 0.1110 - val_mae: 0.2760\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2587 - mae: 0.2671\n",
            "Epoch 53: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2585 - mae: 0.2672 - val_loss: 0.1009 - val_mae: 0.2205\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2705 - mae: 0.2841\n",
            "Epoch 54: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2697 - mae: 0.2836 - val_loss: 0.1784 - val_mae: 0.3375\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2713 - mae: 0.2935\n",
            "Epoch 55: val_loss did not improve from 0.05713\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2715 - mae: 0.2940 - val_loss: 0.0823 - val_mae: 0.2112\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2395 - mae: 0.2532\n",
            "Epoch 56: val_loss improved from 0.05713 to 0.05672, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2396 - mae: 0.2532 - val_loss: 0.0567 - val_mae: 0.1022\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2232 - mae: 0.2465\n",
            "Epoch 57: val_loss did not improve from 0.05672\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2227 - mae: 0.2464 - val_loss: 0.0579 - val_mae: 0.1231\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2269 - mae: 0.2422\n",
            "Epoch 58: val_loss did not improve from 0.05672\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2269 - mae: 0.2421 - val_loss: 0.0627 - val_mae: 0.1500\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2330 - mae: 0.2529\n",
            "Epoch 59: val_loss did not improve from 0.05672\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2323 - mae: 0.2525 - val_loss: 0.0630 - val_mae: 0.1191\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2431 - mae: 0.2572\n",
            "Epoch 60: val_loss improved from 0.05672 to 0.05382, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2435 - mae: 0.2574 - val_loss: 0.0538 - val_mae: 0.1051\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2165 - mae: 0.2368\n",
            "Epoch 61: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2169 - mae: 0.2371 - val_loss: 0.0834 - val_mae: 0.1450\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2286 - mae: 0.2425\n",
            "Epoch 62: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2281 - mae: 0.2422 - val_loss: 0.1138 - val_mae: 0.2723\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2298 - mae: 0.2537\n",
            "Epoch 63: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2296 - mae: 0.2536 - val_loss: 0.0686 - val_mae: 0.1354\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2296 - mae: 0.2366\n",
            "Epoch 64: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2301 - mae: 0.2370 - val_loss: 0.0692 - val_mae: 0.1746\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2239 - mae: 0.2454\n",
            "Epoch 65: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2249 - mae: 0.2454 - val_loss: 0.0543 - val_mae: 0.0905\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3622 - mae: 0.3799\n",
            "Epoch 66: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3612 - mae: 0.3793 - val_loss: 0.0638 - val_mae: 0.1107\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2101 - mae: 0.2266\n",
            "Epoch 67: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2095 - mae: 0.2263 - val_loss: 0.0969 - val_mae: 0.2371\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2232 - mae: 0.2311\n",
            "Epoch 68: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2231 - mae: 0.2311 - val_loss: 0.0585 - val_mae: 0.1054\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3504 - mae: 0.3282\n",
            "Epoch 69: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3505 - mae: 0.3281 - val_loss: 0.3838 - val_mae: 0.2462\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2860 - mae: 0.2766\n",
            "Epoch 70: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2857 - mae: 0.2763 - val_loss: 0.0939 - val_mae: 0.1932\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2207 - mae: 0.2397\n",
            "Epoch 71: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2206 - mae: 0.2398 - val_loss: 0.0543 - val_mae: 0.1111\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2137 - mae: 0.2277\n",
            "Epoch 72: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2136 - mae: 0.2277 - val_loss: 0.0776 - val_mae: 0.1823\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2225 - mae: 0.2322\n",
            "Epoch 73: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2227 - mae: 0.2322 - val_loss: 0.0670 - val_mae: 0.1571\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3327 - mae: 0.3275\n",
            "Epoch 74: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3320 - mae: 0.3271 - val_loss: 0.2024 - val_mae: 0.3854\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2077 - mae: 0.2311\n",
            "Epoch 75: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2088 - mae: 0.2313 - val_loss: 0.0625 - val_mae: 0.1634\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2262 - mae: 0.2402\n",
            "Epoch 76: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2271 - mae: 0.2404 - val_loss: 0.0982 - val_mae: 0.2144\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2208 - mae: 0.2358\n",
            "Epoch 77: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2226 - mae: 0.2363 - val_loss: 0.0780 - val_mae: 0.1951\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2134 - mae: 0.2310\n",
            "Epoch 78: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2130 - mae: 0.2309 - val_loss: 0.0661 - val_mae: 0.1633\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1940 - mae: 0.2191\n",
            "Epoch 79: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1941 - mae: 0.2191 - val_loss: 0.0790 - val_mae: 0.2183\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2112 - mae: 0.2388\n",
            "Epoch 80: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2109 - mae: 0.2387 - val_loss: 0.0574 - val_mae: 0.1478\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3096 - mae: 0.3128\n",
            "Epoch 81: val_loss did not improve from 0.05382\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3087 - mae: 0.3124 - val_loss: 0.0625 - val_mae: 0.1217\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1937 - mae: 0.2162\n",
            "Epoch 82: val_loss improved from 0.05382 to 0.05157, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第4次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.1936 - mae: 0.2160 - val_loss: 0.0516 - val_mae: 0.0970\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2103 - mae: 0.2307\n",
            "Epoch 83: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2102 - mae: 0.2308 - val_loss: 0.0819 - val_mae: 0.1898\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2242 - mae: 0.2430\n",
            "Epoch 84: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2242 - mae: 0.2430 - val_loss: 0.0545 - val_mae: 0.1009\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1974 - mae: 0.2241\n",
            "Epoch 85: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1976 - mae: 0.2249 - val_loss: 0.1807 - val_mae: 0.3489\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2492 - mae: 0.2713\n",
            "Epoch 86: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2514 - mae: 0.2720 - val_loss: 0.0578 - val_mae: 0.1102\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1935 - mae: 0.2184\n",
            "Epoch 87: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1930 - mae: 0.2180 - val_loss: 0.0839 - val_mae: 0.2187\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2047 - mae: 0.2164\n",
            "Epoch 88: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2049 - mae: 0.2165 - val_loss: 0.0584 - val_mae: 0.1117\n",
            "Epoch 89/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.1915 - mae: 0.2204\n",
            "Epoch 89: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1915 - mae: 0.2204 - val_loss: 0.0650 - val_mae: 0.1392\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1883 - mae: 0.2112\n",
            "Epoch 90: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1888 - mae: 0.2116 - val_loss: 0.0533 - val_mae: 0.1151\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2612 - mae: 0.2778\n",
            "Epoch 91: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2609 - mae: 0.2775 - val_loss: 0.0730 - val_mae: 0.1247\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.2074\n",
            "Epoch 92: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1762 - mae: 0.2074 - val_loss: 0.0675 - val_mae: 0.1563\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2029 - mae: 0.2205\n",
            "Epoch 93: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2023 - mae: 0.2203 - val_loss: 0.0658 - val_mae: 0.1517\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.2013\n",
            "Epoch 94: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1771 - mae: 0.2015 - val_loss: 0.0739 - val_mae: 0.1498\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2611 - mae: 0.2930\n",
            "Epoch 95: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2603 - mae: 0.2924 - val_loss: 0.0631 - val_mae: 0.1438\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1706 - mae: 0.1982\n",
            "Epoch 96: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1709 - mae: 0.1984 - val_loss: 0.0572 - val_mae: 0.1200\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1830 - mae: 0.2032\n",
            "Epoch 97: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1827 - mae: 0.2033 - val_loss: 0.0541 - val_mae: 0.0944\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.2082\n",
            "Epoch 98: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1775 - mae: 0.2081 - val_loss: 0.0635 - val_mae: 0.1393\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1976 - mae: 0.2173\n",
            "Epoch 99: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1971 - mae: 0.2172 - val_loss: 0.0555 - val_mae: 0.0937\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1955 - mae: 0.2142\n",
            "Epoch 100: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1957 - mae: 0.2141 - val_loss: 0.0679 - val_mae: 0.1669\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1845 - mae: 0.2201\n",
            "Epoch 101: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1846 - mae: 0.2203 - val_loss: 0.0580 - val_mae: 0.1032\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2003 - mae: 0.2230\n",
            "Epoch 102: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1998 - mae: 0.2229 - val_loss: 0.0620 - val_mae: 0.1047\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3786 - mae: 0.3451\n",
            "Epoch 103: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3789 - mae: 0.3450 - val_loss: 0.0728 - val_mae: 0.1700\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1914 - mae: 0.2226\n",
            "Epoch 104: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1916 - mae: 0.2228 - val_loss: 0.0908 - val_mae: 0.2037\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1876 - mae: 0.2148\n",
            "Epoch 105: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1876 - mae: 0.2149 - val_loss: 0.0609 - val_mae: 0.1521\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1889 - mae: 0.2098\n",
            "Epoch 106: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1891 - mae: 0.2098 - val_loss: 0.0546 - val_mae: 0.1083\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1807 - mae: 0.2183\n",
            "Epoch 107: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1806 - mae: 0.2180 - val_loss: 0.0580 - val_mae: 0.1176\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1663 - mae: 0.1969\n",
            "Epoch 108: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1664 - mae: 0.1970 - val_loss: 0.0566 - val_mae: 0.1169\n",
            "Epoch 109/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3353 - mae: 0.2943\n",
            "Epoch 109: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3356 - mae: 0.2954 - val_loss: 0.4326 - val_mae: 0.4313\n",
            "Epoch 110/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3653 - mae: 0.3488\n",
            "Epoch 110: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3644 - mae: 0.3481 - val_loss: 0.1912 - val_mae: 0.3053\n",
            "Epoch 111/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2025 - mae: 0.2391\n",
            "Epoch 111: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2032 - mae: 0.2394 - val_loss: 0.0646 - val_mae: 0.1118\n",
            "Epoch 112/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1917 - mae: 0.2196\n",
            "Epoch 112: val_loss did not improve from 0.05157\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1914 - mae: 0.2195 - val_loss: 0.0675 - val_mae: 0.1733\n",
            "Epoch 112: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07354951649904251, RMSE:0.27120015025138855, MAE:0.11981920152902603, R2:0.9195682834084178\n",
            "4 0.9195682834084178\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07621617615222931, RMSE:0.27607277035713196, MAE:0.1167503148317337, R2:0.9151527028515862\n",
            "r2: 0.9151527028515862 rmse: 0.27607277\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07621617615222931, RMSE:0.27607277035713196, MAE:0.1167503148317337, R2:0.9151527028515862\n",
            "r2: 0.9151527028515862 rmse: 0.27607277\n",
            "第5次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_8/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_8'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_43\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_32 (Embedding)       (None, 17, 900)      649800      ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_33 (Embedding)       (None, 17, 900)      649800      ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_34 (Embedding)       (None, 17, 900)      649800      ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_35 (Embedding)       (None, 17, 900)      649800      ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 17, 100)      270100      ['embedding_32[0][0]',           \n",
            "                                                                  'embedding_33[0][0]',           \n",
            "                                                                  'embedding_34[0][0]',           \n",
            "                                                                  'embedding_35[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 17, 100)      270100      ['embedding_32[0][0]',           \n",
            "                                                                  'embedding_33[0][0]',           \n",
            "                                                                  'embedding_34[0][0]',           \n",
            "                                                                  'embedding_35[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 17, 100)      270100      ['embedding_32[0][0]',           \n",
            "                                                                  'embedding_33[0][0]',           \n",
            "                                                                  'embedding_34[0][0]',           \n",
            "                                                                  'embedding_35[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_96 (Globa  (None, 100)         0           ['conv1d_24[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_97 (Globa  (None, 100)         0           ['conv1d_24[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_98 (Globa  (None, 100)         0           ['conv1d_24[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_99 (Globa  (None, 100)         0           ['conv1d_24[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_100 (Glob  (None, 100)         0           ['conv1d_25[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_101 (Glob  (None, 100)         0           ['conv1d_25[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_102 (Glob  (None, 100)         0           ['conv1d_25[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_103 (Glob  (None, 100)         0           ['conv1d_25[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_104 (Glob  (None, 100)         0           ['conv1d_26[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_105 (Glob  (None, 100)         0           ['conv1d_26[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_106 (Glob  (None, 100)         0           ['conv1d_26[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_107 (Glob  (None, 100)         0           ['conv1d_26[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 100)          0           ['global_max_pooling1d_96[0][0]',\n",
            "                                                                  'global_max_pooling1d_97[0][0]',\n",
            "                                                                  'global_max_pooling1d_98[0][0]',\n",
            "                                                                  'global_max_pooling1d_99[0][0]']\n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 100)          0           ['global_max_pooling1d_100[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_101[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_102[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_103[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 100)          0           ['global_max_pooling1d_104[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_105[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_106[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_107[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 1000)         11000       ['input_18[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 1300)         0           ['add_24[0][0]',                 \n",
            "                                                                  'add_25[0][0]',                 \n",
            "                                                                  'add_26[0][0]',                 \n",
            "                                                                  'dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_8 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_8[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_8 (Gl  (1, 1300)           0           ['tf.expand_dims_8[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_8 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (1, 1, 1, 650)       845650      ['reshape_8[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_8 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_33[0][0]']               \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_8[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_8 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_8[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_8[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_8 (TFOpLa  (None, 1300)        0           ['multiply_8[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_8[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 1)            3           ['dropout_44[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.0650 - mae: 1.0787\n",
            "Epoch 1: val_loss improved from inf to 0.60009, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 45ms/step - loss: 3.0650 - mae: 1.0787 - val_loss: 0.6001 - val_mae: 0.4928\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8486 - mae: 0.5478\n",
            "Epoch 2: val_loss improved from 0.60009 to 0.45896, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.8494 - mae: 0.5481 - val_loss: 0.4590 - val_mae: 0.4054\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7593 - mae: 0.5295\n",
            "Epoch 3: val_loss did not improve from 0.45896\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.7588 - mae: 0.5288 - val_loss: 0.6750 - val_mae: 0.5590\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7383 - mae: 0.5249\n",
            "Epoch 4: val_loss improved from 0.45896 to 0.45307, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7384 - mae: 0.5250 - val_loss: 0.4531 - val_mae: 0.4116\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6442 - mae: 0.4879\n",
            "Epoch 5: val_loss improved from 0.45307 to 0.44175, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6454 - mae: 0.4884 - val_loss: 0.4417 - val_mae: 0.4070\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6547 - mae: 0.4825\n",
            "Epoch 6: val_loss improved from 0.44175 to 0.38825, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6539 - mae: 0.4828 - val_loss: 0.3883 - val_mae: 0.3449\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6428 - mae: 0.4931\n",
            "Epoch 7: val_loss improved from 0.38825 to 0.37355, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6433 - mae: 0.4931 - val_loss: 0.3735 - val_mae: 0.3192\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6374 - mae: 0.4779\n",
            "Epoch 8: val_loss did not improve from 0.37355\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6393 - mae: 0.4785 - val_loss: 0.3782 - val_mae: 0.3649\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6119 - mae: 0.4701\n",
            "Epoch 9: val_loss did not improve from 0.37355\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6114 - mae: 0.4699 - val_loss: 0.4182 - val_mae: 0.4263\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6333 - mae: 0.4863\n",
            "Epoch 10: val_loss did not improve from 0.37355\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6333 - mae: 0.4864 - val_loss: 0.4098 - val_mae: 0.4647\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5695 - mae: 0.4491\n",
            "Epoch 11: val_loss improved from 0.37355 to 0.35363, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.5698 - mae: 0.4493 - val_loss: 0.3536 - val_mae: 0.3139\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5434 - mae: 0.4275\n",
            "Epoch 12: val_loss improved from 0.35363 to 0.33918, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5424 - mae: 0.4269 - val_loss: 0.3392 - val_mae: 0.3782\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5589 - mae: 0.4538\n",
            "Epoch 13: val_loss improved from 0.33918 to 0.29174, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5601 - mae: 0.4539 - val_loss: 0.2917 - val_mae: 0.3066\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5099 - mae: 0.4209\n",
            "Epoch 14: val_loss did not improve from 0.29174\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5101 - mae: 0.4209 - val_loss: 0.4526 - val_mae: 0.5064\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5396 - mae: 0.4670\n",
            "Epoch 15: val_loss did not improve from 0.29174\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5389 - mae: 0.4667 - val_loss: 0.7330 - val_mae: 0.6888\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5136 - mae: 0.4193\n",
            "Epoch 16: val_loss improved from 0.29174 to 0.26639, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.5154 - mae: 0.4196 - val_loss: 0.2664 - val_mae: 0.3044\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4656 - mae: 0.3900\n",
            "Epoch 17: val_loss did not improve from 0.26639\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4642 - mae: 0.3892 - val_loss: 0.3291 - val_mae: 0.4349\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5017 - mae: 0.4314\n",
            "Epoch 18: val_loss improved from 0.26639 to 0.19096, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.5011 - mae: 0.4309 - val_loss: 0.1910 - val_mae: 0.2179\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3953 - mae: 0.3621\n",
            "Epoch 19: val_loss improved from 0.19096 to 0.18849, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3945 - mae: 0.3618 - val_loss: 0.1885 - val_mae: 0.2410\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4131 - mae: 0.3726\n",
            "Epoch 20: val_loss improved from 0.18849 to 0.14653, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4130 - mae: 0.3724 - val_loss: 0.1465 - val_mae: 0.2215\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3389 - mae: 0.3275\n",
            "Epoch 21: val_loss improved from 0.14653 to 0.10699, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3383 - mae: 0.3275 - val_loss: 0.1070 - val_mae: 0.1736\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3843 - mae: 0.3507\n",
            "Epoch 22: val_loss did not improve from 0.10699\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3835 - mae: 0.3503 - val_loss: 0.2377 - val_mae: 0.3809\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3266 - mae: 0.3303\n",
            "Epoch 23: val_loss did not improve from 0.10699\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.3261 - mae: 0.3304 - val_loss: 0.5563 - val_mae: 0.6659\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3357 - mae: 0.3312\n",
            "Epoch 24: val_loss did not improve from 0.10699\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3352 - mae: 0.3310 - val_loss: 0.1286 - val_mae: 0.2480\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3404 - mae: 0.3359\n",
            "Epoch 25: val_loss improved from 0.10699 to 0.09468, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.3406 - mae: 0.3357 - val_loss: 0.0947 - val_mae: 0.2051\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3044 - mae: 0.3026\n",
            "Epoch 26: val_loss did not improve from 0.09468\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3035 - mae: 0.3022 - val_loss: 0.1178 - val_mae: 0.1476\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3230 - mae: 0.3144\n",
            "Epoch 27: val_loss improved from 0.09468 to 0.07762, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3221 - mae: 0.3140 - val_loss: 0.0776 - val_mae: 0.1681\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3128 - mae: 0.3101\n",
            "Epoch 28: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3133 - mae: 0.3103 - val_loss: 0.0970 - val_mae: 0.1388\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2986 - mae: 0.2965\n",
            "Epoch 29: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2980 - mae: 0.2964 - val_loss: 0.0959 - val_mae: 0.1444\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3039 - mae: 0.3070\n",
            "Epoch 30: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3039 - mae: 0.3072 - val_loss: 0.1184 - val_mae: 0.2145\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3126 - mae: 0.3074\n",
            "Epoch 31: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3126 - mae: 0.3077 - val_loss: 0.1677 - val_mae: 0.2896\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2795 - mae: 0.2812\n",
            "Epoch 32: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2788 - mae: 0.2810 - val_loss: 0.0853 - val_mae: 0.1840\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3277 - mae: 0.3271\n",
            "Epoch 33: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3276 - mae: 0.3272 - val_loss: 0.0796 - val_mae: 0.1509\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2884 - mae: 0.2859\n",
            "Epoch 34: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2877 - mae: 0.2858 - val_loss: 0.1662 - val_mae: 0.3276\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2752 - mae: 0.2852\n",
            "Epoch 35: val_loss did not improve from 0.07762\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2745 - mae: 0.2849 - val_loss: 0.0793 - val_mae: 0.1774\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2560 - mae: 0.2731\n",
            "Epoch 36: val_loss improved from 0.07762 to 0.07081, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2566 - mae: 0.2734 - val_loss: 0.0708 - val_mae: 0.1542\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2622 - mae: 0.2772\n",
            "Epoch 37: val_loss did not improve from 0.07081\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2622 - mae: 0.2773 - val_loss: 0.1333 - val_mae: 0.3010\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2876 - mae: 0.2900\n",
            "Epoch 38: val_loss did not improve from 0.07081\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2882 - mae: 0.2902 - val_loss: 0.0709 - val_mae: 0.1707\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2720 - mae: 0.2754\n",
            "Epoch 39: val_loss did not improve from 0.07081\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2723 - mae: 0.2755 - val_loss: 0.0763 - val_mae: 0.1857\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2692 - mae: 0.2720\n",
            "Epoch 40: val_loss did not improve from 0.07081\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2700 - mae: 0.2720 - val_loss: 0.1089 - val_mae: 0.1898\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2769 - mae: 0.2771\n",
            "Epoch 41: val_loss improved from 0.07081 to 0.06377, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2765 - mae: 0.2771 - val_loss: 0.0638 - val_mae: 0.1202\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3001 - mae: 0.3084\n",
            "Epoch 42: val_loss did not improve from 0.06377\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3016 - mae: 0.3089 - val_loss: 0.0717 - val_mae: 0.1176\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2450 - mae: 0.2666\n",
            "Epoch 43: val_loss improved from 0.06377 to 0.05816, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2451 - mae: 0.2666 - val_loss: 0.0582 - val_mae: 0.1346\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2595 - mae: 0.2806\n",
            "Epoch 44: val_loss did not improve from 0.05816\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2597 - mae: 0.2807 - val_loss: 0.0590 - val_mae: 0.1037\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2520 - mae: 0.2617\n",
            "Epoch 45: val_loss did not improve from 0.05816\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2513 - mae: 0.2614 - val_loss: 0.0903 - val_mae: 0.2151\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2650 - mae: 0.2746\n",
            "Epoch 46: val_loss did not improve from 0.05816\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2651 - mae: 0.2751 - val_loss: 0.0697 - val_mae: 0.1608\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2966 - mae: 0.3133\n",
            "Epoch 47: val_loss did not improve from 0.05816\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2959 - mae: 0.3129 - val_loss: 0.0860 - val_mae: 0.2080\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2601 - mae: 0.2575\n",
            "Epoch 48: val_loss improved from 0.05816 to 0.05410, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第5次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2598 - mae: 0.2575 - val_loss: 0.0541 - val_mae: 0.1215\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2490 - mae: 0.2498\n",
            "Epoch 49: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2495 - mae: 0.2503 - val_loss: 0.1040 - val_mae: 0.2218\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2667 - mae: 0.2889\n",
            "Epoch 50: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2663 - mae: 0.2886 - val_loss: 0.1572 - val_mae: 0.3307\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2442 - mae: 0.2619\n",
            "Epoch 51: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2437 - mae: 0.2620 - val_loss: 0.0658 - val_mae: 0.1666\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2407 - mae: 0.2745\n",
            "Epoch 52: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2401 - mae: 0.2742 - val_loss: 0.0593 - val_mae: 0.1251\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2402 - mae: 0.2561\n",
            "Epoch 53: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2408 - mae: 0.2563 - val_loss: 0.1288 - val_mae: 0.2473\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2525 - mae: 0.2721\n",
            "Epoch 54: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2520 - mae: 0.2721 - val_loss: 0.0788 - val_mae: 0.1151\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2572 - mae: 0.2621\n",
            "Epoch 55: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2576 - mae: 0.2621 - val_loss: 0.0630 - val_mae: 0.1622\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2587 - mae: 0.2745\n",
            "Epoch 56: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2592 - mae: 0.2742 - val_loss: 0.0563 - val_mae: 0.1328\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2270 - mae: 0.2446\n",
            "Epoch 57: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2272 - mae: 0.2448 - val_loss: 0.0584 - val_mae: 0.1416\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2197 - mae: 0.2453\n",
            "Epoch 58: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2192 - mae: 0.2451 - val_loss: 0.0771 - val_mae: 0.1532\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2253 - mae: 0.2514\n",
            "Epoch 59: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2254 - mae: 0.2514 - val_loss: 0.0543 - val_mae: 0.0937\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2445 - mae: 0.2712\n",
            "Epoch 60: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2442 - mae: 0.2708 - val_loss: 0.0751 - val_mae: 0.1896\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2685 - mae: 0.2738\n",
            "Epoch 61: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2692 - mae: 0.2741 - val_loss: 0.0576 - val_mae: 0.0955\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2044 - mae: 0.2344\n",
            "Epoch 62: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2060 - mae: 0.2350 - val_loss: 0.0634 - val_mae: 0.1434\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2364 - mae: 0.2483\n",
            "Epoch 63: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2364 - mae: 0.2482 - val_loss: 0.0802 - val_mae: 0.1647\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2293 - mae: 0.2584\n",
            "Epoch 64: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2290 - mae: 0.2584 - val_loss: 0.0570 - val_mae: 0.1027\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2537 - mae: 0.2708\n",
            "Epoch 65: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2540 - mae: 0.2708 - val_loss: 0.1264 - val_mae: 0.2597\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2441 - mae: 0.2685\n",
            "Epoch 66: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2441 - mae: 0.2687 - val_loss: 0.1723 - val_mae: 0.3583\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2222 - mae: 0.2520\n",
            "Epoch 67: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2244 - mae: 0.2523 - val_loss: 0.0543 - val_mae: 0.1107\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2184 - mae: 0.2342\n",
            "Epoch 68: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2180 - mae: 0.2343 - val_loss: 0.0613 - val_mae: 0.1347\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2284 - mae: 0.2565\n",
            "Epoch 69: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2282 - mae: 0.2564 - val_loss: 0.0548 - val_mae: 0.1001\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2082 - mae: 0.2372\n",
            "Epoch 70: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2080 - mae: 0.2371 - val_loss: 0.0668 - val_mae: 0.1583\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2319 - mae: 0.2503\n",
            "Epoch 71: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2315 - mae: 0.2503 - val_loss: 0.0615 - val_mae: 0.1251\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2181 - mae: 0.2381\n",
            "Epoch 72: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2185 - mae: 0.2382 - val_loss: 0.0557 - val_mae: 0.0997\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2413 - mae: 0.2806\n",
            "Epoch 73: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2407 - mae: 0.2803 - val_loss: 0.0648 - val_mae: 0.1149\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2117 - mae: 0.2452\n",
            "Epoch 74: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2112 - mae: 0.2450 - val_loss: 0.1192 - val_mae: 0.2919\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2158 - mae: 0.2410\n",
            "Epoch 75: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2168 - mae: 0.2417 - val_loss: 0.0797 - val_mae: 0.2047\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2451 - mae: 0.2732\n",
            "Epoch 76: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2452 - mae: 0.2733 - val_loss: 0.0643 - val_mae: 0.1562\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2195 - mae: 0.2433\n",
            "Epoch 77: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2191 - mae: 0.2433 - val_loss: 0.1591 - val_mae: 0.3301\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2445 - mae: 0.2861\n",
            "Epoch 78: val_loss did not improve from 0.05410\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2442 - mae: 0.2859 - val_loss: 0.0643 - val_mae: 0.1478\n",
            "Epoch 78: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07548614591360092, RMSE:0.27474743127822876, MAE:0.1417754888534546, R2:0.9174504361181719\n",
            "5 0.9174504361181719\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07732007652521133, RMSE:0.27806487679481506, MAE:0.14129313826560974, R2:0.9139237875328569\n",
            "r2: 0.9139237875328569 rmse: 0.27806488\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07732007652521133, RMSE:0.27806487679481506, MAE:0.14129313826560974, R2:0.9139237875328569\n",
            "r2: 0.9139237875328569 rmse: 0.27806488\n",
            "第6次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_9/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_9'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_49\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_36 (Embedding)       (None, 17, 900)      649800      ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_37 (Embedding)       (None, 17, 900)      649800      ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_38 (Embedding)       (None, 17, 900)      649800      ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_39 (Embedding)       (None, 17, 900)      649800      ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 17, 100)      270100      ['embedding_36[0][0]',           \n",
            "                                                                  'embedding_37[0][0]',           \n",
            "                                                                  'embedding_38[0][0]',           \n",
            "                                                                  'embedding_39[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 17, 100)      270100      ['embedding_36[0][0]',           \n",
            "                                                                  'embedding_37[0][0]',           \n",
            "                                                                  'embedding_38[0][0]',           \n",
            "                                                                  'embedding_39[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 17, 100)      270100      ['embedding_36[0][0]',           \n",
            "                                                                  'embedding_37[0][0]',           \n",
            "                                                                  'embedding_38[0][0]',           \n",
            "                                                                  'embedding_39[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_108 (Glob  (None, 100)         0           ['conv1d_27[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_109 (Glob  (None, 100)         0           ['conv1d_27[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_110 (Glob  (None, 100)         0           ['conv1d_27[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_111 (Glob  (None, 100)         0           ['conv1d_27[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_112 (Glob  (None, 100)         0           ['conv1d_28[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_113 (Glob  (None, 100)         0           ['conv1d_28[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_114 (Glob  (None, 100)         0           ['conv1d_28[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_115 (Glob  (None, 100)         0           ['conv1d_28[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_116 (Glob  (None, 100)         0           ['conv1d_29[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_117 (Glob  (None, 100)         0           ['conv1d_29[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_118 (Glob  (None, 100)         0           ['conv1d_29[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_119 (Glob  (None, 100)         0           ['conv1d_29[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 100)          0           ['global_max_pooling1d_108[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_109[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_110[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_111[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 100)          0           ['global_max_pooling1d_112[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_113[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_114[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_115[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 100)          0           ['global_max_pooling1d_116[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_117[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_118[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_119[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 1000)         11000       ['input_20[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 1300)         0           ['add_27[0][0]',                 \n",
            "                                                                  'add_28[0][0]',                 \n",
            "                                                                  'add_29[0][0]',                 \n",
            "                                                                  'dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_9 (TFOpLambda)  (1, None, 1300)      0           ['concatenate_9[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_9 (Gl  (1, 1300)           0           ['tf.expand_dims_9[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_9 (Reshape)            (1, 1, 1, 1300)      0           ['global_average_pooling1d_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (1, 1, 1, 650)       845650      ['reshape_9[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_9 (TFOpLambda)      (1, 1, 1, 650)       0           ['dense_37[0][0]']               \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_9[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_9 (TFOpLambda)  (1, 1, 1, 1300)     0           ['dense_38[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (1, 1, None, 1300)   0           ['tf.expand_dims_9[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_9[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_9 (TFOpLa  (None, 1300)        0           ['multiply_9[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_9[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_45[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_46[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 1)            3           ['dropout_49[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.1755 - mae: 0.9287\n",
            "Epoch 1: val_loss improved from inf to 0.66474, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 6s 44ms/step - loss: 2.1718 - mae: 0.9281 - val_loss: 0.6647 - val_mae: 0.6577\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7754 - mae: 0.5505\n",
            "Epoch 2: val_loss improved from 0.66474 to 0.42652, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7747 - mae: 0.5501 - val_loss: 0.4265 - val_mae: 0.3316\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7088 - mae: 0.5203\n",
            "Epoch 3: val_loss improved from 0.42652 to 0.38825, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.7090 - mae: 0.5204 - val_loss: 0.3882 - val_mae: 0.3378\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6703 - mae: 0.5055\n",
            "Epoch 4: val_loss improved from 0.38825 to 0.38234, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6708 - mae: 0.5056 - val_loss: 0.3823 - val_mae: 0.3229\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6681 - mae: 0.5221\n",
            "Epoch 5: val_loss did not improve from 0.38234\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6683 - mae: 0.5222 - val_loss: 0.5212 - val_mae: 0.5687\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6621 - mae: 0.5054\n",
            "Epoch 6: val_loss improved from 0.38234 to 0.37852, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6614 - mae: 0.5053 - val_loss: 0.3785 - val_mae: 0.3575\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6264 - mae: 0.4891\n",
            "Epoch 7: val_loss improved from 0.37852 to 0.35496, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6266 - mae: 0.4891 - val_loss: 0.3550 - val_mae: 0.2976\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6126 - mae: 0.4676\n",
            "Epoch 8: val_loss did not improve from 0.35496\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6116 - mae: 0.4672 - val_loss: 0.6039 - val_mae: 0.5909\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5842 - mae: 0.4490\n",
            "Epoch 9: val_loss improved from 0.35496 to 0.33177, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5843 - mae: 0.4490 - val_loss: 0.3318 - val_mae: 0.3115\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5134 - mae: 0.4256\n",
            "Epoch 10: val_loss did not improve from 0.33177\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5135 - mae: 0.4256 - val_loss: 0.3610 - val_mae: 0.3485\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5804 - mae: 0.4702\n",
            "Epoch 11: val_loss improved from 0.33177 to 0.30753, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5815 - mae: 0.4706 - val_loss: 0.3075 - val_mae: 0.3244\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5037 - mae: 0.4104\n",
            "Epoch 12: val_loss did not improve from 0.30753\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5028 - mae: 0.4102 - val_loss: 0.3334 - val_mae: 0.3254\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4869 - mae: 0.3990\n",
            "Epoch 13: val_loss did not improve from 0.30753\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4896 - mae: 0.3999 - val_loss: 0.4309 - val_mae: 0.5110\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4606 - mae: 0.4097\n",
            "Epoch 14: val_loss improved from 0.30753 to 0.19538, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4625 - mae: 0.4101 - val_loss: 0.1954 - val_mae: 0.2519\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4568 - mae: 0.4043\n",
            "Epoch 15: val_loss did not improve from 0.19538\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4584 - mae: 0.4050 - val_loss: 0.2414 - val_mae: 0.3491\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3836 - mae: 0.3504\n",
            "Epoch 16: val_loss did not improve from 0.19538\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3838 - mae: 0.3506 - val_loss: 0.2514 - val_mae: 0.4009\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3848 - mae: 0.3613\n",
            "Epoch 17: val_loss did not improve from 0.19538\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3859 - mae: 0.3614 - val_loss: 0.1963 - val_mae: 0.2535\n",
            "Epoch 18/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3457 - mae: 0.3383\n",
            "Epoch 18: val_loss improved from 0.19538 to 0.11380, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.3457 - mae: 0.3383 - val_loss: 0.1138 - val_mae: 0.1704\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3575 - mae: 0.3414\n",
            "Epoch 19: val_loss improved from 0.11380 to 0.10642, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3567 - mae: 0.3410 - val_loss: 0.1064 - val_mae: 0.1886\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3432 - mae: 0.3319\n",
            "Epoch 20: val_loss did not improve from 0.10642\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3430 - mae: 0.3318 - val_loss: 0.1213 - val_mae: 0.2243\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3008 - mae: 0.3012\n",
            "Epoch 21: val_loss did not improve from 0.10642\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3001 - mae: 0.3011 - val_loss: 0.3624 - val_mae: 0.5457\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4763 - mae: 0.4169\n",
            "Epoch 22: val_loss did not improve from 0.10642\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4755 - mae: 0.4164 - val_loss: 0.1124 - val_mae: 0.1709\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3818 - mae: 0.3533\n",
            "Epoch 23: val_loss did not improve from 0.10642\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3820 - mae: 0.3532 - val_loss: 0.1167 - val_mae: 0.2411\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3432 - mae: 0.3166\n",
            "Epoch 24: val_loss improved from 0.10642 to 0.10327, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 44ms/step - loss: 0.3430 - mae: 0.3166 - val_loss: 0.1033 - val_mae: 0.1647\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3213 - mae: 0.3131\n",
            "Epoch 25: val_loss improved from 0.10327 to 0.08633, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3211 - mae: 0.3131 - val_loss: 0.0863 - val_mae: 0.2126\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3045 - mae: 0.2985\n",
            "Epoch 26: val_loss did not improve from 0.08633\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3039 - mae: 0.2984 - val_loss: 0.1485 - val_mae: 0.3116\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2836 - mae: 0.2817\n",
            "Epoch 27: val_loss did not improve from 0.08633\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2843 - mae: 0.2820 - val_loss: 0.0957 - val_mae: 0.1914\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3018 - mae: 0.3009\n",
            "Epoch 28: val_loss improved from 0.08633 to 0.06870, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 0.3010 - mae: 0.3005 - val_loss: 0.0687 - val_mae: 0.1512\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2983 - mae: 0.2924\n",
            "Epoch 29: val_loss did not improve from 0.06870\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2977 - mae: 0.2923 - val_loss: 0.1305 - val_mae: 0.3066\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3675 - mae: 0.3400\n",
            "Epoch 30: val_loss did not improve from 0.06870\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3670 - mae: 0.3398 - val_loss: 0.3406 - val_mae: 0.2587\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3318 - mae: 0.3256\n",
            "Epoch 31: val_loss did not improve from 0.06870\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3329 - mae: 0.3257 - val_loss: 0.0823 - val_mae: 0.1842\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2837 - mae: 0.2978\n",
            "Epoch 32: val_loss did not improve from 0.06870\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2855 - mae: 0.2980 - val_loss: 0.0773 - val_mae: 0.1509\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2743 - mae: 0.2778\n",
            "Epoch 33: val_loss did not improve from 0.06870\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2738 - mae: 0.2776 - val_loss: 0.0993 - val_mae: 0.2224\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2984 - mae: 0.2943\n",
            "Epoch 34: val_loss improved from 0.06870 to 0.06616, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2999 - mae: 0.2948 - val_loss: 0.0662 - val_mae: 0.1312\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2724 - mae: 0.2727\n",
            "Epoch 35: val_loss did not improve from 0.06616\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2717 - mae: 0.2724 - val_loss: 0.0948 - val_mae: 0.2346\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2797 - mae: 0.2740\n",
            "Epoch 36: val_loss did not improve from 0.06616\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2797 - mae: 0.2741 - val_loss: 0.1488 - val_mae: 0.3379\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2953 - mae: 0.3063\n",
            "Epoch 37: val_loss improved from 0.06616 to 0.06149, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2950 - mae: 0.3062 - val_loss: 0.0615 - val_mae: 0.1375\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2444 - mae: 0.2555\n",
            "Epoch 38: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2448 - mae: 0.2558 - val_loss: 0.0937 - val_mae: 0.2273\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2564 - mae: 0.2647\n",
            "Epoch 39: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2566 - mae: 0.2649 - val_loss: 0.2364 - val_mae: 0.3875\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2952 - mae: 0.2853\n",
            "Epoch 40: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2950 - mae: 0.2851 - val_loss: 0.0652 - val_mae: 0.1603\n",
            "Epoch 41/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2642 - mae: 0.2688\n",
            "Epoch 41: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2642 - mae: 0.2688 - val_loss: 0.0797 - val_mae: 0.1420\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3206 - mae: 0.3128\n",
            "Epoch 42: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3209 - mae: 0.3130 - val_loss: 0.2990 - val_mae: 0.3671\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2648 - mae: 0.2623\n",
            "Epoch 43: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2644 - mae: 0.2623 - val_loss: 0.0700 - val_mae: 0.1464\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2896 - mae: 0.2886\n",
            "Epoch 44: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2893 - mae: 0.2884 - val_loss: 0.0796 - val_mae: 0.1269\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2390 - mae: 0.2520\n",
            "Epoch 45: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2383 - mae: 0.2517 - val_loss: 0.0746 - val_mae: 0.1920\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2280 - mae: 0.2438\n",
            "Epoch 46: val_loss did not improve from 0.06149\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2292 - mae: 0.2443 - val_loss: 0.0813 - val_mae: 0.2212\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2357 - mae: 0.2559\n",
            "Epoch 47: val_loss improved from 0.06149 to 0.05612, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2352 - mae: 0.2555 - val_loss: 0.0561 - val_mae: 0.1281\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2508 - mae: 0.2618\n",
            "Epoch 48: val_loss did not improve from 0.05612\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2522 - mae: 0.2621 - val_loss: 0.1297 - val_mae: 0.2830\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2351 - mae: 0.2494\n",
            "Epoch 49: val_loss did not improve from 0.05612\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2349 - mae: 0.2493 - val_loss: 0.0744 - val_mae: 0.1184\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2452 - mae: 0.2570\n",
            "Epoch 50: val_loss improved from 0.05612 to 0.05191, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第6次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.2446 - mae: 0.2567 - val_loss: 0.0519 - val_mae: 0.0890\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2421 - mae: 0.2570\n",
            "Epoch 51: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2436 - mae: 0.2578 - val_loss: 0.0926 - val_mae: 0.1811\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4503 - mae: 0.3801\n",
            "Epoch 52: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.4503 - mae: 0.3803 - val_loss: 0.5600 - val_mae: 0.5231\n",
            "Epoch 53/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.6609 - mae: 0.4746\n",
            "Epoch 53: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6609 - mae: 0.4746 - val_loss: 0.4468 - val_mae: 0.4000\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5998 - mae: 0.4441\n",
            "Epoch 54: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.5988 - mae: 0.4436 - val_loss: 0.3877 - val_mae: 0.3443\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5619 - mae: 0.4278\n",
            "Epoch 55: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.5615 - mae: 0.4281 - val_loss: 0.3556 - val_mae: 0.3416\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4157 - mae: 0.3836\n",
            "Epoch 56: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.4156 - mae: 0.3833 - val_loss: 0.1797 - val_mae: 0.2658\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3048 - mae: 0.3108\n",
            "Epoch 57: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3042 - mae: 0.3105 - val_loss: 0.1326 - val_mae: 0.2587\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2407 - mae: 0.2693\n",
            "Epoch 58: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2419 - mae: 0.2698 - val_loss: 0.0707 - val_mae: 0.1238\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2640 - mae: 0.2847\n",
            "Epoch 59: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2642 - mae: 0.2848 - val_loss: 0.0837 - val_mae: 0.1564\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2468 - mae: 0.2608\n",
            "Epoch 60: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2469 - mae: 0.2608 - val_loss: 0.0870 - val_mae: 0.1732\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2377 - mae: 0.2468\n",
            "Epoch 61: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2382 - mae: 0.2472 - val_loss: 0.0770 - val_mae: 0.1437\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2455 - mae: 0.2554\n",
            "Epoch 62: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2457 - mae: 0.2554 - val_loss: 0.1204 - val_mae: 0.2640\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2121 - mae: 0.2429\n",
            "Epoch 63: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2117 - mae: 0.2427 - val_loss: 0.0605 - val_mae: 0.1228\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2169 - mae: 0.2381\n",
            "Epoch 64: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2166 - mae: 0.2379 - val_loss: 0.1159 - val_mae: 0.2536\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2178 - mae: 0.2428\n",
            "Epoch 65: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2176 - mae: 0.2430 - val_loss: 0.0720 - val_mae: 0.1706\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2233 - mae: 0.2307\n",
            "Epoch 66: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2228 - mae: 0.2307 - val_loss: 0.0965 - val_mae: 0.2333\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2483 - mae: 0.2665\n",
            "Epoch 67: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2483 - mae: 0.2665 - val_loss: 0.0620 - val_mae: 0.1498\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2270 - mae: 0.2365\n",
            "Epoch 68: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2263 - mae: 0.2361 - val_loss: 0.0835 - val_mae: 0.2122\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2258 - mae: 0.2350\n",
            "Epoch 69: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2257 - mae: 0.2349 - val_loss: 0.0558 - val_mae: 0.1044\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2555 - mae: 0.2570\n",
            "Epoch 70: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2555 - mae: 0.2570 - val_loss: 0.0671 - val_mae: 0.1701\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2142 - mae: 0.2300\n",
            "Epoch 71: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2141 - mae: 0.2299 - val_loss: 0.0697 - val_mae: 0.1766\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2035 - mae: 0.2227\n",
            "Epoch 72: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2033 - mae: 0.2225 - val_loss: 0.0744 - val_mae: 0.1657\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2097 - mae: 0.2326\n",
            "Epoch 73: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2095 - mae: 0.2328 - val_loss: 0.0704 - val_mae: 0.1428\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2102 - mae: 0.2307\n",
            "Epoch 74: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2102 - mae: 0.2309 - val_loss: 0.0583 - val_mae: 0.1122\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2101 - mae: 0.2207\n",
            "Epoch 75: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2106 - mae: 0.2209 - val_loss: 0.0525 - val_mae: 0.0887\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1888 - mae: 0.2126\n",
            "Epoch 76: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1902 - mae: 0.2128 - val_loss: 0.0764 - val_mae: 0.1938\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3098 - mae: 0.2842\n",
            "Epoch 77: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3137 - mae: 0.2853 - val_loss: 0.5135 - val_mae: 0.3512\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5846 - mae: 0.4445\n",
            "Epoch 78: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.5836 - mae: 0.4439 - val_loss: 0.3752 - val_mae: 0.3051\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3474 - mae: 0.3277\n",
            "Epoch 79: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3472 - mae: 0.3277 - val_loss: 0.0746 - val_mae: 0.1382\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1994 - mae: 0.2218\n",
            "Epoch 80: val_loss did not improve from 0.05191\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.1995 - mae: 0.2220 - val_loss: 0.0624 - val_mae: 0.1089\n",
            "Epoch 80: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 6ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07410497218370438, RMSE:0.27222228050231934, MAE:0.11166403442621231, R2:0.9189608483056998\n",
            "6 0.9189608483056998\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07413321733474731, RMSE:0.2722741663455963, MAE:0.10843803733587265, R2:0.9174715523063106\n",
            "r2: 0.9174715523063106 rmse: 0.27227417\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07413321733474731, RMSE:0.2722741663455963, MAE:0.10843803733587265, R2:0.9174715523063106\n",
            "r2: 0.9174715523063106 rmse: 0.27227417\n",
            "第7次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_10/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_10'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_55\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_40 (Embedding)       (None, 17, 900)      649800      ['input_21[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_41 (Embedding)       (None, 17, 900)      649800      ['input_21[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_42 (Embedding)       (None, 17, 900)      649800      ['input_21[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_43 (Embedding)       (None, 17, 900)      649800      ['input_21[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 17, 100)      270100      ['embedding_40[0][0]',           \n",
            "                                                                  'embedding_41[0][0]',           \n",
            "                                                                  'embedding_42[0][0]',           \n",
            "                                                                  'embedding_43[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 17, 100)      270100      ['embedding_40[0][0]',           \n",
            "                                                                  'embedding_41[0][0]',           \n",
            "                                                                  'embedding_42[0][0]',           \n",
            "                                                                  'embedding_43[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 17, 100)      270100      ['embedding_40[0][0]',           \n",
            "                                                                  'embedding_41[0][0]',           \n",
            "                                                                  'embedding_42[0][0]',           \n",
            "                                                                  'embedding_43[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_120 (Glob  (None, 100)         0           ['conv1d_30[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_121 (Glob  (None, 100)         0           ['conv1d_30[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_122 (Glob  (None, 100)         0           ['conv1d_30[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_123 (Glob  (None, 100)         0           ['conv1d_30[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_124 (Glob  (None, 100)         0           ['conv1d_31[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_125 (Glob  (None, 100)         0           ['conv1d_31[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_126 (Glob  (None, 100)         0           ['conv1d_31[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_127 (Glob  (None, 100)         0           ['conv1d_31[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_128 (Glob  (None, 100)         0           ['conv1d_32[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_129 (Glob  (None, 100)         0           ['conv1d_32[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_130 (Glob  (None, 100)         0           ['conv1d_32[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_131 (Glob  (None, 100)         0           ['conv1d_32[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 100)          0           ['global_max_pooling1d_120[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_121[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_122[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_123[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 100)          0           ['global_max_pooling1d_124[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_125[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_126[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_127[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 100)          0           ['global_max_pooling1d_128[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_129[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_130[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_131[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_40 (Dense)               (None, 1000)         11000       ['input_22[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 1300)         0           ['add_30[0][0]',                 \n",
            "                                                                  'add_31[0][0]',                 \n",
            "                                                                  'add_32[0][0]',                 \n",
            "                                                                  'dense_40[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_10 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_10[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_10 (G  (1, 1300)           0           ['tf.expand_dims_10[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_10 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_41 (Dense)               (1, 1, 1, 650)       845650      ['reshape_10[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_10 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_41[0][0]']               \n",
            "                                                                                                  \n",
            " dense_42 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_10[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_10 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_42[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_10[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_10[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_10 (TFOpL  (None, 1300)        0           ['multiply_10[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_10[0][0]']\n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 1)            3           ['dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n",
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.0510 - mae: 1.0806\n",
            "Epoch 1: val_loss improved from inf to 0.62341, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 47ms/step - loss: 3.0439 - mae: 1.0791 - val_loss: 0.6234 - val_mae: 0.4325\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.8151 - mae: 0.5640\n",
            "Epoch 2: val_loss improved from 0.62341 to 0.46236, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.8150 - mae: 0.5644 - val_loss: 0.4624 - val_mae: 0.3762\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6784 - mae: 0.5014\n",
            "Epoch 3: val_loss improved from 0.46236 to 0.41381, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.6775 - mae: 0.5010 - val_loss: 0.4138 - val_mae: 0.3922\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6819 - mae: 0.5064\n",
            "Epoch 4: val_loss improved from 0.41381 to 0.39325, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 43ms/step - loss: 0.6818 - mae: 0.5065 - val_loss: 0.3933 - val_mae: 0.3206\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6566 - mae: 0.4926\n",
            "Epoch 5: val_loss did not improve from 0.39325\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.6567 - mae: 0.4932 - val_loss: 0.4673 - val_mae: 0.5241\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6053 - mae: 0.4649\n",
            "Epoch 6: val_loss did not improve from 0.39325\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.6051 - mae: 0.4653 - val_loss: 0.4085 - val_mae: 0.3522\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6439 - mae: 0.4790\n",
            "Epoch 7: val_loss did not improve from 0.39325\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.6427 - mae: 0.4785 - val_loss: 0.3985 - val_mae: 0.2967\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6081 - mae: 0.4722\n",
            "Epoch 8: val_loss improved from 0.39325 to 0.36033, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 56ms/step - loss: 0.6081 - mae: 0.4720 - val_loss: 0.3603 - val_mae: 0.3226\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5985 - mae: 0.4586\n",
            "Epoch 9: val_loss did not improve from 0.36033\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.5982 - mae: 0.4586 - val_loss: 0.3724 - val_mae: 0.3755\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5976 - mae: 0.4706\n",
            "Epoch 10: val_loss improved from 0.36033 to 0.33944, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.5973 - mae: 0.4703 - val_loss: 0.3394 - val_mae: 0.3078\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5365 - mae: 0.4245\n",
            "Epoch 11: val_loss improved from 0.33944 to 0.33504, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 43ms/step - loss: 0.5385 - mae: 0.4250 - val_loss: 0.3350 - val_mae: 0.2982\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5419 - mae: 0.4348\n",
            "Epoch 12: val_loss did not improve from 0.33504\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5416 - mae: 0.4347 - val_loss: 0.3725 - val_mae: 0.4746\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5413 - mae: 0.4467\n",
            "Epoch 13: val_loss improved from 0.33504 to 0.29805, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.5403 - mae: 0.4463 - val_loss: 0.2981 - val_mae: 0.2879\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5287 - mae: 0.4380\n",
            "Epoch 14: val_loss improved from 0.29805 to 0.27911, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.5280 - mae: 0.4380 - val_loss: 0.2791 - val_mae: 0.2751\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4902 - mae: 0.4105\n",
            "Epoch 15: val_loss did not improve from 0.27911\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.4899 - mae: 0.4103 - val_loss: 0.3434 - val_mae: 0.3473\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4648 - mae: 0.3971\n",
            "Epoch 16: val_loss improved from 0.27911 to 0.21423, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.4650 - mae: 0.3970 - val_loss: 0.2142 - val_mae: 0.2883\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4714 - mae: 0.4012\n",
            "Epoch 17: val_loss improved from 0.21423 to 0.20969, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.4714 - mae: 0.4011 - val_loss: 0.2097 - val_mae: 0.2877\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4103 - mae: 0.3775\n",
            "Epoch 18: val_loss did not improve from 0.20969\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.4104 - mae: 0.3779 - val_loss: 0.5265 - val_mae: 0.6038\n",
            "Epoch 19/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3876 - mae: 0.3556\n",
            "Epoch 19: val_loss improved from 0.20969 to 0.15617, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 43ms/step - loss: 0.3876 - mae: 0.3556 - val_loss: 0.1562 - val_mae: 0.2866\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3704 - mae: 0.3477\n",
            "Epoch 20: val_loss improved from 0.15617 to 0.14384, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.3708 - mae: 0.3476 - val_loss: 0.1438 - val_mae: 0.2681\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3367 - mae: 0.3283\n",
            "Epoch 21: val_loss did not improve from 0.14384\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3358 - mae: 0.3280 - val_loss: 0.3923 - val_mae: 0.5097\n",
            "Epoch 22/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3465 - mae: 0.3325\n",
            "Epoch 22: val_loss did not improve from 0.14384\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3465 - mae: 0.3325 - val_loss: 0.1658 - val_mae: 0.3376\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3193 - mae: 0.3210\n",
            "Epoch 23: val_loss improved from 0.14384 to 0.08432, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.3189 - mae: 0.3206 - val_loss: 0.0843 - val_mae: 0.1529\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3674 - mae: 0.3542\n",
            "Epoch 24: val_loss did not improve from 0.08432\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3672 - mae: 0.3540 - val_loss: 0.2151 - val_mae: 0.3769\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2905 - mae: 0.2964\n",
            "Epoch 25: val_loss did not improve from 0.08432\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2908 - mae: 0.2966 - val_loss: 0.0965 - val_mae: 0.2197\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2993 - mae: 0.3036\n",
            "Epoch 26: val_loss did not improve from 0.08432\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2986 - mae: 0.3034 - val_loss: 0.0926 - val_mae: 0.1861\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3495 - mae: 0.3544\n",
            "Epoch 27: val_loss improved from 0.08432 to 0.07523, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.3485 - mae: 0.3538 - val_loss: 0.0752 - val_mae: 0.1608\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3298 - mae: 0.3289\n",
            "Epoch 28: val_loss did not improve from 0.07523\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3297 - mae: 0.3288 - val_loss: 0.1162 - val_mae: 0.2543\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2775 - mae: 0.2738\n",
            "Epoch 29: val_loss improved from 0.07523 to 0.06341, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 42ms/step - loss: 0.2770 - mae: 0.2735 - val_loss: 0.0634 - val_mae: 0.1129\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2852 - mae: 0.2951\n",
            "Epoch 30: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2847 - mae: 0.2950 - val_loss: 0.0964 - val_mae: 0.2449\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2874 - mae: 0.2883\n",
            "Epoch 31: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2873 - mae: 0.2885 - val_loss: 0.1334 - val_mae: 0.3220\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2866 - mae: 0.2997\n",
            "Epoch 32: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2869 - mae: 0.3000 - val_loss: 0.1840 - val_mae: 0.3435\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3361 - mae: 0.3207\n",
            "Epoch 33: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3365 - mae: 0.3207 - val_loss: 0.0707 - val_mae: 0.1812\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2495 - mae: 0.2647\n",
            "Epoch 34: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2488 - mae: 0.2642 - val_loss: 0.0820 - val_mae: 0.2000\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2867 - mae: 0.2829\n",
            "Epoch 35: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2860 - mae: 0.2829 - val_loss: 0.1120 - val_mae: 0.2518\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3396 - mae: 0.3328\n",
            "Epoch 36: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3388 - mae: 0.3324 - val_loss: 0.4800 - val_mae: 0.5912\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2825 - mae: 0.2867\n",
            "Epoch 37: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2816 - mae: 0.2863 - val_loss: 0.0950 - val_mae: 0.2085\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2480 - mae: 0.2735\n",
            "Epoch 38: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2481 - mae: 0.2736 - val_loss: 0.1367 - val_mae: 0.2893\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2882 - mae: 0.2958\n",
            "Epoch 39: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2880 - mae: 0.2955 - val_loss: 0.1073 - val_mae: 0.2564\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2698 - mae: 0.2662\n",
            "Epoch 40: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.2697 - mae: 0.2663 - val_loss: 0.1755 - val_mae: 0.3493\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2937 - mae: 0.3062\n",
            "Epoch 41: val_loss did not improve from 0.06341\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2935 - mae: 0.3060 - val_loss: 0.0830 - val_mae: 0.1576\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2900 - mae: 0.2893\n",
            "Epoch 42: val_loss improved from 0.06341 to 0.05992, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2898 - mae: 0.2890 - val_loss: 0.0599 - val_mae: 0.1058\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2578 - mae: 0.2643\n",
            "Epoch 43: val_loss did not improve from 0.05992\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2573 - mae: 0.2640 - val_loss: 0.1001 - val_mae: 0.2528\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2466 - mae: 0.2627\n",
            "Epoch 44: val_loss improved from 0.05992 to 0.05752, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2493 - mae: 0.2632 - val_loss: 0.0575 - val_mae: 0.1089\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2516 - mae: 0.2601\n",
            "Epoch 45: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2517 - mae: 0.2602 - val_loss: 0.1346 - val_mae: 0.2887\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2477 - mae: 0.2687\n",
            "Epoch 46: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2476 - mae: 0.2686 - val_loss: 0.0804 - val_mae: 0.1894\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2676 - mae: 0.2655\n",
            "Epoch 47: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2680 - mae: 0.2656 - val_loss: 0.0761 - val_mae: 0.1698\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2682 - mae: 0.2727\n",
            "Epoch 48: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2684 - mae: 0.2727 - val_loss: 0.0675 - val_mae: 0.1422\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2961 - mae: 0.3081\n",
            "Epoch 49: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2960 - mae: 0.3080 - val_loss: 0.0792 - val_mae: 0.1942\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2423 - mae: 0.2632\n",
            "Epoch 50: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2418 - mae: 0.2631 - val_loss: 0.0582 - val_mae: 0.1134\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2410 - mae: 0.2547\n",
            "Epoch 51: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2408 - mae: 0.2544 - val_loss: 0.1206 - val_mae: 0.2505\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2733 - mae: 0.2921\n",
            "Epoch 52: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2732 - mae: 0.2919 - val_loss: 0.0871 - val_mae: 0.1827\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2690 - mae: 0.2851\n",
            "Epoch 53: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2682 - mae: 0.2846 - val_loss: 0.0797 - val_mae: 0.2061\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2328 - mae: 0.2448\n",
            "Epoch 54: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2323 - mae: 0.2446 - val_loss: 0.0652 - val_mae: 0.1446\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2417 - mae: 0.2530\n",
            "Epoch 55: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2423 - mae: 0.2534 - val_loss: 0.0887 - val_mae: 0.2281\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2148 - mae: 0.2476\n",
            "Epoch 56: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2144 - mae: 0.2475 - val_loss: 0.0687 - val_mae: 0.1271\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2368 - mae: 0.2691\n",
            "Epoch 57: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2365 - mae: 0.2694 - val_loss: 0.0597 - val_mae: 0.1273\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2230 - mae: 0.2502\n",
            "Epoch 58: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2223 - mae: 0.2499 - val_loss: 0.2358 - val_mae: 0.3881\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2394 - mae: 0.2650\n",
            "Epoch 59: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2394 - mae: 0.2653 - val_loss: 0.1587 - val_mae: 0.3471\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2281 - mae: 0.2560\n",
            "Epoch 60: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2281 - mae: 0.2559 - val_loss: 0.0680 - val_mae: 0.1532\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2217 - mae: 0.2460\n",
            "Epoch 61: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2216 - mae: 0.2465 - val_loss: 0.0842 - val_mae: 0.2200\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2481 - mae: 0.2507\n",
            "Epoch 62: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2486 - mae: 0.2512 - val_loss: 0.0601 - val_mae: 0.1138\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2270 - mae: 0.2569\n",
            "Epoch 63: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2265 - mae: 0.2567 - val_loss: 0.0805 - val_mae: 0.2052\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2427 - mae: 0.2634\n",
            "Epoch 64: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2421 - mae: 0.2630 - val_loss: 0.1812 - val_mae: 0.3360\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2486 - mae: 0.2639\n",
            "Epoch 65: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2497 - mae: 0.2644 - val_loss: 0.1015 - val_mae: 0.1508\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2375 - mae: 0.2615\n",
            "Epoch 66: val_loss did not improve from 0.05752\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2374 - mae: 0.2614 - val_loss: 0.1118 - val_mae: 0.2632\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2332 - mae: 0.2688\n",
            "Epoch 67: val_loss improved from 0.05752 to 0.05389, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第7次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2331 - mae: 0.2687 - val_loss: 0.0539 - val_mae: 0.0968\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2097 - mae: 0.2411\n",
            "Epoch 68: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2092 - mae: 0.2409 - val_loss: 0.0666 - val_mae: 0.1372\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2286 - mae: 0.2430\n",
            "Epoch 69: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2289 - mae: 0.2431 - val_loss: 0.1528 - val_mae: 0.3325\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2388 - mae: 0.2703\n",
            "Epoch 70: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2383 - mae: 0.2703 - val_loss: 0.1140 - val_mae: 0.2634\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2256 - mae: 0.2592\n",
            "Epoch 71: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2262 - mae: 0.2595 - val_loss: 0.0924 - val_mae: 0.2120\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2082 - mae: 0.2431\n",
            "Epoch 72: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2077 - mae: 0.2431 - val_loss: 0.1822 - val_mae: 0.3704\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2168 - mae: 0.2402\n",
            "Epoch 73: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2164 - mae: 0.2402 - val_loss: 0.0731 - val_mae: 0.1821\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2218 - mae: 0.2476\n",
            "Epoch 74: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2233 - mae: 0.2480 - val_loss: 0.0569 - val_mae: 0.1091\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2181 - mae: 0.2521\n",
            "Epoch 75: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2189 - mae: 0.2523 - val_loss: 0.1433 - val_mae: 0.3257\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2274 - mae: 0.2431\n",
            "Epoch 76: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2268 - mae: 0.2428 - val_loss: 0.0663 - val_mae: 0.1392\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2067 - mae: 0.2338\n",
            "Epoch 77: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2070 - mae: 0.2340 - val_loss: 0.1015 - val_mae: 0.2499\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2137 - mae: 0.2370\n",
            "Epoch 78: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2138 - mae: 0.2373 - val_loss: 0.0549 - val_mae: 0.0943\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2149 - mae: 0.2462\n",
            "Epoch 79: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2151 - mae: 0.2462 - val_loss: 0.0859 - val_mae: 0.2262\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2081 - mae: 0.2350\n",
            "Epoch 80: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2076 - mae: 0.2349 - val_loss: 0.0645 - val_mae: 0.1361\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2033 - mae: 0.2286\n",
            "Epoch 81: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2037 - mae: 0.2287 - val_loss: 0.3274 - val_mae: 0.5294\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3395 - mae: 0.3278\n",
            "Epoch 82: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3386 - mae: 0.3274 - val_loss: 0.0990 - val_mae: 0.2560\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2004 - mae: 0.2337\n",
            "Epoch 83: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2001 - mae: 0.2335 - val_loss: 0.1120 - val_mae: 0.2755\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2064 - mae: 0.2424\n",
            "Epoch 84: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2068 - mae: 0.2426 - val_loss: 0.0765 - val_mae: 0.1868\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2126 - mae: 0.2353\n",
            "Epoch 85: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2121 - mae: 0.2350 - val_loss: 0.0582 - val_mae: 0.1360\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2113 - mae: 0.2449\n",
            "Epoch 86: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2113 - mae: 0.2447 - val_loss: 0.0601 - val_mae: 0.1050\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2118 - mae: 0.2364\n",
            "Epoch 87: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2120 - mae: 0.2363 - val_loss: 0.0603 - val_mae: 0.1124\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2119 - mae: 0.2456\n",
            "Epoch 88: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2117 - mae: 0.2455 - val_loss: 0.1168 - val_mae: 0.2726\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2071 - mae: 0.2356\n",
            "Epoch 89: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2066 - mae: 0.2353 - val_loss: 0.0607 - val_mae: 0.1450\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1953 - mae: 0.2358\n",
            "Epoch 90: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1968 - mae: 0.2359 - val_loss: 0.0642 - val_mae: 0.1473\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1910 - mae: 0.2257\n",
            "Epoch 91: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1906 - mae: 0.2256 - val_loss: 0.0778 - val_mae: 0.2032\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1905 - mae: 0.2256\n",
            "Epoch 92: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1902 - mae: 0.2256 - val_loss: 0.0791 - val_mae: 0.1490\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2091 - mae: 0.2371\n",
            "Epoch 93: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2091 - mae: 0.2372 - val_loss: 0.0653 - val_mae: 0.1475\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2067 - mae: 0.2475\n",
            "Epoch 94: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2073 - mae: 0.2477 - val_loss: 0.0587 - val_mae: 0.1262\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2160 - mae: 0.2648\n",
            "Epoch 95: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2159 - mae: 0.2647 - val_loss: 0.0574 - val_mae: 0.1224\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1910 - mae: 0.2232\n",
            "Epoch 96: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1913 - mae: 0.2234 - val_loss: 0.0733 - val_mae: 0.1979\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1849 - mae: 0.2270\n",
            "Epoch 97: val_loss did not improve from 0.05389\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1854 - mae: 0.2274 - val_loss: 0.0848 - val_mae: 0.2167\n",
            "Epoch 97: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07052862644195557, RMSE:0.2655722498893738, MAE:0.11307722330093384, R2:0.922871844232307\n",
            "7 0.922871844232307\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0776103138923645, RMSE:0.2785862684249878, MAE:0.11624002456665039, R2:0.91360068106163\n",
            "r2: 0.91360068106163 rmse: 0.27858627\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 18ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.0776103138923645, RMSE:0.2785862684249878, MAE:0.11624002456665039, R2:0.91360068106163\n",
            "r2: 0.91360068106163 rmse: 0.27858627\n",
            "第8次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_11/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_11'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_61\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_23 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_44 (Embedding)       (None, 17, 900)      649800      ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_45 (Embedding)       (None, 17, 900)      649800      ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_46 (Embedding)       (None, 17, 900)      649800      ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_47 (Embedding)       (None, 17, 900)      649800      ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 17, 100)      270100      ['embedding_44[0][0]',           \n",
            "                                                                  'embedding_45[0][0]',           \n",
            "                                                                  'embedding_46[0][0]',           \n",
            "                                                                  'embedding_47[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 17, 100)      270100      ['embedding_44[0][0]',           \n",
            "                                                                  'embedding_45[0][0]',           \n",
            "                                                                  'embedding_46[0][0]',           \n",
            "                                                                  'embedding_47[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 17, 100)      270100      ['embedding_44[0][0]',           \n",
            "                                                                  'embedding_45[0][0]',           \n",
            "                                                                  'embedding_46[0][0]',           \n",
            "                                                                  'embedding_47[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_132 (Glob  (None, 100)         0           ['conv1d_33[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_133 (Glob  (None, 100)         0           ['conv1d_33[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_134 (Glob  (None, 100)         0           ['conv1d_33[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_135 (Glob  (None, 100)         0           ['conv1d_33[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_136 (Glob  (None, 100)         0           ['conv1d_34[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_137 (Glob  (None, 100)         0           ['conv1d_34[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_138 (Glob  (None, 100)         0           ['conv1d_34[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_139 (Glob  (None, 100)         0           ['conv1d_34[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_140 (Glob  (None, 100)         0           ['conv1d_35[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_141 (Glob  (None, 100)         0           ['conv1d_35[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_142 (Glob  (None, 100)         0           ['conv1d_35[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_143 (Glob  (None, 100)         0           ['conv1d_35[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 100)          0           ['global_max_pooling1d_132[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_133[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_134[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_135[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 100)          0           ['global_max_pooling1d_136[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_137[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_138[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_139[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 100)          0           ['global_max_pooling1d_140[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_141[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_142[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_143[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 1000)         11000       ['input_24[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 1300)         0           ['add_33[0][0]',                 \n",
            "                                                                  'add_34[0][0]',                 \n",
            "                                                                  'add_35[0][0]',                 \n",
            "                                                                  'dense_44[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_11 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_11[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_11 (G  (1, 1300)           0           ['tf.expand_dims_11[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_11 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (1, 1, 1, 650)       845650      ['reshape_11[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_11 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_45[0][0]']               \n",
            "                                                                                                  \n",
            " dense_46 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_11[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_11 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_46[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_11[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_11[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_11 (TFOpL  (None, 1300)        0           ['multiply_11[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_11[0][0]']\n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_55[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_56[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_57 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_57[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_58 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_58[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_59 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_47 (Dense)               (None, 1)            3           ['dropout_59[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 2.3557 - mae: 0.9642\n",
            "Epoch 1: val_loss improved from inf to 0.57112, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 45ms/step - loss: 2.3557 - mae: 0.9642 - val_loss: 0.5711 - val_mae: 0.4274\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7601 - mae: 0.5283\n",
            "Epoch 2: val_loss improved from 0.57112 to 0.44768, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7590 - mae: 0.5282 - val_loss: 0.4477 - val_mae: 0.3646\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6919 - mae: 0.5003\n",
            "Epoch 3: val_loss improved from 0.44768 to 0.41434, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6912 - mae: 0.4999 - val_loss: 0.4143 - val_mae: 0.2961\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6718 - mae: 0.5122\n",
            "Epoch 4: val_loss did not improve from 0.41434\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6762 - mae: 0.5129 - val_loss: 0.6258 - val_mae: 0.5785\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6259 - mae: 0.4790\n",
            "Epoch 5: val_loss improved from 0.41434 to 0.36900, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6251 - mae: 0.4792 - val_loss: 0.3690 - val_mae: 0.3397\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6103 - mae: 0.4684\n",
            "Epoch 6: val_loss improved from 0.36900 to 0.36856, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.6108 - mae: 0.4683 - val_loss: 0.3686 - val_mae: 0.2928\n",
            "Epoch 7/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6257 - mae: 0.4745\n",
            "Epoch 7: val_loss improved from 0.36856 to 0.35609, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6244 - mae: 0.4737 - val_loss: 0.3561 - val_mae: 0.3147\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6049 - mae: 0.4570\n",
            "Epoch 8: val_loss improved from 0.35609 to 0.34391, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6059 - mae: 0.4575 - val_loss: 0.3439 - val_mae: 0.3457\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6251 - mae: 0.4774\n",
            "Epoch 9: val_loss did not improve from 0.34391\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6267 - mae: 0.4775 - val_loss: 0.3840 - val_mae: 0.3524\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6013 - mae: 0.4635\n",
            "Epoch 10: val_loss did not improve from 0.34391\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6025 - mae: 0.4637 - val_loss: 0.3515 - val_mae: 0.3314\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5511 - mae: 0.4423\n",
            "Epoch 11: val_loss improved from 0.34391 to 0.31965, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5513 - mae: 0.4423 - val_loss: 0.3196 - val_mae: 0.3300\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5545 - mae: 0.4408\n",
            "Epoch 12: val_loss did not improve from 0.31965\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5558 - mae: 0.4414 - val_loss: 0.3951 - val_mae: 0.4385\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5074 - mae: 0.4130\n",
            "Epoch 13: val_loss improved from 0.31965 to 0.28777, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.5070 - mae: 0.4129 - val_loss: 0.2878 - val_mae: 0.3164\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5033 - mae: 0.4148\n",
            "Epoch 14: val_loss did not improve from 0.28777\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5034 - mae: 0.4151 - val_loss: 0.3047 - val_mae: 0.3573\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4624 - mae: 0.3902\n",
            "Epoch 15: val_loss improved from 0.28777 to 0.23071, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.4625 - mae: 0.3903 - val_loss: 0.2307 - val_mae: 0.2571\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4844 - mae: 0.4190\n",
            "Epoch 16: val_loss did not improve from 0.23071\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4847 - mae: 0.4194 - val_loss: 0.3175 - val_mae: 0.3246\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4152 - mae: 0.3737\n",
            "Epoch 17: val_loss improved from 0.23071 to 0.18632, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4142 - mae: 0.3732 - val_loss: 0.1863 - val_mae: 0.2590\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4038 - mae: 0.3642\n",
            "Epoch 18: val_loss did not improve from 0.18632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4028 - mae: 0.3638 - val_loss: 0.2054 - val_mae: 0.3354\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4012 - mae: 0.3652\n",
            "Epoch 19: val_loss improved from 0.18632 to 0.16313, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4006 - mae: 0.3651 - val_loss: 0.1631 - val_mae: 0.2513\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3621 - mae: 0.3368\n",
            "Epoch 20: val_loss did not improve from 0.16313\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3620 - mae: 0.3371 - val_loss: 0.2112 - val_mae: 0.3643\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3490 - mae: 0.3469\n",
            "Epoch 21: val_loss improved from 0.16313 to 0.14233, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3483 - mae: 0.3465 - val_loss: 0.1423 - val_mae: 0.3027\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3056 - mae: 0.3074\n",
            "Epoch 22: val_loss improved from 0.14233 to 0.11645, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.3052 - mae: 0.3072 - val_loss: 0.1165 - val_mae: 0.2431\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2955 - mae: 0.3000\n",
            "Epoch 23: val_loss did not improve from 0.11645\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2973 - mae: 0.3012 - val_loss: 0.1256 - val_mae: 0.1774\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2958 - mae: 0.3125\n",
            "Epoch 24: val_loss did not improve from 0.11645\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2951 - mae: 0.3120 - val_loss: 0.1547 - val_mae: 0.2607\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3186 - mae: 0.3208\n",
            "Epoch 25: val_loss did not improve from 0.11645\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.3181 - mae: 0.3209 - val_loss: 0.1597 - val_mae: 0.2717\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3086 - mae: 0.3019\n",
            "Epoch 26: val_loss improved from 0.11645 to 0.08975, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.3080 - mae: 0.3016 - val_loss: 0.0897 - val_mae: 0.1984\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3193 - mae: 0.3168\n",
            "Epoch 27: val_loss did not improve from 0.08975\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3192 - mae: 0.3174 - val_loss: 0.1162 - val_mae: 0.2792\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2847 - mae: 0.2839\n",
            "Epoch 28: val_loss did not improve from 0.08975\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2843 - mae: 0.2838 - val_loss: 0.1075 - val_mae: 0.2530\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2984 - mae: 0.2958\n",
            "Epoch 29: val_loss improved from 0.08975 to 0.06481, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.2986 - mae: 0.2959 - val_loss: 0.0648 - val_mae: 0.1589\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3007 - mae: 0.3155\n",
            "Epoch 30: val_loss improved from 0.06481 to 0.06028, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3000 - mae: 0.3152 - val_loss: 0.0603 - val_mae: 0.1344\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3035 - mae: 0.2933\n",
            "Epoch 31: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3027 - mae: 0.2930 - val_loss: 0.1680 - val_mae: 0.3535\n",
            "Epoch 32/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2536 - mae: 0.2732\n",
            "Epoch 32: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2536 - mae: 0.2732 - val_loss: 0.0760 - val_mae: 0.1908\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2607 - mae: 0.2708\n",
            "Epoch 33: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2600 - mae: 0.2705 - val_loss: 0.0781 - val_mae: 0.1342\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2930 - mae: 0.2978\n",
            "Epoch 34: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2938 - mae: 0.2983 - val_loss: 0.0641 - val_mae: 0.1320\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2627 - mae: 0.2864\n",
            "Epoch 35: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2630 - mae: 0.2864 - val_loss: 0.0625 - val_mae: 0.1633\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2848 - mae: 0.2952\n",
            "Epoch 36: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2841 - mae: 0.2949 - val_loss: 0.1023 - val_mae: 0.2383\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3397 - mae: 0.3438\n",
            "Epoch 37: val_loss did not improve from 0.06028\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3387 - mae: 0.3432 - val_loss: 0.0849 - val_mae: 0.1966\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2729 - mae: 0.2696\n",
            "Epoch 38: val_loss improved from 0.06028 to 0.05761, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2728 - mae: 0.2696 - val_loss: 0.0576 - val_mae: 0.1230\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2604 - mae: 0.2763\n",
            "Epoch 39: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2596 - mae: 0.2758 - val_loss: 0.1131 - val_mae: 0.2672\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2789 - mae: 0.2868\n",
            "Epoch 40: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2789 - mae: 0.2874 - val_loss: 0.0859 - val_mae: 0.1598\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2561 - mae: 0.2661\n",
            "Epoch 41: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2557 - mae: 0.2660 - val_loss: 0.1318 - val_mae: 0.2735\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2592 - mae: 0.2690\n",
            "Epoch 42: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2592 - mae: 0.2689 - val_loss: 0.0710 - val_mae: 0.1507\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2908 - mae: 0.2907\n",
            "Epoch 43: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2908 - mae: 0.2910 - val_loss: 0.0945 - val_mae: 0.2376\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2528 - mae: 0.2784\n",
            "Epoch 44: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.2527 - mae: 0.2784 - val_loss: 0.0897 - val_mae: 0.2325\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2626 - mae: 0.2865\n",
            "Epoch 45: val_loss did not improve from 0.05761\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2621 - mae: 0.2862 - val_loss: 0.1287 - val_mae: 0.2649\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2569 - mae: 0.2731\n",
            "Epoch 46: val_loss improved from 0.05761 to 0.05632, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2575 - mae: 0.2735 - val_loss: 0.0563 - val_mae: 0.1040\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2544 - mae: 0.2557\n",
            "Epoch 47: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2537 - mae: 0.2553 - val_loss: 0.0577 - val_mae: 0.1213\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2624 - mae: 0.2712\n",
            "Epoch 48: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2617 - mae: 0.2709 - val_loss: 0.0654 - val_mae: 0.1100\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2611 - mae: 0.2809\n",
            "Epoch 49: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2630 - mae: 0.2810 - val_loss: 0.0871 - val_mae: 0.2090\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2480 - mae: 0.2667\n",
            "Epoch 50: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2493 - mae: 0.2671 - val_loss: 0.1534 - val_mae: 0.3090\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2420 - mae: 0.2684\n",
            "Epoch 51: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2421 - mae: 0.2684 - val_loss: 0.0580 - val_mae: 0.1296\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2827 - mae: 0.3033\n",
            "Epoch 52: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2842 - mae: 0.3037 - val_loss: 0.1666 - val_mae: 0.3002\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2455 - mae: 0.2657\n",
            "Epoch 53: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2450 - mae: 0.2657 - val_loss: 0.0632 - val_mae: 0.1380\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2471 - mae: 0.2592\n",
            "Epoch 54: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2471 - mae: 0.2591 - val_loss: 0.1002 - val_mae: 0.2423\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2200 - mae: 0.2436\n",
            "Epoch 55: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2195 - mae: 0.2433 - val_loss: 0.1134 - val_mae: 0.2530\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2585 - mae: 0.2756\n",
            "Epoch 56: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2580 - mae: 0.2754 - val_loss: 0.0634 - val_mae: 0.1378\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2219 - mae: 0.2471\n",
            "Epoch 57: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2239 - mae: 0.2480 - val_loss: 0.1387 - val_mae: 0.2880\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2414 - mae: 0.2706\n",
            "Epoch 58: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2409 - mae: 0.2704 - val_loss: 0.0907 - val_mae: 0.2321\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2353 - mae: 0.2623\n",
            "Epoch 59: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2357 - mae: 0.2624 - val_loss: 0.0769 - val_mae: 0.1641\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2389 - mae: 0.2541\n",
            "Epoch 60: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2382 - mae: 0.2539 - val_loss: 0.1093 - val_mae: 0.2320\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2289 - mae: 0.2604\n",
            "Epoch 61: val_loss did not improve from 0.05632\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2288 - mae: 0.2603 - val_loss: 0.0722 - val_mae: 0.1930\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2284 - mae: 0.2435\n",
            "Epoch 62: val_loss improved from 0.05632 to 0.05529, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2284 - mae: 0.2434 - val_loss: 0.0553 - val_mae: 0.1080\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2443 - mae: 0.2621\n",
            "Epoch 63: val_loss improved from 0.05529 to 0.05461, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 39ms/step - loss: 0.2446 - mae: 0.2624 - val_loss: 0.0546 - val_mae: 0.1009\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2444 - mae: 0.2757\n",
            "Epoch 64: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2444 - mae: 0.2756 - val_loss: 0.0643 - val_mae: 0.1502\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2306 - mae: 0.2535\n",
            "Epoch 65: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2319 - mae: 0.2539 - val_loss: 0.0773 - val_mae: 0.2085\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3875 - mae: 0.3744\n",
            "Epoch 66: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3865 - mae: 0.3740 - val_loss: 0.0695 - val_mae: 0.1432\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2497 - mae: 0.2570\n",
            "Epoch 67: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2502 - mae: 0.2569 - val_loss: 0.0604 - val_mae: 0.1007\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2286 - mae: 0.2438\n",
            "Epoch 68: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2288 - mae: 0.2439 - val_loss: 0.0548 - val_mae: 0.1135\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2402 - mae: 0.2601\n",
            "Epoch 69: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2411 - mae: 0.2606 - val_loss: 0.0588 - val_mae: 0.1019\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2265 - mae: 0.2480\n",
            "Epoch 70: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2259 - mae: 0.2477 - val_loss: 0.0586 - val_mae: 0.1093\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2352 - mae: 0.2588\n",
            "Epoch 71: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2345 - mae: 0.2582 - val_loss: 0.1062 - val_mae: 0.2573\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2305 - mae: 0.2625\n",
            "Epoch 72: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2305 - mae: 0.2627 - val_loss: 0.2393 - val_mae: 0.4550\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2265 - mae: 0.2521\n",
            "Epoch 73: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2264 - mae: 0.2520 - val_loss: 0.1447 - val_mae: 0.3286\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2189 - mae: 0.2439\n",
            "Epoch 74: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2193 - mae: 0.2440 - val_loss: 0.0957 - val_mae: 0.2224\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2709 - mae: 0.3009\n",
            "Epoch 75: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2705 - mae: 0.3006 - val_loss: 0.0805 - val_mae: 0.1980\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2146 - mae: 0.2589\n",
            "Epoch 76: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2155 - mae: 0.2594 - val_loss: 0.0932 - val_mae: 0.2290\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2229 - mae: 0.2618\n",
            "Epoch 77: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2228 - mae: 0.2617 - val_loss: 0.0824 - val_mae: 0.1670\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2180 - mae: 0.2375\n",
            "Epoch 78: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2182 - mae: 0.2375 - val_loss: 0.0637 - val_mae: 0.1353\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2164 - mae: 0.2558\n",
            "Epoch 79: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2158 - mae: 0.2555 - val_loss: 0.0590 - val_mae: 0.1282\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1935 - mae: 0.2285\n",
            "Epoch 80: val_loss did not improve from 0.05461\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1938 - mae: 0.2286 - val_loss: 0.0985 - val_mae: 0.2286\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2154 - mae: 0.2330\n",
            "Epoch 81: val_loss improved from 0.05461 to 0.05212, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第8次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 48ms/step - loss: 0.2153 - mae: 0.2331 - val_loss: 0.0521 - val_mae: 0.0915\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2021 - mae: 0.2404\n",
            "Epoch 82: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2019 - mae: 0.2407 - val_loss: 0.0564 - val_mae: 0.1165\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2176 - mae: 0.2574\n",
            "Epoch 83: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2181 - mae: 0.2575 - val_loss: 0.1001 - val_mae: 0.1873\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2413 - mae: 0.2753\n",
            "Epoch 84: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2409 - mae: 0.2751 - val_loss: 0.1101 - val_mae: 0.2250\n",
            "Epoch 85/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2217 - mae: 0.2455\n",
            "Epoch 85: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2229 - mae: 0.2460 - val_loss: 0.0872 - val_mae: 0.2237\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2081 - mae: 0.2330\n",
            "Epoch 86: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2077 - mae: 0.2329 - val_loss: 0.0573 - val_mae: 0.1182\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1896 - mae: 0.2308\n",
            "Epoch 87: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1892 - mae: 0.2306 - val_loss: 0.0701 - val_mae: 0.1484\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2181 - mae: 0.2517\n",
            "Epoch 88: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2176 - mae: 0.2516 - val_loss: 0.1876 - val_mae: 0.3612\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2035 - mae: 0.2322\n",
            "Epoch 89: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2029 - mae: 0.2319 - val_loss: 0.0641 - val_mae: 0.1002\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1945 - mae: 0.2290\n",
            "Epoch 90: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1940 - mae: 0.2288 - val_loss: 0.0660 - val_mae: 0.1269\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1820 - mae: 0.2228\n",
            "Epoch 91: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1822 - mae: 0.2230 - val_loss: 0.0555 - val_mae: 0.1034\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2140 - mae: 0.2467\n",
            "Epoch 92: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2134 - mae: 0.2462 - val_loss: 0.2009 - val_mae: 0.3870\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2055 - mae: 0.2421\n",
            "Epoch 93: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2050 - mae: 0.2418 - val_loss: 0.0577 - val_mae: 0.1390\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2072 - mae: 0.2511\n",
            "Epoch 94: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2067 - mae: 0.2507 - val_loss: 0.1095 - val_mae: 0.2561\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1987 - mae: 0.2313\n",
            "Epoch 95: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1985 - mae: 0.2312 - val_loss: 0.0752 - val_mae: 0.1913\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1876 - mae: 0.2186\n",
            "Epoch 96: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1872 - mae: 0.2183 - val_loss: 0.0684 - val_mae: 0.1563\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1789 - mae: 0.2229\n",
            "Epoch 97: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1790 - mae: 0.2230 - val_loss: 0.0658 - val_mae: 0.1303\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2128 - mae: 0.2614\n",
            "Epoch 98: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2138 - mae: 0.2617 - val_loss: 0.0670 - val_mae: 0.1560\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1887 - mae: 0.2287\n",
            "Epoch 99: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1894 - mae: 0.2292 - val_loss: 0.0557 - val_mae: 0.1139\n",
            "Epoch 100/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1804 - mae: 0.2100\n",
            "Epoch 100: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1804 - mae: 0.2100 - val_loss: 0.0611 - val_mae: 0.1440\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1805 - mae: 0.2235\n",
            "Epoch 101: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1807 - mae: 0.2235 - val_loss: 0.1012 - val_mae: 0.2244\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2284 - mae: 0.2872\n",
            "Epoch 102: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2282 - mae: 0.2869 - val_loss: 0.0609 - val_mae: 0.1219\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1988 - mae: 0.2199\n",
            "Epoch 103: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1986 - mae: 0.2199 - val_loss: 0.0875 - val_mae: 0.2245\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.2167\n",
            "Epoch 104: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1768 - mae: 0.2164 - val_loss: 0.1351 - val_mae: 0.2973\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1825 - mae: 0.2345\n",
            "Epoch 105: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1822 - mae: 0.2343 - val_loss: 0.0559 - val_mae: 0.0924\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1819 - mae: 0.2181\n",
            "Epoch 106: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1824 - mae: 0.2181 - val_loss: 0.0590 - val_mae: 0.1219\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1854 - mae: 0.2277\n",
            "Epoch 107: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1864 - mae: 0.2279 - val_loss: 0.0874 - val_mae: 0.1856\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.2173\n",
            "Epoch 108: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1751 - mae: 0.2175 - val_loss: 0.0667 - val_mae: 0.1749\n",
            "Epoch 109/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2599 - mae: 0.3048\n",
            "Epoch 109: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2594 - mae: 0.3047 - val_loss: 0.0942 - val_mae: 0.1661\n",
            "Epoch 110/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.2129\n",
            "Epoch 110: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1758 - mae: 0.2128 - val_loss: 0.0644 - val_mae: 0.1483\n",
            "Epoch 111/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1779 - mae: 0.2066\n",
            "Epoch 111: val_loss did not improve from 0.05212\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1784 - mae: 0.2069 - val_loss: 0.0559 - val_mae: 0.1142\n",
            "Epoch 111: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.07364463806152344, RMSE:0.271375447511673, MAE:0.11432535201311111, R2:0.919464255721107\n",
            "8 0.919464255721107\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 9ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07406847923994064, RMSE:0.2721552550792694, MAE:0.11148115992546082, R2:0.9175436190007902\n",
            "r2: 0.9175436190007902 rmse: 0.27215526\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07406847923994064, RMSE:0.2721552550792694, MAE:0.11148115992546082, R2:0.9175436190007902\n",
            "r2: 0.9175436190007902 rmse: 0.27215526\n",
            "第9次训练\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_12/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_12'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_67\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_25 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_48 (Embedding)       (None, 17, 900)      649800      ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_49 (Embedding)       (None, 17, 900)      649800      ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_50 (Embedding)       (None, 17, 900)      649800      ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_51 (Embedding)       (None, 17, 900)      649800      ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 17, 100)      270100      ['embedding_48[0][0]',           \n",
            "                                                                  'embedding_49[0][0]',           \n",
            "                                                                  'embedding_50[0][0]',           \n",
            "                                                                  'embedding_51[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 17, 100)      270100      ['embedding_48[0][0]',           \n",
            "                                                                  'embedding_49[0][0]',           \n",
            "                                                                  'embedding_50[0][0]',           \n",
            "                                                                  'embedding_51[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 17, 100)      270100      ['embedding_48[0][0]',           \n",
            "                                                                  'embedding_49[0][0]',           \n",
            "                                                                  'embedding_50[0][0]',           \n",
            "                                                                  'embedding_51[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_144 (Glob  (None, 100)         0           ['conv1d_36[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_145 (Glob  (None, 100)         0           ['conv1d_36[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_146 (Glob  (None, 100)         0           ['conv1d_36[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_147 (Glob  (None, 100)         0           ['conv1d_36[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_148 (Glob  (None, 100)         0           ['conv1d_37[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_149 (Glob  (None, 100)         0           ['conv1d_37[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_150 (Glob  (None, 100)         0           ['conv1d_37[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_151 (Glob  (None, 100)         0           ['conv1d_37[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_152 (Glob  (None, 100)         0           ['conv1d_38[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_153 (Glob  (None, 100)         0           ['conv1d_38[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_154 (Glob  (None, 100)         0           ['conv1d_38[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_155 (Glob  (None, 100)         0           ['conv1d_38[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_36 (Add)                   (None, 100)          0           ['global_max_pooling1d_144[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_145[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_146[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_147[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_37 (Add)                   (None, 100)          0           ['global_max_pooling1d_148[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_149[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_150[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_151[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_38 (Add)                   (None, 100)          0           ['global_max_pooling1d_152[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_153[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_154[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_155[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 1000)         11000       ['input_26[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 1300)         0           ['add_36[0][0]',                 \n",
            "                                                                  'add_37[0][0]',                 \n",
            "                                                                  'add_38[0][0]',                 \n",
            "                                                                  'dense_48[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_12 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_12[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_12 (G  (1, 1300)           0           ['tf.expand_dims_12[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_12 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_12[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (1, 1, 1, 650)       845650      ['reshape_12[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_12 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_12[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_12 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_50[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_12[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_12[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_12 (TFOpL  (None, 1300)        0           ['multiply_12[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_12[0][0]']\n",
            "                                                                                                  \n",
            " dropout_60 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_60[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_61 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_61[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_62 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_62[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_63 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_63[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_64 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 1)            3           ['dropout_64[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the ATTENTION_merge model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 2.2700 - mae: 0.9178\n",
            "Epoch 1: val_loss improved from inf to 0.50861, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 7s 46ms/step - loss: 2.2634 - mae: 0.9159 - val_loss: 0.5086 - val_mae: 0.4668\n",
            "Epoch 2/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.7387 - mae: 0.5442\n",
            "Epoch 2: val_loss improved from 0.50861 to 0.42212, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7418 - mae: 0.5453 - val_loss: 0.4221 - val_mae: 0.4266\n",
            "Epoch 3/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6595 - mae: 0.5018\n",
            "Epoch 3: val_loss improved from 0.42212 to 0.39278, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6595 - mae: 0.5018 - val_loss: 0.3928 - val_mae: 0.3292\n",
            "Epoch 4/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6431 - mae: 0.4931\n",
            "Epoch 4: val_loss did not improve from 0.39278\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6442 - mae: 0.4936 - val_loss: 0.4335 - val_mae: 0.4101\n",
            "Epoch 5/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6729 - mae: 0.5124\n",
            "Epoch 5: val_loss improved from 0.39278 to 0.37948, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6732 - mae: 0.5122 - val_loss: 0.3795 - val_mae: 0.3279\n",
            "Epoch 6/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6226 - mae: 0.4839\n",
            "Epoch 6: val_loss did not improve from 0.37948\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6228 - mae: 0.4843 - val_loss: 0.4989 - val_mae: 0.4395\n",
            "Epoch 7/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.6057 - mae: 0.4834\n",
            "Epoch 7: val_loss did not improve from 0.37948\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.6057 - mae: 0.4834 - val_loss: 0.3854 - val_mae: 0.3452\n",
            "Epoch 8/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6433 - mae: 0.4960\n",
            "Epoch 8: val_loss improved from 0.37948 to 0.35649, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.6444 - mae: 0.4966 - val_loss: 0.3565 - val_mae: 0.3834\n",
            "Epoch 9/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5637 - mae: 0.4397\n",
            "Epoch 9: val_loss did not improve from 0.35649\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5624 - mae: 0.4391 - val_loss: 0.5375 - val_mae: 0.5506\n",
            "Epoch 10/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6111 - mae: 0.4920\n",
            "Epoch 10: val_loss improved from 0.35649 to 0.29010, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.6108 - mae: 0.4919 - val_loss: 0.2901 - val_mae: 0.2804\n",
            "Epoch 11/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5378 - mae: 0.4361\n",
            "Epoch 11: val_loss improved from 0.29010 to 0.25984, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 41ms/step - loss: 0.5371 - mae: 0.4362 - val_loss: 0.2598 - val_mae: 0.2562\n",
            "Epoch 12/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4844 - mae: 0.3991\n",
            "Epoch 12: val_loss improved from 0.25984 to 0.22628, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4834 - mae: 0.3986 - val_loss: 0.2263 - val_mae: 0.2548\n",
            "Epoch 13/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4869 - mae: 0.4111\n",
            "Epoch 13: val_loss did not improve from 0.22628\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.4856 - mae: 0.4107 - val_loss: 0.2576 - val_mae: 0.3764\n",
            "Epoch 14/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4075 - mae: 0.3768\n",
            "Epoch 14: val_loss improved from 0.22628 to 0.21893, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.4068 - mae: 0.3764 - val_loss: 0.2189 - val_mae: 0.3168\n",
            "Epoch 15/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3924 - mae: 0.3597\n",
            "Epoch 15: val_loss improved from 0.21893 to 0.12023, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3925 - mae: 0.3601 - val_loss: 0.1202 - val_mae: 0.1587\n",
            "Epoch 16/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3512 - mae: 0.3382\n",
            "Epoch 16: val_loss did not improve from 0.12023\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3514 - mae: 0.3382 - val_loss: 0.2126 - val_mae: 0.3601\n",
            "Epoch 17/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3850 - mae: 0.3577\n",
            "Epoch 17: val_loss improved from 0.12023 to 0.08457, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3844 - mae: 0.3573 - val_loss: 0.0846 - val_mae: 0.1378\n",
            "Epoch 18/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3088 - mae: 0.3038\n",
            "Epoch 18: val_loss improved from 0.08457 to 0.07368, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.3085 - mae: 0.3037 - val_loss: 0.0737 - val_mae: 0.1457\n",
            "Epoch 19/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3497 - mae: 0.3291\n",
            "Epoch 19: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3494 - mae: 0.3288 - val_loss: 0.0871 - val_mae: 0.1419\n",
            "Epoch 20/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3704 - mae: 0.3498\n",
            "Epoch 20: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3711 - mae: 0.3502 - val_loss: 0.3193 - val_mae: 0.4588\n",
            "Epoch 21/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3477 - mae: 0.3333\n",
            "Epoch 21: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3470 - mae: 0.3329 - val_loss: 0.0940 - val_mae: 0.1874\n",
            "Epoch 22/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3190 - mae: 0.3089\n",
            "Epoch 22: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3183 - mae: 0.3086 - val_loss: 0.1113 - val_mae: 0.2308\n",
            "Epoch 23/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3120 - mae: 0.3058\n",
            "Epoch 23: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3131 - mae: 0.3062 - val_loss: 0.0795 - val_mae: 0.1720\n",
            "Epoch 24/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3606 - mae: 0.3372\n",
            "Epoch 24: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3605 - mae: 0.3376 - val_loss: 0.3797 - val_mae: 0.4306\n",
            "Epoch 25/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3214 - mae: 0.3096\n",
            "Epoch 25: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3204 - mae: 0.3090 - val_loss: 0.0792 - val_mae: 0.1896\n",
            "Epoch 26/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2776 - mae: 0.2877\n",
            "Epoch 26: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2781 - mae: 0.2878 - val_loss: 0.4217 - val_mae: 0.5719\n",
            "Epoch 27/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3120 - mae: 0.3077\n",
            "Epoch 27: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3116 - mae: 0.3076 - val_loss: 0.0849 - val_mae: 0.1331\n",
            "Epoch 28/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2932 - mae: 0.3070\n",
            "Epoch 28: val_loss did not improve from 0.07368\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2924 - mae: 0.3064 - val_loss: 0.1026 - val_mae: 0.2438\n",
            "Epoch 29/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2724 - mae: 0.2793\n",
            "Epoch 29: val_loss improved from 0.07368 to 0.06769, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.2719 - mae: 0.2791 - val_loss: 0.0677 - val_mae: 0.1331\n",
            "Epoch 30/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3183 - mae: 0.3192\n",
            "Epoch 30: val_loss did not improve from 0.06769\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3192 - mae: 0.3205 - val_loss: 0.1293 - val_mae: 0.2943\n",
            "Epoch 31/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2824 - mae: 0.2901\n",
            "Epoch 31: val_loss did not improve from 0.06769\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2816 - mae: 0.2897 - val_loss: 0.0873 - val_mae: 0.1807\n",
            "Epoch 32/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2717 - mae: 0.2795\n",
            "Epoch 32: val_loss did not improve from 0.06769\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2716 - mae: 0.2794 - val_loss: 0.0753 - val_mae: 0.1702\n",
            "Epoch 33/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2667 - mae: 0.2791\n",
            "Epoch 33: val_loss improved from 0.06769 to 0.05535, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.2667 - mae: 0.2793 - val_loss: 0.0553 - val_mae: 0.1002\n",
            "Epoch 34/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2734 - mae: 0.2809\n",
            "Epoch 34: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2726 - mae: 0.2806 - val_loss: 0.0775 - val_mae: 0.1963\n",
            "Epoch 35/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2814 - mae: 0.2899\n",
            "Epoch 35: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2809 - mae: 0.2898 - val_loss: 0.0576 - val_mae: 0.1011\n",
            "Epoch 36/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2669 - mae: 0.2633\n",
            "Epoch 36: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2662 - mae: 0.2630 - val_loss: 0.1223 - val_mae: 0.2526\n",
            "Epoch 37/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2621 - mae: 0.2629\n",
            "Epoch 37: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2617 - mae: 0.2630 - val_loss: 0.0642 - val_mae: 0.1234\n",
            "Epoch 38/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2792 - mae: 0.2774\n",
            "Epoch 38: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2802 - mae: 0.2776 - val_loss: 0.0746 - val_mae: 0.1615\n",
            "Epoch 39/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2595 - mae: 0.2838\n",
            "Epoch 39: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2594 - mae: 0.2837 - val_loss: 0.0604 - val_mae: 0.1333\n",
            "Epoch 40/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2834 - mae: 0.2892\n",
            "Epoch 40: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2840 - mae: 0.2899 - val_loss: 0.1153 - val_mae: 0.1712\n",
            "Epoch 41/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2837 - mae: 0.2925\n",
            "Epoch 41: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2842 - mae: 0.2927 - val_loss: 0.0964 - val_mae: 0.2240\n",
            "Epoch 42/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2461 - mae: 0.2624\n",
            "Epoch 42: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2460 - mae: 0.2623 - val_loss: 0.1129 - val_mae: 0.2539\n",
            "Epoch 43/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3223 - mae: 0.3285\n",
            "Epoch 43: val_loss did not improve from 0.05535\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.3220 - mae: 0.3283 - val_loss: 0.0857 - val_mae: 0.1750\n",
            "Epoch 44/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2304 - mae: 0.2491\n",
            "Epoch 44: val_loss improved from 0.05535 to 0.05533, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.2312 - mae: 0.2495 - val_loss: 0.0553 - val_mae: 0.0939\n",
            "Epoch 45/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2702 - mae: 0.2626\n",
            "Epoch 45: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2695 - mae: 0.2624 - val_loss: 0.0711 - val_mae: 0.1168\n",
            "Epoch 46/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2545 - mae: 0.2690\n",
            "Epoch 46: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2541 - mae: 0.2689 - val_loss: 0.0571 - val_mae: 0.1202\n",
            "Epoch 47/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2329 - mae: 0.2514\n",
            "Epoch 47: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2327 - mae: 0.2517 - val_loss: 0.0798 - val_mae: 0.1994\n",
            "Epoch 48/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3018 - mae: 0.3012\n",
            "Epoch 48: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3016 - mae: 0.3010 - val_loss: 0.0975 - val_mae: 0.2160\n",
            "Epoch 49/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2400 - mae: 0.2472\n",
            "Epoch 49: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2406 - mae: 0.2473 - val_loss: 0.1039 - val_mae: 0.2259\n",
            "Epoch 50/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2726 - mae: 0.2819\n",
            "Epoch 50: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2719 - mae: 0.2816 - val_loss: 0.0659 - val_mae: 0.1542\n",
            "Epoch 51/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2483 - mae: 0.2534\n",
            "Epoch 51: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2479 - mae: 0.2535 - val_loss: 0.1616 - val_mae: 0.3427\n",
            "Epoch 52/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2527 - mae: 0.2776\n",
            "Epoch 52: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2524 - mae: 0.2773 - val_loss: 0.0928 - val_mae: 0.1969\n",
            "Epoch 53/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2335 - mae: 0.2499\n",
            "Epoch 53: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2329 - mae: 0.2496 - val_loss: 0.0722 - val_mae: 0.1065\n",
            "Epoch 54/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2182 - mae: 0.2392\n",
            "Epoch 54: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2178 - mae: 0.2390 - val_loss: 0.0613 - val_mae: 0.1269\n",
            "Epoch 55/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2302 - mae: 0.2454\n",
            "Epoch 55: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2299 - mae: 0.2451 - val_loss: 0.0778 - val_mae: 0.1952\n",
            "Epoch 56/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2494 - mae: 0.2588\n",
            "Epoch 56: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2489 - mae: 0.2587 - val_loss: 0.0640 - val_mae: 0.1383\n",
            "Epoch 57/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2324 - mae: 0.2423\n",
            "Epoch 57: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2323 - mae: 0.2422 - val_loss: 0.0711 - val_mae: 0.1474\n",
            "Epoch 58/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2879 - mae: 0.3084\n",
            "Epoch 58: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2898 - mae: 0.3087 - val_loss: 0.0823 - val_mae: 0.1959\n",
            "Epoch 59/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2118 - mae: 0.2314\n",
            "Epoch 59: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2115 - mae: 0.2313 - val_loss: 0.1053 - val_mae: 0.2559\n",
            "Epoch 60/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2129 - mae: 0.2434\n",
            "Epoch 60: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2133 - mae: 0.2435 - val_loss: 0.0834 - val_mae: 0.1504\n",
            "Epoch 61/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2172 - mae: 0.2350\n",
            "Epoch 61: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2169 - mae: 0.2351 - val_loss: 0.1017 - val_mae: 0.2500\n",
            "Epoch 62/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2314 - mae: 0.2503\n",
            "Epoch 62: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2342 - mae: 0.2510 - val_loss: 0.0576 - val_mae: 0.1306\n",
            "Epoch 63/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2457 - mae: 0.2584\n",
            "Epoch 63: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2455 - mae: 0.2582 - val_loss: 0.0968 - val_mae: 0.2373\n",
            "Epoch 64/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2151 - mae: 0.2289\n",
            "Epoch 64: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2150 - mae: 0.2290 - val_loss: 0.0634 - val_mae: 0.1421\n",
            "Epoch 65/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2495 - mae: 0.2595\n",
            "Epoch 65: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2499 - mae: 0.2597 - val_loss: 0.0635 - val_mae: 0.1169\n",
            "Epoch 66/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2499 - mae: 0.2837\n",
            "Epoch 66: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2497 - mae: 0.2834 - val_loss: 0.0805 - val_mae: 0.1958\n",
            "Epoch 67/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2132 - mae: 0.2291\n",
            "Epoch 67: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2134 - mae: 0.2293 - val_loss: 0.0555 - val_mae: 0.1244\n",
            "Epoch 68/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2268 - mae: 0.2435\n",
            "Epoch 68: val_loss did not improve from 0.05533\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2262 - mae: 0.2433 - val_loss: 0.0765 - val_mae: 0.1880\n",
            "Epoch 69/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2207 - mae: 0.2399\n",
            "Epoch 69: val_loss improved from 0.05533 to 0.05310, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 45ms/step - loss: 0.2221 - mae: 0.2404 - val_loss: 0.0531 - val_mae: 0.1183\n",
            "Epoch 70/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2198 - mae: 0.2290\n",
            "Epoch 70: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2192 - mae: 0.2288 - val_loss: 0.1056 - val_mae: 0.2497\n",
            "Epoch 71/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3947 - mae: 0.3673\n",
            "Epoch 71: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3939 - mae: 0.3667 - val_loss: 0.0634 - val_mae: 0.1222\n",
            "Epoch 72/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2254 - mae: 0.2370\n",
            "Epoch 72: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2254 - mae: 0.2372 - val_loss: 0.0636 - val_mae: 0.1151\n",
            "Epoch 73/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2222 - mae: 0.2377\n",
            "Epoch 73: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2216 - mae: 0.2374 - val_loss: 0.0554 - val_mae: 0.1056\n",
            "Epoch 74/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2245 - mae: 0.2354\n",
            "Epoch 74: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2246 - mae: 0.2362 - val_loss: 0.0778 - val_mae: 0.2000\n",
            "Epoch 75/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2213 - mae: 0.2433\n",
            "Epoch 75: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2214 - mae: 0.2433 - val_loss: 0.2284 - val_mae: 0.3865\n",
            "Epoch 76/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2244 - mae: 0.2489\n",
            "Epoch 76: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2241 - mae: 0.2486 - val_loss: 0.1163 - val_mae: 0.2618\n",
            "Epoch 77/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2166 - mae: 0.2348\n",
            "Epoch 77: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2162 - mae: 0.2347 - val_loss: 0.0534 - val_mae: 0.0936\n",
            "Epoch 78/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2048 - mae: 0.2241\n",
            "Epoch 78: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2046 - mae: 0.2240 - val_loss: 0.0635 - val_mae: 0.1624\n",
            "Epoch 79/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2044 - mae: 0.2195\n",
            "Epoch 79: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2043 - mae: 0.2193 - val_loss: 0.0624 - val_mae: 0.1460\n",
            "Epoch 80/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2030 - mae: 0.2263\n",
            "Epoch 80: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2024 - mae: 0.2259 - val_loss: 0.1309 - val_mae: 0.2516\n",
            "Epoch 81/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2599 - mae: 0.2769\n",
            "Epoch 81: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2593 - mae: 0.2767 - val_loss: 0.0538 - val_mae: 0.0969\n",
            "Epoch 82/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2093 - mae: 0.2479\n",
            "Epoch 82: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2089 - mae: 0.2476 - val_loss: 0.0751 - val_mae: 0.1568\n",
            "Epoch 83/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2007 - mae: 0.2262\n",
            "Epoch 83: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2020 - mae: 0.2266 - val_loss: 0.0614 - val_mae: 0.1570\n",
            "Epoch 84/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1965 - mae: 0.2144\n",
            "Epoch 84: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1959 - mae: 0.2140 - val_loss: 0.0983 - val_mae: 0.2407\n",
            "Epoch 85/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2320 - mae: 0.2597\n",
            "Epoch 85: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2320 - mae: 0.2597 - val_loss: 0.0919 - val_mae: 0.2032\n",
            "Epoch 86/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1978 - mae: 0.2154\n",
            "Epoch 86: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1976 - mae: 0.2154 - val_loss: 0.0648 - val_mae: 0.1359\n",
            "Epoch 87/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1869 - mae: 0.2167\n",
            "Epoch 87: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1869 - mae: 0.2167 - val_loss: 0.0764 - val_mae: 0.1719\n",
            "Epoch 88/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1921 - mae: 0.2107\n",
            "Epoch 88: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1918 - mae: 0.2107 - val_loss: 0.0716 - val_mae: 0.1783\n",
            "Epoch 89/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2041 - mae: 0.2182\n",
            "Epoch 89: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2040 - mae: 0.2181 - val_loss: 0.0562 - val_mae: 0.0974\n",
            "Epoch 90/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2335 - mae: 0.2566\n",
            "Epoch 90: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2329 - mae: 0.2563 - val_loss: 0.0713 - val_mae: 0.1886\n",
            "Epoch 91/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1832 - mae: 0.2008\n",
            "Epoch 91: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1832 - mae: 0.2007 - val_loss: 0.0560 - val_mae: 0.1015\n",
            "Epoch 92/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1959 - mae: 0.2142\n",
            "Epoch 92: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1959 - mae: 0.2144 - val_loss: 0.0558 - val_mae: 0.1011\n",
            "Epoch 93/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1895 - mae: 0.2084\n",
            "Epoch 93: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1889 - mae: 0.2080 - val_loss: 0.0636 - val_mae: 0.1252\n",
            "Epoch 94/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2864 - mae: 0.3055\n",
            "Epoch 94: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2859 - mae: 0.3050 - val_loss: 0.1110 - val_mae: 0.2722\n",
            "Epoch 95/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2037 - mae: 0.2141\n",
            "Epoch 95: val_loss did not improve from 0.05310\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.2036 - mae: 0.2141 - val_loss: 0.0562 - val_mae: 0.1276\n",
            "Epoch 96/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1960 - mae: 0.2096\n",
            "Epoch 96: val_loss improved from 0.05310 to 0.05075, saving model to /content/gdrive/MyDrive/cnn_model/没有时间间隔_5月16日(下午)_第9次训练_MG_mergeweighted_29:59_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.1962 - mae: 0.2096 - val_loss: 0.0507 - val_mae: 0.0986\n",
            "Epoch 97/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.2104\n",
            "Epoch 97: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1724 - mae: 0.2105 - val_loss: 0.0807 - val_mae: 0.2011\n",
            "Epoch 98/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1897 - mae: 0.2088\n",
            "Epoch 98: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1904 - mae: 0.2092 - val_loss: 0.0535 - val_mae: 0.1025\n",
            "Epoch 99/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1921 - mae: 0.2264\n",
            "Epoch 99: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1917 - mae: 0.2262 - val_loss: 0.0844 - val_mae: 0.1964\n",
            "Epoch 100/200\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.1902 - mae: 0.2052\n",
            "Epoch 100: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1902 - mae: 0.2052 - val_loss: 0.0591 - val_mae: 0.1278\n",
            "Epoch 101/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.1961\n",
            "Epoch 101: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1752 - mae: 0.1961 - val_loss: 0.0587 - val_mae: 0.1371\n",
            "Epoch 102/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1914 - mae: 0.2026\n",
            "Epoch 102: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1909 - mae: 0.2026 - val_loss: 0.0533 - val_mae: 0.1146\n",
            "Epoch 103/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1661 - mae: 0.1931\n",
            "Epoch 103: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1672 - mae: 0.1935 - val_loss: 0.0777 - val_mae: 0.1747\n",
            "Epoch 104/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1786 - mae: 0.2010\n",
            "Epoch 104: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1786 - mae: 0.2010 - val_loss: 0.0670 - val_mae: 0.1342\n",
            "Epoch 105/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.1963\n",
            "Epoch 105: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1771 - mae: 0.1964 - val_loss: 0.0570 - val_mae: 0.1236\n",
            "Epoch 106/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2122 - mae: 0.2470\n",
            "Epoch 106: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2115 - mae: 0.2465 - val_loss: 0.0742 - val_mae: 0.1078\n",
            "Epoch 107/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.1937\n",
            "Epoch 107: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1691 - mae: 0.1937 - val_loss: 0.0621 - val_mae: 0.1233\n",
            "Epoch 108/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.2034\n",
            "Epoch 108: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1749 - mae: 0.2032 - val_loss: 0.0583 - val_mae: 0.1187\n",
            "Epoch 109/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1628 - mae: 0.1830\n",
            "Epoch 109: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1627 - mae: 0.1830 - val_loss: 0.0608 - val_mae: 0.0913\n",
            "Epoch 110/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1837 - mae: 0.1961\n",
            "Epoch 110: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1834 - mae: 0.1962 - val_loss: 0.0645 - val_mae: 0.1159\n",
            "Epoch 111/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1666 - mae: 0.1919\n",
            "Epoch 111: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1662 - mae: 0.1917 - val_loss: 0.0630 - val_mae: 0.1217\n",
            "Epoch 112/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.3916 - mae: 0.3197\n",
            "Epoch 112: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.3922 - mae: 0.3204 - val_loss: 0.5427 - val_mae: 0.4971\n",
            "Epoch 113/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.6331 - mae: 0.4496\n",
            "Epoch 113: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.6328 - mae: 0.4494 - val_loss: 0.5125 - val_mae: 0.3027\n",
            "Epoch 114/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5837 - mae: 0.4245\n",
            "Epoch 114: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5852 - mae: 0.4251 - val_loss: 0.4544 - val_mae: 0.4098\n",
            "Epoch 115/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5295 - mae: 0.4015\n",
            "Epoch 115: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.5280 - mae: 0.4008 - val_loss: 0.4650 - val_mae: 0.3485\n",
            "Epoch 116/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.5002 - mae: 0.3816\n",
            "Epoch 116: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.5008 - mae: 0.3816 - val_loss: 0.3912 - val_mae: 0.2962\n",
            "Epoch 117/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4690 - mae: 0.3725\n",
            "Epoch 117: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.4683 - mae: 0.3717 - val_loss: 0.3853 - val_mae: 0.3004\n",
            "Epoch 118/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4674 - mae: 0.3588\n",
            "Epoch 118: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.4659 - mae: 0.3581 - val_loss: 0.3927 - val_mae: 0.2813\n",
            "Epoch 119/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.4379 - mae: 0.3368\n",
            "Epoch 119: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.4372 - mae: 0.3366 - val_loss: 0.1932 - val_mae: 0.2253\n",
            "Epoch 120/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2502 - mae: 0.2496\n",
            "Epoch 120: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2497 - mae: 0.2493 - val_loss: 0.0878 - val_mae: 0.1396\n",
            "Epoch 121/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2201 - mae: 0.2271\n",
            "Epoch 121: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2199 - mae: 0.2270 - val_loss: 0.0718 - val_mae: 0.1061\n",
            "Epoch 122/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.1927\n",
            "Epoch 122: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1733 - mae: 0.1925 - val_loss: 0.0607 - val_mae: 0.1376\n",
            "Epoch 123/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1661 - mae: 0.1885\n",
            "Epoch 123: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1657 - mae: 0.1884 - val_loss: 0.0609 - val_mae: 0.1149\n",
            "Epoch 124/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1583 - mae: 0.1869\n",
            "Epoch 124: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.1584 - mae: 0.1870 - val_loss: 0.0865 - val_mae: 0.1492\n",
            "Epoch 125/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.1908\n",
            "Epoch 125: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 37ms/step - loss: 0.1719 - mae: 0.1909 - val_loss: 0.5635 - val_mae: 0.2765\n",
            "Epoch 126/200\n",
            "89/90 [============================>.] - ETA: 0s - loss: 0.2898 - mae: 0.2659\n",
            "Epoch 126: val_loss did not improve from 0.05075\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.2889 - mae: 0.2653 - val_loss: 0.0549 - val_mae: 0.1012\n",
            "Epoch 126: early stopping\n",
            "4、开始画图：损失\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412, 1)\n",
            "float32\n",
            "MSE:0.0721464529633522, RMSE:0.2686009109020233, MAE:0.12018810957670212, R2:0.9211026264086222\n",
            "9 0.9211026264086222\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07312426716089249, RMSE:0.27041497826576233, MAE:0.11622720956802368, R2:0.918594758214431\n",
            "r2: 0.918594758214431 rmse: 0.27041498\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 16ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07312426716089249, RMSE:0.27041497826576233, MAE:0.11622720956802368, R2:0.918594758214431\n",
            "r2: 0.918594758214431 rmse: 0.27041498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_CNN_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIVEsia_3s_R",
        "outputId": "c4e95ccc-9244-44c4-d733-6c4013fbaffa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9135039079983848,\n",
              " 0.9171583225677296,\n",
              " 0.9232899830262921,\n",
              " 0.9099373038593238,\n",
              " 0.9195682834084178,\n",
              " 0.9174504361181719,\n",
              " 0.9189608483056998,\n",
              " 0.922871844232307,\n",
              " 0.919464255721107,\n",
              " 0.9211026264086222]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_CNN_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDXZSdwm3vJY",
        "outputId": "148b3bf6-0a26-43dc-856f-7b30cc5b1284"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9097156799067575,\n",
              " 0.9133483565999173,\n",
              " 0.9199765740763938,\n",
              " 0.9078222057681373,\n",
              " 0.9151527028515862,\n",
              " 0.9139237875328569,\n",
              " 0.9174715523063106,\n",
              " 0.91360068106163,\n",
              " 0.9175436190007902,\n",
              " 0.918594758214431]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1.5 Grad-CAM可视化**"
      ],
      "metadata": {
        "id": "0eaYwlzGKMeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **第一次尝试**"
      ],
      "metadata": {
        "id": "zlYxkyW6cFu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tG5MPcLcKs-",
        "outputId": "181c22a0-3642-43f6-a0a1-345b931e6a78",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_36\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 3)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding_24 (Embedding)       (None, 3, 900)       649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_25 (Embedding)       (None, 3, 900)       649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_26 (Embedding)       (None, 3, 900)       649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_27 (Embedding)       (None, 3, 900)       649800      ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 3, 300)       810300      ['embedding_24[0][0]',           \n",
            "                                                                  'embedding_25[0][0]',           \n",
            "                                                                  'embedding_26[0][0]',           \n",
            "                                                                  'embedding_27[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 3, 300)       810300      ['embedding_24[0][0]',           \n",
            "                                                                  'embedding_25[0][0]',           \n",
            "                                                                  'embedding_26[0][0]',           \n",
            "                                                                  'embedding_27[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 3, 300)       810300      ['embedding_24[0][0]',           \n",
            "                                                                  'embedding_25[0][0]',           \n",
            "                                                                  'embedding_26[0][0]',           \n",
            "                                                                  'embedding_27[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_72 (Globa  (None, 300)         0           ['conv1d_18[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_73 (Globa  (None, 300)         0           ['conv1d_18[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_74 (Globa  (None, 300)         0           ['conv1d_18[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_75 (Globa  (None, 300)         0           ['conv1d_18[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_76 (Globa  (None, 300)         0           ['conv1d_19[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_77 (Globa  (None, 300)         0           ['conv1d_19[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_78 (Globa  (None, 300)         0           ['conv1d_19[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_79 (Globa  (None, 300)         0           ['conv1d_19[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_80 (Globa  (None, 300)         0           ['conv1d_20[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_81 (Globa  (None, 300)         0           ['conv1d_20[1][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_82 (Globa  (None, 300)         0           ['conv1d_20[2][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_83 (Globa  (None, 300)         0           ['conv1d_20[3][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 300)          0           ['global_max_pooling1d_72[0][0]',\n",
            "                                                                  'global_max_pooling1d_73[0][0]',\n",
            "                                                                  'global_max_pooling1d_74[0][0]',\n",
            "                                                                  'global_max_pooling1d_75[0][0]']\n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 300)          0           ['global_max_pooling1d_76[0][0]',\n",
            "                                                                  'global_max_pooling1d_77[0][0]',\n",
            "                                                                  'global_max_pooling1d_78[0][0]',\n",
            "                                                                  'global_max_pooling1d_79[0][0]']\n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 300)          0           ['global_max_pooling1d_80[0][0]',\n",
            "                                                                  'global_max_pooling1d_81[0][0]',\n",
            "                                                                  'global_max_pooling1d_82[0][0]',\n",
            "                                                                  'global_max_pooling1d_83[0][0]']\n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 1000)         11000       ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 1900)         0           ['add_18[0][0]',                 \n",
            "                                                                  'add_19[0][0]',                 \n",
            "                                                                  'add_20[0][0]',                 \n",
            "                                                                  'dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_6 (TFOpLambda)  (1, None, 1900)      0           ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (1, 1900)           0           ['tf.expand_dims_6[0][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)            (1, 1, 1, 1900)      0           ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (1, 1, 1, 950)       1805950     ['reshape_6[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.relu_6 (TFOpLambda)      (1, 1, 1, 950)       0           ['dense_25[0][0]']               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (1, 1, 1, 1900)      1806900     ['tf.nn.relu_6[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_6 (TFOpLambda)  (1, 1, 1, 1900)     0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (1, 1, None, 1900)   0           ['tf.expand_dims_6[0][0]',       \n",
            "                                                                  'tf.math.sigmoid_6[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_6 (TFOpLa  (None, 1900)        0           ['multiply_6[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1901000     ['tf.compat.v1.squeeze_6[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_30[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_31[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_32[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_33[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 1)            3           ['dropout_34[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,110,705\n",
            "Trainable params: 11,110,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **第二次尝试**"
      ],
      "metadata": {
        "id": "cQCUTlTVXTL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "# import cv2\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        " \n",
        "#from keras.applications.resnet50 import preprocess_input\n",
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "#K.set_learning_phase(1) #set learning phase\n",
        "# 注释掉的是针对输入为图片的情况 \n",
        "# 需根据自己情况修改1.训练好的模型路径\n",
        "weight_file_dir = '/content/gdrive/MyDrive/cnn_model/5月13日(1)_第0次训练_MG_mergeweighted_55:38_Falsedays_Trueinit_Truetrainable_0.000000dpt_0.005000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5'\n",
        "\n",
        "\n",
        "# x为你的输入，现在的代码逻辑是针对一条数据的\n",
        "# 如果一次计算很多条就需要考虑多出来的那个维度了\n",
        "x = [X1_test, demographics_test]\n",
        "print(\"X1_test.ndim:\",X1_test.ndim)\n",
        "print(\"demographics_test.ndim:\",demographics_test.ndim)\n",
        "print(\"X1_test.shape:\",X1_test.shape)\n",
        "x1=np.squeeze(X1_test)\n",
        "x2=np.squeeze(demographics_test)\n",
        "print(\"x1.ndim:\",x1.ndim)\n",
        "print(\"x2.ndim:\",x2.ndim)\n",
        "print(\"x1.shape:\",x1.shape)\n",
        "print(\"x2.shape:\",x2.shape)\n",
        "x3=[x1,x2]\n",
        "print(len(x3))\n",
        "\n",
        "modelpath = '5月13日(1)_'+'第0次训练_'+'MG_merge' + path\n",
        "\n",
        "#test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3) #预测模型\n",
        "model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "\n",
        "y_CNN_pred = model.predict([x1,x2], batch_size=1, verbose=1) #开始预测\n",
        "y_CNN_pred1=tf.convert_to_tensor(y_CNN_pred) #将预测结果转换为张量\n",
        "class_output = y_CNN_pred1\n",
        "\n",
        "last_conv_layer = model.get_layer(\"conv1d_2\") # 需根据自己情况修改2. 把block5_conv3改成需要关注的网络层名字\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk0q3y8cXhOg",
        "outputId": "8112e065-a966-42f3-fde4-2389aad482a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1_test.ndim: 2\n",
            "demographics_test.ndim: 2\n",
            "X1_test.shape: (1569, 3)\n",
            "x1.ndim: 2\n",
            "x2.ndim: 2\n",
            "x1.shape: (1569, 3)\n",
            "x2.shape: (1569, 10)\n",
            "2\n",
            "1569/1569 [==============================] - 10s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGc2zBl7o6-q",
        "outputId": "409d9f9f-3222-470d-dfa2-d43232f725bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_27')>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6oGboQU2wL",
        "outputId": "7d02581c-78aa-47b5-c09b-2765d3f8c455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1569, 1), dtype=float32, numpy=\n",
              "array([[5.6822214],\n",
              "       [5.6830072],\n",
              "       [5.717518 ],\n",
              "       ...,\n",
              "       [5.6804886],\n",
              "       [5.6824117],\n",
              "       [5.6923018]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2kIDFMdVG7F",
        "outputId": "54c63ba8-b279-4e01-9c1b-3ef2a65566d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 300) dtype=float32 (created by layer 'conv1d_19')>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ZZjlfSVJ5h",
        "outputId": "5af0f5a9-6652-4f2f-d6eb-986827ddf479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_27')>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFPu0yflY5-A",
        "outputId": "4286c6ed-78d5-4a99-c91d-074dda199c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'input_13')>,\n",
              " <KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'input_14')>]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM 算法实现\n",
        "import tensorflow.keras.backend as K\n",
        "with tf.GradientTape() as gtape: #keras.backend.gradients\n",
        "    print(\"class_output:\",class_output)\n",
        "    print(last_conv_layer.output)\n",
        "    #print(model.output[0])\n",
        "    #aa=tf.squeeze(last_conv_layer.output,axis=0)\n",
        "    #print(aa[0])\n",
        "    grads = gtape.gradient(model.output,last_conv_layer.output)[0] # last_conv_layer.output\n",
        "pooled_grads = K.mean(grads,axis=(0))\n",
        "iterate = K.function([model.input],[pooled_grads,last_conv_layer.output[0]])\n",
        "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "# 需根据自己情况修改3. 100是此层卷积层的通道数，根据自己情况修改\n",
        "# 因为你的卷积是一维卷积，所以和二维的图像卷积相比，输出也自然少了一维\n",
        "for i in range(100):\n",
        "    # conv_layer_output_value[:,:,i] *= pooled_grads_value[i]\n",
        "    conv_layer_output_value[:,i] *= pooled_grads_value[i]\n",
        " \n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "heatmap = np.maximum(heatmap,0)\n",
        "heatmap /= np.max(heatmap)\n",
        "\n",
        "# 下面这里相当于把热力图加到原始输入图像中\n",
        "# 因为你的输入不是图片，所以可考虑直接打印热力图矩阵？\n",
        "# 和输入同形状，意义为输入各个部分的重要程度\n",
        "heatmap = np.resize(heatmap, x.shape[0])\n",
        "print(heatmap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "6P9PgUJWFIFX",
        "outputId": "ebded07a-66aa-42ba-c853-a7d27120f939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_output: tf.Tensor(\n",
            "[[5.7471347]\n",
            " [5.737805 ]\n",
            " [5.7776055]\n",
            " ...\n",
            " [5.7335706]\n",
            " [5.7313743]\n",
            " [5.750322 ]], shape=(1569, 1), dtype=float32)\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 3, 300), dtype=tf.float32, name=None), name='conv1d_2/Relu:0', description=\"created by layer 'conv1d_2'\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1d99d28480e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#aa=tf.squeeze(last_conv_layer.output,axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(aa[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_conv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# last_conv_layer.output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpooled_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0miterate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpooled_grads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_conv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute '_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_output.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h81j377rGPIn",
        "outputId": "6cb0d84a-d863-449a-c2dd-377db1245727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_conv_layer.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI88fcs9Fh4D",
        "outputId": "fa694698-426c-43fe-bb5b-0403e7c306cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 300) dtype=float32 (created by layer 'conv1d_5')>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **第三次尝试**"
      ],
      "metadata": {
        "id": "mNBl5Mo2UWbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        " \n",
        "#from keras.applications.resnet50 import preprocess_input\n",
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "K.set_learning_phase(1) #set learning phase\n",
        "\n",
        "# 注释掉的是针对输入为图片的情况 \n",
        "# 需根据自己情况修改1.训练好的模型路径\n",
        "weight_file_dir = '/content/gdrive/MyDrive/cnn_model/5月13日(1)_第0次训练_MG_mergeweighted_35:39_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.005000lr_3fz_300fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5'\n",
        "model = keras.models.load_model(weight_file_dir)\n",
        "x = [X1_test, demographics_test]\n",
        "x = [np.array([x[0][0]]),np.array([x[1][0]])] # x为你的输入，现在的代码逻辑是针对一条数据的 # 如果一次计算很多条就需要考虑多出来的那个维度了\n",
        "class_output = model.predict(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "2fc8co2LNnQB",
        "outputId": "a10e27c0-ea95-4cc5-a7fd-642705f444a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-64665ba4348b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# x为你的输入，现在的代码逻辑是针对一条数据的 # 如果一次计算很多条就需要考虑多出来的那个维度了\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mclass_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50haS9yBPy6f",
        "outputId": "432cc5e0-5a0e-48a0-d17c-4fb4689a8375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.7459364],\n",
              "       [5.738635 ],\n",
              "       [5.723418 ],\n",
              "       ...,\n",
              "       [5.738988 ],\n",
              "       [5.7416162],\n",
              "       [5.742443 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 需根据自己情况修改2. 把block5_conv3改成需要关注的网络层名字\n",
        "# last_conv_layer = model.get_layer(\"block5_conv3\")\n",
        "last_conv_layer = model.get_layer(\"conv1d_5\")\n",
        "with tf.GradientTape() as gtape: #keras.backend.gradients\n",
        "    print(\"class_output:\",class_output)\n",
        "    print(last_conv_layer.output)\n",
        "    grads = gtape.gradient(model.output,last_conv_layer.output)[0]\n",
        "#grads = K.gradients(class_output,last_conv_layer.output)[0]\n",
        "pooled_grads = K.mean(grads,axis=(0))\n",
        "iterate = K.function([model.input],[pooled_grads,last_conv_layer.output[0]])\n",
        "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "# 需根据自己情况修改3. 100是此层卷积层的通道数，根据自己情况修改\n",
        "# 因为你的卷积是一维卷积，所以和二维的图像卷积相比，输出也自然少了一维\n",
        "for i in range(100):\n",
        "    # conv_layer_output_value[:,:,i] *= pooled_grads_value[i]\n",
        "    conv_layer_output_value[:,i] *= pooled_grads_value[i]\n",
        " \n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "heatmap = np.maximum(heatmap,0)\n",
        "heatmap /= np.max(heatmap)\n",
        " \n",
        "# 下面这里相当于把热力图加到原始输入图像中\n",
        "# 因为你的输入不是图片，所以可考虑直接打印热力图矩阵？\n",
        "# 和输入同形状，意义为输入各个部分的重要程度\n",
        "heatmap = np.resize(heatmap, x.shape[0])\n",
        "print(heatmap)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "ldyLjD4_PrXB",
        "outputId": "cddb5c5f-bf5f-4d9a-f8d9-67fe105f2437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_output: [[5.7459364]\n",
            " [5.738635 ]\n",
            " [5.723418 ]\n",
            " ...\n",
            " [5.738988 ]\n",
            " [5.7416162]\n",
            " [5.742443 ]]\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 3, 300), dtype=tf.float32, name=None), name='conv1d_5/Relu:0', description=\"created by layer 'conv1d_5'\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1d625115f4d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class_output:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_conv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_conv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#grads = K.gradients(class_output,last_conv_layer.output)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpooled_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute '_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 原卷积神经网络模型预测**"
      ],
      "metadata": {
        "id": "OsvnheVgdzCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.1 原卷积神经网络模型预测(1)**"
      ],
      "metadata": {
        "id": "B5DtlqyvcuB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2_SG_merge=[]\n",
        "seed = 1234\n",
        "def test_single_channel_merge_model(model, modelpath,X_test,demographics_test,y_test, index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"./cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    print(r2)\n",
        "    r2_SG_merge.append(r2)\n",
        "    name = 'SG_merge%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "r2_SG_split=[]\n",
        "def test_single_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"./cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test,  demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    r2_SG_split.append(r2)\n",
        "    name = 'SG_merge%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "r2_MG_merge=[]\n",
        "def test_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"./cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    print(\"y_pred:{}\".format(y_pred.shape))\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    r2_MG_merge.append(r2)\n",
        "    name = 'MG_split%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  modelpath = 'SG_merge' + path\n",
        "  model = single_channel_merge_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_single_channel_merge_model(model, modelpath, X_train, demographics_train, y_train)\n",
        "  print(f,'testing on the testing datasets.....')\n",
        "  test_single_channel_merge_model(model, modelpath, X_test, demographics_test, y_test, 3)\n",
        "  print(f,'testing on the filtered testing datasets.....')\n",
        "  print(r2_SG_merge)\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  modelpath = 'SG_split' + path\n",
        "  model = single_channel_split_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_single_channel_split_model(model, modelpath, X1_train, demographics_train, y_train)\n",
        "  print(f,'testing on the testing datasets.....')\n",
        "  test_single_channel_split_model(model, modelpath, X1_test,  demographics_test,y_test,3)\n",
        "  #print(f,'testing on the filtered testing datasets.....')\n",
        "  print(r2_SG_split)\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  modelpath = 'MG_merge' + path\n",
        "  model= multi_channel_split_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_multi_channel_split_model(model, modelpath, X1_train, demographics_train, y_train)\n",
        "  extract_patientvec(model, modelpath, disease, demographics)\n",
        "  print(f,'testing on the testing datasets.....')\n",
        "  test_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
        "  print(r2_MG_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnmHHz36eGmp",
        "outputId": "be9c6e1f-0cd1-4981-a719-dec66247b4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n",
            "Model: \"model_120\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_41 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_80 (Embedding)       (None, 17, 900)      649800      ['input_41[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_60 (Conv1D)             (None, 17, 100)      270100      ['embedding_80[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_61 (Conv1D)             (None, 17, 100)      270100      ['embedding_80[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_62 (Conv1D)             (None, 17, 100)      270100      ['embedding_80[0][0]']           \n",
            "                                                                                                  \n",
            " input_42 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_240 (Glob  (None, 100)         0           ['conv1d_60[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_241 (Glob  (None, 100)         0           ['conv1d_61[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_242 (Glob  (None, 100)         0           ['conv1d_62[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_80 (Dense)               (None, 3)            33          ['input_42[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_240[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_241[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_242[0][0]'\n",
            "                                                                 , 'dense_80[0][0]']              \n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (None, 500)          152000      ['concatenate_20[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_100 (Dropout)          (None, 500)          0           ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 100)          50100       ['dropout_100[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_101 (Dropout)          (None, 100)          0           ['dense_82[0][0]']               \n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 1)            101         ['dropout_101[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1465 - mae: 1.0259\n",
            "Epoch 1: val_loss improved from inf to 0.76035, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 54ms/step - loss: 3.1465 - mae: 1.0259 - val_loss: 0.7604 - val_mae: 0.5859\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6987 - mae: 0.5482\n",
            "Epoch 2: val_loss improved from 0.76035 to 0.61641, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.6958 - mae: 0.5479 - val_loss: 0.6164 - val_mae: 0.5126\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5893 - mae: 0.5105\n",
            "Epoch 3: val_loss improved from 0.61641 to 0.52411, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.5857 - mae: 0.5080 - val_loss: 0.5241 - val_mae: 0.4834\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4962 - mae: 0.4747\n",
            "Epoch 4: val_loss improved from 0.52411 to 0.45705, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4962 - mae: 0.4747 - val_loss: 0.4570 - val_mae: 0.4477\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4338 - mae: 0.4466\n",
            "Epoch 5: val_loss improved from 0.45705 to 0.40323, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.4326 - mae: 0.4463 - val_loss: 0.4032 - val_mae: 0.4277\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3832 - mae: 0.4222\n",
            "Epoch 6: val_loss improved from 0.40323 to 0.35877, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3832 - mae: 0.4222 - val_loss: 0.3588 - val_mae: 0.4013\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3422 - mae: 0.3972\n",
            "Epoch 7: val_loss improved from 0.35877 to 0.32305, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3404 - mae: 0.3960 - val_loss: 0.3231 - val_mae: 0.3779\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3039 - mae: 0.3700\n",
            "Epoch 8: val_loss improved from 0.32305 to 0.28092, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3033 - mae: 0.3704 - val_loss: 0.2809 - val_mae: 0.3550\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2791 - mae: 0.3577\n",
            "Epoch 9: val_loss improved from 0.28092 to 0.25065, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2785 - mae: 0.3569 - val_loss: 0.2507 - val_mae: 0.3362\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2455 - mae: 0.3306\n",
            "Epoch 10: val_loss improved from 0.25065 to 0.24398, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2458 - mae: 0.3311 - val_loss: 0.2440 - val_mae: 0.3293\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2215 - mae: 0.3101\n",
            "Epoch 11: val_loss improved from 0.24398 to 0.21201, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2204 - mae: 0.3100 - val_loss: 0.2120 - val_mae: 0.3071\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1972 - mae: 0.2883\n",
            "Epoch 12: val_loss improved from 0.21201 to 0.20820, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1973 - mae: 0.2898 - val_loss: 0.2082 - val_mae: 0.3320\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2022 - mae: 0.3098\n",
            "Epoch 13: val_loss improved from 0.20820 to 0.17949, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2011 - mae: 0.3082 - val_loss: 0.1795 - val_mae: 0.2856\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1690 - mae: 0.2649\n",
            "Epoch 14: val_loss did not improve from 0.17949\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1724 - mae: 0.2702 - val_loss: 0.2453 - val_mae: 0.3818\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2359 - mae: 0.3756\n",
            "Epoch 15: val_loss did not improve from 0.17949\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2358 - mae: 0.3756 - val_loss: 0.2453 - val_mae: 0.4082\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2224 - mae: 0.3592\n",
            "Epoch 16: val_loss did not improve from 0.17949\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2220 - mae: 0.3586 - val_loss: 0.2204 - val_mae: 0.3620\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2066 - mae: 0.3423\n",
            "Epoch 17: val_loss improved from 0.17949 to 0.14917, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2044 - mae: 0.3394 - val_loss: 0.1492 - val_mae: 0.2682\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1459 - mae: 0.2432\n",
            "Epoch 18: val_loss improved from 0.14917 to 0.14478, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1450 - mae: 0.2430 - val_loss: 0.1448 - val_mae: 0.2631\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1773 - mae: 0.3071\n",
            "Epoch 19: val_loss did not improve from 0.14478\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1772 - mae: 0.3074 - val_loss: 0.1889 - val_mae: 0.3292\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1977 - mae: 0.3351\n",
            "Epoch 20: val_loss improved from 0.14478 to 0.13386, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1977 - mae: 0.3351 - val_loss: 0.1339 - val_mae: 0.2448\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.3012\n",
            "Epoch 21: val_loss did not improve from 0.13386\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1751 - mae: 0.3017 - val_loss: 0.1723 - val_mae: 0.3075\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1780 - mae: 0.3115\n",
            "Epoch 22: val_loss improved from 0.13386 to 0.11440, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1764 - mae: 0.3085 - val_loss: 0.1144 - val_mae: 0.2003\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1474 - mae: 0.2561\n",
            "Epoch 23: val_loss did not improve from 0.11440\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1467 - mae: 0.2559 - val_loss: 0.1280 - val_mae: 0.2368\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1856 - mae: 0.3254\n",
            "Epoch 24: val_loss did not improve from 0.11440\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1854 - mae: 0.3252 - val_loss: 0.1679 - val_mae: 0.3040\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1350 - mae: 0.2360\n",
            "Epoch 25: val_loss did not improve from 0.11440\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1359 - mae: 0.2382 - val_loss: 0.1691 - val_mae: 0.3166\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1899 - mae: 0.3367\n",
            "Epoch 26: val_loss did not improve from 0.11440\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1893 - mae: 0.3358 - val_loss: 0.1756 - val_mae: 0.3168\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1317 - mae: 0.2348\n",
            "Epoch 27: val_loss improved from 0.11440 to 0.10986, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1315 - mae: 0.2336 - val_loss: 0.1099 - val_mae: 0.1938\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1415 - mae: 0.2594\n",
            "Epoch 28: val_loss did not improve from 0.10986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1444 - mae: 0.2621 - val_loss: 0.1326 - val_mae: 0.2477\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1559 - mae: 0.2862\n",
            "Epoch 29: val_loss did not improve from 0.10986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1574 - mae: 0.2867 - val_loss: 0.1343 - val_mae: 0.2564\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1234 - mae: 0.2171\n",
            "Epoch 30: val_loss did not improve from 0.10986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1220 - mae: 0.2160 - val_loss: 0.1157 - val_mae: 0.2102\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1264 - mae: 0.2272\n",
            "Epoch 31: val_loss did not improve from 0.10986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1264 - mae: 0.2272 - val_loss: 0.1229 - val_mae: 0.2329\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1585 - mae: 0.2878\n",
            "Epoch 32: val_loss did not improve from 0.10986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1585 - mae: 0.2878 - val_loss: 0.1183 - val_mae: 0.2178\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1186 - mae: 0.2102\n",
            "Epoch 33: val_loss did not improve from 0.10986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1190 - mae: 0.2117 - val_loss: 0.1224 - val_mae: 0.2314\n",
            "Epoch 33: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13675035536289215, RMSE:0.3697977364063263, MAE:0.21510738134384155, R2:0.8404397759662624\n",
            "0.8404397759662624\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n",
            "Model: \"model_121\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_43 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_81 (Embedding)       (None, 17, 900)      649800      ['input_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_63 (Conv1D)             (None, 17, 100)      270100      ['embedding_81[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 17, 100)      270100      ['embedding_81[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 17, 100)      270100      ['embedding_81[0][0]']           \n",
            "                                                                                                  \n",
            " input_44 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_243 (Glob  (None, 100)         0           ['conv1d_63[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_244 (Glob  (None, 100)         0           ['conv1d_64[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_245 (Glob  (None, 100)         0           ['conv1d_65[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 3)            33          ['input_44[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_243[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_244[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_245[0][0]'\n",
            "                                                                 , 'dense_84[0][0]']              \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 500)          152000      ['concatenate_21[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 500)          0           ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 100)          50100       ['dropout_102[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 100)          0           ['dense_86[0][0]']               \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 1)            101         ['dropout_103[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 3.7781 - mae: 1.1025\n",
            "Epoch 1: val_loss improved from inf to 0.71625, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 41ms/step - loss: 3.5604 - mae: 1.0676 - val_loss: 0.7163 - val_mae: 0.5344\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6444 - mae: 0.5277\n",
            "Epoch 2: val_loss improved from 0.71625 to 0.55512, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.6363 - mae: 0.5238 - val_loss: 0.5551 - val_mae: 0.4916\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5140 - mae: 0.4805\n",
            "Epoch 3: val_loss improved from 0.55512 to 0.47182, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.5140 - mae: 0.4805 - val_loss: 0.4718 - val_mae: 0.4823\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4368 - mae: 0.4487\n",
            "Epoch 4: val_loss improved from 0.47182 to 0.39956, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4362 - mae: 0.4486 - val_loss: 0.3996 - val_mae: 0.4239\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3769 - mae: 0.4155\n",
            "Epoch 5: val_loss improved from 0.39956 to 0.34944, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.3763 - mae: 0.4154 - val_loss: 0.3494 - val_mae: 0.3970\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3289 - mae: 0.3873\n",
            "Epoch 6: val_loss improved from 0.34944 to 0.30634, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3280 - mae: 0.3870 - val_loss: 0.3063 - val_mae: 0.3677\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2860 - mae: 0.3561\n",
            "Epoch 7: val_loss improved from 0.30634 to 0.26607, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2860 - mae: 0.3561 - val_loss: 0.2661 - val_mae: 0.3485\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2505 - mae: 0.3323\n",
            "Epoch 8: val_loss improved from 0.26607 to 0.23099, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2505 - mae: 0.3316 - val_loss: 0.2310 - val_mae: 0.3143\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2191 - mae: 0.3052\n",
            "Epoch 9: val_loss improved from 0.23099 to 0.20290, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2191 - mae: 0.3052 - val_loss: 0.2029 - val_mae: 0.2938\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2096 - mae: 0.3036\n",
            "Epoch 10: val_loss improved from 0.20290 to 0.19189, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2068 - mae: 0.3014 - val_loss: 0.1919 - val_mae: 0.2975\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1848 - mae: 0.2779\n",
            "Epoch 11: val_loss improved from 0.19189 to 0.17592, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1844 - mae: 0.2789 - val_loss: 0.1759 - val_mae: 0.2834\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1990 - mae: 0.3132\n",
            "Epoch 12: val_loss did not improve from 0.17592\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2001 - mae: 0.3141 - val_loss: 0.2023 - val_mae: 0.3234\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1894 - mae: 0.3052\n",
            "Epoch 13: val_loss did not improve from 0.17592\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1896 - mae: 0.3066 - val_loss: 0.2015 - val_mae: 0.3443\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1832 - mae: 0.3014\n",
            "Epoch 14: val_loss did not improve from 0.17592\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1836 - mae: 0.3005 - val_loss: 0.1819 - val_mae: 0.3037\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1848 - mae: 0.3089\n",
            "Epoch 15: val_loss improved from 0.17592 to 0.14680, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1829 - mae: 0.3064 - val_loss: 0.1468 - val_mae: 0.2565\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1603 - mae: 0.2718\n",
            "Epoch 16: val_loss did not improve from 0.14680\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1620 - mae: 0.2734 - val_loss: 0.1822 - val_mae: 0.3107\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2268 - mae: 0.3729\n",
            "Epoch 17: val_loss did not improve from 0.14680\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2262 - mae: 0.3711 - val_loss: 0.1751 - val_mae: 0.3154\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2051 - mae: 0.3430\n",
            "Epoch 18: val_loss did not improve from 0.14680\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2051 - mae: 0.3430 - val_loss: 0.2247 - val_mae: 0.3704\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1601 - mae: 0.2707\n",
            "Epoch 19: val_loss improved from 0.14680 to 0.12540, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1579 - mae: 0.2681 - val_loss: 0.1254 - val_mae: 0.2213\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2063 - mae: 0.3415\n",
            "Epoch 20: val_loss did not improve from 0.12540\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2059 - mae: 0.3421 - val_loss: 0.2091 - val_mae: 0.3535\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1586 - mae: 0.2749\n",
            "Epoch 21: val_loss did not improve from 0.12540\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1595 - mae: 0.2772 - val_loss: 0.1548 - val_mae: 0.2866\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1590 - mae: 0.2806\n",
            "Epoch 22: val_loss did not improve from 0.12540\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1595 - mae: 0.2796 - val_loss: 0.1486 - val_mae: 0.2644\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1509 - mae: 0.2659\n",
            "Epoch 23: val_loss did not improve from 0.12540\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1500 - mae: 0.2655 - val_loss: 0.1516 - val_mae: 0.2829\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1501 - mae: 0.2639\n",
            "Epoch 24: val_loss improved from 0.12540 to 0.11549, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1464 - mae: 0.2589 - val_loss: 0.1155 - val_mae: 0.2034\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1681 - mae: 0.2948\n",
            "Epoch 25: val_loss did not improve from 0.11549\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1686 - mae: 0.2960 - val_loss: 0.1694 - val_mae: 0.3020\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1357 - mae: 0.2389\n",
            "Epoch 26: val_loss did not improve from 0.11549\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1357 - mae: 0.2389 - val_loss: 0.1392 - val_mae: 0.2621\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1539 - mae: 0.2767\n",
            "Epoch 27: val_loss did not improve from 0.11549\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1528 - mae: 0.2763 - val_loss: 0.1639 - val_mae: 0.2957\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1584 - mae: 0.2865\n",
            "Epoch 28: val_loss did not improve from 0.11549\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1585 - mae: 0.2869 - val_loss: 0.1472 - val_mae: 0.2797\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1903 - mae: 0.3358\n",
            "Epoch 29: val_loss did not improve from 0.11549\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1899 - mae: 0.3348 - val_loss: 0.1542 - val_mae: 0.2792\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1288 - mae: 0.2274\n",
            "Epoch 30: val_loss did not improve from 0.11549\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1288 - mae: 0.2274 - val_loss: 0.1780 - val_mae: 0.3177\n",
            "Epoch 30: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13684609532356262, RMSE:0.36992713809013367, MAE:0.22378920018672943, R2:0.8403280665778942\n",
            "0.8403280665778942\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_122\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_45 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_82 (Embedding)       (None, 17, 900)      649800      ['input_45[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 17, 100)      270100      ['embedding_82[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_67 (Conv1D)             (None, 17, 100)      270100      ['embedding_82[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_68 (Conv1D)             (None, 17, 100)      270100      ['embedding_82[0][0]']           \n",
            "                                                                                                  \n",
            " input_46 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_246 (Glob  (None, 100)         0           ['conv1d_66[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_247 (Glob  (None, 100)         0           ['conv1d_67[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_248 (Glob  (None, 100)         0           ['conv1d_68[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 3)            33          ['input_46[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_246[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_247[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_248[0][0]'\n",
            "                                                                 , 'dense_88[0][0]']              \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 500)          152000      ['concatenate_22[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 500)          0           ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 100)          50100       ['dropout_104[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 100)          0           ['dense_90[0][0]']               \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 1)            101         ['dropout_105[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.3044 - mae: 1.0415\n",
            "Epoch 1: val_loss improved from inf to 0.76509, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 39ms/step - loss: 3.2244 - mae: 1.0257 - val_loss: 0.7651 - val_mae: 0.6334\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7017 - mae: 0.5535\n",
            "Epoch 2: val_loss improved from 0.76509 to 0.59802, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.6912 - mae: 0.5482 - val_loss: 0.5980 - val_mae: 0.4865\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5570 - mae: 0.4933\n",
            "Epoch 3: val_loss improved from 0.59802 to 0.50213, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5584 - mae: 0.4939 - val_loss: 0.5021 - val_mae: 0.4967\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4754 - mae: 0.4681\n",
            "Epoch 4: val_loss improved from 0.50213 to 0.42934, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.4713 - mae: 0.4661 - val_loss: 0.4293 - val_mae: 0.4343\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4108 - mae: 0.4390\n",
            "Epoch 5: val_loss improved from 0.42934 to 0.37367, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4072 - mae: 0.4367 - val_loss: 0.3737 - val_mae: 0.4066\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3520 - mae: 0.4038\n",
            "Epoch 6: val_loss improved from 0.37367 to 0.33134, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3542 - mae: 0.4043 - val_loss: 0.3313 - val_mae: 0.3776\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3122 - mae: 0.3765\n",
            "Epoch 7: val_loss improved from 0.33134 to 0.28644, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.3113 - mae: 0.3753 - val_loss: 0.2864 - val_mae: 0.3504\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2892 - mae: 0.3635\n",
            "Epoch 8: val_loss did not improve from 0.28644\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2868 - mae: 0.3622 - val_loss: 0.2917 - val_mae: 0.3606\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2581 - mae: 0.3408\n",
            "Epoch 9: val_loss improved from 0.28644 to 0.27165, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2568 - mae: 0.3407 - val_loss: 0.2717 - val_mae: 0.3910\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2203 - mae: 0.3127\n",
            "Epoch 10: val_loss improved from 0.27165 to 0.24209, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2220 - mae: 0.3140 - val_loss: 0.2421 - val_mae: 0.3660\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2671 - mae: 0.3878\n",
            "Epoch 11: val_loss improved from 0.24209 to 0.20451, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2661 - mae: 0.3858 - val_loss: 0.2045 - val_mae: 0.3031\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2425 - mae: 0.3627\n",
            "Epoch 12: val_loss did not improve from 0.20451\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2425 - mae: 0.3627 - val_loss: 0.3126 - val_mae: 0.4722\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2420 - mae: 0.3714\n",
            "Epoch 13: val_loss did not improve from 0.20451\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2420 - mae: 0.3714 - val_loss: 0.2448 - val_mae: 0.3729\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2033 - mae: 0.3247\n",
            "Epoch 14: val_loss improved from 0.20451 to 0.17736, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2029 - mae: 0.3245 - val_loss: 0.1774 - val_mae: 0.2994\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2702 - mae: 0.4106\n",
            "Epoch 15: val_loss did not improve from 0.17736\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.2758 - mae: 0.4160 - val_loss: 0.2985 - val_mae: 0.4409\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2234 - mae: 0.3566\n",
            "Epoch 16: val_loss did not improve from 0.17736\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2216 - mae: 0.3547 - val_loss: 0.1919 - val_mae: 0.3339\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2656 - mae: 0.4122\n",
            "Epoch 17: val_loss did not improve from 0.17736\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2619 - mae: 0.4085 - val_loss: 0.1806 - val_mae: 0.3023\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1512 - mae: 0.2517\n",
            "Epoch 18: val_loss improved from 0.17736 to 0.13160, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1500 - mae: 0.2503 - val_loss: 0.1316 - val_mae: 0.2205\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1429 - mae: 0.2385\n",
            "Epoch 19: val_loss did not improve from 0.13160\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1470 - mae: 0.2461 - val_loss: 0.2207 - val_mae: 0.3813\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2372 - mae: 0.3862\n",
            "Epoch 20: val_loss did not improve from 0.13160\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2372 - mae: 0.3862 - val_loss: 0.1557 - val_mae: 0.2726\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1781 - mae: 0.3070\n",
            "Epoch 21: val_loss did not improve from 0.13160\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1785 - mae: 0.3085 - val_loss: 0.1965 - val_mae: 0.3518\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2022 - mae: 0.3453\n",
            "Epoch 22: val_loss did not improve from 0.13160\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2022 - mae: 0.3453 - val_loss: 0.1483 - val_mae: 0.2609\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1429 - mae: 0.2493\n",
            "Epoch 23: val_loss did not improve from 0.13160\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1424 - mae: 0.2487 - val_loss: 0.1360 - val_mae: 0.2458\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1856 - mae: 0.3197\n",
            "Epoch 24: val_loss improved from 0.13160 to 0.12161, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1811 - mae: 0.3116 - val_loss: 0.1216 - val_mae: 0.2096\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1261 - mae: 0.2154\n",
            "Epoch 25: val_loss improved from 0.12161 to 0.11630, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1252 - mae: 0.2145 - val_loss: 0.1163 - val_mae: 0.2001\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1573 - mae: 0.2711\n",
            "Epoch 26: val_loss did not improve from 0.11630\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1626 - mae: 0.2773 - val_loss: 0.2569 - val_mae: 0.4328\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1576 - mae: 0.2710\n",
            "Epoch 27: val_loss did not improve from 0.11630\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1563 - mae: 0.2688 - val_loss: 0.1197 - val_mae: 0.2092\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1368 - mae: 0.2470\n",
            "Epoch 28: val_loss did not improve from 0.11630\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1375 - mae: 0.2476 - val_loss: 0.1391 - val_mae: 0.2612\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1343 - mae: 0.2426\n",
            "Epoch 29: val_loss did not improve from 0.11630\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1344 - mae: 0.2424 - val_loss: 0.1388 - val_mae: 0.2533\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1259 - mae: 0.2270\n",
            "Epoch 30: val_loss did not improve from 0.11630\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1267 - mae: 0.2271 - val_loss: 0.1225 - val_mae: 0.2251\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1593 - mae: 0.2819\n",
            "Epoch 31: val_loss did not improve from 0.11630\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1582 - mae: 0.2814 - val_loss: 0.1705 - val_mae: 0.3101\n",
            "Epoch 31: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.14098358154296875, RMSE:0.37547779083251953, MAE:0.21988902986049652, R2:0.8355004636743163\n",
            "0.8355004636743163\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_123\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_83 (Embedding)       (None, 17, 900)      649800      ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_69 (Conv1D)             (None, 17, 100)      270100      ['embedding_83[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_70 (Conv1D)             (None, 17, 100)      270100      ['embedding_83[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_71 (Conv1D)             (None, 17, 100)      270100      ['embedding_83[0][0]']           \n",
            "                                                                                                  \n",
            " input_48 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_249 (Glob  (None, 100)         0           ['conv1d_69[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_250 (Glob  (None, 100)         0           ['conv1d_70[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_251 (Glob  (None, 100)         0           ['conv1d_71[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 3)            33          ['input_48[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_249[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_250[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_251[0][0]'\n",
            "                                                                 , 'dense_92[0][0]']              \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 500)          152000      ['concatenate_23[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_106 (Dropout)          (None, 500)          0           ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 100)          50100       ['dropout_106[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 100)          0           ['dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 1)            101         ['dropout_107[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.4372 - mae: 0.8896\n",
            "Epoch 1: val_loss improved from inf to 0.71434, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 39ms/step - loss: 2.4372 - mae: 0.8896 - val_loss: 0.7143 - val_mae: 0.4662\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6481 - mae: 0.5289\n",
            "Epoch 2: val_loss improved from 0.71434 to 0.57414, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.6467 - mae: 0.5270 - val_loss: 0.5741 - val_mae: 0.5685\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5365 - mae: 0.4928\n",
            "Epoch 3: val_loss improved from 0.57414 to 0.46627, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5323 - mae: 0.4906 - val_loss: 0.4663 - val_mae: 0.4514\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4519 - mae: 0.4547\n",
            "Epoch 4: val_loss improved from 0.46627 to 0.40863, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4493 - mae: 0.4535 - val_loss: 0.4086 - val_mae: 0.4227\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3884 - mae: 0.4197\n",
            "Epoch 5: val_loss improved from 0.40863 to 0.38520, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3896 - mae: 0.4207 - val_loss: 0.3852 - val_mae: 0.4636\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3552 - mae: 0.4062\n",
            "Epoch 6: val_loss improved from 0.38520 to 0.30964, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3535 - mae: 0.4047 - val_loss: 0.3096 - val_mae: 0.3782\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3197 - mae: 0.3862\n",
            "Epoch 7: val_loss did not improve from 0.30964\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3197 - mae: 0.3863 - val_loss: 0.3203 - val_mae: 0.3816\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2699 - mae: 0.3441\n",
            "Epoch 8: val_loss improved from 0.30964 to 0.24482, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2699 - mae: 0.3441 - val_loss: 0.2448 - val_mae: 0.3245\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2954 - mae: 0.3906\n",
            "Epoch 9: val_loss improved from 0.24482 to 0.22011, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2895 - mae: 0.3840 - val_loss: 0.2201 - val_mae: 0.3074\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2421 - mae: 0.3407\n",
            "Epoch 10: val_loss did not improve from 0.22011\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2419 - mae: 0.3411 - val_loss: 0.2545 - val_mae: 0.3942\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2782 - mae: 0.4020\n",
            "Epoch 11: val_loss did not improve from 0.22011\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.2783 - mae: 0.4016 - val_loss: 0.2215 - val_mae: 0.3295\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2267 - mae: 0.3457\n",
            "Epoch 12: val_loss did not improve from 0.22011\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2293 - mae: 0.3474 - val_loss: 0.2487 - val_mae: 0.4019\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2358 - mae: 0.3681\n",
            "Epoch 13: val_loss improved from 0.22011 to 0.17389, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2360 - mae: 0.3666 - val_loss: 0.1739 - val_mae: 0.2815\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2137 - mae: 0.3400\n",
            "Epoch 14: val_loss improved from 0.17389 to 0.17046, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2130 - mae: 0.3387 - val_loss: 0.1705 - val_mae: 0.2948\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2737 - mae: 0.4188\n",
            "Epoch 15: val_loss did not improve from 0.17046\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2711 - mae: 0.4162 - val_loss: 0.2010 - val_mae: 0.3354\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1836 - mae: 0.3053\n",
            "Epoch 16: val_loss improved from 0.17046 to 0.16647, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1838 - mae: 0.3050 - val_loss: 0.1665 - val_mae: 0.2972\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2482 - mae: 0.3851\n",
            "Epoch 17: val_loss did not improve from 0.16647\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2482 - mae: 0.3851 - val_loss: 0.3203 - val_mae: 0.4737\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2063 - mae: 0.3372\n",
            "Epoch 18: val_loss improved from 0.16647 to 0.13879, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2037 - mae: 0.3341 - val_loss: 0.1388 - val_mae: 0.2494\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1702 - mae: 0.2930\n",
            "Epoch 19: val_loss did not improve from 0.13879\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1717 - mae: 0.2938 - val_loss: 0.1933 - val_mae: 0.3355\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1651 - mae: 0.2855\n",
            "Epoch 20: val_loss improved from 0.13879 to 0.12207, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1617 - mae: 0.2809 - val_loss: 0.1221 - val_mae: 0.2165\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.2936\n",
            "Epoch 21: val_loss did not improve from 0.12207\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1710 - mae: 0.2925 - val_loss: 0.1423 - val_mae: 0.2553\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1739 - mae: 0.3008\n",
            "Epoch 22: val_loss did not improve from 0.12207\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1782 - mae: 0.3083 - val_loss: 0.2361 - val_mae: 0.4054\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1962 - mae: 0.3383\n",
            "Epoch 23: val_loss did not improve from 0.12207\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1962 - mae: 0.3388 - val_loss: 0.2080 - val_mae: 0.3587\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1671 - mae: 0.2910\n",
            "Epoch 24: val_loss did not improve from 0.12207\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1682 - mae: 0.2925 - val_loss: 0.1719 - val_mae: 0.3188\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1394 - mae: 0.2483\n",
            "Epoch 25: val_loss did not improve from 0.12207\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1394 - mae: 0.2483 - val_loss: 0.1285 - val_mae: 0.2389\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1552 - mae: 0.2795\n",
            "Epoch 26: val_loss did not improve from 0.12207\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1552 - mae: 0.2795 - val_loss: 0.1313 - val_mae: 0.2412\n",
            "Epoch 26: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.15355364978313446, RMSE:0.391859233379364, MAE:0.24127265810966492, R2:0.8208336965200509\n",
            "0.8208336965200509\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_124\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_49 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_84 (Embedding)       (None, 17, 900)      649800      ['input_49[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_72 (Conv1D)             (None, 17, 100)      270100      ['embedding_84[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_73 (Conv1D)             (None, 17, 100)      270100      ['embedding_84[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_74 (Conv1D)             (None, 17, 100)      270100      ['embedding_84[0][0]']           \n",
            "                                                                                                  \n",
            " input_50 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_252 (Glob  (None, 100)         0           ['conv1d_72[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_253 (Glob  (None, 100)         0           ['conv1d_73[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_254 (Glob  (None, 100)         0           ['conv1d_74[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 3)            33          ['input_50[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_252[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_253[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_254[0][0]'\n",
            "                                                                 , 'dense_96[0][0]']              \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 500)          152000      ['concatenate_24[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_108 (Dropout)          (None, 500)          0           ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 100)          50100       ['dropout_108[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_109 (Dropout)          (None, 100)          0           ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 1)            101         ['dropout_109[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.2200 - mae: 1.0582\n",
            "Epoch 1: val_loss improved from inf to 0.79215, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 38ms/step - loss: 3.1416 - mae: 1.0422 - val_loss: 0.7921 - val_mae: 0.6140\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7118 - mae: 0.5550\n",
            "Epoch 2: val_loss improved from 0.79215 to 0.64436, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7128 - mae: 0.5543 - val_loss: 0.6444 - val_mae: 0.5939\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5886 - mae: 0.5032\n",
            "Epoch 3: val_loss improved from 0.64436 to 0.55210, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.5855 - mae: 0.5027 - val_loss: 0.5521 - val_mae: 0.5456\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5099 - mae: 0.4822\n",
            "Epoch 4: val_loss improved from 0.55210 to 0.46639, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5094 - mae: 0.4817 - val_loss: 0.4664 - val_mae: 0.4456\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4401 - mae: 0.4489\n",
            "Epoch 5: val_loss improved from 0.46639 to 0.40696, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4380 - mae: 0.4476 - val_loss: 0.4070 - val_mae: 0.4270\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3813 - mae: 0.4169\n",
            "Epoch 6: val_loss improved from 0.40696 to 0.36695, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3813 - mae: 0.4169 - val_loss: 0.3669 - val_mae: 0.4249\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3352 - mae: 0.3926\n",
            "Epoch 7: val_loss improved from 0.36695 to 0.30996, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3327 - mae: 0.3909 - val_loss: 0.3100 - val_mae: 0.3747\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2932 - mae: 0.3653\n",
            "Epoch 8: val_loss improved from 0.30996 to 0.28137, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.2917 - mae: 0.3648 - val_loss: 0.2814 - val_mae: 0.3529\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2651 - mae: 0.3478\n",
            "Epoch 9: val_loss improved from 0.28137 to 0.23697, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2616 - mae: 0.3461 - val_loss: 0.2370 - val_mae: 0.3311\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2240 - mae: 0.3152\n",
            "Epoch 10: val_loss improved from 0.23697 to 0.20769, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2240 - mae: 0.3152 - val_loss: 0.2077 - val_mae: 0.3007\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2085 - mae: 0.3069\n",
            "Epoch 11: val_loss improved from 0.20769 to 0.20697, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2081 - mae: 0.3096 - val_loss: 0.2070 - val_mae: 0.3115\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2081 - mae: 0.3210\n",
            "Epoch 12: val_loss did not improve from 0.20697\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2109 - mae: 0.3254 - val_loss: 0.2564 - val_mae: 0.4130\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2560 - mae: 0.3924\n",
            "Epoch 13: val_loss did not improve from 0.20697\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2560 - mae: 0.3924 - val_loss: 0.3148 - val_mae: 0.4495\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2100 - mae: 0.3388\n",
            "Epoch 14: val_loss improved from 0.20697 to 0.16842, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2100 - mae: 0.3388 - val_loss: 0.1684 - val_mae: 0.2928\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2567 - mae: 0.3999\n",
            "Epoch 15: val_loss did not improve from 0.16842\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2529 - mae: 0.3965 - val_loss: 0.2059 - val_mae: 0.3363\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2234 - mae: 0.3633\n",
            "Epoch 16: val_loss did not improve from 0.16842\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2226 - mae: 0.3627 - val_loss: 0.1871 - val_mae: 0.3346\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2122 - mae: 0.3519\n",
            "Epoch 17: val_loss did not improve from 0.16842\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2118 - mae: 0.3519 - val_loss: 0.2220 - val_mae: 0.3615\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1807 - mae: 0.3095\n",
            "Epoch 18: val_loss improved from 0.16842 to 0.14434, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1796 - mae: 0.3081 - val_loss: 0.1443 - val_mae: 0.2607\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1994 - mae: 0.3297\n",
            "Epoch 19: val_loss did not improve from 0.14434\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2025 - mae: 0.3339 - val_loss: 0.2737 - val_mae: 0.4275\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1774 - mae: 0.3051\n",
            "Epoch 20: val_loss improved from 0.14434 to 0.13693, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1774 - mae: 0.3051 - val_loss: 0.1369 - val_mae: 0.2492\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1624 - mae: 0.2866\n",
            "Epoch 21: val_loss did not improve from 0.13693\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1631 - mae: 0.2876 - val_loss: 0.1700 - val_mae: 0.2981\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1600 - mae: 0.2832\n",
            "Epoch 22: val_loss did not improve from 0.13693\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1600 - mae: 0.2832 - val_loss: 0.2199 - val_mae: 0.3900\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1884 - mae: 0.3261\n",
            "Epoch 23: val_loss improved from 0.13693 to 0.11607, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1816 - mae: 0.3162 - val_loss: 0.1161 - val_mae: 0.1985\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1190 - mae: 0.2021\n",
            "Epoch 24: val_loss did not improve from 0.11607\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1188 - mae: 0.2023 - val_loss: 0.1223 - val_mae: 0.2126\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2015 - mae: 0.3468\n",
            "Epoch 25: val_loss did not improve from 0.11607\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2006 - mae: 0.3453 - val_loss: 0.1360 - val_mae: 0.2540\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1469 - mae: 0.2642\n",
            "Epoch 26: val_loss did not improve from 0.11607\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1455 - mae: 0.2593 - val_loss: 0.1163 - val_mae: 0.2055\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1272 - mae: 0.2237\n",
            "Epoch 27: val_loss did not improve from 0.11607\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1251 - mae: 0.2219 - val_loss: 0.1212 - val_mae: 0.2192\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1366 - mae: 0.2500\n",
            "Epoch 28: val_loss did not improve from 0.11607\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1402 - mae: 0.2537 - val_loss: 0.1770 - val_mae: 0.3151\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1442 - mae: 0.2653\n",
            "Epoch 29: val_loss did not improve from 0.11607\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1442 - mae: 0.2653 - val_loss: 0.1505 - val_mae: 0.2844\n",
            "Epoch 29: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.1348511129617691, RMSE:0.36722078919410706, MAE:0.2178720086812973, R2:0.8426558181999703\n",
            "0.8426558181999703\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_125\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_51 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_85 (Embedding)       (None, 17, 900)      649800      ['input_51[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_75 (Conv1D)             (None, 17, 100)      270100      ['embedding_85[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_76 (Conv1D)             (None, 17, 100)      270100      ['embedding_85[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_77 (Conv1D)             (None, 17, 100)      270100      ['embedding_85[0][0]']           \n",
            "                                                                                                  \n",
            " input_52 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_255 (Glob  (None, 100)         0           ['conv1d_75[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_256 (Glob  (None, 100)         0           ['conv1d_76[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_257 (Glob  (None, 100)         0           ['conv1d_77[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 3)            33          ['input_52[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_255[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_256[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_257[0][0]'\n",
            "                                                                 , 'dense_100[0][0]']             \n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 500)          152000      ['concatenate_25[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_110 (Dropout)          (None, 500)          0           ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 100)          50100       ['dropout_110[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 100)          0           ['dense_102[0][0]']              \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 1)            101         ['dropout_111[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.2411 - mae: 1.0309\n",
            "Epoch 1: val_loss improved from inf to 0.73687, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 40ms/step - loss: 3.1598 - mae: 1.0155 - val_loss: 0.7369 - val_mae: 0.5759\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6661 - mae: 0.5365\n",
            "Epoch 2: val_loss improved from 0.73687 to 0.58145, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6615 - mae: 0.5335 - val_loss: 0.5815 - val_mae: 0.5060\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5497 - mae: 0.4907\n",
            "Epoch 3: val_loss improved from 0.58145 to 0.49095, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.5447 - mae: 0.4895 - val_loss: 0.4910 - val_mae: 0.4621\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4698 - mae: 0.4583\n",
            "Epoch 4: val_loss improved from 0.49095 to 0.42653, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.4622 - mae: 0.4552 - val_loss: 0.4265 - val_mae: 0.4475\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4124 - mae: 0.4401\n",
            "Epoch 5: val_loss improved from 0.42653 to 0.36411, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4098 - mae: 0.4381 - val_loss: 0.3641 - val_mae: 0.3988\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3502 - mae: 0.4013\n",
            "Epoch 6: val_loss improved from 0.36411 to 0.31665, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3502 - mae: 0.4013 - val_loss: 0.3166 - val_mae: 0.3694\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3229 - mae: 0.3904\n",
            "Epoch 7: val_loss did not improve from 0.31665\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.3213 - mae: 0.3905 - val_loss: 0.3315 - val_mae: 0.3957\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3000 - mae: 0.3858\n",
            "Epoch 8: val_loss did not improve from 0.31665\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3047 - mae: 0.3919 - val_loss: 0.3402 - val_mae: 0.4748\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2448 - mae: 0.3318\n",
            "Epoch 9: val_loss improved from 0.31665 to 0.22672, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2436 - mae: 0.3311 - val_loss: 0.2267 - val_mae: 0.3325\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2784 - mae: 0.3935\n",
            "Epoch 10: val_loss did not improve from 0.22672\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.2798 - mae: 0.3945 - val_loss: 0.2482 - val_mae: 0.3594\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2850 - mae: 0.4123\n",
            "Epoch 11: val_loss improved from 0.22672 to 0.22200, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2783 - mae: 0.4065 - val_loss: 0.2220 - val_mae: 0.3578\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2085 - mae: 0.3249\n",
            "Epoch 12: val_loss improved from 0.22200 to 0.18862, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2066 - mae: 0.3237 - val_loss: 0.1886 - val_mae: 0.3136\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2779 - mae: 0.4080\n",
            "Epoch 13: val_loss improved from 0.18862 to 0.18825, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2762 - mae: 0.4064 - val_loss: 0.1882 - val_mae: 0.3085\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1968 - mae: 0.3221\n",
            "Epoch 14: val_loss did not improve from 0.18825\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1994 - mae: 0.3235 - val_loss: 0.2507 - val_mae: 0.4150\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2634 - mae: 0.4089\n",
            "Epoch 15: val_loss improved from 0.18825 to 0.17995, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2634 - mae: 0.4089 - val_loss: 0.1799 - val_mae: 0.3051\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1922 - mae: 0.3248\n",
            "Epoch 16: val_loss did not improve from 0.17995\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1922 - mae: 0.3248 - val_loss: 0.1962 - val_mae: 0.3469\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2499 - mae: 0.3970\n",
            "Epoch 17: val_loss improved from 0.17995 to 0.16761, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2499 - mae: 0.3970 - val_loss: 0.1676 - val_mae: 0.2904\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.3015\n",
            "Epoch 18: val_loss did not improve from 0.16761\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1760 - mae: 0.3029 - val_loss: 0.1889 - val_mae: 0.3396\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2338 - mae: 0.3837\n",
            "Epoch 19: val_loss did not improve from 0.16761\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2334 - mae: 0.3830 - val_loss: 0.2145 - val_mae: 0.3601\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1779 - mae: 0.3092\n",
            "Epoch 20: val_loss improved from 0.16761 to 0.12113, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1762 - mae: 0.3038 - val_loss: 0.1211 - val_mae: 0.2093\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1381 - mae: 0.2387\n",
            "Epoch 21: val_loss did not improve from 0.12113\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1381 - mae: 0.2387 - val_loss: 0.1880 - val_mae: 0.3409\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1929 - mae: 0.3356\n",
            "Epoch 22: val_loss did not improve from 0.12113\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1884 - mae: 0.3297 - val_loss: 0.1300 - val_mae: 0.2314\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1582 - mae: 0.2776\n",
            "Epoch 23: val_loss did not improve from 0.12113\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1657 - mae: 0.2876 - val_loss: 0.2518 - val_mae: 0.4269\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1681 - mae: 0.2938\n",
            "Epoch 24: val_loss improved from 0.12113 to 0.11769, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1644 - mae: 0.2873 - val_loss: 0.1177 - val_mae: 0.2067\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1641 - mae: 0.2861\n",
            "Epoch 25: val_loss did not improve from 0.11769\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1649 - mae: 0.2904 - val_loss: 0.1821 - val_mae: 0.3369\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1489 - mae: 0.2666\n",
            "Epoch 26: val_loss did not improve from 0.11769\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1466 - mae: 0.2623 - val_loss: 0.1193 - val_mae: 0.2116\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1439 - mae: 0.2564\n",
            "Epoch 27: val_loss did not improve from 0.11769\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1445 - mae: 0.2597 - val_loss: 0.1458 - val_mae: 0.2773\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1469 - mae: 0.2642\n",
            "Epoch 28: val_loss improved from 0.11769 to 0.11040, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1469 - mae: 0.2642 - val_loss: 0.1104 - val_mae: 0.1908\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1371 - mae: 0.2497\n",
            "Epoch 29: val_loss did not improve from 0.11040\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1391 - mae: 0.2518 - val_loss: 0.1522 - val_mae: 0.2777\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1409 - mae: 0.2551\n",
            "Epoch 30: val_loss did not improve from 0.11040\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1399 - mae: 0.2530 - val_loss: 0.1191 - val_mae: 0.2184\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1166 - mae: 0.2039\n",
            "Epoch 31: val_loss did not improve from 0.11040\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1154 - mae: 0.2032 - val_loss: 0.1142 - val_mae: 0.2030\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1207 - mae: 0.2200\n",
            "Epoch 32: val_loss did not improve from 0.11040\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1213 - mae: 0.2197 - val_loss: 0.1127 - val_mae: 0.2023\n",
            "Epoch 33/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1128 - mae: 0.1966\n",
            "Epoch 33: val_loss did not improve from 0.11040\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1133 - mae: 0.1993 - val_loss: 0.1359 - val_mae: 0.2514\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1427 - mae: 0.2669\n",
            "Epoch 34: val_loss did not improve from 0.11040\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1420 - mae: 0.2647 - val_loss: 0.1110 - val_mae: 0.1978\n",
            "Epoch 34: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13882583379745483, RMSE:0.37259340286254883, MAE:0.21711894869804382, R2:0.8380181065462868\n",
            "0.8380181065462868\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_126\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_53 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_86 (Embedding)       (None, 17, 900)      649800      ['input_53[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_78 (Conv1D)             (None, 17, 100)      270100      ['embedding_86[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_79 (Conv1D)             (None, 17, 100)      270100      ['embedding_86[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_80 (Conv1D)             (None, 17, 100)      270100      ['embedding_86[0][0]']           \n",
            "                                                                                                  \n",
            " input_54 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_258 (Glob  (None, 100)         0           ['conv1d_78[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_259 (Glob  (None, 100)         0           ['conv1d_79[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_260 (Glob  (None, 100)         0           ['conv1d_80[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 3)            33          ['input_54[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_258[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_259[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_260[0][0]'\n",
            "                                                                 , 'dense_104[0][0]']             \n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 500)          152000      ['concatenate_26[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_112 (Dropout)          (None, 500)          0           ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " dense_106 (Dense)              (None, 100)          50100       ['dropout_112[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 100)          0           ['dense_106[0][0]']              \n",
            "                                                                                                  \n",
            " dense_107 (Dense)              (None, 1)            101         ['dropout_113[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.5682 - mae: 1.0986\n",
            "Epoch 1: val_loss improved from inf to 0.76374, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 39ms/step - loss: 3.4770 - mae: 1.0837 - val_loss: 0.7637 - val_mae: 0.5668\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7185 - mae: 0.5566\n",
            "Epoch 2: val_loss improved from 0.76374 to 0.62653, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7144 - mae: 0.5562 - val_loss: 0.6265 - val_mae: 0.5298\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5960 - mae: 0.5074\n",
            "Epoch 3: val_loss improved from 0.62653 to 0.53282, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.5954 - mae: 0.5069 - val_loss: 0.5328 - val_mae: 0.4926\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5104 - mae: 0.4750\n",
            "Epoch 4: val_loss improved from 0.53282 to 0.46691, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.5095 - mae: 0.4758 - val_loss: 0.4669 - val_mae: 0.4426\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4538 - mae: 0.4559\n",
            "Epoch 5: val_loss improved from 0.46691 to 0.40984, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4491 - mae: 0.4537 - val_loss: 0.4098 - val_mae: 0.4355\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3963 - mae: 0.4295\n",
            "Epoch 6: val_loss improved from 0.40984 to 0.36662, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3933 - mae: 0.4272 - val_loss: 0.3666 - val_mae: 0.4028\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3460 - mae: 0.3978\n",
            "Epoch 7: val_loss improved from 0.36662 to 0.31952, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3452 - mae: 0.3970 - val_loss: 0.3195 - val_mae: 0.3778\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3060 - mae: 0.3724\n",
            "Epoch 8: val_loss improved from 0.31952 to 0.28643, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3060 - mae: 0.3724 - val_loss: 0.2864 - val_mae: 0.3629\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2692 - mae: 0.3467\n",
            "Epoch 9: val_loss improved from 0.28643 to 0.25055, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2685 - mae: 0.3466 - val_loss: 0.2505 - val_mae: 0.3303\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2385 - mae: 0.3242\n",
            "Epoch 10: val_loss did not improve from 0.25055\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2397 - mae: 0.3245 - val_loss: 0.2569 - val_mae: 0.3725\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2278 - mae: 0.3236\n",
            "Epoch 11: val_loss improved from 0.25055 to 0.21669, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.2281 - mae: 0.3249 - val_loss: 0.2167 - val_mae: 0.3279\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2012 - mae: 0.2965\n",
            "Epoch 12: val_loss improved from 0.21669 to 0.17445, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1993 - mae: 0.2948 - val_loss: 0.1744 - val_mae: 0.2664\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1879 - mae: 0.2880\n",
            "Epoch 13: val_loss improved from 0.17445 to 0.16703, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1876 - mae: 0.2884 - val_loss: 0.1670 - val_mae: 0.2687\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1635 - mae: 0.2584\n",
            "Epoch 14: val_loss did not improve from 0.16703\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1639 - mae: 0.2592 - val_loss: 0.1701 - val_mae: 0.2815\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1947 - mae: 0.3153\n",
            "Epoch 15: val_loss improved from 0.16703 to 0.14541, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1947 - mae: 0.3153 - val_loss: 0.1454 - val_mae: 0.2443\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1534 - mae: 0.2555\n",
            "Epoch 16: val_loss improved from 0.14541 to 0.13521, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1530 - mae: 0.2536 - val_loss: 0.1352 - val_mae: 0.2290\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2059 - mae: 0.3341\n",
            "Epoch 17: val_loss did not improve from 0.13521\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2046 - mae: 0.3316 - val_loss: 0.1494 - val_mae: 0.2610\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1722 - mae: 0.2914\n",
            "Epoch 18: val_loss improved from 0.13521 to 0.12417, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1695 - mae: 0.2884 - val_loss: 0.1242 - val_mae: 0.2110\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1469 - mae: 0.2486\n",
            "Epoch 19: val_loss did not improve from 0.12417\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1469 - mae: 0.2503 - val_loss: 0.1562 - val_mae: 0.2847\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1379 - mae: 0.2347\n",
            "Epoch 20: val_loss did not improve from 0.12417\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1363 - mae: 0.2331 - val_loss: 0.1292 - val_mae: 0.2286\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1563 - mae: 0.2688\n",
            "Epoch 21: val_loss did not improve from 0.12417\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1575 - mae: 0.2713 - val_loss: 0.1595 - val_mae: 0.2939\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1432 - mae: 0.2455\n",
            "Epoch 22: val_loss improved from 0.12417 to 0.11982, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1414 - mae: 0.2439 - val_loss: 0.1198 - val_mae: 0.2101\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1995 - mae: 0.3356\n",
            "Epoch 23: val_loss did not improve from 0.11982\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1963 - mae: 0.3327 - val_loss: 0.1618 - val_mae: 0.2996\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1507 - mae: 0.2659\n",
            "Epoch 24: val_loss did not improve from 0.11982\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1507 - mae: 0.2659 - val_loss: 0.1317 - val_mae: 0.2380\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1302 - mae: 0.2318\n",
            "Epoch 25: val_loss improved from 0.11982 to 0.11260, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1307 - mae: 0.2308 - val_loss: 0.1126 - val_mae: 0.1943\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1646 - mae: 0.2904\n",
            "Epoch 26: val_loss did not improve from 0.11260\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1646 - mae: 0.2904 - val_loss: 0.1543 - val_mae: 0.2797\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1276 - mae: 0.2247\n",
            "Epoch 27: val_loss did not improve from 0.11260\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1276 - mae: 0.2247 - val_loss: 0.1135 - val_mae: 0.1973\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1357 - mae: 0.2424\n",
            "Epoch 28: val_loss did not improve from 0.11260\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1357 - mae: 0.2424 - val_loss: 0.1249 - val_mae: 0.2257\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1513 - mae: 0.2739\n",
            "Epoch 29: val_loss did not improve from 0.11260\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.1513 - mae: 0.2739 - val_loss: 0.1210 - val_mae: 0.2198\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1228 - mae: 0.2170\n",
            "Epoch 30: val_loss did not improve from 0.11260\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1228 - mae: 0.2170 - val_loss: 0.1150 - val_mae: 0.2032\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1401 - mae: 0.2547\n",
            "Epoch 31: val_loss did not improve from 0.11260\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1395 - mae: 0.2541 - val_loss: 0.1255 - val_mae: 0.2332\n",
            "Epoch 31: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13322259485721588, RMSE:0.36499670147895813, MAE:0.21315407752990723, R2:0.8445559758494802\n",
            "0.8445559758494802\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_127\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_55 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_87 (Embedding)       (None, 17, 900)      649800      ['input_55[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_81 (Conv1D)             (None, 17, 100)      270100      ['embedding_87[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_82 (Conv1D)             (None, 17, 100)      270100      ['embedding_87[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_83 (Conv1D)             (None, 17, 100)      270100      ['embedding_87[0][0]']           \n",
            "                                                                                                  \n",
            " input_56 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_261 (Glob  (None, 100)         0           ['conv1d_81[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_262 (Glob  (None, 100)         0           ['conv1d_82[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_263 (Glob  (None, 100)         0           ['conv1d_83[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_108 (Dense)              (None, 3)            33          ['input_56[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_261[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_262[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_263[0][0]'\n",
            "                                                                 , 'dense_108[0][0]']             \n",
            "                                                                                                  \n",
            " dense_109 (Dense)              (None, 500)          152000      ['concatenate_27[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_114 (Dropout)          (None, 500)          0           ['dense_109[0][0]']              \n",
            "                                                                                                  \n",
            " dense_110 (Dense)              (None, 100)          50100       ['dropout_114[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 100)          0           ['dense_110[0][0]']              \n",
            "                                                                                                  \n",
            " dense_111 (Dense)              (None, 1)            101         ['dropout_115[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 2.6703 - mae: 0.9537\n",
            "Epoch 1: val_loss improved from inf to 0.73675, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 41ms/step - loss: 2.6051 - mae: 0.9433 - val_loss: 0.7368 - val_mae: 0.4645\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6527 - mae: 0.5292\n",
            "Epoch 2: val_loss improved from 0.73675 to 0.55566, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6404 - mae: 0.5232 - val_loss: 0.5557 - val_mae: 0.4653\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5150 - mae: 0.4742\n",
            "Epoch 3: val_loss improved from 0.55566 to 0.46482, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.5125 - mae: 0.4729 - val_loss: 0.4648 - val_mae: 0.4556\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4358 - mae: 0.4422\n",
            "Epoch 4: val_loss improved from 0.46482 to 0.40896, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4358 - mae: 0.4422 - val_loss: 0.4090 - val_mae: 0.4465\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3831 - mae: 0.4185\n",
            "Epoch 5: val_loss improved from 0.40896 to 0.35655, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3804 - mae: 0.4173 - val_loss: 0.3566 - val_mae: 0.3998\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3373 - mae: 0.3924\n",
            "Epoch 6: val_loss improved from 0.35655 to 0.32487, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3373 - mae: 0.3924 - val_loss: 0.3249 - val_mae: 0.3773\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2958 - mae: 0.3610\n",
            "Epoch 7: val_loss improved from 0.32487 to 0.27654, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2953 - mae: 0.3610 - val_loss: 0.2765 - val_mae: 0.3536\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2644 - mae: 0.3396\n",
            "Epoch 8: val_loss improved from 0.27654 to 0.26024, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2637 - mae: 0.3388 - val_loss: 0.2602 - val_mae: 0.3376\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2360 - mae: 0.3173\n",
            "Epoch 9: val_loss improved from 0.26024 to 0.22285, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2361 - mae: 0.3174 - val_loss: 0.2229 - val_mae: 0.3176\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2121 - mae: 0.3007\n",
            "Epoch 10: val_loss improved from 0.22285 to 0.22024, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2145 - mae: 0.3013 - val_loss: 0.2202 - val_mae: 0.3199\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2013 - mae: 0.2974\n",
            "Epoch 11: val_loss improved from 0.22024 to 0.19328, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2026 - mae: 0.2983 - val_loss: 0.1933 - val_mae: 0.3040\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1920 - mae: 0.2936\n",
            "Epoch 12: val_loss improved from 0.19328 to 0.18243, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1929 - mae: 0.2945 - val_loss: 0.1824 - val_mae: 0.2969\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1945 - mae: 0.3062\n",
            "Epoch 13: val_loss did not improve from 0.18243\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1963 - mae: 0.3086 - val_loss: 0.2752 - val_mae: 0.4131\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1968 - mae: 0.3109\n",
            "Epoch 14: val_loss improved from 0.18243 to 0.17708, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1971 - mae: 0.3108 - val_loss: 0.1771 - val_mae: 0.3012\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2674 - mae: 0.4078\n",
            "Epoch 15: val_loss did not improve from 0.17708\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2647 - mae: 0.4050 - val_loss: 0.1948 - val_mae: 0.3260\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1794 - mae: 0.2981\n",
            "Epoch 16: val_loss improved from 0.17708 to 0.14941, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1797 - mae: 0.2978 - val_loss: 0.1494 - val_mae: 0.2595\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2042 - mae: 0.3339\n",
            "Epoch 17: val_loss did not improve from 0.14941\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.2041 - mae: 0.3332 - val_loss: 0.1745 - val_mae: 0.3011\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1917 - mae: 0.3234\n",
            "Epoch 18: val_loss did not improve from 0.14941\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1913 - mae: 0.3226 - val_loss: 0.1614 - val_mae: 0.2889\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1594 - mae: 0.2734\n",
            "Epoch 19: val_loss improved from 0.14941 to 0.14779, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1578 - mae: 0.2715 - val_loss: 0.1478 - val_mae: 0.2615\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1665 - mae: 0.2889\n",
            "Epoch 20: val_loss improved from 0.14779 to 0.13194, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1660 - mae: 0.2870 - val_loss: 0.1319 - val_mae: 0.2341\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1394 - mae: 0.2381\n",
            "Epoch 21: val_loss did not improve from 0.13194\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1395 - mae: 0.2411 - val_loss: 0.1452 - val_mae: 0.2624\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1576 - mae: 0.2787\n",
            "Epoch 22: val_loss did not improve from 0.13194\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1603 - mae: 0.2813 - val_loss: 0.2155 - val_mae: 0.3772\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1808 - mae: 0.3017\n",
            "Epoch 23: val_loss improved from 0.13194 to 0.12694, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1808 - mae: 0.3017 - val_loss: 0.1269 - val_mae: 0.2263\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1266 - mae: 0.2185\n",
            "Epoch 24: val_loss improved from 0.12694 to 0.12644, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.1266 - mae: 0.2185 - val_loss: 0.1264 - val_mae: 0.2268\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1248 - mae: 0.2163\n",
            "Epoch 25: val_loss did not improve from 0.12644\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1248 - mae: 0.2163 - val_loss: 0.1320 - val_mae: 0.2397\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1471 - mae: 0.2646\n",
            "Epoch 26: val_loss did not improve from 0.12644\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1458 - mae: 0.2629 - val_loss: 0.1269 - val_mae: 0.2323\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1325 - mae: 0.2387\n",
            "Epoch 27: val_loss improved from 0.12644 to 0.11874, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1327 - mae: 0.2378 - val_loss: 0.1187 - val_mae: 0.2117\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1249 - mae: 0.2187\n",
            "Epoch 28: val_loss did not improve from 0.11874\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1259 - mae: 0.2213 - val_loss: 0.1861 - val_mae: 0.3338\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1623 - mae: 0.2936\n",
            "Epoch 29: val_loss did not improve from 0.11874\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1600 - mae: 0.2904 - val_loss: 0.1341 - val_mae: 0.2500\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1506 - mae: 0.2746\n",
            "Epoch 30: val_loss did not improve from 0.11874\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1528 - mae: 0.2769 - val_loss: 0.1836 - val_mae: 0.3295\n",
            "Epoch 31/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1318 - mae: 0.2353\n",
            "Epoch 31: val_loss did not improve from 0.11874\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1310 - mae: 0.2346 - val_loss: 0.1196 - val_mae: 0.2186\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1588 - mae: 0.2887\n",
            "Epoch 32: val_loss did not improve from 0.11874\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1576 - mae: 0.2875 - val_loss: 0.1657 - val_mae: 0.3032\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1289 - mae: 0.2356\n",
            "Epoch 33: val_loss improved from 0.11874 to 0.10909, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1289 - mae: 0.2356 - val_loss: 0.1091 - val_mae: 0.1913\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1168 - mae: 0.2087\n",
            "Epoch 34: val_loss did not improve from 0.10909\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1181 - mae: 0.2101 - val_loss: 0.1408 - val_mae: 0.2608\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1403 - mae: 0.2595\n",
            "Epoch 35: val_loss did not improve from 0.10909\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1403 - mae: 0.2595 - val_loss: 0.1726 - val_mae: 0.3238\n",
            "Epoch 36/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1326 - mae: 0.2438\n",
            "Epoch 36: val_loss did not improve from 0.10909\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1320 - mae: 0.2418 - val_loss: 0.1136 - val_mae: 0.2037\n",
            "Epoch 37/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1197 - mae: 0.2179\n",
            "Epoch 37: val_loss did not improve from 0.10909\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1198 - mae: 0.2174 - val_loss: 0.1114 - val_mae: 0.2012\n",
            "Epoch 38/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1113 - mae: 0.1986\n",
            "Epoch 38: val_loss did not improve from 0.10909\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1157 - mae: 0.2070 - val_loss: 0.1567 - val_mae: 0.2908\n",
            "Epoch 39/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1335 - mae: 0.2506\n",
            "Epoch 39: val_loss did not improve from 0.10909\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1330 - mae: 0.2480 - val_loss: 0.1232 - val_mae: 0.2325\n",
            "Epoch 39: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13267888128757477, RMSE:0.36425113677978516, MAE:0.21171912550926208, R2:0.8451903626131092\n",
            "0.8451903626131092\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_128\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_88 (Embedding)       (None, 17, 900)      649800      ['input_57[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_84 (Conv1D)             (None, 17, 100)      270100      ['embedding_88[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_85 (Conv1D)             (None, 17, 100)      270100      ['embedding_88[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_86 (Conv1D)             (None, 17, 100)      270100      ['embedding_88[0][0]']           \n",
            "                                                                                                  \n",
            " input_58 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_264 (Glob  (None, 100)         0           ['conv1d_84[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_265 (Glob  (None, 100)         0           ['conv1d_85[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_266 (Glob  (None, 100)         0           ['conv1d_86[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_112 (Dense)              (None, 3)            33          ['input_58[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_264[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_265[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_266[0][0]'\n",
            "                                                                 , 'dense_112[0][0]']             \n",
            "                                                                                                  \n",
            " dense_113 (Dense)              (None, 500)          152000      ['concatenate_28[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 500)          0           ['dense_113[0][0]']              \n",
            "                                                                                                  \n",
            " dense_114 (Dense)              (None, 100)          50100       ['dropout_116[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_117 (Dropout)          (None, 100)          0           ['dense_114[0][0]']              \n",
            "                                                                                                  \n",
            " dense_115 (Dense)              (None, 1)            101         ['dropout_117[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 3.1833 - mae: 1.0297\n",
            "Epoch 1: val_loss improved from inf to 0.76023, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 39ms/step - loss: 3.0106 - mae: 0.9963 - val_loss: 0.7602 - val_mae: 0.6667\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6721 - mae: 0.5394\n",
            "Epoch 2: val_loss improved from 0.76023 to 0.59090, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6759 - mae: 0.5422 - val_loss: 0.5909 - val_mae: 0.4866\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5665 - mae: 0.5026\n",
            "Epoch 3: val_loss improved from 0.59090 to 0.50341, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.5632 - mae: 0.5019 - val_loss: 0.5034 - val_mae: 0.4681\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4736 - mae: 0.4635\n",
            "Epoch 4: val_loss improved from 0.50341 to 0.44647, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4749 - mae: 0.4652 - val_loss: 0.4465 - val_mae: 0.4391\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4159 - mae: 0.4379\n",
            "Epoch 5: val_loss improved from 0.44647 to 0.38846, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.4159 - mae: 0.4379 - val_loss: 0.3885 - val_mae: 0.4134\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3658 - mae: 0.4065\n",
            "Epoch 6: val_loss improved from 0.38846 to 0.36084, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.3674 - mae: 0.4089 - val_loss: 0.3608 - val_mae: 0.4247\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3336 - mae: 0.3889\n",
            "Epoch 7: val_loss improved from 0.36084 to 0.32246, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3291 - mae: 0.3866 - val_loss: 0.3225 - val_mae: 0.3724\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2897 - mae: 0.3570\n",
            "Epoch 8: val_loss improved from 0.32246 to 0.27409, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2886 - mae: 0.3559 - val_loss: 0.2741 - val_mae: 0.3401\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2600 - mae: 0.3365\n",
            "Epoch 9: val_loss improved from 0.27409 to 0.24144, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2600 - mae: 0.3365 - val_loss: 0.2414 - val_mae: 0.3155\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2322 - mae: 0.3105\n",
            "Epoch 10: val_loss improved from 0.24144 to 0.21448, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2288 - mae: 0.3085 - val_loss: 0.2145 - val_mae: 0.2962\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2074 - mae: 0.2932\n",
            "Epoch 11: val_loss improved from 0.21448 to 0.20030, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2070 - mae: 0.2933 - val_loss: 0.2003 - val_mae: 0.2880\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2114 - mae: 0.3133\n",
            "Epoch 12: val_loss improved from 0.20030 to 0.18229, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2114 - mae: 0.3133 - val_loss: 0.1823 - val_mae: 0.2764\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1733 - mae: 0.2624\n",
            "Epoch 13: val_loss improved from 0.18229 to 0.17055, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1732 - mae: 0.2622 - val_loss: 0.1705 - val_mae: 0.2655\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1726 - mae: 0.2717\n",
            "Epoch 14: val_loss did not improve from 0.17055\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1726 - mae: 0.2717 - val_loss: 0.1901 - val_mae: 0.3155\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2012 - mae: 0.3241\n",
            "Epoch 15: val_loss improved from 0.17055 to 0.15286, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1975 - mae: 0.3189 - val_loss: 0.1529 - val_mae: 0.2476\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1647 - mae: 0.2669\n",
            "Epoch 16: val_loss did not improve from 0.15286\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1643 - mae: 0.2704 - val_loss: 0.1924 - val_mae: 0.3185\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2204 - mae: 0.3531\n",
            "Epoch 17: val_loss improved from 0.15286 to 0.14466, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2204 - mae: 0.3531 - val_loss: 0.1447 - val_mae: 0.2454\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1656 - mae: 0.2829\n",
            "Epoch 18: val_loss did not improve from 0.14466\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1664 - mae: 0.2836 - val_loss: 0.1744 - val_mae: 0.2981\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2127 - mae: 0.3553\n",
            "Epoch 19: val_loss did not improve from 0.14466\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2125 - mae: 0.3539 - val_loss: 0.1820 - val_mae: 0.3227\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1514 - mae: 0.2596\n",
            "Epoch 20: val_loss did not improve from 0.14466\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1515 - mae: 0.2613 - val_loss: 0.1965 - val_mae: 0.3369\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1551 - mae: 0.2682\n",
            "Epoch 21: val_loss did not improve from 0.14466\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1551 - mae: 0.2682 - val_loss: 0.1532 - val_mae: 0.2757\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1819 - mae: 0.3160\n",
            "Epoch 22: val_loss did not improve from 0.14466\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1784 - mae: 0.3118 - val_loss: 0.1468 - val_mae: 0.2607\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1384 - mae: 0.2452\n",
            "Epoch 23: val_loss did not improve from 0.14466\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1421 - mae: 0.2489 - val_loss: 0.1502 - val_mae: 0.2742\n",
            "Epoch 23: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.17014653980731964, RMSE:0.4124882221221924, MAE:0.2641732692718506, R2:0.8014731286273536\n",
            "0.8014731286273536\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_129\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_89 (Embedding)       (None, 17, 900)      649800      ['input_59[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_87 (Conv1D)             (None, 17, 100)      270100      ['embedding_89[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_88 (Conv1D)             (None, 17, 100)      270100      ['embedding_89[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_89 (Conv1D)             (None, 17, 100)      270100      ['embedding_89[0][0]']           \n",
            "                                                                                                  \n",
            " input_60 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_267 (Glob  (None, 100)         0           ['conv1d_87[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_268 (Glob  (None, 100)         0           ['conv1d_88[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_269 (Glob  (None, 100)         0           ['conv1d_89[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_116 (Dense)              (None, 3)            33          ['input_60[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_267[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_268[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_269[0][0]'\n",
            "                                                                 , 'dense_116[0][0]']             \n",
            "                                                                                                  \n",
            " dense_117 (Dense)              (None, 500)          152000      ['concatenate_29[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_118 (Dropout)          (None, 500)          0           ['dense_117[0][0]']              \n",
            "                                                                                                  \n",
            " dense_118 (Dense)              (None, 100)          50100       ['dropout_118[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_119 (Dropout)          (None, 100)          0           ['dense_118[0][0]']              \n",
            "                                                                                                  \n",
            " dense_119 (Dense)              (None, 1)            101         ['dropout_119[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.4538 - mae: 1.0642\n",
            "Epoch 1: val_loss improved from inf to 0.77828, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 42ms/step - loss: 3.3622 - mae: 1.0484 - val_loss: 0.7783 - val_mae: 0.4918\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6815 - mae: 0.5353\n",
            "Epoch 2: val_loss improved from 0.77828 to 0.61158, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6811 - mae: 0.5380 - val_loss: 0.6116 - val_mae: 0.5706\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5553 - mae: 0.4973\n",
            "Epoch 3: val_loss improved from 0.61158 to 0.50626, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5508 - mae: 0.4948 - val_loss: 0.5063 - val_mae: 0.4655\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4724 - mae: 0.4604\n",
            "Epoch 4: val_loss improved from 0.50626 to 0.44597, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4724 - mae: 0.4604 - val_loss: 0.4460 - val_mae: 0.4538\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4189 - mae: 0.4353\n",
            "Epoch 5: val_loss improved from 0.44597 to 0.41144, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4186 - mae: 0.4360 - val_loss: 0.4114 - val_mae: 0.4240\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3781 - mae: 0.4148\n",
            "Epoch 6: val_loss improved from 0.41144 to 0.36123, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3755 - mae: 0.4123 - val_loss: 0.3612 - val_mae: 0.4090\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3385 - mae: 0.3892\n",
            "Epoch 7: val_loss improved from 0.36123 to 0.32200, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3373 - mae: 0.3883 - val_loss: 0.3220 - val_mae: 0.3761\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3093 - mae: 0.3716\n",
            "Epoch 8: val_loss improved from 0.32200 to 0.29966, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.3076 - mae: 0.3705 - val_loss: 0.2997 - val_mae: 0.3802\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2773 - mae: 0.3489\n",
            "Epoch 9: val_loss improved from 0.29966 to 0.25644, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2745 - mae: 0.3472 - val_loss: 0.2564 - val_mae: 0.3345\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2488 - mae: 0.3288\n",
            "Epoch 10: val_loss did not improve from 0.25644\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2479 - mae: 0.3292 - val_loss: 0.2713 - val_mae: 0.3551\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2633 - mae: 0.3646\n",
            "Epoch 11: val_loss improved from 0.25644 to 0.22256, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2633 - mae: 0.3646 - val_loss: 0.2226 - val_mae: 0.3312\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2183 - mae: 0.3169\n",
            "Epoch 12: val_loss did not improve from 0.22256\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2174 - mae: 0.3170 - val_loss: 0.2371 - val_mae: 0.3452\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2397 - mae: 0.3570\n",
            "Epoch 13: val_loss did not improve from 0.22256\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2396 - mae: 0.3569 - val_loss: 0.2469 - val_mae: 0.3919\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2112 - mae: 0.3306\n",
            "Epoch 14: val_loss did not improve from 0.22256\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2112 - mae: 0.3306 - val_loss: 0.2369 - val_mae: 0.3652\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2799 - mae: 0.4209\n",
            "Epoch 15: val_loss improved from 0.22256 to 0.18867, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2772 - mae: 0.4182 - val_loss: 0.1887 - val_mae: 0.3201\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1838 - mae: 0.2998\n",
            "Epoch 16: val_loss improved from 0.18867 to 0.15598, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1836 - mae: 0.2991 - val_loss: 0.1560 - val_mae: 0.2596\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2476 - mae: 0.3746\n",
            "Epoch 17: val_loss did not improve from 0.15598\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2476 - mae: 0.3746 - val_loss: 0.2935 - val_mae: 0.4616\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2127 - mae: 0.3452\n",
            "Epoch 18: val_loss did not improve from 0.15598\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2165 - mae: 0.3488 - val_loss: 0.2648 - val_mae: 0.4139\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1991 - mae: 0.3332\n",
            "Epoch 19: val_loss did not improve from 0.15598\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1985 - mae: 0.3330 - val_loss: 0.1761 - val_mae: 0.3144\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1981 - mae: 0.3326\n",
            "Epoch 20: val_loss did not improve from 0.15598\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1981 - mae: 0.3328 - val_loss: 0.2040 - val_mae: 0.3441\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2385 - mae: 0.3834\n",
            "Epoch 21: val_loss improved from 0.15598 to 0.14877, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2363 - mae: 0.3801 - val_loss: 0.1488 - val_mae: 0.2669\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1789 - mae: 0.3044\n",
            "Epoch 22: val_loss did not improve from 0.14877\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1793 - mae: 0.3073 - val_loss: 0.1884 - val_mae: 0.3252\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1568 - mae: 0.2694\n",
            "Epoch 23: val_loss improved from 0.14877 to 0.13400, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.1568 - mae: 0.2694 - val_loss: 0.1340 - val_mae: 0.2414\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1775 - mae: 0.3071\n",
            "Epoch 24: val_loss did not improve from 0.13400\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1775 - mae: 0.3071 - val_loss: 0.1940 - val_mae: 0.3345\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1800 - mae: 0.3151\n",
            "Epoch 25: val_loss did not improve from 0.13400\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1800 - mae: 0.3151 - val_loss: 0.1725 - val_mae: 0.3155\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1438 - mae: 0.2520\n",
            "Epoch 26: val_loss improved from 0.13400 to 0.11957, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1418 - mae: 0.2499 - val_loss: 0.1196 - val_mae: 0.2078\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1530 - mae: 0.2715\n",
            "Epoch 27: val_loss did not improve from 0.11957\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1530 - mae: 0.2715 - val_loss: 0.1760 - val_mae: 0.3128\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1517 - mae: 0.2718\n",
            "Epoch 28: val_loss improved from 0.11957 to 0.11867, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1517 - mae: 0.2703 - val_loss: 0.1187 - val_mae: 0.2106\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1485 - mae: 0.2647\n",
            "Epoch 29: val_loss did not improve from 0.11867\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1519 - mae: 0.2704 - val_loss: 0.2098 - val_mae: 0.3622\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1783 - mae: 0.3063\n",
            "Epoch 30: val_loss did not improve from 0.11867\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1759 - mae: 0.3037 - val_loss: 0.1301 - val_mae: 0.2395\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1402 - mae: 0.2519\n",
            "Epoch 31: val_loss did not improve from 0.11867\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1405 - mae: 0.2532 - val_loss: 0.1822 - val_mae: 0.3257\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1585 - mae: 0.2842\n",
            "Epoch 32: val_loss did not improve from 0.11867\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1592 - mae: 0.2870 - val_loss: 0.1540 - val_mae: 0.2889\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1657 - mae: 0.2980\n",
            "Epoch 33: val_loss did not improve from 0.11867\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1645 - mae: 0.2957 - val_loss: 0.1300 - val_mae: 0.2313\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1253 - mae: 0.2223\n",
            "Epoch 34: val_loss did not improve from 0.11867\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1249 - mae: 0.2232 - val_loss: 0.1328 - val_mae: 0.2480\n",
            "Epoch 34: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.14425654709339142, RMSE:0.37981119751930237, MAE:0.23043303191661835, R2:0.8316815533481572\n",
            "0.8316815533481572\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_130\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_61 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_90 (Embedding)       (None, 17, 900)      649800      ['input_61[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_90 (Conv1D)             (None, 17, 100)      270100      ['embedding_90[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_91 (Conv1D)             (None, 17, 100)      270100      ['embedding_90[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_92 (Conv1D)             (None, 17, 100)      270100      ['embedding_90[0][0]']           \n",
            "                                                                                                  \n",
            " input_62 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_270 (Glob  (None, 100)         0           ['conv1d_90[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_271 (Glob  (None, 100)         0           ['conv1d_91[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_272 (Glob  (None, 100)         0           ['conv1d_92[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_120 (Dense)              (None, 3)            33          ['input_62[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_270[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_271[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_272[0][0]'\n",
            "                                                                 , 'dense_120[0][0]']             \n",
            "                                                                                                  \n",
            " dense_121 (Dense)              (None, 500)          152000      ['concatenate_30[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_120 (Dropout)          (None, 500)          0           ['dense_121[0][0]']              \n",
            "                                                                                                  \n",
            " dense_122 (Dense)              (None, 100)          50100       ['dropout_120[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_121 (Dropout)          (None, 100)          0           ['dense_122[0][0]']              \n",
            "                                                                                                  \n",
            " dense_123 (Dense)              (None, 1)            101         ['dropout_121[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 3.1338 - mae: 1.0331\n",
            "Epoch 1: val_loss improved from inf to 0.73423, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 40ms/step - loss: 2.9595 - mae: 0.9999 - val_loss: 0.7342 - val_mae: 0.5212\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6855 - mae: 0.5433\n",
            "Epoch 2: val_loss improved from 0.73423 to 0.60309, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6817 - mae: 0.5416 - val_loss: 0.6031 - val_mae: 0.5020\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5742 - mae: 0.4977\n",
            "Epoch 3: val_loss improved from 0.60309 to 0.52018, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5728 - mae: 0.4984 - val_loss: 0.5202 - val_mae: 0.4811\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4912 - mae: 0.4641\n",
            "Epoch 4: val_loss improved from 0.52018 to 0.46012, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.4912 - mae: 0.4641 - val_loss: 0.4601 - val_mae: 0.4526\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4402 - mae: 0.4450\n",
            "Epoch 5: val_loss improved from 0.46012 to 0.41110, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4374 - mae: 0.4438 - val_loss: 0.4111 - val_mae: 0.4186\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3965 - mae: 0.4220\n",
            "Epoch 6: val_loss improved from 0.41110 to 0.37155, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3948 - mae: 0.4213 - val_loss: 0.3716 - val_mae: 0.4037\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3580 - mae: 0.4006\n",
            "Epoch 7: val_loss improved from 0.37155 to 0.33796, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.3580 - mae: 0.4006 - val_loss: 0.3380 - val_mae: 0.3846\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3231 - mae: 0.3764\n",
            "Epoch 8: val_loss improved from 0.33796 to 0.31022, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3230 - mae: 0.3761 - val_loss: 0.3102 - val_mae: 0.3755\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2959 - mae: 0.3608\n",
            "Epoch 9: val_loss improved from 0.31022 to 0.27707, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2947 - mae: 0.3605 - val_loss: 0.2771 - val_mae: 0.3397\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2634 - mae: 0.3394\n",
            "Epoch 10: val_loss improved from 0.27707 to 0.25520, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2652 - mae: 0.3395 - val_loss: 0.2552 - val_mae: 0.3382\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2465 - mae: 0.3281\n",
            "Epoch 11: val_loss improved from 0.25520 to 0.22341, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2447 - mae: 0.3268 - val_loss: 0.2234 - val_mae: 0.3024\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2556 - mae: 0.3533\n",
            "Epoch 12: val_loss did not improve from 0.22341\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2556 - mae: 0.3533 - val_loss: 0.2552 - val_mae: 0.3799\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2076 - mae: 0.3012\n",
            "Epoch 13: val_loss improved from 0.22341 to 0.20617, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2068 - mae: 0.3010 - val_loss: 0.2062 - val_mae: 0.3126\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2424 - mae: 0.3583\n",
            "Epoch 14: val_loss did not improve from 0.20617\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2424 - mae: 0.3583 - val_loss: 0.3935 - val_mae: 0.5136\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2390 - mae: 0.3572\n",
            "Epoch 15: val_loss improved from 0.20617 to 0.16567, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2312 - mae: 0.3484 - val_loss: 0.1657 - val_mae: 0.2546\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1607 - mae: 0.2511\n",
            "Epoch 16: val_loss did not improve from 0.16567\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1607 - mae: 0.2511 - val_loss: 0.1690 - val_mae: 0.2711\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3013 - mae: 0.4293\n",
            "Epoch 17: val_loss did not improve from 0.16567\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.3013 - mae: 0.4293 - val_loss: 0.2386 - val_mae: 0.3918\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1779 - mae: 0.2904\n",
            "Epoch 18: val_loss did not improve from 0.16567\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1764 - mae: 0.2885 - val_loss: 0.1701 - val_mae: 0.2840\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2102 - mae: 0.3373\n",
            "Epoch 19: val_loss did not improve from 0.16567\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2088 - mae: 0.3382 - val_loss: 0.2147 - val_mae: 0.3650\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2212 - mae: 0.3612\n",
            "Epoch 20: val_loss did not improve from 0.16567\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2218 - mae: 0.3624 - val_loss: 0.2762 - val_mae: 0.4249\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2041 - mae: 0.3395\n",
            "Epoch 21: val_loss improved from 0.16567 to 0.15384, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2040 - mae: 0.3372 - val_loss: 0.1538 - val_mae: 0.2667\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1630 - mae: 0.2766\n",
            "Epoch 22: val_loss did not improve from 0.15384\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1643 - mae: 0.2789 - val_loss: 0.1980 - val_mae: 0.3354\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2161 - mae: 0.3606\n",
            "Epoch 23: val_loss did not improve from 0.15384\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2168 - mae: 0.3621 - val_loss: 0.2170 - val_mae: 0.3744\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1570 - mae: 0.2719\n",
            "Epoch 24: val_loss improved from 0.15384 to 0.12660, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1545 - mae: 0.2680 - val_loss: 0.1266 - val_mae: 0.2111\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1405 - mae: 0.2388\n",
            "Epoch 25: val_loss did not improve from 0.12660\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1405 - mae: 0.2388 - val_loss: 0.1807 - val_mae: 0.3242\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1797 - mae: 0.3173\n",
            "Epoch 26: val_loss did not improve from 0.12660\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1797 - mae: 0.3173 - val_loss: 0.1583 - val_mae: 0.2788\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1279 - mae: 0.2210\n",
            "Epoch 27: val_loss improved from 0.12660 to 0.12449, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1279 - mae: 0.2210 - val_loss: 0.1245 - val_mae: 0.2130\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1803 - mae: 0.3153\n",
            "Epoch 28: val_loss did not improve from 0.12449\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1821 - mae: 0.3170 - val_loss: 0.1854 - val_mae: 0.3238\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1633 - mae: 0.2906\n",
            "Epoch 29: val_loss did not improve from 0.12449\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1636 - mae: 0.2920 - val_loss: 0.1808 - val_mae: 0.3280\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1600 - mae: 0.2847\n",
            "Epoch 30: val_loss did not improve from 0.12449\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1581 - mae: 0.2819 - val_loss: 0.1316 - val_mae: 0.2316\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1542 - mae: 0.2761\n",
            "Epoch 31: val_loss did not improve from 0.12449\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1542 - mae: 0.2761 - val_loss: 0.1628 - val_mae: 0.3000\n",
            "Epoch 32/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1229 - mae: 0.2156\n",
            "Epoch 32: val_loss did not improve from 0.12449\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1226 - mae: 0.2159 - val_loss: 0.1392 - val_mae: 0.2528\n",
            "Epoch 33/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2006 - mae: 0.3476\n",
            "Epoch 33: val_loss did not improve from 0.12449\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1961 - mae: 0.3434 - val_loss: 0.1392 - val_mae: 0.2473\n",
            "Epoch 33: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.14412157237529755, RMSE:0.37963345646858215, MAE:0.23183982074260712, R2:0.8318390389158271\n",
            "0.8318390389158271\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_131\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_63 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_91 (Embedding)       (None, 17, 900)      649800      ['input_63[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_93 (Conv1D)             (None, 17, 100)      270100      ['embedding_91[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_94 (Conv1D)             (None, 17, 100)      270100      ['embedding_91[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_95 (Conv1D)             (None, 17, 100)      270100      ['embedding_91[0][0]']           \n",
            "                                                                                                  \n",
            " input_64 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_273 (Glob  (None, 100)         0           ['conv1d_93[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_274 (Glob  (None, 100)         0           ['conv1d_94[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_275 (Glob  (None, 100)         0           ['conv1d_95[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_124 (Dense)              (None, 3)            33          ['input_64[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_273[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_274[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_275[0][0]'\n",
            "                                                                 , 'dense_124[0][0]']             \n",
            "                                                                                                  \n",
            " dense_125 (Dense)              (None, 500)          152000      ['concatenate_31[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_122 (Dropout)          (None, 500)          0           ['dense_125[0][0]']              \n",
            "                                                                                                  \n",
            " dense_126 (Dense)              (None, 100)          50100       ['dropout_122[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_123 (Dropout)          (None, 100)          0           ['dense_126[0][0]']              \n",
            "                                                                                                  \n",
            " dense_127 (Dense)              (None, 1)            101         ['dropout_123[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 2.7765 - mae: 0.9502\n",
            "Epoch 1: val_loss improved from inf to 0.73301, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 40ms/step - loss: 2.7075 - mae: 0.9389 - val_loss: 0.7330 - val_mae: 0.4979\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6661 - mae: 0.5352\n",
            "Epoch 2: val_loss improved from 0.73301 to 0.59312, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6667 - mae: 0.5345 - val_loss: 0.5931 - val_mae: 0.5644\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5463 - mae: 0.4907\n",
            "Epoch 3: val_loss improved from 0.59312 to 0.49118, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5433 - mae: 0.4896 - val_loss: 0.4912 - val_mae: 0.4815\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4598 - mae: 0.4567\n",
            "Epoch 4: val_loss improved from 0.49118 to 0.42863, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.4608 - mae: 0.4574 - val_loss: 0.4286 - val_mae: 0.4559\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4099 - mae: 0.4378\n",
            "Epoch 5: val_loss improved from 0.42863 to 0.37251, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4060 - mae: 0.4356 - val_loss: 0.3725 - val_mae: 0.4098\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3563 - mae: 0.4066\n",
            "Epoch 6: val_loss improved from 0.37251 to 0.33629, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3565 - mae: 0.4063 - val_loss: 0.3363 - val_mae: 0.3861\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3137 - mae: 0.3781\n",
            "Epoch 7: val_loss improved from 0.33629 to 0.28494, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3131 - mae: 0.3779 - val_loss: 0.2849 - val_mae: 0.3539\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2709 - mae: 0.3457\n",
            "Epoch 8: val_loss improved from 0.28494 to 0.25301, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2709 - mae: 0.3457 - val_loss: 0.2530 - val_mae: 0.3394\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2379 - mae: 0.3209\n",
            "Epoch 9: val_loss improved from 0.25301 to 0.21717, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2378 - mae: 0.3209 - val_loss: 0.2172 - val_mae: 0.3022\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2556 - mae: 0.3534\n",
            "Epoch 10: val_loss did not improve from 0.21717\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2613 - mae: 0.3586 - val_loss: 0.3356 - val_mae: 0.4921\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2584 - mae: 0.3770\n",
            "Epoch 11: val_loss did not improve from 0.21717\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2576 - mae: 0.3771 - val_loss: 0.2851 - val_mae: 0.4046\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2571 - mae: 0.3865\n",
            "Epoch 12: val_loss did not improve from 0.21717\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2564 - mae: 0.3872 - val_loss: 0.2621 - val_mae: 0.4221\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2932 - mae: 0.4320\n",
            "Epoch 13: val_loss did not improve from 0.21717\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2877 - mae: 0.4261 - val_loss: 0.2602 - val_mae: 0.3953\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2727 - mae: 0.4139\n",
            "Epoch 14: val_loss improved from 0.21717 to 0.19002, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2727 - mae: 0.4139 - val_loss: 0.1900 - val_mae: 0.3314\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1898 - mae: 0.3107\n",
            "Epoch 15: val_loss improved from 0.19002 to 0.15069, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1869 - mae: 0.3081 - val_loss: 0.1507 - val_mae: 0.2544\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2150 - mae: 0.3479\n",
            "Epoch 16: val_loss did not improve from 0.15069\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2181 - mae: 0.3522 - val_loss: 0.2706 - val_mae: 0.4426\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2622 - mae: 0.4068\n",
            "Epoch 17: val_loss improved from 0.15069 to 0.13674, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2546 - mae: 0.3976 - val_loss: 0.1367 - val_mae: 0.2321\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1474 - mae: 0.2477\n",
            "Epoch 18: val_loss did not improve from 0.13674\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1497 - mae: 0.2524 - val_loss: 0.1915 - val_mae: 0.3441\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1911 - mae: 0.3312\n",
            "Epoch 19: val_loss did not improve from 0.13674\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1923 - mae: 0.3325 - val_loss: 0.2206 - val_mae: 0.3701\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2390 - mae: 0.3914\n",
            "Epoch 20: val_loss did not improve from 0.13674\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2357 - mae: 0.3874 - val_loss: 0.1447 - val_mae: 0.2670\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1561 - mae: 0.2687\n",
            "Epoch 21: val_loss did not improve from 0.13674\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1585 - mae: 0.2722 - val_loss: 0.2241 - val_mae: 0.3783\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1639 - mae: 0.2834\n",
            "Epoch 22: val_loss improved from 0.13674 to 0.12808, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1632 - mae: 0.2820 - val_loss: 0.1281 - val_mae: 0.2350\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1661 - mae: 0.2927\n",
            "Epoch 23: val_loss did not improve from 0.12808\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1661 - mae: 0.2927 - val_loss: 0.1732 - val_mae: 0.3104\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1742 - mae: 0.3087\n",
            "Epoch 24: val_loss did not improve from 0.12808\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1728 - mae: 0.3064 - val_loss: 0.1363 - val_mae: 0.2583\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1523 - mae: 0.2715\n",
            "Epoch 25: val_loss improved from 0.12808 to 0.12783, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1506 - mae: 0.2693 - val_loss: 0.1278 - val_mae: 0.2299\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1243 - mae: 0.2123\n",
            "Epoch 26: val_loss improved from 0.12783 to 0.11598, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1229 - mae: 0.2128 - val_loss: 0.1160 - val_mae: 0.2095\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1198 - mae: 0.2092\n",
            "Epoch 27: val_loss improved from 0.11598 to 0.11106, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1209 - mae: 0.2100 - val_loss: 0.1111 - val_mae: 0.1938\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1233 - mae: 0.2173\n",
            "Epoch 28: val_loss did not improve from 0.11106\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1246 - mae: 0.2192 - val_loss: 0.1273 - val_mae: 0.2411\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1785 - mae: 0.3197\n",
            "Epoch 29: val_loss did not improve from 0.11106\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1747 - mae: 0.3153 - val_loss: 0.1366 - val_mae: 0.2525\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1382 - mae: 0.2513\n",
            "Epoch 30: val_loss did not improve from 0.11106\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1377 - mae: 0.2507 - val_loss: 0.1334 - val_mae: 0.2572\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1577 - mae: 0.2877\n",
            "Epoch 31: val_loss did not improve from 0.11106\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1575 - mae: 0.2880 - val_loss: 0.1613 - val_mae: 0.2975\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1431 - mae: 0.2631\n",
            "Epoch 32: val_loss did not improve from 0.11106\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1454 - mae: 0.2665 - val_loss: 0.1692 - val_mae: 0.3216\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1430 - mae: 0.2626\n",
            "Epoch 33: val_loss improved from 0.11106 to 0.10923, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1423 - mae: 0.2603 - val_loss: 0.1092 - val_mae: 0.1913\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1136 - mae: 0.2004\n",
            "Epoch 34: val_loss did not improve from 0.10923\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1149 - mae: 0.2022 - val_loss: 0.1287 - val_mae: 0.2394\n",
            "Epoch 35/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1386 - mae: 0.2570\n",
            "Epoch 35: val_loss did not improve from 0.10923\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1395 - mae: 0.2579 - val_loss: 0.1590 - val_mae: 0.3074\n",
            "Epoch 36/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1240 - mae: 0.2207\n",
            "Epoch 36: val_loss improved from 0.10923 to 0.10603, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1213 - mae: 0.2177 - val_loss: 0.1060 - val_mae: 0.1855\n",
            "Epoch 37/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1369 - mae: 0.2477\n",
            "Epoch 37: val_loss did not improve from 0.10603\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1377 - mae: 0.2502 - val_loss: 0.1986 - val_mae: 0.3552\n",
            "Epoch 38/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1647 - mae: 0.3012\n",
            "Epoch 38: val_loss did not improve from 0.10603\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1631 - mae: 0.2962 - val_loss: 0.1372 - val_mae: 0.2691\n",
            "Epoch 39/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1302 - mae: 0.2375\n",
            "Epoch 39: val_loss improved from 0.10603 to 0.10550, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1286 - mae: 0.2354 - val_loss: 0.1055 - val_mae: 0.1849\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1360 - mae: 0.2438\n",
            "Epoch 40: val_loss did not improve from 0.10550\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1360 - mae: 0.2438 - val_loss: 0.1817 - val_mae: 0.3293\n",
            "Epoch 41/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1493 - mae: 0.2776\n",
            "Epoch 41: val_loss did not improve from 0.10550\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1482 - mae: 0.2751 - val_loss: 0.1070 - val_mae: 0.1941\n",
            "Epoch 42/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1071 - mae: 0.1866\n",
            "Epoch 42: val_loss improved from 0.10550 to 0.10402, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1059 - mae: 0.1859 - val_loss: 0.1040 - val_mae: 0.1824\n",
            "Epoch 43/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1085 - mae: 0.1915\n",
            "Epoch 43: val_loss did not improve from 0.10402\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1091 - mae: 0.1948 - val_loss: 0.1191 - val_mae: 0.2296\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1081 - mae: 0.1951\n",
            "Epoch 44: val_loss improved from 0.10402 to 0.10384, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.1081 - mae: 0.1951 - val_loss: 0.1038 - val_mae: 0.1820\n",
            "Epoch 45/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1120 - mae: 0.2041\n",
            "Epoch 45: val_loss did not improve from 0.10384\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1120 - mae: 0.2049 - val_loss: 0.1140 - val_mae: 0.2173\n",
            "Epoch 46/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1226 - mae: 0.2298\n",
            "Epoch 46: val_loss did not improve from 0.10384\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1206 - mae: 0.2272 - val_loss: 0.1045 - val_mae: 0.1844\n",
            "Epoch 47/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1024 - mae: 0.1823\n",
            "Epoch 47: val_loss improved from 0.10384 to 0.10262, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1027 - mae: 0.1823 - val_loss: 0.1026 - val_mae: 0.1795\n",
            "Epoch 48/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1111 - mae: 0.2088\n",
            "Epoch 48: val_loss did not improve from 0.10262\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1128 - mae: 0.2101 - val_loss: 0.1298 - val_mae: 0.2567\n",
            "Epoch 49/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1140 - mae: 0.2114\n",
            "Epoch 49: val_loss did not improve from 0.10262\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1132 - mae: 0.2101 - val_loss: 0.1038 - val_mae: 0.1843\n",
            "Epoch 50/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1077 - mae: 0.1977\n",
            "Epoch 50: val_loss improved from 0.10262 to 0.10189, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1063 - mae: 0.1956 - val_loss: 0.1019 - val_mae: 0.1784\n",
            "Epoch 51/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1118 - mae: 0.2081\n",
            "Epoch 51: val_loss did not improve from 0.10189\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1115 - mae: 0.2088 - val_loss: 0.1150 - val_mae: 0.2208\n",
            "Epoch 52/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1148 - mae: 0.2149\n",
            "Epoch 52: val_loss did not improve from 0.10189\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1139 - mae: 0.2140 - val_loss: 0.1175 - val_mae: 0.2188\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1215 - mae: 0.2343\n",
            "Epoch 53: val_loss did not improve from 0.10189\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1215 - mae: 0.2343 - val_loss: 0.1421 - val_mae: 0.2828\n",
            "Epoch 54/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1149 - mae: 0.2160\n",
            "Epoch 54: val_loss improved from 0.10189 to 0.10131, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1145 - mae: 0.2147 - val_loss: 0.1013 - val_mae: 0.1789\n",
            "Epoch 55/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1097 - mae: 0.2044\n",
            "Epoch 55: val_loss did not improve from 0.10131\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1089 - mae: 0.2034 - val_loss: 0.1052 - val_mae: 0.1855\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1040 - mae: 0.1920\n",
            "Epoch 56: val_loss did not improve from 0.10131\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1040 - mae: 0.1920 - val_loss: 0.1021 - val_mae: 0.1826\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1162 - mae: 0.2230\n",
            "Epoch 57: val_loss did not improve from 0.10131\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1162 - mae: 0.2230 - val_loss: 0.1318 - val_mae: 0.2495\n",
            "Epoch 58/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1161 - mae: 0.2262\n",
            "Epoch 58: val_loss did not improve from 0.10131\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1162 - mae: 0.2244 - val_loss: 0.1111 - val_mae: 0.2100\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1493 - mae: 0.2866\n",
            "Epoch 59: val_loss did not improve from 0.10131\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1493 - mae: 0.2866 - val_loss: 0.1723 - val_mae: 0.3190\n",
            "Epoch 60/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1207 - mae: 0.2311\n",
            "Epoch 60: val_loss did not improve from 0.10131\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1207 - mae: 0.2298 - val_loss: 0.1016 - val_mae: 0.1813\n",
            "Epoch 60: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.12772956490516663, RMSE:0.35739272832870483, MAE:0.2032681107521057, R2:0.8509652318431318\n",
            "0.8509652318431318\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_132\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_65 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_92 (Embedding)       (None, 17, 900)      649800      ['input_65[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_96 (Conv1D)             (None, 17, 100)      270100      ['embedding_92[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_97 (Conv1D)             (None, 17, 100)      270100      ['embedding_92[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_98 (Conv1D)             (None, 17, 100)      270100      ['embedding_92[0][0]']           \n",
            "                                                                                                  \n",
            " input_66 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_276 (Glob  (None, 100)         0           ['conv1d_96[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_277 (Glob  (None, 100)         0           ['conv1d_97[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_278 (Glob  (None, 100)         0           ['conv1d_98[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_128 (Dense)              (None, 3)            33          ['input_66[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_276[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_277[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_278[0][0]'\n",
            "                                                                 , 'dense_128[0][0]']             \n",
            "                                                                                                  \n",
            " dense_129 (Dense)              (None, 500)          152000      ['concatenate_32[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_124 (Dropout)          (None, 500)          0           ['dense_129[0][0]']              \n",
            "                                                                                                  \n",
            " dense_130 (Dense)              (None, 100)          50100       ['dropout_124[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_125 (Dropout)          (None, 100)          0           ['dense_130[0][0]']              \n",
            "                                                                                                  \n",
            " dense_131 (Dense)              (None, 1)            101         ['dropout_125[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.9759 - mae: 1.1623\n",
            "Epoch 1: val_loss improved from inf to 0.77205, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 41ms/step - loss: 3.9759 - mae: 1.1623 - val_loss: 0.7720 - val_mae: 0.6118\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6967 - mae: 0.5525\n",
            "Epoch 2: val_loss improved from 0.77205 to 0.60824, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6943 - mae: 0.5502 - val_loss: 0.6082 - val_mae: 0.5328\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5634 - mae: 0.4972\n",
            "Epoch 3: val_loss improved from 0.60824 to 0.50351, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.5623 - mae: 0.4973 - val_loss: 0.5035 - val_mae: 0.4658\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4723 - mae: 0.4609\n",
            "Epoch 4: val_loss improved from 0.50351 to 0.43122, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.4696 - mae: 0.4597 - val_loss: 0.4312 - val_mae: 0.4547\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4010 - mae: 0.4297\n",
            "Epoch 5: val_loss improved from 0.43122 to 0.36543, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3999 - mae: 0.4295 - val_loss: 0.3654 - val_mae: 0.4127\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3425 - mae: 0.3958\n",
            "Epoch 6: val_loss improved from 0.36543 to 0.31546, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3387 - mae: 0.3938 - val_loss: 0.3155 - val_mae: 0.3840\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2888 - mae: 0.3617\n",
            "Epoch 7: val_loss improved from 0.31546 to 0.26955, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2892 - mae: 0.3608 - val_loss: 0.2696 - val_mae: 0.3457\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2500 - mae: 0.3302\n",
            "Epoch 8: val_loss improved from 0.26955 to 0.23446, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2500 - mae: 0.3302 - val_loss: 0.2345 - val_mae: 0.3168\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2190 - mae: 0.3079\n",
            "Epoch 9: val_loss improved from 0.23446 to 0.20726, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2215 - mae: 0.3090 - val_loss: 0.2073 - val_mae: 0.2978\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2007 - mae: 0.2950\n",
            "Epoch 10: val_loss did not improve from 0.20726\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2042 - mae: 0.3002 - val_loss: 0.2127 - val_mae: 0.3353\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1829 - mae: 0.2770\n",
            "Epoch 11: val_loss improved from 0.20726 to 0.16236, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1796 - mae: 0.2745 - val_loss: 0.1624 - val_mae: 0.2543\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1934 - mae: 0.3095\n",
            "Epoch 12: val_loss did not improve from 0.16236\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1934 - mae: 0.3098 - val_loss: 0.2119 - val_mae: 0.3545\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2248 - mae: 0.3605\n",
            "Epoch 13: val_loss did not improve from 0.16236\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.2205 - mae: 0.3550 - val_loss: 0.1748 - val_mae: 0.2925\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1666 - mae: 0.2814\n",
            "Epoch 14: val_loss did not improve from 0.16236\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1684 - mae: 0.2831 - val_loss: 0.1899 - val_mae: 0.3322\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2562 - mae: 0.4038\n",
            "Epoch 15: val_loss did not improve from 0.16236\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2536 - mae: 0.4012 - val_loss: 0.1939 - val_mae: 0.3281\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1759 - mae: 0.3016\n",
            "Epoch 16: val_loss improved from 0.16236 to 0.15706, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1769 - mae: 0.3032 - val_loss: 0.1571 - val_mae: 0.2798\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1333 - mae: 0.2249\n",
            "Epoch 17: val_loss improved from 0.15706 to 0.12691, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1333 - mae: 0.2242 - val_loss: 0.1269 - val_mae: 0.2179\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1836 - mae: 0.3185\n",
            "Epoch 18: val_loss did not improve from 0.12691\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1836 - mae: 0.3185 - val_loss: 0.1517 - val_mae: 0.2676\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1650 - mae: 0.2890\n",
            "Epoch 19: val_loss did not improve from 0.12691\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1680 - mae: 0.2921 - val_loss: 0.2503 - val_mae: 0.4207\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2345 - mae: 0.3882\n",
            "Epoch 20: val_loss did not improve from 0.12691\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2317 - mae: 0.3842 - val_loss: 0.1405 - val_mae: 0.2453\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1602 - mae: 0.2801\n",
            "Epoch 21: val_loss did not improve from 0.12691\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1596 - mae: 0.2812 - val_loss: 0.1504 - val_mae: 0.2797\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1803 - mae: 0.3140\n",
            "Epoch 22: val_loss did not improve from 0.12691\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1755 - mae: 0.3083 - val_loss: 0.1385 - val_mae: 0.2464\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1594 - mae: 0.2846\n",
            "Epoch 23: val_loss did not improve from 0.12691\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1593 - mae: 0.2838 - val_loss: 0.1317 - val_mae: 0.2416\n",
            "Epoch 23: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.14975875616073608, RMSE:0.38698676228523254, MAE:0.23619601130485535, R2:0.8252615713353465\n",
            "0.8252615713353465\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_133\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_67 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_93 (Embedding)       (None, 17, 900)      649800      ['input_67[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_99 (Conv1D)             (None, 17, 100)      270100      ['embedding_93[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_100 (Conv1D)            (None, 17, 100)      270100      ['embedding_93[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_101 (Conv1D)            (None, 17, 100)      270100      ['embedding_93[0][0]']           \n",
            "                                                                                                  \n",
            " input_68 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_279 (Glob  (None, 100)         0           ['conv1d_99[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_280 (Glob  (None, 100)         0           ['conv1d_100[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_281 (Glob  (None, 100)         0           ['conv1d_101[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_132 (Dense)              (None, 3)            33          ['input_68[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_279[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_280[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_281[0][0]'\n",
            "                                                                 , 'dense_132[0][0]']             \n",
            "                                                                                                  \n",
            " dense_133 (Dense)              (None, 500)          152000      ['concatenate_33[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_126 (Dropout)          (None, 500)          0           ['dense_133[0][0]']              \n",
            "                                                                                                  \n",
            " dense_134 (Dense)              (None, 100)          50100       ['dropout_126[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_127 (Dropout)          (None, 100)          0           ['dense_134[0][0]']              \n",
            "                                                                                                  \n",
            " dense_135 (Dense)              (None, 1)            101         ['dropout_127[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.0009 - mae: 0.9966\n",
            "Epoch 1: val_loss improved from inf to 0.68947, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 39ms/step - loss: 2.9290 - mae: 0.9846 - val_loss: 0.6895 - val_mae: 0.5337\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6267 - mae: 0.5201\n",
            "Epoch 2: val_loss improved from 0.68947 to 0.54114, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6207 - mae: 0.5169 - val_loss: 0.5411 - val_mae: 0.4838\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5009 - mae: 0.4740\n",
            "Epoch 3: val_loss improved from 0.54114 to 0.46852, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5021 - mae: 0.4735 - val_loss: 0.4685 - val_mae: 0.4927\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4309 - mae: 0.4444\n",
            "Epoch 4: val_loss improved from 0.46852 to 0.39688, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4278 - mae: 0.4431 - val_loss: 0.3969 - val_mae: 0.4330\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3710 - mae: 0.4102\n",
            "Epoch 5: val_loss improved from 0.39688 to 0.34531, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3702 - mae: 0.4099 - val_loss: 0.3453 - val_mae: 0.3952\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3237 - mae: 0.3806\n",
            "Epoch 6: val_loss improved from 0.34531 to 0.31031, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3234 - mae: 0.3803 - val_loss: 0.3103 - val_mae: 0.3754\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3005 - mae: 0.3699\n",
            "Epoch 7: val_loss improved from 0.31031 to 0.27112, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2965 - mae: 0.3664 - val_loss: 0.2711 - val_mae: 0.3400\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2676 - mae: 0.3480\n",
            "Epoch 8: val_loss improved from 0.27112 to 0.24332, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2649 - mae: 0.3453 - val_loss: 0.2433 - val_mae: 0.3150\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2359 - mae: 0.3178\n",
            "Epoch 9: val_loss improved from 0.24332 to 0.21894, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2324 - mae: 0.3154 - val_loss: 0.2189 - val_mae: 0.2998\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2082 - mae: 0.2932\n",
            "Epoch 10: val_loss improved from 0.21894 to 0.19460, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2066 - mae: 0.2924 - val_loss: 0.1946 - val_mae: 0.2778\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2056 - mae: 0.3023\n",
            "Epoch 11: val_loss did not improve from 0.19460\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2090 - mae: 0.3075 - val_loss: 0.2272 - val_mae: 0.3342\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1976 - mae: 0.3016\n",
            "Epoch 12: val_loss did not improve from 0.19460\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1987 - mae: 0.3027 - val_loss: 0.2152 - val_mae: 0.3462\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2124 - mae: 0.3288\n",
            "Epoch 13: val_loss improved from 0.19460 to 0.19181, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2124 - mae: 0.3288 - val_loss: 0.1918 - val_mae: 0.3062\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1906 - mae: 0.3048\n",
            "Epoch 14: val_loss improved from 0.19181 to 0.16260, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1894 - mae: 0.3032 - val_loss: 0.1626 - val_mae: 0.2668\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2089 - mae: 0.3282\n",
            "Epoch 15: val_loss did not improve from 0.16260\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2133 - mae: 0.3352 - val_loss: 0.2361 - val_mae: 0.3915\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1658 - mae: 0.2693\n",
            "Epoch 16: val_loss improved from 0.16260 to 0.14969, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1634 - mae: 0.2662 - val_loss: 0.1497 - val_mae: 0.2481\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.2935\n",
            "Epoch 17: val_loss did not improve from 0.14969\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1772 - mae: 0.2959 - val_loss: 0.2100 - val_mae: 0.3588\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1977 - mae: 0.3252\n",
            "Epoch 18: val_loss improved from 0.14969 to 0.14131, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1964 - mae: 0.3229 - val_loss: 0.1413 - val_mae: 0.2365\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1360 - mae: 0.2303\n",
            "Epoch 19: val_loss improved from 0.14131 to 0.12837, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1368 - mae: 0.2302 - val_loss: 0.1284 - val_mae: 0.2127\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1547 - mae: 0.2643\n",
            "Epoch 20: val_loss did not improve from 0.12837\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1519 - mae: 0.2608 - val_loss: 0.1357 - val_mae: 0.2292\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2001 - mae: 0.3340\n",
            "Epoch 21: val_loss did not improve from 0.12837\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1989 - mae: 0.3325 - val_loss: 0.1846 - val_mae: 0.3285\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1601 - mae: 0.2805\n",
            "Epoch 22: val_loss improved from 0.12837 to 0.12414, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1587 - mae: 0.2757 - val_loss: 0.1241 - val_mae: 0.2083\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1581 - mae: 0.2706\n",
            "Epoch 23: val_loss did not improve from 0.12414\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1655 - mae: 0.2809 - val_loss: 0.2522 - val_mae: 0.4067\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1929 - mae: 0.3226\n",
            "Epoch 24: val_loss improved from 0.12414 to 0.12048, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1896 - mae: 0.3157 - val_loss: 0.1205 - val_mae: 0.2019\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1339 - mae: 0.2322\n",
            "Epoch 25: val_loss did not improve from 0.12048\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1338 - mae: 0.2324 - val_loss: 0.1301 - val_mae: 0.2309\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1498 - mae: 0.2658\n",
            "Epoch 26: val_loss did not improve from 0.12048\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1502 - mae: 0.2674 - val_loss: 0.1353 - val_mae: 0.2347\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1306 - mae: 0.2250\n",
            "Epoch 27: val_loss did not improve from 0.12048\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1354 - mae: 0.2347 - val_loss: 0.2195 - val_mae: 0.3703\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1577 - mae: 0.2804\n",
            "Epoch 28: val_loss did not improve from 0.12048\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1550 - mae: 0.2761 - val_loss: 0.1288 - val_mae: 0.2313\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1410 - mae: 0.2522\n",
            "Epoch 29: val_loss did not improve from 0.12048\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1397 - mae: 0.2497 - val_loss: 0.1349 - val_mae: 0.2351\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1388 - mae: 0.2486\n",
            "Epoch 30: val_loss did not improve from 0.12048\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1388 - mae: 0.2486 - val_loss: 0.1300 - val_mae: 0.2366\n",
            "Epoch 30: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13963599503040314, RMSE:0.3736790120601654, MAE:0.22308678925037384, R2:0.8370728042445453\n",
            "0.8370728042445453\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_134\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_69 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_94 (Embedding)       (None, 17, 900)      649800      ['input_69[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_102 (Conv1D)            (None, 17, 100)      270100      ['embedding_94[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_103 (Conv1D)            (None, 17, 100)      270100      ['embedding_94[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_104 (Conv1D)            (None, 17, 100)      270100      ['embedding_94[0][0]']           \n",
            "                                                                                                  \n",
            " input_70 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_282 (Glob  (None, 100)         0           ['conv1d_102[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_283 (Glob  (None, 100)         0           ['conv1d_103[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_284 (Glob  (None, 100)         0           ['conv1d_104[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_136 (Dense)              (None, 3)            33          ['input_70[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_282[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_283[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_284[0][0]'\n",
            "                                                                 , 'dense_136[0][0]']             \n",
            "                                                                                                  \n",
            " dense_137 (Dense)              (None, 500)          152000      ['concatenate_34[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_128 (Dropout)          (None, 500)          0           ['dense_137[0][0]']              \n",
            "                                                                                                  \n",
            " dense_138 (Dense)              (None, 100)          50100       ['dropout_128[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_129 (Dropout)          (None, 100)          0           ['dense_138[0][0]']              \n",
            "                                                                                                  \n",
            " dense_139 (Dense)              (None, 1)            101         ['dropout_129[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 2.8213 - mae: 0.9717\n",
            "Epoch 1: val_loss improved from inf to 0.73858, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 41ms/step - loss: 2.7512 - mae: 0.9567 - val_loss: 0.7386 - val_mae: 0.5301\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6674 - mae: 0.5350\n",
            "Epoch 2: val_loss improved from 0.73858 to 0.58117, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6674 - mae: 0.5350 - val_loss: 0.5812 - val_mae: 0.5141\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5409 - mae: 0.4913\n",
            "Epoch 3: val_loss improved from 0.58117 to 0.48890, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5401 - mae: 0.4902 - val_loss: 0.4889 - val_mae: 0.4814\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4552 - mae: 0.4537\n",
            "Epoch 4: val_loss improved from 0.48890 to 0.42819, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4559 - mae: 0.4538 - val_loss: 0.4282 - val_mae: 0.4612\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3992 - mae: 0.4271\n",
            "Epoch 5: val_loss improved from 0.42819 to 0.37287, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3993 - mae: 0.4265 - val_loss: 0.3729 - val_mae: 0.4263\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3450 - mae: 0.3949\n",
            "Epoch 6: val_loss improved from 0.37287 to 0.32201, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.3450 - mae: 0.3949 - val_loss: 0.3220 - val_mae: 0.3741\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3123 - mae: 0.3751\n",
            "Epoch 7: val_loss improved from 0.32201 to 0.28875, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3131 - mae: 0.3749 - val_loss: 0.2888 - val_mae: 0.3733\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2718 - mae: 0.3438\n",
            "Epoch 8: val_loss improved from 0.28875 to 0.24338, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2695 - mae: 0.3425 - val_loss: 0.2434 - val_mae: 0.3259\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2372 - mae: 0.3180\n",
            "Epoch 9: val_loss improved from 0.24338 to 0.22395, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.2363 - mae: 0.3173 - val_loss: 0.2240 - val_mae: 0.3114\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2094 - mae: 0.2961\n",
            "Epoch 10: val_loss improved from 0.22395 to 0.18890, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2105 - mae: 0.2963 - val_loss: 0.1889 - val_mae: 0.2823\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2056 - mae: 0.3023\n",
            "Epoch 11: val_loss did not improve from 0.18890\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2056 - mae: 0.3023 - val_loss: 0.2612 - val_mae: 0.4094\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2458 - mae: 0.3726\n",
            "Epoch 12: val_loss did not improve from 0.18890\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2450 - mae: 0.3722 - val_loss: 0.2340 - val_mae: 0.3620\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2739 - mae: 0.4101\n",
            "Epoch 13: val_loss did not improve from 0.18890\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2710 - mae: 0.4073 - val_loss: 0.2098 - val_mae: 0.3534\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1881 - mae: 0.2999\n",
            "Epoch 14: val_loss improved from 0.18890 to 0.14986, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1880 - mae: 0.2988 - val_loss: 0.1499 - val_mae: 0.2489\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2183 - mae: 0.3496\n",
            "Epoch 15: val_loss did not improve from 0.14986\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2183 - mae: 0.3496 - val_loss: 0.1562 - val_mae: 0.2734\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1949 - mae: 0.3256\n",
            "Epoch 16: val_loss did not improve from 0.14986\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1957 - mae: 0.3265 - val_loss: 0.1811 - val_mae: 0.3095\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2524 - mae: 0.4013\n",
            "Epoch 17: val_loss did not improve from 0.14986\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2495 - mae: 0.3986 - val_loss: 0.1565 - val_mae: 0.2827\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1478 - mae: 0.2483\n",
            "Epoch 18: val_loss did not improve from 0.14986\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1491 - mae: 0.2496 - val_loss: 0.1507 - val_mae: 0.2738\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1888 - mae: 0.3091\n",
            "Epoch 19: val_loss did not improve from 0.14986\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1913 - mae: 0.3158 - val_loss: 0.2496 - val_mae: 0.4050\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.2965\n",
            "Epoch 20: val_loss did not improve from 0.14986\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1756 - mae: 0.2975 - val_loss: 0.1822 - val_mae: 0.3304\n",
            "Epoch 20: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.17238342761993408, RMSE:0.41519084572792053, MAE:0.26779165863990784, R2:0.798863135421634\n",
            "0.798863135421634\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453, 0.798863135421634]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_135\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_71 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_95 (Embedding)       (None, 17, 900)      649800      ['input_71[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_105 (Conv1D)            (None, 17, 100)      270100      ['embedding_95[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_106 (Conv1D)            (None, 17, 100)      270100      ['embedding_95[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_107 (Conv1D)            (None, 17, 100)      270100      ['embedding_95[0][0]']           \n",
            "                                                                                                  \n",
            " input_72 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_285 (Glob  (None, 100)         0           ['conv1d_105[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_286 (Glob  (None, 100)         0           ['conv1d_106[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_287 (Glob  (None, 100)         0           ['conv1d_107[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_140 (Dense)              (None, 3)            33          ['input_72[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_285[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_286[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_287[0][0]'\n",
            "                                                                 , 'dense_140[0][0]']             \n",
            "                                                                                                  \n",
            " dense_141 (Dense)              (None, 500)          152000      ['concatenate_35[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_130 (Dropout)          (None, 500)          0           ['dense_141[0][0]']              \n",
            "                                                                                                  \n",
            " dense_142 (Dense)              (None, 100)          50100       ['dropout_130[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_131 (Dropout)          (None, 100)          0           ['dense_142[0][0]']              \n",
            "                                                                                                  \n",
            " dense_143 (Dense)              (None, 1)            101         ['dropout_131[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.0083 - mae: 0.9947\n",
            "Epoch 1: val_loss improved from inf to 0.75549, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 41ms/step - loss: 3.0083 - mae: 0.9947 - val_loss: 0.7555 - val_mae: 0.5911\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6944 - mae: 0.5465\n",
            "Epoch 2: val_loss improved from 0.75549 to 0.60216, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6892 - mae: 0.5453 - val_loss: 0.6022 - val_mae: 0.5269\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5618 - mae: 0.4965\n",
            "Epoch 3: val_loss improved from 0.60216 to 0.49627, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.5563 - mae: 0.4938 - val_loss: 0.4963 - val_mae: 0.4574\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4627 - mae: 0.4571\n",
            "Epoch 4: val_loss improved from 0.49627 to 0.41791, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4614 - mae: 0.4564 - val_loss: 0.4179 - val_mae: 0.4411\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3897 - mae: 0.4261\n",
            "Epoch 5: val_loss improved from 0.41791 to 0.37091, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3897 - mae: 0.4261 - val_loss: 0.3709 - val_mae: 0.4089\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3303 - mae: 0.3914\n",
            "Epoch 6: val_loss improved from 0.37091 to 0.29864, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3303 - mae: 0.3914 - val_loss: 0.2986 - val_mae: 0.3705\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2940 - mae: 0.3727\n",
            "Epoch 7: val_loss improved from 0.29864 to 0.25531, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2945 - mae: 0.3729 - val_loss: 0.2553 - val_mae: 0.3395\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2556 - mae: 0.3449\n",
            "Epoch 8: val_loss improved from 0.25531 to 0.22014, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2519 - mae: 0.3421 - val_loss: 0.2201 - val_mae: 0.3169\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2530 - mae: 0.3588\n",
            "Epoch 9: val_loss did not improve from 0.22014\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2530 - mae: 0.3588 - val_loss: 0.2496 - val_mae: 0.3850\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2840 - mae: 0.4101\n",
            "Epoch 10: val_loss did not improve from 0.22014\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2840 - mae: 0.4101 - val_loss: 0.2603 - val_mae: 0.3806\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1852 - mae: 0.2871\n",
            "Epoch 11: val_loss improved from 0.22014 to 0.15601, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1837 - mae: 0.2857 - val_loss: 0.1560 - val_mae: 0.2520\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2699 - mae: 0.3802\n",
            "Epoch 12: val_loss did not improve from 0.15601\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2729 - mae: 0.3845 - val_loss: 0.3568 - val_mae: 0.4922\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2066 - mae: 0.3324\n",
            "Epoch 13: val_loss improved from 0.15601 to 0.13847, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2020 - mae: 0.3249 - val_loss: 0.1385 - val_mae: 0.2324\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2301 - mae: 0.3548\n",
            "Epoch 14: val_loss did not improve from 0.13847\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2335 - mae: 0.3593 - val_loss: 0.3124 - val_mae: 0.4845\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2258 - mae: 0.3675\n",
            "Epoch 15: val_loss did not improve from 0.13847\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2261 - mae: 0.3668 - val_loss: 0.1924 - val_mae: 0.3277\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1883 - mae: 0.3182\n",
            "Epoch 16: val_loss did not improve from 0.13847\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1886 - mae: 0.3192 - val_loss: 0.1835 - val_mae: 0.3311\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2171 - mae: 0.3616\n",
            "Epoch 17: val_loss did not improve from 0.13847\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2138 - mae: 0.3586 - val_loss: 0.1450 - val_mae: 0.2584\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1790 - mae: 0.3085\n",
            "Epoch 18: val_loss did not improve from 0.13847\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1805 - mae: 0.3099 - val_loss: 0.2057 - val_mae: 0.3664\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1702 - mae: 0.2953\n",
            "Epoch 19: val_loss did not improve from 0.13847\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1683 - mae: 0.2928 - val_loss: 0.1396 - val_mae: 0.2537\n",
            "Epoch 19: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.16580180823802948, RMSE:0.40718767046928406, MAE:0.24813738465309143, R2:0.8065425692152892\n",
            "0.8065425692152892\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453, 0.798863135421634, 0.8065425692152892]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n",
            "Model: \"model_136\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_73 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_96 (Embedding)       (None, 17, 900)      649800      ['input_73[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_108 (Conv1D)            (None, 17, 100)      270100      ['embedding_96[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_109 (Conv1D)            (None, 17, 100)      270100      ['embedding_96[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_110 (Conv1D)            (None, 17, 100)      270100      ['embedding_96[0][0]']           \n",
            "                                                                                                  \n",
            " input_74 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_288 (Glob  (None, 100)         0           ['conv1d_108[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_289 (Glob  (None, 100)         0           ['conv1d_109[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_290 (Glob  (None, 100)         0           ['conv1d_110[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_144 (Dense)              (None, 3)            33          ['input_74[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_288[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_289[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_290[0][0]'\n",
            "                                                                 , 'dense_144[0][0]']             \n",
            "                                                                                                  \n",
            " dense_145 (Dense)              (None, 500)          152000      ['concatenate_36[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_132 (Dropout)          (None, 500)          0           ['dense_145[0][0]']              \n",
            "                                                                                                  \n",
            " dense_146 (Dense)              (None, 100)          50100       ['dropout_132[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_133 (Dropout)          (None, 100)          0           ['dense_146[0][0]']              \n",
            "                                                                                                  \n",
            " dense_147 (Dense)              (None, 1)            101         ['dropout_133[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 3.1223 - mae: 1.0108\n",
            "Epoch 1: val_loss improved from inf to 0.70382, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 42ms/step - loss: 2.9449 - mae: 0.9780 - val_loss: 0.7038 - val_mae: 0.4873\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6272 - mae: 0.5200\n",
            "Epoch 2: val_loss improved from 0.70382 to 0.55301, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6237 - mae: 0.5189 - val_loss: 0.5530 - val_mae: 0.4920\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.5129 - mae: 0.4813\n",
            "Epoch 3: val_loss improved from 0.55301 to 0.48642, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5133 - mae: 0.4819 - val_loss: 0.4864 - val_mae: 0.4998\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.4436 - mae: 0.4570\n",
            "Epoch 4: val_loss improved from 0.48642 to 0.41076, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4414 - mae: 0.4554 - val_loss: 0.4108 - val_mae: 0.4322\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3856 - mae: 0.4255\n",
            "Epoch 5: val_loss improved from 0.41076 to 0.36687, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3833 - mae: 0.4242 - val_loss: 0.3669 - val_mae: 0.4064\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3388 - mae: 0.3974\n",
            "Epoch 6: val_loss improved from 0.36687 to 0.31906, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.3394 - mae: 0.3971 - val_loss: 0.3191 - val_mae: 0.3842\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2998 - mae: 0.3707\n",
            "Epoch 7: val_loss improved from 0.31906 to 0.30911, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2998 - mae: 0.3707 - val_loss: 0.3091 - val_mae: 0.3714\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2650 - mae: 0.3455\n",
            "Epoch 8: val_loss improved from 0.30911 to 0.24853, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.2656 - mae: 0.3447 - val_loss: 0.2485 - val_mae: 0.3342\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2355 - mae: 0.3217\n",
            "Epoch 9: val_loss improved from 0.24853 to 0.22768, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2355 - mae: 0.3217 - val_loss: 0.2277 - val_mae: 0.3131\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2118 - mae: 0.3022\n",
            "Epoch 10: val_loss improved from 0.22768 to 0.19728, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2122 - mae: 0.3022 - val_loss: 0.1973 - val_mae: 0.2887\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2054 - mae: 0.3037\n",
            "Epoch 11: val_loss improved from 0.19728 to 0.17941, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2054 - mae: 0.3037 - val_loss: 0.1794 - val_mae: 0.2711\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1774 - mae: 0.2723\n",
            "Epoch 12: val_loss improved from 0.17941 to 0.16357, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1774 - mae: 0.2723 - val_loss: 0.1636 - val_mae: 0.2575\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1717 - mae: 0.2673\n",
            "Epoch 13: val_loss did not improve from 0.16357\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1711 - mae: 0.2692 - val_loss: 0.1769 - val_mae: 0.2975\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1601 - mae: 0.2588\n",
            "Epoch 14: val_loss did not improve from 0.16357\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1601 - mae: 0.2588 - val_loss: 0.1679 - val_mae: 0.2882\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1590 - mae: 0.2607\n",
            "Epoch 15: val_loss improved from 0.16357 to 0.14306, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.1568 - mae: 0.2584 - val_loss: 0.1431 - val_mae: 0.2402\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1591 - mae: 0.2665\n",
            "Epoch 16: val_loss did not improve from 0.14306\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1620 - mae: 0.2698 - val_loss: 0.2147 - val_mae: 0.3676\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1448 - mae: 0.2442\n",
            "Epoch 17: val_loss improved from 0.14306 to 0.13592, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1448 - mae: 0.2442 - val_loss: 0.1359 - val_mae: 0.2323\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1545 - mae: 0.2634\n",
            "Epoch 18: val_loss did not improve from 0.13592\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1545 - mae: 0.2634 - val_loss: 0.1857 - val_mae: 0.3302\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1638 - mae: 0.2824\n",
            "Epoch 19: val_loss improved from 0.13592 to 0.12389, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.1638 - mae: 0.2824 - val_loss: 0.1239 - val_mae: 0.2137\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1379 - mae: 0.2390\n",
            "Epoch 20: val_loss did not improve from 0.12389\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1386 - mae: 0.2401 - val_loss: 0.1410 - val_mae: 0.2554\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1371 - mae: 0.2397\n",
            "Epoch 21: val_loss improved from 0.12389 to 0.12357, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1364 - mae: 0.2384 - val_loss: 0.1236 - val_mae: 0.2200\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1279 - mae: 0.2225\n",
            "Epoch 22: val_loss did not improve from 0.12357\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1281 - mae: 0.2233 - val_loss: 0.1268 - val_mae: 0.2199\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1228 - mae: 0.2154\n",
            "Epoch 23: val_loss improved from 0.12357 to 0.12049, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1235 - mae: 0.2156 - val_loss: 0.1205 - val_mae: 0.2091\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1456 - mae: 0.2594\n",
            "Epoch 24: val_loss did not improve from 0.12049\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1473 - mae: 0.2623 - val_loss: 0.1888 - val_mae: 0.3411\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1852 - mae: 0.3240\n",
            "Epoch 25: val_loss did not improve from 0.12049\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1840 - mae: 0.3234 - val_loss: 0.1866 - val_mae: 0.3250\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1409 - mae: 0.2546\n",
            "Epoch 26: val_loss did not improve from 0.12049\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1409 - mae: 0.2546 - val_loss: 0.1253 - val_mae: 0.2290\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1303 - mae: 0.2321\n",
            "Epoch 27: val_loss did not improve from 0.12049\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1291 - mae: 0.2315 - val_loss: 0.1317 - val_mae: 0.2343\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1164 - mae: 0.2045\n",
            "Epoch 28: val_loss improved from 0.12049 to 0.11236, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1169 - mae: 0.2047 - val_loss: 0.1124 - val_mae: 0.1963\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1156 - mae: 0.2048\n",
            "Epoch 29: val_loss did not improve from 0.11236\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1166 - mae: 0.2053 - val_loss: 0.1217 - val_mae: 0.2143\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1160 - mae: 0.2047\n",
            "Epoch 30: val_loss did not improve from 0.11236\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1154 - mae: 0.2047 - val_loss: 0.1247 - val_mae: 0.2229\n",
            "Epoch 31/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1791 - mae: 0.3179\n",
            "Epoch 31: val_loss did not improve from 0.11236\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1738 - mae: 0.3108 - val_loss: 0.1195 - val_mae: 0.2187\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1136 - mae: 0.2009\n",
            "Epoch 32: val_loss did not improve from 0.11236\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1141 - mae: 0.2015 - val_loss: 0.1193 - val_mae: 0.2189\n",
            "Epoch 33/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1163 - mae: 0.2093\n",
            "Epoch 33: val_loss improved from 0.11236 to 0.11189, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1159 - mae: 0.2077 - val_loss: 0.1119 - val_mae: 0.1993\n",
            "Epoch 34/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1226 - mae: 0.2211\n",
            "Epoch 34: val_loss did not improve from 0.11189\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1247 - mae: 0.2254 - val_loss: 0.1648 - val_mae: 0.3098\n",
            "Epoch 35/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1343 - mae: 0.2473\n",
            "Epoch 35: val_loss did not improve from 0.11189\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1336 - mae: 0.2461 - val_loss: 0.1291 - val_mae: 0.2329\n",
            "Epoch 36/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1136 - mae: 0.2071\n",
            "Epoch 36: val_loss improved from 0.11189 to 0.10933, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.1135 - mae: 0.2066 - val_loss: 0.1093 - val_mae: 0.1901\n",
            "Epoch 37/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1219 - mae: 0.2221\n",
            "Epoch 37: val_loss did not improve from 0.10933\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1197 - mae: 0.2202 - val_loss: 0.1257 - val_mae: 0.2263\n",
            "Epoch 38/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1130 - mae: 0.2079\n",
            "Epoch 38: val_loss improved from 0.10933 to 0.10924, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1132 - mae: 0.2073 - val_loss: 0.1092 - val_mae: 0.1938\n",
            "Epoch 39/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1305 - mae: 0.2439\n",
            "Epoch 39: val_loss did not improve from 0.10924\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1311 - mae: 0.2457 - val_loss: 0.1557 - val_mae: 0.2848\n",
            "Epoch 40/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1472 - mae: 0.2747\n",
            "Epoch 40: val_loss did not improve from 0.10924\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1457 - mae: 0.2730 - val_loss: 0.1335 - val_mae: 0.2536\n",
            "Epoch 41/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1357 - mae: 0.2582\n",
            "Epoch 41: val_loss did not improve from 0.10924\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1355 - mae: 0.2575 - val_loss: 0.1356 - val_mae: 0.2468\n",
            "Epoch 42/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1200 - mae: 0.2254\n",
            "Epoch 42: val_loss did not improve from 0.10924\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1206 - mae: 0.2259 - val_loss: 0.1310 - val_mae: 0.2492\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1335 - mae: 0.2525\n",
            "Epoch 43: val_loss did not improve from 0.10924\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1335 - mae: 0.2525 - val_loss: 0.1463 - val_mae: 0.2695\n",
            "Epoch 44/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1136 - mae: 0.2101\n",
            "Epoch 44: val_loss improved from 0.10924 to 0.10787, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1114 - mae: 0.2074 - val_loss: 0.1079 - val_mae: 0.1921\n",
            "Epoch 45/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1216 - mae: 0.2281\n",
            "Epoch 45: val_loss did not improve from 0.10787\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1220 - mae: 0.2300 - val_loss: 0.1474 - val_mae: 0.2703\n",
            "Epoch 46/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1323 - mae: 0.2471\n",
            "Epoch 46: val_loss improved from 0.10787 to 0.10580, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1291 - mae: 0.2421 - val_loss: 0.1058 - val_mae: 0.1850\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1049 - mae: 0.1923\n",
            "Epoch 47: val_loss did not improve from 0.10580\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1049 - mae: 0.1923 - val_loss: 0.1081 - val_mae: 0.1933\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1202 - mae: 0.2258\n",
            "Epoch 48: val_loss improved from 0.10580 to 0.10561, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1202 - mae: 0.2258 - val_loss: 0.1056 - val_mae: 0.1835\n",
            "Epoch 49/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1047 - mae: 0.1952\n",
            "Epoch 49: val_loss did not improve from 0.10561\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1075 - mae: 0.1992 - val_loss: 0.1231 - val_mae: 0.2319\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1021 - mae: 0.1871\n",
            "Epoch 50: val_loss improved from 0.10561 to 0.10533, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1021 - mae: 0.1871 - val_loss: 0.1053 - val_mae: 0.1828\n",
            "Epoch 51/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1329 - mae: 0.2516\n",
            "Epoch 51: val_loss did not improve from 0.10533\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1337 - mae: 0.2545 - val_loss: 0.1430 - val_mae: 0.2641\n",
            "Epoch 52/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1106 - mae: 0.2087\n",
            "Epoch 52: val_loss improved from 0.10533 to 0.10471, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1101 - mae: 0.2079 - val_loss: 0.1047 - val_mae: 0.1832\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1065 - mae: 0.2010\n",
            "Epoch 53: val_loss did not improve from 0.10471\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1065 - mae: 0.2010 - val_loss: 0.1060 - val_mae: 0.1840\n",
            "Epoch 54/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1099 - mae: 0.2089\n",
            "Epoch 54: val_loss did not improve from 0.10471\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1104 - mae: 0.2093 - val_loss: 0.1211 - val_mae: 0.2188\n",
            "Epoch 55/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1338 - mae: 0.2590\n",
            "Epoch 55: val_loss did not improve from 0.10471\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1343 - mae: 0.2603 - val_loss: 0.1265 - val_mae: 0.2420\n",
            "Epoch 56/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1127 - mae: 0.2169\n",
            "Epoch 56: val_loss did not improve from 0.10471\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1129 - mae: 0.2172 - val_loss: 0.1308 - val_mae: 0.2397\n",
            "Epoch 57/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1121 - mae: 0.2127\n",
            "Epoch 57: val_loss did not improve from 0.10471\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1096 - mae: 0.2093 - val_loss: 0.1053 - val_mae: 0.1865\n",
            "Epoch 58/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1322 - mae: 0.2535\n",
            "Epoch 58: val_loss did not improve from 0.10471\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1332 - mae: 0.2563 - val_loss: 0.1453 - val_mae: 0.2695\n",
            "Epoch 58: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.125795379281044, RMSE:0.3546764552593231, MAE:0.20084185898303986, R2:0.8532220419550329\n",
            "0.8532220419550329\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453, 0.798863135421634, 0.8065425692152892, 0.8532220419550329]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_137\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_75 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_97 (Embedding)       (None, 17, 900)      649800      ['input_75[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_111 (Conv1D)            (None, 17, 100)      270100      ['embedding_97[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_112 (Conv1D)            (None, 17, 100)      270100      ['embedding_97[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_113 (Conv1D)            (None, 17, 100)      270100      ['embedding_97[0][0]']           \n",
            "                                                                                                  \n",
            " input_76 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_291 (Glob  (None, 100)         0           ['conv1d_111[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_292 (Glob  (None, 100)         0           ['conv1d_112[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_293 (Glob  (None, 100)         0           ['conv1d_113[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_148 (Dense)              (None, 3)            33          ['input_76[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_291[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_292[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_293[0][0]'\n",
            "                                                                 , 'dense_148[0][0]']             \n",
            "                                                                                                  \n",
            " dense_149 (Dense)              (None, 500)          152000      ['concatenate_37[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_134 (Dropout)          (None, 500)          0           ['dense_149[0][0]']              \n",
            "                                                                                                  \n",
            " dense_150 (Dense)              (None, 100)          50100       ['dropout_134[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_135 (Dropout)          (None, 100)          0           ['dense_150[0][0]']              \n",
            "                                                                                                  \n",
            " dense_151 (Dense)              (None, 1)            101         ['dropout_135[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.6601 - mae: 1.2360\n",
            "Epoch 1: val_loss improved from inf to 0.79159, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 41ms/step - loss: 4.5379 - mae: 1.2153 - val_loss: 0.7916 - val_mae: 0.6358\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7279 - mae: 0.5628\n",
            "Epoch 2: val_loss improved from 0.79159 to 0.64574, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.7279 - mae: 0.5628 - val_loss: 0.6457 - val_mae: 0.4629\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.6084 - mae: 0.5106\n",
            "Epoch 3: val_loss improved from 0.64574 to 0.54441, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.6021 - mae: 0.5079 - val_loss: 0.5444 - val_mae: 0.4632\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5184 - mae: 0.4750\n",
            "Epoch 4: val_loss improved from 0.54441 to 0.47572, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.5184 - mae: 0.4750 - val_loss: 0.4757 - val_mae: 0.4500\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4538 - mae: 0.4459\n",
            "Epoch 5: val_loss improved from 0.47572 to 0.42410, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.4532 - mae: 0.4452 - val_loss: 0.4241 - val_mae: 0.4391\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4063 - mae: 0.4239\n",
            "Epoch 6: val_loss improved from 0.42410 to 0.38832, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4053 - mae: 0.4230 - val_loss: 0.3883 - val_mae: 0.4309\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3718 - mae: 0.4063\n",
            "Epoch 7: val_loss improved from 0.38832 to 0.36083, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.3702 - mae: 0.4061 - val_loss: 0.3608 - val_mae: 0.3961\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3315 - mae: 0.3792\n",
            "Epoch 8: val_loss improved from 0.36083 to 0.31448, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3332 - mae: 0.3802 - val_loss: 0.3145 - val_mae: 0.3737\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3079 - mae: 0.3684\n",
            "Epoch 9: val_loss improved from 0.31448 to 0.29037, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.3079 - mae: 0.3684 - val_loss: 0.2904 - val_mae: 0.3626\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2823 - mae: 0.3482\n",
            "Epoch 10: val_loss improved from 0.29037 to 0.26197, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2807 - mae: 0.3476 - val_loss: 0.2620 - val_mae: 0.3378\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2581 - mae: 0.3315\n",
            "Epoch 11: val_loss improved from 0.26197 to 0.24403, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2581 - mae: 0.3315 - val_loss: 0.2440 - val_mae: 0.3225\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2420 - mae: 0.3221\n",
            "Epoch 12: val_loss improved from 0.24403 to 0.21819, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2389 - mae: 0.3196 - val_loss: 0.2182 - val_mae: 0.3022\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2199 - mae: 0.3044\n",
            "Epoch 13: val_loss improved from 0.21819 to 0.20364, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2191 - mae: 0.3042 - val_loss: 0.2036 - val_mae: 0.2896\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2054 - mae: 0.2946\n",
            "Epoch 14: val_loss did not improve from 0.20364\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2054 - mae: 0.2946 - val_loss: 0.2458 - val_mae: 0.3568\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2048 - mae: 0.3065\n",
            "Epoch 15: val_loss improved from 0.20364 to 0.17191, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2038 - mae: 0.3043 - val_loss: 0.1719 - val_mae: 0.2629\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2379 - mae: 0.3492\n",
            "Epoch 16: val_loss did not improve from 0.17191\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2395 - mae: 0.3537 - val_loss: 0.2533 - val_mae: 0.4055\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2022 - mae: 0.3077\n",
            "Epoch 17: val_loss improved from 0.17191 to 0.17022, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2002 - mae: 0.3061 - val_loss: 0.1702 - val_mae: 0.2744\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1776 - mae: 0.2797\n",
            "Epoch 18: val_loss did not improve from 0.17022\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1821 - mae: 0.2854 - val_loss: 0.3157 - val_mae: 0.4804\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2214 - mae: 0.3525\n",
            "Epoch 19: val_loss did not improve from 0.17022\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2214 - mae: 0.3525 - val_loss: 0.2298 - val_mae: 0.3682\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2070 - mae: 0.3363\n",
            "Epoch 20: val_loss did not improve from 0.17022\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2070 - mae: 0.3363 - val_loss: 0.1765 - val_mae: 0.3081\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1621 - mae: 0.2683\n",
            "Epoch 21: val_loss did not improve from 0.17022\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1679 - mae: 0.2763 - val_loss: 0.2242 - val_mae: 0.3815\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2043 - mae: 0.3380\n",
            "Epoch 22: val_loss improved from 0.17022 to 0.13555, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1999 - mae: 0.3300 - val_loss: 0.1355 - val_mae: 0.2291\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1385 - mae: 0.2269\n",
            "Epoch 23: val_loss did not improve from 0.13555\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1374 - mae: 0.2268 - val_loss: 0.1419 - val_mae: 0.2449\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2337 - mae: 0.3802\n",
            "Epoch 24: val_loss did not improve from 0.13555\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2309 - mae: 0.3746 - val_loss: 0.1503 - val_mae: 0.2692\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1431 - mae: 0.2409\n",
            "Epoch 25: val_loss did not improve from 0.13555\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1479 - mae: 0.2489 - val_loss: 0.2302 - val_mae: 0.3962\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2107 - mae: 0.3555\n",
            "Epoch 26: val_loss did not improve from 0.13555\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2065 - mae: 0.3490 - val_loss: 0.1574 - val_mae: 0.2782\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1811 - mae: 0.3103\n",
            "Epoch 27: val_loss did not improve from 0.13555\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1811 - mae: 0.3119 - val_loss: 0.1623 - val_mae: 0.2977\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1328 - mae: 0.2278\n",
            "Epoch 28: val_loss improved from 0.13555 to 0.11954, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1328 - mae: 0.2278 - val_loss: 0.1195 - val_mae: 0.2041\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1376 - mae: 0.2372\n",
            "Epoch 29: val_loss did not improve from 0.11954\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1376 - mae: 0.2372 - val_loss: 0.2182 - val_mae: 0.3696\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1851 - mae: 0.3223\n",
            "Epoch 30: val_loss did not improve from 0.11954\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1830 - mae: 0.3200 - val_loss: 0.1395 - val_mae: 0.2554\n",
            "Epoch 31/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1309 - mae: 0.2297\n",
            "Epoch 31: val_loss did not improve from 0.11954\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1306 - mae: 0.2287 - val_loss: 0.1314 - val_mae: 0.2347\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1570 - mae: 0.2787\n",
            "Epoch 32: val_loss did not improve from 0.11954\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1593 - mae: 0.2825 - val_loss: 0.1782 - val_mae: 0.3283\n",
            "Epoch 33/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1483 - mae: 0.2649\n",
            "Epoch 33: val_loss improved from 0.11954 to 0.11589, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1448 - mae: 0.2597 - val_loss: 0.1159 - val_mae: 0.2002\n",
            "Epoch 34/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1391 - mae: 0.2471\n",
            "Epoch 34: val_loss did not improve from 0.11589\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1418 - mae: 0.2510 - val_loss: 0.1826 - val_mae: 0.3255\n",
            "Epoch 35/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1434 - mae: 0.2606\n",
            "Epoch 35: val_loss did not improve from 0.11589\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1420 - mae: 0.2585 - val_loss: 0.1167 - val_mae: 0.2046\n",
            "Epoch 36/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1220 - mae: 0.2156\n",
            "Epoch 36: val_loss did not improve from 0.11589\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1226 - mae: 0.2163 - val_loss: 0.1309 - val_mae: 0.2425\n",
            "Epoch 37/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1387 - mae: 0.2535\n",
            "Epoch 37: val_loss did not improve from 0.11589\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1389 - mae: 0.2525 - val_loss: 0.1283 - val_mae: 0.2329\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1267 - mae: 0.2292\n",
            "Epoch 38: val_loss did not improve from 0.11589\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1267 - mae: 0.2292 - val_loss: 0.1228 - val_mae: 0.2245\n",
            "Epoch 39/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1318 - mae: 0.2427\n",
            "Epoch 39: val_loss did not improve from 0.11589\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1326 - mae: 0.2440 - val_loss: 0.1372 - val_mae: 0.2530\n",
            "Epoch 39: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.1387411206960678, RMSE:0.3724796772003174, MAE:0.2206609547138214, R2:0.8381169469272299\n",
            "0.8381169469272299\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453, 0.798863135421634, 0.8065425692152892, 0.8532220419550329, 0.8381169469272299]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_138\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_77 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_98 (Embedding)       (None, 17, 900)      649800      ['input_77[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_114 (Conv1D)            (None, 17, 100)      270100      ['embedding_98[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_115 (Conv1D)            (None, 17, 100)      270100      ['embedding_98[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_116 (Conv1D)            (None, 17, 100)      270100      ['embedding_98[0][0]']           \n",
            "                                                                                                  \n",
            " input_78 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_294 (Glob  (None, 100)         0           ['conv1d_114[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_295 (Glob  (None, 100)         0           ['conv1d_115[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_296 (Glob  (None, 100)         0           ['conv1d_116[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_152 (Dense)              (None, 3)            33          ['input_78[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_294[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_295[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_296[0][0]'\n",
            "                                                                 , 'dense_152[0][0]']             \n",
            "                                                                                                  \n",
            " dense_153 (Dense)              (None, 500)          152000      ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_136 (Dropout)          (None, 500)          0           ['dense_153[0][0]']              \n",
            "                                                                                                  \n",
            " dense_154 (Dense)              (None, 100)          50100       ['dropout_136[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_137 (Dropout)          (None, 100)          0           ['dense_154[0][0]']              \n",
            "                                                                                                  \n",
            " dense_155 (Dense)              (None, 1)            101         ['dropout_137[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.5117 - mae: 1.0596\n",
            "Epoch 1: val_loss improved from inf to 0.75556, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 40ms/step - loss: 3.4204 - mae: 1.0432 - val_loss: 0.7556 - val_mae: 0.5722\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6898 - mae: 0.5476\n",
            "Epoch 2: val_loss improved from 0.75556 to 0.60751, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.6831 - mae: 0.5440 - val_loss: 0.6075 - val_mae: 0.4675\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5599 - mae: 0.4951\n",
            "Epoch 3: val_loss improved from 0.60751 to 0.51015, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5581 - mae: 0.4941 - val_loss: 0.5101 - val_mae: 0.4900\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4878 - mae: 0.4722\n",
            "Epoch 4: val_loss improved from 0.51015 to 0.44968, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4852 - mae: 0.4712 - val_loss: 0.4497 - val_mae: 0.4439\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4289 - mae: 0.4418\n",
            "Epoch 5: val_loss improved from 0.44968 to 0.41025, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4279 - mae: 0.4421 - val_loss: 0.4103 - val_mae: 0.4254\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3841 - mae: 0.4181\n",
            "Epoch 6: val_loss improved from 0.41025 to 0.36869, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3861 - mae: 0.4197 - val_loss: 0.3687 - val_mae: 0.4041\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3521 - mae: 0.4003\n",
            "Epoch 7: val_loss improved from 0.36869 to 0.34894, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3535 - mae: 0.4004 - val_loss: 0.3489 - val_mae: 0.4181\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3196 - mae: 0.3780\n",
            "Epoch 8: val_loss improved from 0.34894 to 0.32803, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.3211 - mae: 0.3781 - val_loss: 0.3280 - val_mae: 0.4122\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2946 - mae: 0.3609\n",
            "Epoch 9: val_loss improved from 0.32803 to 0.28748, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2950 - mae: 0.3619 - val_loss: 0.2875 - val_mae: 0.3496\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2690 - mae: 0.3406\n",
            "Epoch 10: val_loss improved from 0.28748 to 0.25064, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2672 - mae: 0.3395 - val_loss: 0.2506 - val_mae: 0.3285\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2428 - mae: 0.3194\n",
            "Epoch 11: val_loss improved from 0.25064 to 0.22709, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2409 - mae: 0.3186 - val_loss: 0.2271 - val_mae: 0.3070\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2287 - mae: 0.3142\n",
            "Epoch 12: val_loss improved from 0.22709 to 0.22355, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2285 - mae: 0.3142 - val_loss: 0.2236 - val_mae: 0.3120\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2078 - mae: 0.2955\n",
            "Epoch 13: val_loss improved from 0.22355 to 0.18688, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2053 - mae: 0.2932 - val_loss: 0.1869 - val_mae: 0.2745\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2309 - mae: 0.3395\n",
            "Epoch 14: val_loss did not improve from 0.18688\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2309 - mae: 0.3395 - val_loss: 0.2001 - val_mae: 0.3223\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1830 - mae: 0.2803\n",
            "Epoch 15: val_loss did not improve from 0.18688\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1830 - mae: 0.2803 - val_loss: 0.2033 - val_mae: 0.3178\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2362 - mae: 0.3674\n",
            "Epoch 16: val_loss did not improve from 0.18688\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2362 - mae: 0.3674 - val_loss: 0.3026 - val_mae: 0.4673\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2151 - mae: 0.3426\n",
            "Epoch 17: val_loss did not improve from 0.18688\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.2162 - mae: 0.3439 - val_loss: 0.2286 - val_mae: 0.3622\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2364 - mae: 0.3732\n",
            "Epoch 18: val_loss improved from 0.18688 to 0.14515, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2338 - mae: 0.3694 - val_loss: 0.1452 - val_mae: 0.2466\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1663 - mae: 0.2752\n",
            "Epoch 19: val_loss improved from 0.14515 to 0.14129, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1655 - mae: 0.2736 - val_loss: 0.1413 - val_mae: 0.2368\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1552 - mae: 0.2611\n",
            "Epoch 20: val_loss did not improve from 0.14129\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1593 - mae: 0.2674 - val_loss: 0.2240 - val_mae: 0.3845\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2993 - mae: 0.4558\n",
            "Epoch 21: val_loss did not improve from 0.14129\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2993 - mae: 0.4558 - val_loss: 0.2945 - val_mae: 0.4475\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2172 - mae: 0.3611\n",
            "Epoch 22: val_loss did not improve from 0.14129\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2138 - mae: 0.3556 - val_loss: 0.1515 - val_mae: 0.2749\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1645 - mae: 0.2820\n",
            "Epoch 23: val_loss did not improve from 0.14129\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1613 - mae: 0.2784 - val_loss: 0.1438 - val_mae: 0.2519\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1419 - mae: 0.2431\n",
            "Epoch 24: val_loss did not improve from 0.14129\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1419 - mae: 0.2431 - val_loss: 0.1568 - val_mae: 0.2871\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2145 - mae: 0.3574\n",
            "Epoch 25: val_loss did not improve from 0.14129\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2145 - mae: 0.3574 - val_loss: 0.1631 - val_mae: 0.2868\n",
            "Epoch 25: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.163901686668396, RMSE:0.40484774112701416, MAE:0.2562088966369629, R2:0.8087596296790454\n",
            "0.8087596296790454\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453, 0.798863135421634, 0.8065425692152892, 0.8532220419550329, 0.8381169469272299, 0.8087596296790454]\n",
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "添加人口统计信息，融合\n",
            "Model: \"model_139\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_79 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_99 (Embedding)       (None, 17, 900)      649800      ['input_79[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_117 (Conv1D)            (None, 17, 100)      270100      ['embedding_99[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_118 (Conv1D)            (None, 17, 100)      270100      ['embedding_99[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_119 (Conv1D)            (None, 17, 100)      270100      ['embedding_99[0][0]']           \n",
            "                                                                                                  \n",
            " input_80 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_297 (Glob  (None, 100)         0           ['conv1d_117[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_298 (Glob  (None, 100)         0           ['conv1d_118[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_299 (Glob  (None, 100)         0           ['conv1d_119[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_156 (Dense)              (None, 3)            33          ['input_80[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_297[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_298[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_299[0][0]'\n",
            "                                                                 , 'dense_156[0][0]']             \n",
            "                                                                                                  \n",
            " dense_157 (Dense)              (None, 500)          152000      ['concatenate_39[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_138 (Dropout)          (None, 500)          0           ['dense_157[0][0]']              \n",
            "                                                                                                  \n",
            " dense_158 (Dense)              (None, 100)          50100       ['dropout_138[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_139 (Dropout)          (None, 100)          0           ['dense_158[0][0]']              \n",
            "                                                                                                  \n",
            " dense_159 (Dense)              (None, 1)            101         ['dropout_139[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.2217 - mae: 1.0245\n",
            "Epoch 1: val_loss improved from inf to 0.71681, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 40ms/step - loss: 3.2217 - mae: 1.0245 - val_loss: 0.7168 - val_mae: 0.5270\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.6501 - mae: 0.5254\n",
            "Epoch 2: val_loss improved from 0.71681 to 0.56584, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6488 - mae: 0.5275 - val_loss: 0.5658 - val_mae: 0.4789\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.5342 - mae: 0.4828\n",
            "Epoch 3: val_loss improved from 0.56584 to 0.48522, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.5306 - mae: 0.4818 - val_loss: 0.4852 - val_mae: 0.4491\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4608 - mae: 0.4542\n",
            "Epoch 4: val_loss improved from 0.48522 to 0.43058, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4608 - mae: 0.4542 - val_loss: 0.4306 - val_mae: 0.4430\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.4193 - mae: 0.4356\n",
            "Epoch 5: val_loss improved from 0.43058 to 0.38497, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.4175 - mae: 0.4355 - val_loss: 0.3850 - val_mae: 0.4114\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3700 - mae: 0.4052\n",
            "Epoch 6: val_loss improved from 0.38497 to 0.34948, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3702 - mae: 0.4057 - val_loss: 0.3495 - val_mae: 0.3859\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.3368 - mae: 0.3836\n",
            "Epoch 7: val_loss improved from 0.34948 to 0.32136, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.3343 - mae: 0.3824 - val_loss: 0.3214 - val_mae: 0.3652\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3053 - mae: 0.3625\n",
            "Epoch 8: val_loss improved from 0.32136 to 0.28869, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.3053 - mae: 0.3625 - val_loss: 0.2887 - val_mae: 0.3483\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2775 - mae: 0.3456\n",
            "Epoch 9: val_loss improved from 0.28869 to 0.26370, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2775 - mae: 0.3456 - val_loss: 0.2637 - val_mae: 0.3285\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2481 - mae: 0.3216\n",
            "Epoch 10: val_loss improved from 0.26370 to 0.23489, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2484 - mae: 0.3218 - val_loss: 0.2349 - val_mae: 0.3095\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2253 - mae: 0.3051\n",
            "Epoch 11: val_loss improved from 0.23489 to 0.21864, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.2252 - mae: 0.3040 - val_loss: 0.2186 - val_mae: 0.3085\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.2123 - mae: 0.2978\n",
            "Epoch 12: val_loss improved from 0.21864 to 0.21836, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.2126 - mae: 0.2991 - val_loss: 0.2184 - val_mae: 0.3116\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1985 - mae: 0.2905\n",
            "Epoch 13: val_loss improved from 0.21836 to 0.17991, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1964 - mae: 0.2893 - val_loss: 0.1799 - val_mae: 0.2695\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.2625\n",
            "Epoch 14: val_loss improved from 0.17991 to 0.17704, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1751 - mae: 0.2628 - val_loss: 0.1770 - val_mae: 0.2793\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1656 - mae: 0.2569\n",
            "Epoch 15: val_loss improved from 0.17704 to 0.17121, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1656 - mae: 0.2569 - val_loss: 0.1712 - val_mae: 0.2708\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1573 - mae: 0.2504\n",
            "Epoch 16: val_loss improved from 0.17121 to 0.15048, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1573 - mae: 0.2504 - val_loss: 0.1505 - val_mae: 0.2444\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1582 - mae: 0.2562\n",
            "Epoch 17: val_loss did not improve from 0.15048\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1600 - mae: 0.2614 - val_loss: 0.1992 - val_mae: 0.3402\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.3415 - mae: 0.4846\n",
            "Epoch 18: val_loss did not improve from 0.15048\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.3373 - mae: 0.4795 - val_loss: 0.2037 - val_mae: 0.3271\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1642 - mae: 0.2770\n",
            "Epoch 19: val_loss did not improve from 0.15048\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1649 - mae: 0.2769 - val_loss: 0.1714 - val_mae: 0.2986\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1939 - mae: 0.3284\n",
            "Epoch 20: val_loss improved from 0.15048 to 0.14679, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1925 - mae: 0.3238 - val_loss: 0.1468 - val_mae: 0.2450\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1455 - mae: 0.2459\n",
            "Epoch 21: val_loss improved from 0.14679 to 0.13171, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.1455 - mae: 0.2459 - val_loss: 0.1317 - val_mae: 0.2195\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1920 - mae: 0.3249\n",
            "Epoch 22: val_loss did not improve from 0.13171\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1900 - mae: 0.3234 - val_loss: 0.1487 - val_mae: 0.2645\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1493 - mae: 0.2611\n",
            "Epoch 23: val_loss did not improve from 0.13171\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1493 - mae: 0.2611 - val_loss: 0.1361 - val_mae: 0.2330\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1502 - mae: 0.2618\n",
            "Epoch 24: val_loss did not improve from 0.13171\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1502 - mae: 0.2618 - val_loss: 0.1552 - val_mae: 0.2807\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1457 - mae: 0.2556\n",
            "Epoch 25: val_loss did not improve from 0.13171\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1457 - mae: 0.2556 - val_loss: 0.2213 - val_mae: 0.3683\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.2537 - mae: 0.4089\n",
            "Epoch 26: val_loss improved from 0.13171 to 0.12842, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.2439 - mae: 0.3965 - val_loss: 0.1284 - val_mae: 0.2258\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1425 - mae: 0.2477\n",
            "Epoch 27: val_loss did not improve from 0.12842\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1454 - mae: 0.2534 - val_loss: 0.1529 - val_mae: 0.2678\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1520 - mae: 0.2740\n",
            "Epoch 28: val_loss improved from 0.12842 to 0.12322, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1518 - mae: 0.2707 - val_loss: 0.1232 - val_mae: 0.2150\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1477 - mae: 0.2638\n",
            "Epoch 29: val_loss did not improve from 0.12322\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1466 - mae: 0.2630 - val_loss: 0.1481 - val_mae: 0.2615\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1434 - mae: 0.2573\n",
            "Epoch 30: val_loss did not improve from 0.12322\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1434 - mae: 0.2573 - val_loss: 0.1635 - val_mae: 0.3016\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1974 - mae: 0.3432\n",
            "Epoch 31: val_loss did not improve from 0.12322\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1948 - mae: 0.3392 - val_loss: 0.1241 - val_mae: 0.2122\n",
            "Epoch 32/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1261 - mae: 0.2221\n",
            "Epoch 32: val_loss did not improve from 0.12322\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1256 - mae: 0.2219 - val_loss: 0.1311 - val_mae: 0.2300\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1192 - mae: 0.2090\n",
            "Epoch 33: val_loss improved from 0.12322 to 0.11439, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1188 - mae: 0.2079 - val_loss: 0.1144 - val_mae: 0.1933\n",
            "Epoch 34/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1327 - mae: 0.2402\n",
            "Epoch 34: val_loss did not improve from 0.11439\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1326 - mae: 0.2415 - val_loss: 0.1441 - val_mae: 0.2585\n",
            "Epoch 35/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1159 - mae: 0.2041\n",
            "Epoch 35: val_loss did not improve from 0.11439\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1161 - mae: 0.2047 - val_loss: 0.1254 - val_mae: 0.2195\n",
            "Epoch 36/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1460 - mae: 0.2686\n",
            "Epoch 36: val_loss improved from 0.11439 to 0.11251, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.1435 - mae: 0.2644 - val_loss: 0.1125 - val_mae: 0.1911\n",
            "Epoch 37/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1205 - mae: 0.2180\n",
            "Epoch 37: val_loss did not improve from 0.11251\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1205 - mae: 0.2182 - val_loss: 0.1236 - val_mae: 0.2225\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1191 - mae: 0.2161\n",
            "Epoch 38: val_loss did not improve from 0.11251\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1191 - mae: 0.2161 - val_loss: 0.1147 - val_mae: 0.1953\n",
            "Epoch 39/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1777 - mae: 0.3089\n",
            "Epoch 39: val_loss did not improve from 0.11251\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1755 - mae: 0.3055 - val_loss: 0.1344 - val_mae: 0.2503\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1130 - mae: 0.2012\n",
            "Epoch 40: val_loss improved from 0.11251 to 0.11225, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1130 - mae: 0.2012 - val_loss: 0.1123 - val_mae: 0.1932\n",
            "Epoch 41/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1193 - mae: 0.2142\n",
            "Epoch 41: val_loss did not improve from 0.11225\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1227 - mae: 0.2213 - val_loss: 0.1484 - val_mae: 0.2706\n",
            "Epoch 42/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1448 - mae: 0.2702\n",
            "Epoch 42: val_loss did not improve from 0.11225\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1428 - mae: 0.2681 - val_loss: 0.1324 - val_mae: 0.2479\n",
            "Epoch 43/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1122 - mae: 0.2024\n",
            "Epoch 43: val_loss improved from 0.11225 to 0.11008, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.1114 - mae: 0.2016 - val_loss: 0.1101 - val_mae: 0.1876\n",
            "Epoch 44/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1146 - mae: 0.2099\n",
            "Epoch 44: val_loss improved from 0.11008 to 0.10884, saving model to ./cnn_model/SG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.1142 - mae: 0.2085 - val_loss: 0.1088 - val_mae: 0.1834\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1270 - mae: 0.2380\n",
            "Epoch 45: val_loss did not improve from 0.10884\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1270 - mae: 0.2380 - val_loss: 0.1559 - val_mae: 0.2851\n",
            "Epoch 46/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.1459 - mae: 0.2777\n",
            "Epoch 46: val_loss did not improve from 0.10884\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1441 - mae: 0.2726 - val_loss: 0.1110 - val_mae: 0.1920\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1091 - mae: 0.1993\n",
            "Epoch 47: val_loss did not improve from 0.10884\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1091 - mae: 0.1993 - val_loss: 0.1097 - val_mae: 0.1861\n",
            "Epoch 48/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1084 - mae: 0.1975\n",
            "Epoch 48: val_loss did not improve from 0.10884\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1080 - mae: 0.1974 - val_loss: 0.1188 - val_mae: 0.2106\n",
            "Epoch 49/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.1508 - mae: 0.2820\n",
            "Epoch 49: val_loss did not improve from 0.10884\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1495 - mae: 0.2815 - val_loss: 0.1335 - val_mae: 0.2520\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1078 - mae: 0.1981\n",
            "Epoch 50: val_loss did not improve from 0.10884\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1078 - mae: 0.1981 - val_loss: 0.1107 - val_mae: 0.1927\n",
            "Epoch 50: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.13159696757793427, RMSE:0.3627629578113556, MAE:0.20683041214942932, R2:0.8464527605056376\n",
            "0.8464527605056376\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the filtered testing datasets.....\n",
            "[0.8404397759662624, 0.8403280665778942, 0.8355004636743163, 0.8208336965200509, 0.8426558181999703, 0.8380181065462868, 0.8445559758494802, 0.8451903626131092, 0.8014731286273536, 0.8316815533481572, 0.8318390389158271, 0.8509652318431318, 0.8252615713353465, 0.8370728042445453, 0.798863135421634, 0.8065425692152892, 0.8532220419550329, 0.8381169469272299, 0.8087596296790454, 0.8464527605056376]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_140\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_81 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_100 (Embedding)      (None, 17, 900)      649800      ['input_81[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_120 (Conv1D)            (None, 17, 100)      270100      ['embedding_100[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_121 (Conv1D)            (None, 17, 100)      270100      ['embedding_100[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_122 (Conv1D)            (None, 17, 100)      270100      ['embedding_100[0][0]']          \n",
            "                                                                                                  \n",
            " input_82 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_300 (Glob  (None, 100)         0           ['conv1d_120[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_301 (Glob  (None, 100)         0           ['conv1d_121[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_302 (Glob  (None, 100)         0           ['conv1d_122[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_160 (Dense)              (None, 3)            33          ['input_82[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_300[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_301[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_302[0][0]'\n",
            "                                                                 , 'dense_160[0][0]']             \n",
            "                                                                                                  \n",
            " dense_161 (Dense)              (None, 1000)         304000      ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_140 (Dropout)          (None, 1000)         0           ['dense_161[0][0]']              \n",
            "                                                                                                  \n",
            " dense_162 (Dense)              (None, 500)          500500      ['dropout_140[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_141 (Dropout)          (None, 500)          0           ['dense_162[0][0]']              \n",
            "                                                                                                  \n",
            " dense_163 (Dense)              (None, 100)          50100       ['dropout_141[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_142 (Dropout)          (None, 100)          0           ['dense_163[0][0]']              \n",
            "                                                                                                  \n",
            " dense_164 (Dense)              (None, 1)            101         ['dropout_142[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.4299 - mae: 1.2470\n",
            "Epoch 1: val_loss improved from inf to 0.93799, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 41ms/step - loss: 4.4299 - mae: 1.2470 - val_loss: 0.9380 - val_mae: 0.3870\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0451 - mae: 0.6937\n",
            "Epoch 2: val_loss did not improve from 0.93799\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.0442 - mae: 0.6874 - val_loss: 0.9543 - val_mae: 0.8550\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9573 - mae: 0.6396\n",
            "Epoch 3: val_loss did not improve from 0.93799\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9568 - mae: 0.6468 - val_loss: 1.0495 - val_mae: 0.5093\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9110 - mae: 0.6161\n",
            "Epoch 4: val_loss improved from 0.93799 to 0.88589, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9110 - mae: 0.6161 - val_loss: 0.8859 - val_mae: 0.7755\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9269 - mae: 0.6295\n",
            "Epoch 5: val_loss improved from 0.88589 to 0.85084, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.9130 - mae: 0.6210 - val_loss: 0.8508 - val_mae: 0.4832\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8898 - mae: 0.6080\n",
            "Epoch 6: val_loss did not improve from 0.85084\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8925 - mae: 0.6154 - val_loss: 0.8896 - val_mae: 0.4199\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9060 - mae: 0.6173\n",
            "Epoch 7: val_loss did not improve from 0.85084\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9060 - mae: 0.6173 - val_loss: 0.9505 - val_mae: 0.4181\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9818 - mae: 0.6426\n",
            "Epoch 8: val_loss did not improve from 0.85084\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9862 - mae: 0.6503 - val_loss: 0.9738 - val_mae: 0.8790\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9128 - mae: 0.6320\n",
            "Epoch 9: val_loss improved from 0.85084 to 0.81869, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.9050 - mae: 0.6225 - val_loss: 0.8187 - val_mae: 0.5585\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8634 - mae: 0.6142\n",
            "Epoch 10: val_loss improved from 0.81869 to 0.81397, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8573 - mae: 0.6096 - val_loss: 0.8140 - val_mae: 0.5899\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8771 - mae: 0.6182\n",
            "Epoch 11: val_loss did not improve from 0.81397\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8834 - mae: 0.6135 - val_loss: 0.9593 - val_mae: 0.8682\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8683 - mae: 0.6209\n",
            "Epoch 12: val_loss did not improve from 0.81397\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8669 - mae: 0.6242 - val_loss: 0.8630 - val_mae: 0.4339\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9344 - mae: 0.6352\n",
            "Epoch 13: val_loss did not improve from 0.81397\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9276 - mae: 0.6357 - val_loss: 0.8633 - val_mae: 0.4297\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9038 - mae: 0.6246\n",
            "Epoch 14: val_loss did not improve from 0.81397\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9038 - mae: 0.6246 - val_loss: 0.8586 - val_mae: 0.4318\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8778 - mae: 0.6085\n",
            "Epoch 15: val_loss did not improve from 0.81397\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8778 - mae: 0.6085 - val_loss: 0.8325 - val_mae: 0.7107\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8927 - mae: 0.6176\n",
            "Epoch 16: val_loss did not improve from 0.81397\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8974 - mae: 0.6179 - val_loss: 0.9497 - val_mae: 0.4456\n",
            "Epoch 16: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8438117504119873, RMSE:0.9185922741889954, MAE:0.603941798210144, R2:0.015441060128791428\n",
            "[0.015441060128791428]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_141\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_83 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_101 (Embedding)      (None, 17, 900)      649800      ['input_83[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_123 (Conv1D)            (None, 17, 100)      270100      ['embedding_101[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_124 (Conv1D)            (None, 17, 100)      270100      ['embedding_101[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_125 (Conv1D)            (None, 17, 100)      270100      ['embedding_101[0][0]']          \n",
            "                                                                                                  \n",
            " input_84 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_303 (Glob  (None, 100)         0           ['conv1d_123[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_304 (Glob  (None, 100)         0           ['conv1d_124[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_305 (Glob  (None, 100)         0           ['conv1d_125[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_165 (Dense)              (None, 3)            33          ['input_84[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_303[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_304[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_305[0][0]'\n",
            "                                                                 , 'dense_165[0][0]']             \n",
            "                                                                                                  \n",
            " dense_166 (Dense)              (None, 1000)         304000      ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_143 (Dropout)          (None, 1000)         0           ['dense_166[0][0]']              \n",
            "                                                                                                  \n",
            " dense_167 (Dense)              (None, 500)          500500      ['dropout_143[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_144 (Dropout)          (None, 500)          0           ['dense_167[0][0]']              \n",
            "                                                                                                  \n",
            " dense_168 (Dense)              (None, 100)          50100       ['dropout_144[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_145 (Dropout)          (None, 100)          0           ['dense_168[0][0]']              \n",
            "                                                                                                  \n",
            " dense_169 (Dense)              (None, 1)            101         ['dropout_145[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.4769 - mae: 1.2873\n",
            "Epoch 1: val_loss improved from inf to 0.88716, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 42ms/step - loss: 4.3600 - mae: 1.2704 - val_loss: 0.8872 - val_mae: 0.4377\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9524 - mae: 0.6497\n",
            "Epoch 2: val_loss did not improve from 0.88716\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9524 - mae: 0.6497 - val_loss: 0.9556 - val_mae: 0.4116\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0367 - mae: 0.6928\n",
            "Epoch 3: val_loss did not improve from 0.88716\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.0400 - mae: 0.6878 - val_loss: 0.9739 - val_mae: 0.8755\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9815 - mae: 0.6687\n",
            "Epoch 4: val_loss did not improve from 0.88716\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9680 - mae: 0.6611 - val_loss: 0.9327 - val_mae: 0.3896\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9841 - mae: 0.6549\n",
            "Epoch 5: val_loss did not improve from 0.88716\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9837 - mae: 0.6473 - val_loss: 0.9223 - val_mae: 0.8246\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8568 - mae: 0.6165\n",
            "Epoch 6: val_loss improved from 0.88716 to 0.82027, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8571 - mae: 0.6168 - val_loss: 0.8203 - val_mae: 0.5857\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8792 - mae: 0.6195\n",
            "Epoch 7: val_loss improved from 0.82027 to 0.81950, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8758 - mae: 0.6157 - val_loss: 0.8195 - val_mae: 0.6235\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9297 - mae: 0.6385\n",
            "Epoch 8: val_loss did not improve from 0.81950\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9295 - mae: 0.6434 - val_loss: 0.8726 - val_mae: 0.4327\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8707 - mae: 0.6067\n",
            "Epoch 9: val_loss did not improve from 0.81950\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8732 - mae: 0.6046 - val_loss: 0.8606 - val_mae: 0.7479\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9746 - mae: 0.6756\n",
            "Epoch 10: val_loss improved from 0.81950 to 0.81592, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9658 - mae: 0.6676 - val_loss: 0.8159 - val_mae: 0.5517\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9357 - mae: 0.6444\n",
            "Epoch 11: val_loss improved from 0.81592 to 0.81483, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9322 - mae: 0.6442 - val_loss: 0.8148 - val_mae: 0.5465\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9020 - mae: 0.6282\n",
            "Epoch 12: val_loss did not improve from 0.81483\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9020 - mae: 0.6282 - val_loss: 0.8900 - val_mae: 0.3976\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9390 - mae: 0.6356\n",
            "Epoch 13: val_loss did not improve from 0.81483\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9356 - mae: 0.6394 - val_loss: 0.9342 - val_mae: 0.4197\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9063 - mae: 0.6201\n",
            "Epoch 14: val_loss improved from 0.81483 to 0.80402, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.9013 - mae: 0.6206 - val_loss: 0.8040 - val_mae: 0.5878\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8574 - mae: 0.6042\n",
            "Epoch 15: val_loss did not improve from 0.80402\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8604 - mae: 0.6058 - val_loss: 0.9099 - val_mae: 0.8237\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9383 - mae: 0.6497\n",
            "Epoch 16: val_loss did not improve from 0.80402\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9386 - mae: 0.6568 - val_loss: 0.9425 - val_mae: 0.4396\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8594 - mae: 0.6083\n",
            "Epoch 17: val_loss did not improve from 0.80402\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8731 - mae: 0.6091 - val_loss: 1.0662 - val_mae: 0.9587\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9782 - mae: 0.6719\n",
            "Epoch 18: val_loss did not improve from 0.80402\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9732 - mae: 0.6658 - val_loss: 0.8321 - val_mae: 0.4513\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8339 - mae: 0.5987\n",
            "Epoch 19: val_loss improved from 0.80402 to 0.79274, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8312 - mae: 0.5967 - val_loss: 0.7927 - val_mae: 0.6038\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8693 - mae: 0.6061\n",
            "Epoch 20: val_loss did not improve from 0.79274\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8693 - mae: 0.6061 - val_loss: 0.9097 - val_mae: 0.8304\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9015 - mae: 0.6160\n",
            "Epoch 21: val_loss did not improve from 0.79274\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8992 - mae: 0.6199 - val_loss: 0.8468 - val_mae: 0.4115\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8295 - mae: 0.5895\n",
            "Epoch 22: val_loss improved from 0.79274 to 0.78316, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8293 - mae: 0.5911 - val_loss: 0.7832 - val_mae: 0.5733\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8689 - mae: 0.5998\n",
            "Epoch 23: val_loss did not improve from 0.78316\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8689 - mae: 0.5998 - val_loss: 0.9848 - val_mae: 0.9015\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8753 - mae: 0.6120\n",
            "Epoch 24: val_loss improved from 0.78316 to 0.78223, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8683 - mae: 0.6096 - val_loss: 0.7822 - val_mae: 0.6266\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8551 - mae: 0.5967\n",
            "Epoch 25: val_loss did not improve from 0.78223\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8551 - mae: 0.5967 - val_loss: 0.8876 - val_mae: 0.8144\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8786 - mae: 0.6093\n",
            "Epoch 26: val_loss did not improve from 0.78223\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8757 - mae: 0.6112 - val_loss: 0.8229 - val_mae: 0.4151\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8548 - mae: 0.5932\n",
            "Epoch 27: val_loss did not improve from 0.78223\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8548 - mae: 0.6005 - val_loss: 0.9916 - val_mae: 0.5348\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8559 - mae: 0.6036\n",
            "Epoch 28: val_loss improved from 0.78223 to 0.76551, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8523 - mae: 0.6028 - val_loss: 0.7655 - val_mae: 0.5467\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8733 - mae: 0.6044\n",
            "Epoch 29: val_loss did not improve from 0.76551\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8735 - mae: 0.6056 - val_loss: 0.7676 - val_mae: 0.6228\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8062 - mae: 0.5837\n",
            "Epoch 30: val_loss did not improve from 0.76551\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8106 - mae: 0.5845 - val_loss: 0.8104 - val_mae: 0.4060\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8671 - mae: 0.5928\n",
            "Epoch 31: val_loss did not improve from 0.76551\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8690 - mae: 0.6016 - val_loss: 0.9136 - val_mae: 0.4809\n",
            "Epoch 32/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9317 - mae: 0.6559\n",
            "Epoch 32: val_loss did not improve from 0.76551\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9254 - mae: 0.6485 - val_loss: 0.7874 - val_mae: 0.6950\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8804 - mae: 0.6204\n",
            "Epoch 33: val_loss did not improve from 0.76551\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8804 - mae: 0.6204 - val_loss: 0.8932 - val_mae: 0.8302\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8406 - mae: 0.5826\n",
            "Epoch 34: val_loss did not improve from 0.76551\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8415 - mae: 0.5897 - val_loss: 0.8894 - val_mae: 0.4712\n",
            "Epoch 34: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7999609708786011, RMSE:0.8944053649902344, MAE:0.5628620982170105, R2:0.06660595280856729\n",
            "[0.015441060128791428, 0.06660595280856729]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_142\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_85 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_102 (Embedding)      (None, 17, 900)      649800      ['input_85[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_126 (Conv1D)            (None, 17, 100)      270100      ['embedding_102[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_127 (Conv1D)            (None, 17, 100)      270100      ['embedding_102[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_128 (Conv1D)            (None, 17, 100)      270100      ['embedding_102[0][0]']          \n",
            "                                                                                                  \n",
            " input_86 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_306 (Glob  (None, 100)         0           ['conv1d_126[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_307 (Glob  (None, 100)         0           ['conv1d_127[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_308 (Glob  (None, 100)         0           ['conv1d_128[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_170 (Dense)              (None, 3)            33          ['input_86[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_306[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_307[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_308[0][0]'\n",
            "                                                                 , 'dense_170[0][0]']             \n",
            "                                                                                                  \n",
            " dense_171 (Dense)              (None, 1000)         304000      ['concatenate_42[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_146 (Dropout)          (None, 1000)         0           ['dense_171[0][0]']              \n",
            "                                                                                                  \n",
            " dense_172 (Dense)              (None, 500)          500500      ['dropout_146[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_147 (Dropout)          (None, 500)          0           ['dense_172[0][0]']              \n",
            "                                                                                                  \n",
            " dense_173 (Dense)              (None, 100)          50100       ['dropout_147[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_148 (Dropout)          (None, 100)          0           ['dense_173[0][0]']              \n",
            "                                                                                                  \n",
            " dense_174 (Dense)              (None, 1)            101         ['dropout_148[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 5.7179 - mae: 1.4865\n",
            "Epoch 1: val_loss improved from inf to 0.95863, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 42ms/step - loss: 5.5646 - mae: 1.4536 - val_loss: 0.9586 - val_mae: 0.8584\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9056 - mae: 0.6354\n",
            "Epoch 2: val_loss improved from 0.95863 to 0.82807, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9054 - mae: 0.6344 - val_loss: 0.8281 - val_mae: 0.5913\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9712 - mae: 0.6778\n",
            "Epoch 3: val_loss did not improve from 0.82807\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9869 - mae: 0.6798 - val_loss: 1.1639 - val_mae: 1.0180\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9967 - mae: 0.6968\n",
            "Epoch 4: val_loss did not improve from 0.82807\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9967 - mae: 0.6968 - val_loss: 0.8519 - val_mae: 0.7181\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9014 - mae: 0.6405\n",
            "Epoch 5: val_loss did not improve from 0.82807\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9127 - mae: 0.6408 - val_loss: 1.0558 - val_mae: 0.9452\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9466 - mae: 0.6489\n",
            "Epoch 6: val_loss improved from 0.82807 to 0.82321, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9344 - mae: 0.6418 - val_loss: 0.8232 - val_mae: 0.5558\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8978 - mae: 0.6261\n",
            "Epoch 7: val_loss did not improve from 0.82321\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9021 - mae: 0.6223 - val_loss: 0.9560 - val_mae: 0.8624\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9382 - mae: 0.6317\n",
            "Epoch 8: val_loss did not improve from 0.82321\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9327 - mae: 0.6358 - val_loss: 0.9890 - val_mae: 0.4659\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9215 - mae: 0.6230\n",
            "Epoch 9: val_loss did not improve from 0.82321\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.9215 - mae: 0.6230 - val_loss: 0.8402 - val_mae: 0.7103\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9185 - mae: 0.6397\n",
            "Epoch 10: val_loss did not improve from 0.82321\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9185 - mae: 0.6397 - val_loss: 0.8498 - val_mae: 0.7330\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8635 - mae: 0.6196\n",
            "Epoch 11: val_loss did not improve from 0.82321\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8699 - mae: 0.6147 - val_loss: 0.9543 - val_mae: 0.8651\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8888 - mae: 0.6153\n",
            "Epoch 12: val_loss improved from 0.82321 to 0.81377, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 0.8867 - mae: 0.6163 - val_loss: 0.8138 - val_mae: 0.5323\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8787 - mae: 0.6172\n",
            "Epoch 13: val_loss improved from 0.81377 to 0.80869, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 40ms/step - loss: 0.8787 - mae: 0.6172 - val_loss: 0.8087 - val_mae: 0.5434\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8984 - mae: 0.6265\n",
            "Epoch 14: val_loss improved from 0.80869 to 0.80499, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 0.8984 - mae: 0.6265 - val_loss: 0.8050 - val_mae: 0.5485\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8845 - mae: 0.6079\n",
            "Epoch 15: val_loss did not improve from 0.80499\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8845 - mae: 0.6079 - val_loss: 0.9120 - val_mae: 0.8275\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8997 - mae: 0.6313\n",
            "Epoch 16: val_loss improved from 0.80499 to 0.79732, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 61ms/step - loss: 0.8997 - mae: 0.6313 - val_loss: 0.7973 - val_mae: 0.5651\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8750 - mae: 0.6160\n",
            "Epoch 17: val_loss improved from 0.79732 to 0.79666, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 45ms/step - loss: 0.8750 - mae: 0.6160 - val_loss: 0.7967 - val_mae: 0.6235\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8368 - mae: 0.6010\n",
            "Epoch 18: val_loss did not improve from 0.79666\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8368 - mae: 0.6010 - val_loss: 0.8012 - val_mae: 0.5120\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8863 - mae: 0.6152\n",
            "Epoch 19: val_loss did not improve from 0.79666\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8863 - mae: 0.6152 - val_loss: 1.0242 - val_mae: 0.5361\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8625 - mae: 0.6054\n",
            "Epoch 20: val_loss did not improve from 0.79666\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8589 - mae: 0.5992 - val_loss: 0.8014 - val_mae: 0.6714\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8392 - mae: 0.5979\n",
            "Epoch 21: val_loss improved from 0.79666 to 0.79003, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.8392 - mae: 0.5979 - val_loss: 0.7900 - val_mae: 0.5143\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9168 - mae: 0.6377\n",
            "Epoch 22: val_loss did not improve from 0.79003\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.9149 - mae: 0.6426 - val_loss: 0.9140 - val_mae: 0.4403\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8458 - mae: 0.5993\n",
            "Epoch 23: val_loss improved from 0.79003 to 0.77525, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.8470 - mae: 0.6006 - val_loss: 0.7753 - val_mae: 0.5786\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8105 - mae: 0.5891\n",
            "Epoch 24: val_loss did not improve from 0.77525\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8116 - mae: 0.5915 - val_loss: 0.7800 - val_mae: 0.5090\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9003 - mae: 0.6316\n",
            "Epoch 25: val_loss did not improve from 0.77525\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8947 - mae: 0.6325 - val_loss: 0.8154 - val_mae: 0.4209\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8298 - mae: 0.5875\n",
            "Epoch 26: val_loss did not improve from 0.77525\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8298 - mae: 0.5875 - val_loss: 0.8850 - val_mae: 0.4303\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8465 - mae: 0.5904\n",
            "Epoch 27: val_loss did not improve from 0.77525\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8465 - mae: 0.5904 - val_loss: 0.8556 - val_mae: 0.7855\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8842 - mae: 0.6277\n",
            "Epoch 28: val_loss did not improve from 0.77525\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8842 - mae: 0.6277 - val_loss: 0.8453 - val_mae: 0.3942\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8724 - mae: 0.6179\n",
            "Epoch 29: val_loss did not improve from 0.77525\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8724 - mae: 0.6179 - val_loss: 0.7961 - val_mae: 0.4193\n",
            "Epoch 29: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8076046109199524, RMSE:0.8986682295799255, MAE:0.5932631492614746, R2:0.057687555104453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n",
            "Model: \"model_143\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_87 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_103 (Embedding)      (None, 17, 900)      649800      ['input_87[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_129 (Conv1D)            (None, 17, 100)      270100      ['embedding_103[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_130 (Conv1D)            (None, 17, 100)      270100      ['embedding_103[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_131 (Conv1D)            (None, 17, 100)      270100      ['embedding_103[0][0]']          \n",
            "                                                                                                  \n",
            " input_88 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_309 (Glob  (None, 100)         0           ['conv1d_129[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_310 (Glob  (None, 100)         0           ['conv1d_130[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_311 (Glob  (None, 100)         0           ['conv1d_131[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_175 (Dense)              (None, 3)            33          ['input_88[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_309[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_310[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_311[0][0]'\n",
            "                                                                 , 'dense_175[0][0]']             \n",
            "                                                                                                  \n",
            " dense_176 (Dense)              (None, 1000)         304000      ['concatenate_43[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_149 (Dropout)          (None, 1000)         0           ['dense_176[0][0]']              \n",
            "                                                                                                  \n",
            " dense_177 (Dense)              (None, 500)          500500      ['dropout_149[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_150 (Dropout)          (None, 500)          0           ['dense_177[0][0]']              \n",
            "                                                                                                  \n",
            " dense_178 (Dense)              (None, 100)          50100       ['dropout_150[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_151 (Dropout)          (None, 100)          0           ['dense_178[0][0]']              \n",
            "                                                                                                  \n",
            " dense_179 (Dense)              (None, 1)            101         ['dropout_151[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.9433 - mae: 1.3286\n",
            "Epoch 1: val_loss improved from inf to 0.90373, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 4s 67ms/step - loss: 4.8112 - mae: 1.3008 - val_loss: 0.9037 - val_mae: 0.8075\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9208 - mae: 0.6356\n",
            "Epoch 2: val_loss did not improve from 0.90373\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.9208 - mae: 0.6356 - val_loss: 0.9491 - val_mae: 0.4306\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8896 - mae: 0.6141\n",
            "Epoch 3: val_loss improved from 0.90373 to 0.86682, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 62ms/step - loss: 0.8896 - mae: 0.6141 - val_loss: 0.8668 - val_mae: 0.7731\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8653 - mae: 0.6094\n",
            "Epoch 4: val_loss improved from 0.86682 to 0.80780, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8669 - mae: 0.6085 - val_loss: 0.8078 - val_mae: 0.6603\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8601 - mae: 0.6067\n",
            "Epoch 5: val_loss did not improve from 0.80780\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8612 - mae: 0.6072 - val_loss: 0.8665 - val_mae: 0.3983\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8714 - mae: 0.6020\n",
            "Epoch 6: val_loss did not improve from 0.80780\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8711 - mae: 0.6037 - val_loss: 0.8537 - val_mae: 0.7677\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9067 - mae: 0.6217\n",
            "Epoch 7: val_loss did not improve from 0.80780\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9022 - mae: 0.6198 - val_loss: 0.8186 - val_mae: 0.7152\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9011 - mae: 0.6118\n",
            "Epoch 8: val_loss did not improve from 0.80780\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9025 - mae: 0.6201 - val_loss: 0.8931 - val_mae: 0.4030\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9103 - mae: 0.6233\n",
            "Epoch 9: val_loss did not improve from 0.80780\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9103 - mae: 0.6233 - val_loss: 0.9284 - val_mae: 0.8584\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9050 - mae: 0.6320\n",
            "Epoch 10: val_loss did not improve from 0.80780\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9135 - mae: 0.6420 - val_loss: 0.9041 - val_mae: 0.8369\n",
            "Epoch 10: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8362711071968079, RMSE:0.914478600025177, MAE:0.669856607913971, R2:0.024239409486910612\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_144\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_89 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_104 (Embedding)      (None, 17, 900)      649800      ['input_89[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_132 (Conv1D)            (None, 17, 100)      270100      ['embedding_104[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_133 (Conv1D)            (None, 17, 100)      270100      ['embedding_104[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_134 (Conv1D)            (None, 17, 100)      270100      ['embedding_104[0][0]']          \n",
            "                                                                                                  \n",
            " input_90 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_312 (Glob  (None, 100)         0           ['conv1d_132[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_313 (Glob  (None, 100)         0           ['conv1d_133[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_314 (Glob  (None, 100)         0           ['conv1d_134[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_180 (Dense)              (None, 3)            33          ['input_90[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_312[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_313[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_314[0][0]'\n",
            "                                                                 , 'dense_180[0][0]']             \n",
            "                                                                                                  \n",
            " dense_181 (Dense)              (None, 1000)         304000      ['concatenate_44[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_152 (Dropout)          (None, 1000)         0           ['dense_181[0][0]']              \n",
            "                                                                                                  \n",
            " dense_182 (Dense)              (None, 500)          500500      ['dropout_152[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_153 (Dropout)          (None, 500)          0           ['dense_182[0][0]']              \n",
            "                                                                                                  \n",
            " dense_183 (Dense)              (None, 100)          50100       ['dropout_153[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_154 (Dropout)          (None, 100)          0           ['dense_183[0][0]']              \n",
            "                                                                                                  \n",
            " dense_184 (Dense)              (None, 1)            101         ['dropout_154[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3.6724 - mae: 1.1080\n",
            "Epoch 1: val_loss improved from inf to 0.85481, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 43ms/step - loss: 3.5826 - mae: 1.0913 - val_loss: 0.8548 - val_mae: 0.7153\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.0763 - mae: 0.7328\n",
            "Epoch 2: val_loss did not improve from 0.85481\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 1.0763 - mae: 0.7328 - val_loss: 0.9908 - val_mae: 0.4520\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0480 - mae: 0.7036\n",
            "Epoch 3: val_loss did not improve from 0.85481\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 1.0452 - mae: 0.6972 - val_loss: 0.8746 - val_mae: 0.7585\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8910 - mae: 0.6208\n",
            "Epoch 4: val_loss did not improve from 0.85481\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8857 - mae: 0.6195 - val_loss: 0.8572 - val_mae: 0.4696\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0288 - mae: 0.7036\n",
            "Epoch 5: val_loss did not improve from 0.85481\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 1.0233 - mae: 0.7034 - val_loss: 0.8587 - val_mae: 0.4627\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8998 - mae: 0.6241\n",
            "Epoch 6: val_loss did not improve from 0.85481\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8998 - mae: 0.6241 - val_loss: 0.8681 - val_mae: 0.7553\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8566 - mae: 0.6119\n",
            "Epoch 7: val_loss did not improve from 0.85481\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8564 - mae: 0.6168 - val_loss: 0.8752 - val_mae: 0.4293\n",
            "Epoch 7: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8781919479370117, RMSE:0.9371189475059509, MAE:0.7237956523895264, R2:-0.02467382782391736\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_145\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_91 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_105 (Embedding)      (None, 17, 900)      649800      ['input_91[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_135 (Conv1D)            (None, 17, 100)      270100      ['embedding_105[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_136 (Conv1D)            (None, 17, 100)      270100      ['embedding_105[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_137 (Conv1D)            (None, 17, 100)      270100      ['embedding_105[0][0]']          \n",
            "                                                                                                  \n",
            " input_92 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_315 (Glob  (None, 100)         0           ['conv1d_135[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_316 (Glob  (None, 100)         0           ['conv1d_136[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_317 (Glob  (None, 100)         0           ['conv1d_137[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_185 (Dense)              (None, 3)            33          ['input_92[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_315[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_316[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_317[0][0]'\n",
            "                                                                 , 'dense_185[0][0]']             \n",
            "                                                                                                  \n",
            " dense_186 (Dense)              (None, 1000)         304000      ['concatenate_45[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_155 (Dropout)          (None, 1000)         0           ['dense_186[0][0]']              \n",
            "                                                                                                  \n",
            " dense_187 (Dense)              (None, 500)          500500      ['dropout_155[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_156 (Dropout)          (None, 500)          0           ['dense_187[0][0]']              \n",
            "                                                                                                  \n",
            " dense_188 (Dense)              (None, 100)          50100       ['dropout_156[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_157 (Dropout)          (None, 100)          0           ['dense_188[0][0]']              \n",
            "                                                                                                  \n",
            " dense_189 (Dense)              (None, 1)            101         ['dropout_157[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 5.8606 - mae: 1.4669\n",
            "Epoch 1: val_loss improved from inf to 0.84950, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 42ms/step - loss: 5.7036 - mae: 1.4466 - val_loss: 0.8495 - val_mae: 0.5102\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8953 - mae: 0.6180\n",
            "Epoch 2: val_loss did not improve from 0.84950\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8973 - mae: 0.6194 - val_loss: 0.9157 - val_mae: 0.4044\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8898 - mae: 0.6098\n",
            "Epoch 3: val_loss did not improve from 0.84950\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8930 - mae: 0.6124 - val_loss: 0.8959 - val_mae: 0.7869\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9215 - mae: 0.6224\n",
            "Epoch 4: val_loss improved from 0.84950 to 0.83297, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9185 - mae: 0.6226 - val_loss: 0.8330 - val_mae: 0.5431\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9070 - mae: 0.6141\n",
            "Epoch 5: val_loss did not improve from 0.83297\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9039 - mae: 0.6188 - val_loss: 0.9873 - val_mae: 0.4518\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8989 - mae: 0.6145\n",
            "Epoch 6: val_loss did not improve from 0.83297\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8958 - mae: 0.6128 - val_loss: 0.8610 - val_mae: 0.7372\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8664 - mae: 0.6153\n",
            "Epoch 7: val_loss improved from 0.83297 to 0.82614, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8669 - mae: 0.6176 - val_loss: 0.8261 - val_mae: 0.5481\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8957 - mae: 0.6220\n",
            "Epoch 8: val_loss did not improve from 0.82614\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8957 - mae: 0.6220 - val_loss: 0.8474 - val_mae: 0.7157\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8919 - mae: 0.6128\n",
            "Epoch 9: val_loss did not improve from 0.82614\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8923 - mae: 0.6181 - val_loss: 0.8679 - val_mae: 0.4422\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9164 - mae: 0.6233\n",
            "Epoch 10: val_loss did not improve from 0.82614\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.9164 - mae: 0.6233 - val_loss: 1.0573 - val_mae: 0.5297\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8754 - mae: 0.6116\n",
            "Epoch 11: val_loss improved from 0.82614 to 0.81765, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8768 - mae: 0.6145 - val_loss: 0.8176 - val_mae: 0.5508\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9213 - mae: 0.6128\n",
            "Epoch 12: val_loss did not improve from 0.81765\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9264 - mae: 0.6181 - val_loss: 0.8770 - val_mae: 0.7755\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8698 - mae: 0.6163\n",
            "Epoch 13: val_loss did not improve from 0.81765\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8708 - mae: 0.6115 - val_loss: 0.8528 - val_mae: 0.7399\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8825 - mae: 0.6165\n",
            "Epoch 14: val_loss improved from 0.81765 to 0.81611, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8825 - mae: 0.6165 - val_loss: 0.8161 - val_mae: 0.6546\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8483 - mae: 0.6091\n",
            "Epoch 15: val_loss improved from 0.81611 to 0.80908, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8487 - mae: 0.6085 - val_loss: 0.8091 - val_mae: 0.6282\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8907 - mae: 0.6221\n",
            "Epoch 16: val_loss did not improve from 0.80908\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8892 - mae: 0.6153 - val_loss: 0.8230 - val_mae: 0.6868\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8640 - mae: 0.6072\n",
            "Epoch 17: val_loss did not improve from 0.80908\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8640 - mae: 0.6072 - val_loss: 0.8658 - val_mae: 0.7686\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8831 - mae: 0.6228\n",
            "Epoch 18: val_loss did not improve from 0.80908\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8925 - mae: 0.6186 - val_loss: 0.9553 - val_mae: 0.8698\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8677 - mae: 0.6096\n",
            "Epoch 19: val_loss improved from 0.80908 to 0.80890, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8640 - mae: 0.6078 - val_loss: 0.8089 - val_mae: 0.6607\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8738 - mae: 0.6049\n",
            "Epoch 20: val_loss improved from 0.80890 to 0.80737, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8796 - mae: 0.6136 - val_loss: 0.8074 - val_mae: 0.5111\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8597 - mae: 0.6049\n",
            "Epoch 21: val_loss did not improve from 0.80737\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8598 - mae: 0.6059 - val_loss: 0.8098 - val_mae: 0.6753\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8427 - mae: 0.6058\n",
            "Epoch 22: val_loss improved from 0.80737 to 0.80481, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8408 - mae: 0.6010 - val_loss: 0.8048 - val_mae: 0.6667\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8505 - mae: 0.5980\n",
            "Epoch 23: val_loss did not improve from 0.80481\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8551 - mae: 0.5983 - val_loss: 0.8284 - val_mae: 0.7244\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8656 - mae: 0.6002\n",
            "Epoch 24: val_loss did not improve from 0.80481\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8657 - mae: 0.6071 - val_loss: 0.9430 - val_mae: 0.4612\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8764 - mae: 0.6117\n",
            "Epoch 25: val_loss improved from 0.80481 to 0.78669, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8762 - mae: 0.6107 - val_loss: 0.7867 - val_mae: 0.6226\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8549 - mae: 0.6034\n",
            "Epoch 26: val_loss did not improve from 0.78669\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8549 - mae: 0.6034 - val_loss: 0.8551 - val_mae: 0.7727\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8276 - mae: 0.6005\n",
            "Epoch 27: val_loss improved from 0.78669 to 0.78313, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8276 - mae: 0.6005 - val_loss: 0.7831 - val_mae: 0.6310\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8329 - mae: 0.5979\n",
            "Epoch 28: val_loss improved from 0.78313 to 0.77971, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8283 - mae: 0.5927 - val_loss: 0.7797 - val_mae: 0.6271\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8242 - mae: 0.5875\n",
            "Epoch 29: val_loss did not improve from 0.77971\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8251 - mae: 0.5933 - val_loss: 0.8284 - val_mae: 0.4076\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8465 - mae: 0.5931\n",
            "Epoch 30: val_loss did not improve from 0.77971\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8419 - mae: 0.5910 - val_loss: 0.7890 - val_mae: 0.6741\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8116 - mae: 0.5890\n",
            "Epoch 31: val_loss did not improve from 0.77971\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8167 - mae: 0.5853 - val_loss: 0.8855 - val_mae: 0.8169\n",
            "Epoch 32/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8605 - mae: 0.6060\n",
            "Epoch 32: val_loss improved from 0.77971 to 0.76088, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8593 - mae: 0.6057 - val_loss: 0.7609 - val_mae: 0.5707\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8209 - mae: 0.5858\n",
            "Epoch 33: val_loss improved from 0.76088 to 0.75742, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8194 - mae: 0.5848 - val_loss: 0.7574 - val_mae: 0.5592\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8265 - mae: 0.5800\n",
            "Epoch 34: val_loss did not improve from 0.75742\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8265 - mae: 0.5800 - val_loss: 0.7691 - val_mae: 0.6488\n",
            "Epoch 35/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8738 - mae: 0.6088\n",
            "Epoch 35: val_loss did not improve from 0.75742\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8716 - mae: 0.6063 - val_loss: 0.7659 - val_mae: 0.4738\n",
            "Epoch 36/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8019 - mae: 0.5763\n",
            "Epoch 36: val_loss did not improve from 0.75742\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.7971 - mae: 0.5732 - val_loss: 0.7732 - val_mae: 0.6721\n",
            "Epoch 37/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8459 - mae: 0.5833\n",
            "Epoch 37: val_loss did not improve from 0.75742\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8459 - mae: 0.5874 - val_loss: 0.8076 - val_mae: 0.3817\n",
            "Epoch 38/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8156 - mae: 0.5709\n",
            "Epoch 38: val_loss improved from 0.75742 to 0.74647, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8169 - mae: 0.5726 - val_loss: 0.7465 - val_mae: 0.6100\n",
            "Epoch 39/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8277 - mae: 0.5806\n",
            "Epoch 39: val_loss did not improve from 0.74647\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8267 - mae: 0.5759 - val_loss: 0.7800 - val_mae: 0.6977\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8209 - mae: 0.5891\n",
            "Epoch 40: val_loss improved from 0.74647 to 0.73437, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8209 - mae: 0.5891 - val_loss: 0.7344 - val_mae: 0.5759\n",
            "Epoch 41/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7900 - mae: 0.5708\n",
            "Epoch 41: val_loss did not improve from 0.73437\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7938 - mae: 0.5684 - val_loss: 0.8823 - val_mae: 0.8236\n",
            "Epoch 42/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8313 - mae: 0.5970\n",
            "Epoch 42: val_loss did not improve from 0.73437\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8267 - mae: 0.5967 - val_loss: 0.7419 - val_mae: 0.4541\n",
            "Epoch 43/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8147 - mae: 0.5833\n",
            "Epoch 43: val_loss did not improve from 0.73437\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8180 - mae: 0.5914 - val_loss: 0.8628 - val_mae: 0.4807\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8275 - mae: 0.5811\n",
            "Epoch 44: val_loss improved from 0.73437 to 0.73060, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8275 - mae: 0.5811 - val_loss: 0.7306 - val_mae: 0.6106\n",
            "Epoch 45/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8706 - mae: 0.6215\n",
            "Epoch 45: val_loss did not improve from 0.73060\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8682 - mae: 0.6267 - val_loss: 0.9528 - val_mae: 0.5753\n",
            "Epoch 46/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8464 - mae: 0.5941\n",
            "Epoch 46: val_loss did not improve from 0.73060\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8459 - mae: 0.5999 - val_loss: 0.7633 - val_mae: 0.6886\n",
            "Epoch 47/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7756 - mae: 0.5552\n",
            "Epoch 47: val_loss improved from 0.73060 to 0.71096, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7744 - mae: 0.5551 - val_loss: 0.7110 - val_mae: 0.5271\n",
            "Epoch 48/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8338 - mae: 0.5953\n",
            "Epoch 48: val_loss did not improve from 0.71096\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8284 - mae: 0.5880 - val_loss: 0.7343 - val_mae: 0.6400\n",
            "Epoch 49/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9194 - mae: 0.6642\n",
            "Epoch 49: val_loss improved from 0.71096 to 0.70750, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9141 - mae: 0.6609 - val_loss: 0.7075 - val_mae: 0.5063\n",
            "Epoch 50/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7835 - mae: 0.5571\n",
            "Epoch 50: val_loss did not improve from 0.70750\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7916 - mae: 0.5590 - val_loss: 0.8688 - val_mae: 0.8130\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8154 - mae: 0.5864\n",
            "Epoch 51: val_loss did not improve from 0.70750\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8154 - mae: 0.5864 - val_loss: 0.7213 - val_mae: 0.6197\n",
            "Epoch 52/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7766 - mae: 0.5423\n",
            "Epoch 52: val_loss improved from 0.70750 to 0.70694, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.7753 - mae: 0.5437 - val_loss: 0.7069 - val_mae: 0.4567\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7495 - mae: 0.5352\n",
            "Epoch 53: val_loss improved from 0.70694 to 0.69725, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.7495 - mae: 0.5352 - val_loss: 0.6972 - val_mae: 0.4915\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8472 - mae: 0.6116\n",
            "Epoch 54: val_loss did not improve from 0.69725\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8472 - mae: 0.6116 - val_loss: 0.7064 - val_mae: 0.5911\n",
            "Epoch 55/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7585 - mae: 0.5356\n",
            "Epoch 55: val_loss did not improve from 0.69725\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.7538 - mae: 0.5350 - val_loss: 0.6999 - val_mae: 0.4495\n",
            "Epoch 56/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7625 - mae: 0.5423\n",
            "Epoch 56: val_loss improved from 0.69725 to 0.68931, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.7633 - mae: 0.5407 - val_loss: 0.6893 - val_mae: 0.5181\n",
            "Epoch 57/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9006 - mae: 0.6618\n",
            "Epoch 57: val_loss did not improve from 0.68931\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8934 - mae: 0.6593 - val_loss: 0.7385 - val_mae: 0.3830\n",
            "Epoch 58/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7465 - mae: 0.5241\n",
            "Epoch 58: val_loss improved from 0.68931 to 0.68576, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7479 - mae: 0.5273 - val_loss: 0.6858 - val_mae: 0.4993\n",
            "Epoch 59/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8779 - mae: 0.6382\n",
            "Epoch 59: val_loss did not improve from 0.68576\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8696 - mae: 0.6318 - val_loss: 0.6865 - val_mae: 0.4766\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8617 - mae: 0.6383\n",
            "Epoch 60: val_loss did not improve from 0.68576\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8617 - mae: 0.6383 - val_loss: 0.6966 - val_mae: 0.5867\n",
            "Epoch 61/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7943 - mae: 0.5852\n",
            "Epoch 61: val_loss did not improve from 0.68576\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7985 - mae: 0.5821 - val_loss: 0.7722 - val_mae: 0.7183\n",
            "Epoch 62/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8805 - mae: 0.6523\n",
            "Epoch 62: val_loss did not improve from 0.68576\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8684 - mae: 0.6429 - val_loss: 0.7695 - val_mae: 0.4437\n",
            "Epoch 63/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7591 - mae: 0.5367\n",
            "Epoch 63: val_loss did not improve from 0.68576\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.7586 - mae: 0.5354 - val_loss: 0.6966 - val_mae: 0.5980\n",
            "Epoch 64/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7536 - mae: 0.5419\n",
            "Epoch 64: val_loss did not improve from 0.68576\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7649 - mae: 0.5529 - val_loss: 0.9980 - val_mae: 0.6629\n",
            "Epoch 64: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7275179028511047, RMSE:0.8529465794563293, MAE:0.517865777015686, R2:0.15113259425115744\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_146\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_93 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_106 (Embedding)      (None, 17, 900)      649800      ['input_93[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_138 (Conv1D)            (None, 17, 100)      270100      ['embedding_106[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_139 (Conv1D)            (None, 17, 100)      270100      ['embedding_106[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_140 (Conv1D)            (None, 17, 100)      270100      ['embedding_106[0][0]']          \n",
            "                                                                                                  \n",
            " input_94 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_318 (Glob  (None, 100)         0           ['conv1d_138[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_319 (Glob  (None, 100)         0           ['conv1d_139[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_320 (Glob  (None, 100)         0           ['conv1d_140[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_190 (Dense)              (None, 3)            33          ['input_94[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_318[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_319[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_320[0][0]'\n",
            "                                                                 , 'dense_190[0][0]']             \n",
            "                                                                                                  \n",
            " dense_191 (Dense)              (None, 1000)         304000      ['concatenate_46[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_158 (Dropout)          (None, 1000)         0           ['dense_191[0][0]']              \n",
            "                                                                                                  \n",
            " dense_192 (Dense)              (None, 500)          500500      ['dropout_158[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_159 (Dropout)          (None, 500)          0           ['dense_192[0][0]']              \n",
            "                                                                                                  \n",
            " dense_193 (Dense)              (None, 100)          50100       ['dropout_159[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_160 (Dropout)          (None, 100)          0           ['dense_193[0][0]']              \n",
            "                                                                                                  \n",
            " dense_194 (Dense)              (None, 1)            101         ['dropout_160[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 4.4587 - mae: 1.2688\n",
            "Epoch 1: val_loss improved from inf to 0.86790, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 42ms/step - loss: 4.2053 - mae: 1.2200 - val_loss: 0.8679 - val_mae: 0.4748\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9806 - mae: 0.6525\n",
            "Epoch 2: val_loss improved from 0.86790 to 0.84738, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9775 - mae: 0.6490 - val_loss: 0.8474 - val_mae: 0.6918\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8839 - mae: 0.6182\n",
            "Epoch 3: val_loss did not improve from 0.84738\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8855 - mae: 0.6208 - val_loss: 0.8538 - val_mae: 0.4927\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9549 - mae: 0.6544\n",
            "Epoch 4: val_loss improved from 0.84738 to 0.83957, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9475 - mae: 0.6439 - val_loss: 0.8396 - val_mae: 0.6805\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9082 - mae: 0.6157\n",
            "Epoch 5: val_loss improved from 0.83957 to 0.82874, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9069 - mae: 0.6160 - val_loss: 0.8287 - val_mae: 0.6417\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8965 - mae: 0.6279\n",
            "Epoch 6: val_loss did not improve from 0.82874\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9057 - mae: 0.6291 - val_loss: 1.1170 - val_mae: 0.5705\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9590 - mae: 0.6446\n",
            "Epoch 7: val_loss did not improve from 0.82874\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9476 - mae: 0.6394 - val_loss: 0.8294 - val_mae: 0.6586\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9085 - mae: 0.6315\n",
            "Epoch 8: val_loss improved from 0.82874 to 0.82176, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9050 - mae: 0.6289 - val_loss: 0.8218 - val_mae: 0.6225\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9652 - mae: 0.6499\n",
            "Epoch 9: val_loss did not improve from 0.82176\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9636 - mae: 0.6459 - val_loss: 0.9462 - val_mae: 0.4146\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9046 - mae: 0.6203\n",
            "Epoch 10: val_loss did not improve from 0.82176\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9084 - mae: 0.6144 - val_loss: 0.9460 - val_mae: 0.8528\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8792 - mae: 0.6154\n",
            "Epoch 11: val_loss improved from 0.82176 to 0.81556, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8811 - mae: 0.6188 - val_loss: 0.8156 - val_mae: 0.5820\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9458 - mae: 0.6428\n",
            "Epoch 12: val_loss did not improve from 0.81556\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.9465 - mae: 0.6393 - val_loss: 0.8317 - val_mae: 0.5013\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8504 - mae: 0.6047\n",
            "Epoch 13: val_loss did not improve from 0.81556\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8528 - mae: 0.6072 - val_loss: 0.8185 - val_mae: 0.6517\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9402 - mae: 0.6316\n",
            "Epoch 14: val_loss did not improve from 0.81556\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9430 - mae: 0.6402 - val_loss: 0.9883 - val_mae: 0.4732\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9430 - mae: 0.6371\n",
            "Epoch 15: val_loss did not improve from 0.81556\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9377 - mae: 0.6372 - val_loss: 0.8277 - val_mae: 0.4942\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8534 - mae: 0.6063\n",
            "Epoch 16: val_loss did not improve from 0.81556\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8576 - mae: 0.6024 - val_loss: 0.8788 - val_mae: 0.7833\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8579 - mae: 0.6161\n",
            "Epoch 17: val_loss did not improve from 0.81556\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8568 - mae: 0.6076 - val_loss: 0.8498 - val_mae: 0.7423\n",
            "Epoch 17: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8467437028884888, RMSE:0.9201867580413818, MAE:0.5964784622192383, R2:0.012020036471809936\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_147\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_95 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_107 (Embedding)      (None, 17, 900)      649800      ['input_95[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_141 (Conv1D)            (None, 17, 100)      270100      ['embedding_107[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_142 (Conv1D)            (None, 17, 100)      270100      ['embedding_107[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_143 (Conv1D)            (None, 17, 100)      270100      ['embedding_107[0][0]']          \n",
            "                                                                                                  \n",
            " input_96 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_321 (Glob  (None, 100)         0           ['conv1d_141[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_322 (Glob  (None, 100)         0           ['conv1d_142[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_323 (Glob  (None, 100)         0           ['conv1d_143[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_195 (Dense)              (None, 3)            33          ['input_96[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_321[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_322[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_323[0][0]'\n",
            "                                                                 , 'dense_195[0][0]']             \n",
            "                                                                                                  \n",
            " dense_196 (Dense)              (None, 1000)         304000      ['concatenate_47[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_161 (Dropout)          (None, 1000)         0           ['dense_196[0][0]']              \n",
            "                                                                                                  \n",
            " dense_197 (Dense)              (None, 500)          500500      ['dropout_161[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_162 (Dropout)          (None, 500)          0           ['dense_197[0][0]']              \n",
            "                                                                                                  \n",
            " dense_198 (Dense)              (None, 100)          50100       ['dropout_162[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_163 (Dropout)          (None, 100)          0           ['dense_198[0][0]']              \n",
            "                                                                                                  \n",
            " dense_199 (Dense)              (None, 1)            101         ['dropout_163[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 5.9725 - mae: 1.4881\n",
            "Epoch 1: val_loss improved from inf to 0.83588, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 44ms/step - loss: 5.8058 - mae: 1.4586 - val_loss: 0.8359 - val_mae: 0.6617\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9132 - mae: 0.6264\n",
            "Epoch 2: val_loss did not improve from 0.83588\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9138 - mae: 0.6281 - val_loss: 0.8878 - val_mae: 0.7777\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8683 - mae: 0.6142\n",
            "Epoch 3: val_loss improved from 0.83588 to 0.83560, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8696 - mae: 0.6174 - val_loss: 0.8356 - val_mae: 0.5198\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8662 - mae: 0.6103\n",
            "Epoch 4: val_loss did not improve from 0.83560\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8655 - mae: 0.6155 - val_loss: 0.9408 - val_mae: 0.4047\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9318 - mae: 0.6251\n",
            "Epoch 5: val_loss improved from 0.83560 to 0.83042, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.9308 - mae: 0.6217 - val_loss: 0.8304 - val_mae: 0.6908\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8555 - mae: 0.6113\n",
            "Epoch 6: val_loss improved from 0.83042 to 0.80733, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8519 - mae: 0.6102 - val_loss: 0.8073 - val_mae: 0.6332\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8849 - mae: 0.6182\n",
            "Epoch 7: val_loss did not improve from 0.80733\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8849 - mae: 0.6182 - val_loss: 0.8119 - val_mae: 0.5191\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8856 - mae: 0.6152\n",
            "Epoch 8: val_loss did not improve from 0.80733\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8856 - mae: 0.6152 - val_loss: 0.8361 - val_mae: 0.4487\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9516 - mae: 0.6362\n",
            "Epoch 9: val_loss improved from 0.80733 to 0.79273, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9494 - mae: 0.6380 - val_loss: 0.7927 - val_mae: 0.5705\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9259 - mae: 0.6394\n",
            "Epoch 10: val_loss did not improve from 0.79273\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9287 - mae: 0.6325 - val_loss: 0.9061 - val_mae: 0.8280\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8646 - mae: 0.6124\n",
            "Epoch 11: val_loss improved from 0.79273 to 0.78826, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8617 - mae: 0.6106 - val_loss: 0.7883 - val_mae: 0.5418\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8574 - mae: 0.6131\n",
            "Epoch 12: val_loss did not improve from 0.78826\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8738 - mae: 0.6193 - val_loss: 0.8792 - val_mae: 0.3827\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8557 - mae: 0.5976\n",
            "Epoch 13: val_loss did not improve from 0.78826\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8557 - mae: 0.5976 - val_loss: 0.8073 - val_mae: 0.4646\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9008 - mae: 0.6176\n",
            "Epoch 14: val_loss improved from 0.78826 to 0.78093, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8983 - mae: 0.6159 - val_loss: 0.7809 - val_mae: 0.6150\n",
            "Epoch 15/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8270 - mae: 0.5959\n",
            "Epoch 15: val_loss improved from 0.78093 to 0.77790, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8261 - mae: 0.5967 - val_loss: 0.7779 - val_mae: 0.5452\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8604 - mae: 0.6024\n",
            "Epoch 16: val_loss did not improve from 0.77790\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8655 - mae: 0.6066 - val_loss: 0.8626 - val_mae: 0.7829\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8596 - mae: 0.6189\n",
            "Epoch 17: val_loss improved from 0.77790 to 0.76681, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8573 - mae: 0.6177 - val_loss: 0.7668 - val_mae: 0.6112\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8851 - mae: 0.6179\n",
            "Epoch 18: val_loss did not improve from 0.76681\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8851 - mae: 0.6179 - val_loss: 0.8563 - val_mae: 0.7675\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8418 - mae: 0.5891\n",
            "Epoch 19: val_loss improved from 0.76681 to 0.76418, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8471 - mae: 0.5964 - val_loss: 0.7642 - val_mae: 0.5213\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8393 - mae: 0.5933\n",
            "Epoch 20: val_loss did not improve from 0.76418\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8347 - mae: 0.5940 - val_loss: 0.7809 - val_mae: 0.4584\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8809 - mae: 0.6145\n",
            "Epoch 21: val_loss did not improve from 0.76418\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8807 - mae: 0.6151 - val_loss: 0.8058 - val_mae: 0.7149\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8070 - mae: 0.5821\n",
            "Epoch 22: val_loss did not improve from 0.76418\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8148 - mae: 0.5878 - val_loss: 0.8075 - val_mae: 0.7376\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8205 - mae: 0.5829\n",
            "Epoch 23: val_loss improved from 0.76418 to 0.74563, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8258 - mae: 0.5896 - val_loss: 0.7456 - val_mae: 0.5425\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8012 - mae: 0.5831\n",
            "Epoch 24: val_loss did not improve from 0.74563\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8012 - mae: 0.5831 - val_loss: 0.7625 - val_mae: 0.4518\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8320 - mae: 0.5794\n",
            "Epoch 25: val_loss did not improve from 0.74563\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8350 - mae: 0.5780 - val_loss: 0.7936 - val_mae: 0.7101\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8658 - mae: 0.6141\n",
            "Epoch 26: val_loss did not improve from 0.74563\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8658 - mae: 0.6141 - val_loss: 0.8505 - val_mae: 0.3964\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8448 - mae: 0.5867\n",
            "Epoch 27: val_loss improved from 0.74563 to 0.74558, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8428 - mae: 0.5847 - val_loss: 0.7456 - val_mae: 0.6315\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8265 - mae: 0.5823\n",
            "Epoch 28: val_loss did not improve from 0.74558\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8269 - mae: 0.5862 - val_loss: 0.7531 - val_mae: 0.4407\n",
            "Epoch 29/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8160 - mae: 0.5797\n",
            "Epoch 29: val_loss improved from 0.74558 to 0.74551, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8079 - mae: 0.5744 - val_loss: 0.7455 - val_mae: 0.4935\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7954 - mae: 0.5757\n",
            "Epoch 30: val_loss improved from 0.74551 to 0.73075, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.7985 - mae: 0.5779 - val_loss: 0.7307 - val_mae: 0.6269\n",
            "Epoch 31/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7926 - mae: 0.5735\n",
            "Epoch 31: val_loss did not improve from 0.73075\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.7884 - mae: 0.5710 - val_loss: 0.7525 - val_mae: 0.6836\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8050 - mae: 0.5728\n",
            "Epoch 32: val_loss improved from 0.73075 to 0.72124, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8050 - mae: 0.5728 - val_loss: 0.7212 - val_mae: 0.4786\n",
            "Epoch 33/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7981 - mae: 0.5654\n",
            "Epoch 33: val_loss did not improve from 0.72124\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.7962 - mae: 0.5636 - val_loss: 0.8098 - val_mae: 0.7630\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7763 - mae: 0.5648\n",
            "Epoch 34: val_loss improved from 0.72124 to 0.70774, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7755 - mae: 0.5646 - val_loss: 0.7077 - val_mae: 0.5256\n",
            "Epoch 35/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8144 - mae: 0.5865\n",
            "Epoch 35: val_loss did not improve from 0.70774\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8155 - mae: 0.5909 - val_loss: 0.7327 - val_mae: 0.4096\n",
            "Epoch 36/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9523 - mae: 0.6831\n",
            "Epoch 36: val_loss did not improve from 0.70774\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9509 - mae: 0.6753 - val_loss: 0.7846 - val_mae: 0.7274\n",
            "Epoch 37/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7932 - mae: 0.5644\n",
            "Epoch 37: val_loss improved from 0.70774 to 0.69709, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7896 - mae: 0.5620 - val_loss: 0.6971 - val_mae: 0.5629\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8424 - mae: 0.6089\n",
            "Epoch 38: val_loss did not improve from 0.69709\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8424 - mae: 0.6089 - val_loss: 0.7020 - val_mae: 0.6038\n",
            "Epoch 39/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8286 - mae: 0.5897\n",
            "Epoch 39: val_loss did not improve from 0.69709\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8247 - mae: 0.5917 - val_loss: 0.7583 - val_mae: 0.3737\n",
            "Epoch 40/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8304 - mae: 0.5914\n",
            "Epoch 40: val_loss improved from 0.69709 to 0.69248, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8222 - mae: 0.5868 - val_loss: 0.6925 - val_mae: 0.4382\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7593 - mae: 0.5439\n",
            "Epoch 41: val_loss did not improve from 0.69248\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7593 - mae: 0.5439 - val_loss: 0.7338 - val_mae: 0.3958\n",
            "Epoch 42/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7854 - mae: 0.5569\n",
            "Epoch 42: val_loss improved from 0.69248 to 0.68592, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.7786 - mae: 0.5517 - val_loss: 0.6859 - val_mae: 0.4314\n",
            "Epoch 43/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8882 - mae: 0.6468\n",
            "Epoch 43: val_loss did not improve from 0.68592\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8860 - mae: 0.6476 - val_loss: 0.8700 - val_mae: 0.8261\n",
            "Epoch 44/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8577 - mae: 0.6311\n",
            "Epoch 44: val_loss did not improve from 0.68592\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8492 - mae: 0.6231 - val_loss: 0.7121 - val_mae: 0.3859\n",
            "Epoch 45/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7685 - mae: 0.5522\n",
            "Epoch 45: val_loss did not improve from 0.68592\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.7650 - mae: 0.5530 - val_loss: 0.7219 - val_mae: 0.3729\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8411 - mae: 0.6087\n",
            "Epoch 46: val_loss improved from 0.68592 to 0.68266, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8411 - mae: 0.6087 - val_loss: 0.6827 - val_mae: 0.5898\n",
            "Epoch 47/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8922 - mae: 0.6555\n",
            "Epoch 47: val_loss did not improve from 0.68266\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8922 - mae: 0.6504 - val_loss: 0.7944 - val_mae: 0.7537\n",
            "Epoch 48/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7656 - mae: 0.5507\n",
            "Epoch 48: val_loss improved from 0.68266 to 0.67636, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.7650 - mae: 0.5523 - val_loss: 0.6764 - val_mae: 0.4094\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8433 - mae: 0.6195\n",
            "Epoch 49: val_loss did not improve from 0.67636\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8433 - mae: 0.6195 - val_loss: 0.6829 - val_mae: 0.4490\n",
            "Epoch 50/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7839 - mae: 0.5666\n",
            "Epoch 50: val_loss did not improve from 0.67636\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.7874 - mae: 0.5752 - val_loss: 0.9657 - val_mae: 0.6211\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8001 - mae: 0.5698\n",
            "Epoch 51: val_loss did not improve from 0.67636\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.8001 - mae: 0.5698 - val_loss: 0.6999 - val_mae: 0.6240\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8095 - mae: 0.5898\n",
            "Epoch 52: val_loss did not improve from 0.67636\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8095 - mae: 0.5898 - val_loss: 0.6988 - val_mae: 0.3864\n",
            "Epoch 53/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7760 - mae: 0.5602\n",
            "Epoch 53: val_loss improved from 0.67636 to 0.67181, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.7765 - mae: 0.5611 - val_loss: 0.6718 - val_mae: 0.5526\n",
            "Epoch 54/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7268 - mae: 0.5171\n",
            "Epoch 54: val_loss improved from 0.67181 to 0.64664, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.7288 - mae: 0.5206 - val_loss: 0.6466 - val_mae: 0.5245\n",
            "Epoch 55/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8859 - mae: 0.6619\n",
            "Epoch 55: val_loss did not improve from 0.64664\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8835 - mae: 0.6596 - val_loss: 0.6541 - val_mae: 0.4846\n",
            "Epoch 56/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7217 - mae: 0.5188\n",
            "Epoch 56: val_loss did not improve from 0.64664\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.7247 - mae: 0.5196 - val_loss: 0.7348 - val_mae: 0.4474\n",
            "Epoch 57/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7551 - mae: 0.5365\n",
            "Epoch 57: val_loss did not improve from 0.64664\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7546 - mae: 0.5367 - val_loss: 0.6639 - val_mae: 0.5591\n",
            "Epoch 58/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8558 - mae: 0.6383\n",
            "Epoch 58: val_loss did not improve from 0.64664\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8588 - mae: 0.6451 - val_loss: 0.8548 - val_mae: 0.5526\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8641 - mae: 0.6364\n",
            "Epoch 59: val_loss did not improve from 0.64664\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8641 - mae: 0.6364 - val_loss: 0.7017 - val_mae: 0.6466\n",
            "Epoch 60/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8178 - mae: 0.5987\n",
            "Epoch 60: val_loss did not improve from 0.64664\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8189 - mae: 0.6052 - val_loss: 0.8747 - val_mae: 0.5773\n",
            "Epoch 60: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.701095700263977, RMSE:0.8373146057128906, MAE:0.5454868078231812, R2:0.18196206547355598\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_148\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_97 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_108 (Embedding)      (None, 17, 900)      649800      ['input_97[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_144 (Conv1D)            (None, 17, 100)      270100      ['embedding_108[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_145 (Conv1D)            (None, 17, 100)      270100      ['embedding_108[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_146 (Conv1D)            (None, 17, 100)      270100      ['embedding_108[0][0]']          \n",
            "                                                                                                  \n",
            " input_98 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_324 (Glob  (None, 100)         0           ['conv1d_144[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_325 (Glob  (None, 100)         0           ['conv1d_145[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_326 (Glob  (None, 100)         0           ['conv1d_146[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_200 (Dense)              (None, 3)            33          ['input_98[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_48 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_324[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_325[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_326[0][0]'\n",
            "                                                                 , 'dense_200[0][0]']             \n",
            "                                                                                                  \n",
            " dense_201 (Dense)              (None, 1000)         304000      ['concatenate_48[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_164 (Dropout)          (None, 1000)         0           ['dense_201[0][0]']              \n",
            "                                                                                                  \n",
            " dense_202 (Dense)              (None, 500)          500500      ['dropout_164[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_165 (Dropout)          (None, 500)          0           ['dense_202[0][0]']              \n",
            "                                                                                                  \n",
            " dense_203 (Dense)              (None, 100)          50100       ['dropout_165[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_166 (Dropout)          (None, 100)          0           ['dense_203[0][0]']              \n",
            "                                                                                                  \n",
            " dense_204 (Dense)              (None, 1)            101         ['dropout_166[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 5.3896 - mae: 1.4221\n",
            "Epoch 1: val_loss improved from inf to 0.90207, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 43ms/step - loss: 5.2469 - mae: 1.3940 - val_loss: 0.9021 - val_mae: 0.7740\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0918 - mae: 0.7217\n",
            "Epoch 2: val_loss did not improve from 0.90207\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 1.0865 - mae: 0.7251 - val_loss: 0.9895 - val_mae: 0.4331\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9539 - mae: 0.6392\n",
            "Epoch 3: val_loss improved from 0.90207 to 0.89452, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9547 - mae: 0.6451 - val_loss: 0.8945 - val_mae: 0.4457\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 1.0532 - mae: 0.7004\n",
            "Epoch 4: val_loss did not improve from 0.89452\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.0476 - mae: 0.6976 - val_loss: 1.0000 - val_mae: 0.8928\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9379 - mae: 0.6409\n",
            "Epoch 5: val_loss improved from 0.89452 to 0.83123, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.9269 - mae: 0.6353 - val_loss: 0.8312 - val_mae: 0.6193\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8805 - mae: 0.6249\n",
            "Epoch 6: val_loss did not improve from 0.83123\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8953 - mae: 0.6303 - val_loss: 0.9882 - val_mae: 0.4494\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9351 - mae: 0.6318\n",
            "Epoch 7: val_loss did not improve from 0.83123\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9308 - mae: 0.6300 - val_loss: 0.8701 - val_mae: 0.7497\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8695 - mae: 0.6183\n",
            "Epoch 8: val_loss did not improve from 0.83123\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8712 - mae: 0.6201 - val_loss: 0.8745 - val_mae: 0.4430\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9710 - mae: 0.6490\n",
            "Epoch 9: val_loss did not improve from 0.83123\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9533 - mae: 0.6430 - val_loss: 0.8876 - val_mae: 0.4219\n",
            "Epoch 10/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9299 - mae: 0.6179\n",
            "Epoch 10: val_loss did not improve from 0.83123\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9263 - mae: 0.6165 - val_loss: 0.8658 - val_mae: 0.7505\n",
            "Epoch 11/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8807 - mae: 0.6146\n",
            "Epoch 11: val_loss improved from 0.83123 to 0.82530, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8802 - mae: 0.6150 - val_loss: 0.8253 - val_mae: 0.6592\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9001 - mae: 0.6245\n",
            "Epoch 12: val_loss did not improve from 0.82530\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8955 - mae: 0.6242 - val_loss: 0.8525 - val_mae: 0.4622\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8725 - mae: 0.6104\n",
            "Epoch 13: val_loss did not improve from 0.82530\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8696 - mae: 0.6068 - val_loss: 0.8263 - val_mae: 0.6736\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9603 - mae: 0.6532\n",
            "Epoch 14: val_loss did not improve from 0.82530\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9557 - mae: 0.6555 - val_loss: 0.9010 - val_mae: 0.3914\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8812 - mae: 0.6020\n",
            "Epoch 15: val_loss did not improve from 0.82530\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8813 - mae: 0.6024 - val_loss: 0.8608 - val_mae: 0.7532\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8841 - mae: 0.6093\n",
            "Epoch 16: val_loss improved from 0.82530 to 0.81073, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8840 - mae: 0.6133 - val_loss: 0.8107 - val_mae: 0.6295\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8789 - mae: 0.6169\n",
            "Epoch 17: val_loss did not improve from 0.81073\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8769 - mae: 0.6120 - val_loss: 0.8332 - val_mae: 0.4756\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8810 - mae: 0.6018\n",
            "Epoch 18: val_loss did not improve from 0.81073\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8832 - mae: 0.6057 - val_loss: 0.8341 - val_mae: 0.7135\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8593 - mae: 0.6076\n",
            "Epoch 19: val_loss improved from 0.81073 to 0.80688, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8600 - mae: 0.6058 - val_loss: 0.8069 - val_mae: 0.6414\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8552 - mae: 0.6043\n",
            "Epoch 20: val_loss did not improve from 0.80688\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8602 - mae: 0.6040 - val_loss: 0.8611 - val_mae: 0.7649\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8493 - mae: 0.6078\n",
            "Epoch 21: val_loss improved from 0.80688 to 0.79707, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8494 - mae: 0.6048 - val_loss: 0.7971 - val_mae: 0.5687\n",
            "Epoch 22/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9497 - mae: 0.6638\n",
            "Epoch 22: val_loss did not improve from 0.79707\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9489 - mae: 0.6628 - val_loss: 0.8994 - val_mae: 0.3970\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8809 - mae: 0.6094\n",
            "Epoch 23: val_loss improved from 0.79707 to 0.79265, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8816 - mae: 0.6124 - val_loss: 0.7927 - val_mae: 0.5546\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9077 - mae: 0.6305\n",
            "Epoch 24: val_loss did not improve from 0.79265\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9102 - mae: 0.6380 - val_loss: 0.8811 - val_mae: 0.3822\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9055 - mae: 0.6248\n",
            "Epoch 25: val_loss did not improve from 0.79265\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9197 - mae: 0.6242 - val_loss: 1.1659 - val_mae: 1.0241\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9705 - mae: 0.6822\n",
            "Epoch 26: val_loss did not improve from 0.79265\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9705 - mae: 0.6822 - val_loss: 0.9556 - val_mae: 0.4776\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8978 - mae: 0.6118\n",
            "Epoch 27: val_loss improved from 0.79265 to 0.78307, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8919 - mae: 0.6090 - val_loss: 0.7831 - val_mae: 0.5395\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8488 - mae: 0.6007\n",
            "Epoch 28: val_loss did not improve from 0.78307\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8537 - mae: 0.5960 - val_loss: 0.8957 - val_mae: 0.8217\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9092 - mae: 0.6315\n",
            "Epoch 29: val_loss did not improve from 0.78307\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9092 - mae: 0.6315 - val_loss: 0.7952 - val_mae: 0.6774\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8436 - mae: 0.5914\n",
            "Epoch 30: val_loss did not improve from 0.78307\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8417 - mae: 0.5959 - val_loss: 0.8544 - val_mae: 0.3795\n",
            "Epoch 31/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8850 - mae: 0.6193\n",
            "Epoch 31: val_loss did not improve from 0.78307\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8876 - mae: 0.6212 - val_loss: 0.8152 - val_mae: 0.7256\n",
            "Epoch 32/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8445 - mae: 0.5936\n",
            "Epoch 32: val_loss did not improve from 0.78307\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8427 - mae: 0.5968 - val_loss: 0.7965 - val_mae: 0.4437\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8579 - mae: 0.6024\n",
            "Epoch 33: val_loss did not improve from 0.78307\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8579 - mae: 0.6024 - val_loss: 0.7835 - val_mae: 0.6738\n",
            "Epoch 33: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8174133896827698, RMSE:0.9041091799736023, MAE:0.5561703443527222, R2:0.04624251756716846\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_149\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_99 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_109 (Embedding)      (None, 17, 900)      649800      ['input_99[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_147 (Conv1D)            (None, 17, 100)      270100      ['embedding_109[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_148 (Conv1D)            (None, 17, 100)      270100      ['embedding_109[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_149 (Conv1D)            (None, 17, 100)      270100      ['embedding_109[0][0]']          \n",
            "                                                                                                  \n",
            " input_100 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_327 (Glob  (None, 100)         0           ['conv1d_147[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_328 (Glob  (None, 100)         0           ['conv1d_148[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_329 (Glob  (None, 100)         0           ['conv1d_149[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_205 (Dense)              (None, 3)            33          ['input_100[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_49 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_327[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_328[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_329[0][0]'\n",
            "                                                                 , 'dense_205[0][0]']             \n",
            "                                                                                                  \n",
            " dense_206 (Dense)              (None, 1000)         304000      ['concatenate_49[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_167 (Dropout)          (None, 1000)         0           ['dense_206[0][0]']              \n",
            "                                                                                                  \n",
            " dense_207 (Dense)              (None, 500)          500500      ['dropout_167[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_168 (Dropout)          (None, 500)          0           ['dense_207[0][0]']              \n",
            "                                                                                                  \n",
            " dense_208 (Dense)              (None, 100)          50100       ['dropout_168[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_169 (Dropout)          (None, 100)          0           ['dense_208[0][0]']              \n",
            "                                                                                                  \n",
            " dense_209 (Dense)              (None, 1)            101         ['dropout_169[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.4686 - mae: 1.3387\n",
            "Epoch 1: val_loss improved from inf to 0.86902, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 45ms/step - loss: 4.3564 - mae: 1.3227 - val_loss: 0.8690 - val_mae: 0.4645\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9916 - mae: 0.6674\n",
            "Epoch 2: val_loss did not improve from 0.86902\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9916 - mae: 0.6604 - val_loss: 0.9337 - val_mae: 0.8338\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 1.1170 - mae: 0.7638\n",
            "Epoch 3: val_loss did not improve from 0.86902\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.1046 - mae: 0.7509 - val_loss: 0.8853 - val_mae: 0.4325\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9407 - mae: 0.6441\n",
            "Epoch 4: val_loss did not improve from 0.86902\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9505 - mae: 0.6504 - val_loss: 0.9601 - val_mae: 0.8636\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9012 - mae: 0.6333\n",
            "Epoch 5: val_loss improved from 0.86902 to 0.84892, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9012 - mae: 0.6333 - val_loss: 0.8489 - val_mae: 0.4820\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9221 - mae: 0.6363\n",
            "Epoch 6: val_loss improved from 0.84892 to 0.82471, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9108 - mae: 0.6251 - val_loss: 0.8247 - val_mae: 0.6508\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8956 - mae: 0.6214\n",
            "Epoch 7: val_loss did not improve from 0.82471\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8956 - mae: 0.6214 - val_loss: 0.9017 - val_mae: 0.3984\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9152 - mae: 0.6287\n",
            "Epoch 8: val_loss did not improve from 0.82471\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9132 - mae: 0.6232 - val_loss: 0.9012 - val_mae: 0.8050\n",
            "Epoch 9/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8665 - mae: 0.6159\n",
            "Epoch 9: val_loss improved from 0.82471 to 0.81211, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8628 - mae: 0.6135 - val_loss: 0.8121 - val_mae: 0.5927\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9716 - mae: 0.6688\n",
            "Epoch 10: val_loss did not improve from 0.81211\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9716 - mae: 0.6688 - val_loss: 0.8335 - val_mae: 0.7015\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8924 - mae: 0.6055\n",
            "Epoch 11: val_loss did not improve from 0.81211\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8955 - mae: 0.6134 - val_loss: 0.9179 - val_mae: 0.3976\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9221 - mae: 0.6362\n",
            "Epoch 12: val_loss did not improve from 0.81211\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9221 - mae: 0.6347 - val_loss: 0.8247 - val_mae: 0.6895\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8462 - mae: 0.6056\n",
            "Epoch 13: val_loss did not improve from 0.81211\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8462 - mae: 0.6056 - val_loss: 0.8788 - val_mae: 0.7860\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9281 - mae: 0.6515\n",
            "Epoch 14: val_loss did not improve from 0.81211\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9289 - mae: 0.6486 - val_loss: 0.8363 - val_mae: 0.7229\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9255 - mae: 0.6377\n",
            "Epoch 15: val_loss did not improve from 0.81211\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9255 - mae: 0.6377 - val_loss: 0.8806 - val_mae: 0.3901\n",
            "Epoch 15: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8420739769935608, RMSE:0.9176458716392517, MAE:0.6066699028015137, R2:0.017468553496777006\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_150\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_101 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_110 (Embedding)      (None, 17, 900)      649800      ['input_101[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_150 (Conv1D)            (None, 17, 100)      270100      ['embedding_110[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_151 (Conv1D)            (None, 17, 100)      270100      ['embedding_110[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_152 (Conv1D)            (None, 17, 100)      270100      ['embedding_110[0][0]']          \n",
            "                                                                                                  \n",
            " input_102 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_330 (Glob  (None, 100)         0           ['conv1d_150[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_331 (Glob  (None, 100)         0           ['conv1d_151[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_332 (Glob  (None, 100)         0           ['conv1d_152[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_210 (Dense)              (None, 3)            33          ['input_102[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_50 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_330[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_331[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_332[0][0]'\n",
            "                                                                 , 'dense_210[0][0]']             \n",
            "                                                                                                  \n",
            " dense_211 (Dense)              (None, 1000)         304000      ['concatenate_50[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_170 (Dropout)          (None, 1000)         0           ['dense_211[0][0]']              \n",
            "                                                                                                  \n",
            " dense_212 (Dense)              (None, 500)          500500      ['dropout_170[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_171 (Dropout)          (None, 500)          0           ['dense_212[0][0]']              \n",
            "                                                                                                  \n",
            " dense_213 (Dense)              (None, 100)          50100       ['dropout_171[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_172 (Dropout)          (None, 100)          0           ['dense_213[0][0]']              \n",
            "                                                                                                  \n",
            " dense_214 (Dense)              (None, 1)            101         ['dropout_172[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 4.3308 - mae: 1.2541\n",
            "Epoch 1: val_loss improved from inf to 0.85228, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 45ms/step - loss: 4.0798 - mae: 1.2093 - val_loss: 0.8523 - val_mae: 0.7090\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9268 - mae: 0.6465\n",
            "Epoch 2: val_loss did not improve from 0.85228\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9268 - mae: 0.6465 - val_loss: 1.3041 - val_mae: 0.7011\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0289 - mae: 0.7067\n",
            "Epoch 3: val_loss did not improve from 0.85228\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 1.0262 - mae: 0.7007 - val_loss: 0.8829 - val_mae: 0.7699\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9059 - mae: 0.6283\n",
            "Epoch 4: val_loss did not improve from 0.85228\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9055 - mae: 0.6246 - val_loss: 0.8578 - val_mae: 0.4728\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9440 - mae: 0.6467\n",
            "Epoch 5: val_loss did not improve from 0.85228\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9551 - mae: 0.6479 - val_loss: 1.0977 - val_mae: 0.9750\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9362 - mae: 0.6541\n",
            "Epoch 6: val_loss improved from 0.85228 to 0.82129, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9362 - mae: 0.6541 - val_loss: 0.8213 - val_mae: 0.6089\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8890 - mae: 0.6166\n",
            "Epoch 7: val_loss did not improve from 0.82129\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8890 - mae: 0.6166 - val_loss: 0.9175 - val_mae: 0.8209\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0177 - mae: 0.6902\n",
            "Epoch 8: val_loss did not improve from 0.82129\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 1.0129 - mae: 0.6912 - val_loss: 0.8330 - val_mae: 0.5102\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8761 - mae: 0.6122\n",
            "Epoch 9: val_loss improved from 0.82129 to 0.81961, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8705 - mae: 0.6095 - val_loss: 0.8196 - val_mae: 0.5529\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8529 - mae: 0.6088\n",
            "Epoch 10: val_loss did not improve from 0.81961\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8529 - mae: 0.6088 - val_loss: 0.8198 - val_mae: 0.6482\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8700 - mae: 0.6149\n",
            "Epoch 11: val_loss did not improve from 0.81961\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8703 - mae: 0.6085 - val_loss: 0.8596 - val_mae: 0.7488\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9023 - mae: 0.6139\n",
            "Epoch 12: val_loss did not improve from 0.81961\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9043 - mae: 0.6212 - val_loss: 0.8856 - val_mae: 0.4054\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9133 - mae: 0.6187\n",
            "Epoch 13: val_loss did not improve from 0.81961\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9173 - mae: 0.6283 - val_loss: 1.0235 - val_mae: 0.5092\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8945 - mae: 0.6177\n",
            "Epoch 14: val_loss improved from 0.81961 to 0.81908, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8867 - mae: 0.6131 - val_loss: 0.8191 - val_mae: 0.5110\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8637 - mae: 0.6043\n",
            "Epoch 15: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8736 - mae: 0.6114 - val_loss: 0.8812 - val_mae: 0.7882\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8977 - mae: 0.6215\n",
            "Epoch 16: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8966 - mae: 0.6254 - val_loss: 0.8387 - val_mae: 0.4547\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9009 - mae: 0.6266\n",
            "Epoch 17: val_loss improved from 0.81908 to 0.81091, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9009 - mae: 0.6266 - val_loss: 0.8109 - val_mae: 0.5107\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8540 - mae: 0.6082\n",
            "Epoch 18: val_loss improved from 0.81091 to 0.79739, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8530 - mae: 0.6083 - val_loss: 0.7974 - val_mae: 0.5625\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8794 - mae: 0.6182\n",
            "Epoch 19: val_loss did not improve from 0.79739\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8817 - mae: 0.6116 - val_loss: 0.8897 - val_mae: 0.8057\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8515 - mae: 0.6105\n",
            "Epoch 20: val_loss did not improve from 0.79739\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8513 - mae: 0.6066 - val_loss: 0.8208 - val_mae: 0.7061\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8301 - mae: 0.6007\n",
            "Epoch 21: val_loss improved from 0.79739 to 0.78770, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8301 - mae: 0.6007 - val_loss: 0.7877 - val_mae: 0.5795\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9040 - mae: 0.6323\n",
            "Epoch 22: val_loss did not improve from 0.78770\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9042 - mae: 0.6387 - val_loss: 0.9050 - val_mae: 0.4198\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8495 - mae: 0.5915\n",
            "Epoch 23: val_loss improved from 0.78770 to 0.78532, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8495 - mae: 0.5915 - val_loss: 0.7853 - val_mae: 0.6215\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8378 - mae: 0.5916\n",
            "Epoch 24: val_loss did not improve from 0.78532\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8424 - mae: 0.5923 - val_loss: 0.8586 - val_mae: 0.7784\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9680 - mae: 0.6726\n",
            "Epoch 25: val_loss did not improve from 0.78532\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9680 - mae: 0.6726 - val_loss: 0.8606 - val_mae: 0.3798\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8500 - mae: 0.5922\n",
            "Epoch 26: val_loss did not improve from 0.78532\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8422 - mae: 0.5893 - val_loss: 0.7972 - val_mae: 0.4622\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8855 - mae: 0.6150\n",
            "Epoch 27: val_loss did not improve from 0.78532\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8846 - mae: 0.6150 - val_loss: 0.8460 - val_mae: 0.7688\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8273 - mae: 0.5976\n",
            "Epoch 28: val_loss did not improve from 0.78532\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8212 - mae: 0.5963 - val_loss: 0.8171 - val_mae: 0.4108\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9689 - mae: 0.6751\n",
            "Epoch 29: val_loss did not improve from 0.78532\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9624 - mae: 0.6671 - val_loss: 0.8104 - val_mae: 0.7231\n",
            "Epoch 29: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8154297471046448, RMSE:0.9030115008354187, MAE:0.6339781284332275, R2:0.04855712997036077\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_151\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_103 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_111 (Embedding)      (None, 17, 900)      649800      ['input_103[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_153 (Conv1D)            (None, 17, 100)      270100      ['embedding_111[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_154 (Conv1D)            (None, 17, 100)      270100      ['embedding_111[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_155 (Conv1D)            (None, 17, 100)      270100      ['embedding_111[0][0]']          \n",
            "                                                                                                  \n",
            " input_104 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_333 (Glob  (None, 100)         0           ['conv1d_153[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_334 (Glob  (None, 100)         0           ['conv1d_154[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_335 (Glob  (None, 100)         0           ['conv1d_155[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_215 (Dense)              (None, 3)            33          ['input_104[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_51 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_333[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_334[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_335[0][0]'\n",
            "                                                                 , 'dense_215[0][0]']             \n",
            "                                                                                                  \n",
            " dense_216 (Dense)              (None, 1000)         304000      ['concatenate_51[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_173 (Dropout)          (None, 1000)         0           ['dense_216[0][0]']              \n",
            "                                                                                                  \n",
            " dense_217 (Dense)              (None, 500)          500500      ['dropout_173[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_174 (Dropout)          (None, 500)          0           ['dense_217[0][0]']              \n",
            "                                                                                                  \n",
            " dense_218 (Dense)              (None, 100)          50100       ['dropout_174[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_175 (Dropout)          (None, 100)          0           ['dense_218[0][0]']              \n",
            "                                                                                                  \n",
            " dense_219 (Dense)              (None, 1)            101         ['dropout_175[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.7977 - mae: 1.2867\n",
            "Epoch 1: val_loss improved from inf to 0.83680, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 45ms/step - loss: 4.7977 - mae: 1.2867 - val_loss: 0.8368 - val_mae: 0.6604\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8630 - mae: 0.6172\n",
            "Epoch 2: val_loss improved from 0.83680 to 0.83292, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8630 - mae: 0.6172 - val_loss: 0.8329 - val_mae: 0.6508\n",
            "Epoch 3/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9138 - mae: 0.6233\n",
            "Epoch 3: val_loss did not improve from 0.83292\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9097 - mae: 0.6180 - val_loss: 0.8395 - val_mae: 0.6824\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8706 - mae: 0.6112\n",
            "Epoch 4: val_loss improved from 0.83292 to 0.82444, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8751 - mae: 0.6161 - val_loss: 0.8244 - val_mae: 0.5835\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8621 - mae: 0.6141\n",
            "Epoch 5: val_loss improved from 0.82444 to 0.82252, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8621 - mae: 0.6141 - val_loss: 0.8225 - val_mae: 0.6147\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8774 - mae: 0.6087\n",
            "Epoch 6: val_loss did not improve from 0.82252\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8792 - mae: 0.6114 - val_loss: 0.8452 - val_mae: 0.7100\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9109 - mae: 0.6255\n",
            "Epoch 7: val_loss did not improve from 0.82252\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9125 - mae: 0.6260 - val_loss: 0.8806 - val_mae: 0.7749\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8578 - mae: 0.6134\n",
            "Epoch 8: val_loss did not improve from 0.82252\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8578 - mae: 0.6134 - val_loss: 0.8258 - val_mae: 0.6661\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8622 - mae: 0.6095\n",
            "Epoch 9: val_loss did not improve from 0.82252\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8619 - mae: 0.6133 - val_loss: 0.8528 - val_mae: 0.4578\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8694 - mae: 0.6063\n",
            "Epoch 10: val_loss did not improve from 0.82252\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8674 - mae: 0.6101 - val_loss: 0.9041 - val_mae: 0.3878\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8588 - mae: 0.6002\n",
            "Epoch 11: val_loss improved from 0.82252 to 0.81451, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8592 - mae: 0.6032 - val_loss: 0.8145 - val_mae: 0.5421\n",
            "Epoch 12/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9288 - mae: 0.6400\n",
            "Epoch 12: val_loss did not improve from 0.81451\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9207 - mae: 0.6337 - val_loss: 0.8188 - val_mae: 0.5158\n",
            "Epoch 13/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8729 - mae: 0.6015\n",
            "Epoch 13: val_loss did not improve from 0.81451\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8724 - mae: 0.6075 - val_loss: 0.8962 - val_mae: 0.3852\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8713 - mae: 0.6021\n",
            "Epoch 14: val_loss improved from 0.81451 to 0.80454, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8736 - mae: 0.6049 - val_loss: 0.8045 - val_mae: 0.5555\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8472 - mae: 0.6065\n",
            "Epoch 15: val_loss did not improve from 0.80454\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8473 - mae: 0.6055 - val_loss: 0.8491 - val_mae: 0.4346\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8812 - mae: 0.6042\n",
            "Epoch 16: val_loss improved from 0.80454 to 0.80297, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8830 - mae: 0.6063 - val_loss: 0.8030 - val_mae: 0.6404\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8354 - mae: 0.6031\n",
            "Epoch 17: val_loss did not improve from 0.80297\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8354 - mae: 0.6031 - val_loss: 0.8077 - val_mae: 0.5046\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8301 - mae: 0.6004\n",
            "Epoch 18: val_loss did not improve from 0.80297\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8311 - mae: 0.5965 - val_loss: 0.8487 - val_mae: 0.7539\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8674 - mae: 0.6134\n",
            "Epoch 19: val_loss improved from 0.80297 to 0.79252, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8655 - mae: 0.6135 - val_loss: 0.7925 - val_mae: 0.5383\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8275 - mae: 0.5969\n",
            "Epoch 20: val_loss improved from 0.79252 to 0.78891, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8275 - mae: 0.5963 - val_loss: 0.7889 - val_mae: 0.6216\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8159 - mae: 0.5927\n",
            "Epoch 21: val_loss did not improve from 0.78891\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8197 - mae: 0.5953 - val_loss: 0.7903 - val_mae: 0.6419\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9050 - mae: 0.6300\n",
            "Epoch 22: val_loss did not improve from 0.78891\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8975 - mae: 0.6284 - val_loss: 0.8031 - val_mae: 0.4700\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8507 - mae: 0.5909\n",
            "Epoch 23: val_loss did not improve from 0.78891\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8481 - mae: 0.5928 - val_loss: 0.7909 - val_mae: 0.4904\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8171 - mae: 0.5902\n",
            "Epoch 24: val_loss did not improve from 0.78891\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8131 - mae: 0.5904 - val_loss: 0.8031 - val_mae: 0.4522\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8330 - mae: 0.5891\n",
            "Epoch 25: val_loss did not improve from 0.78891\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8371 - mae: 0.5906 - val_loss: 0.8249 - val_mae: 0.7392\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8477 - mae: 0.6027\n",
            "Epoch 26: val_loss improved from 0.78891 to 0.77300, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8477 - mae: 0.6027 - val_loss: 0.7730 - val_mae: 0.6299\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8290 - mae: 0.5932\n",
            "Epoch 27: val_loss did not improve from 0.77300\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8269 - mae: 0.5912 - val_loss: 0.7751 - val_mae: 0.4870\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8344 - mae: 0.5884\n",
            "Epoch 28: val_loss did not improve from 0.77300\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8331 - mae: 0.5929 - val_loss: 0.8891 - val_mae: 0.4465\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8536 - mae: 0.5923\n",
            "Epoch 29: val_loss did not improve from 0.77300\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8538 - mae: 0.5878 - val_loss: 0.7976 - val_mae: 0.7092\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8536 - mae: 0.6063\n",
            "Epoch 30: val_loss improved from 0.77300 to 0.76970, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8475 - mae: 0.6038 - val_loss: 0.7697 - val_mae: 0.4649\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8370 - mae: 0.5914\n",
            "Epoch 31: val_loss did not improve from 0.76970\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8401 - mae: 0.5903 - val_loss: 0.7887 - val_mae: 0.7019\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8395 - mae: 0.5986\n",
            "Epoch 32: val_loss did not improve from 0.76970\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8395 - mae: 0.5986 - val_loss: 0.9063 - val_mae: 0.4903\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8482 - mae: 0.5910\n",
            "Epoch 33: val_loss improved from 0.76970 to 0.75951, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8482 - mae: 0.5910 - val_loss: 0.7595 - val_mae: 0.6515\n",
            "Epoch 34/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8565 - mae: 0.6146\n",
            "Epoch 34: val_loss improved from 0.75951 to 0.73764, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8519 - mae: 0.6118 - val_loss: 0.7376 - val_mae: 0.5226\n",
            "Epoch 35/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8917 - mae: 0.6370\n",
            "Epoch 35: val_loss improved from 0.73764 to 0.73469, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8901 - mae: 0.6354 - val_loss: 0.7347 - val_mae: 0.5215\n",
            "Epoch 36/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7855 - mae: 0.5626\n",
            "Epoch 36: val_loss did not improve from 0.73469\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7865 - mae: 0.5702 - val_loss: 0.9585 - val_mae: 0.5606\n",
            "Epoch 37/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 1.0203 - mae: 0.7285\n",
            "Epoch 37: val_loss did not improve from 0.73469\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.0095 - mae: 0.7186 - val_loss: 0.8135 - val_mae: 0.7514\n",
            "Epoch 38/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8483 - mae: 0.5935\n",
            "Epoch 38: val_loss did not improve from 0.73469\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8483 - mae: 0.5990 - val_loss: 0.8067 - val_mae: 0.4101\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8570 - mae: 0.6067\n",
            "Epoch 39: val_loss improved from 0.73469 to 0.72279, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8570 - mae: 0.6067 - val_loss: 0.7228 - val_mae: 0.5074\n",
            "Epoch 40/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8103 - mae: 0.5740\n",
            "Epoch 40: val_loss did not improve from 0.72279\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8127 - mae: 0.5819 - val_loss: 0.7625 - val_mae: 0.3895\n",
            "Epoch 41/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8024 - mae: 0.5634\n",
            "Epoch 41: val_loss did not improve from 0.72279\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.7993 - mae: 0.5658 - val_loss: 0.7245 - val_mae: 0.6035\n",
            "Epoch 42/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8267 - mae: 0.5948\n",
            "Epoch 42: val_loss improved from 0.72279 to 0.71651, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8238 - mae: 0.5915 - val_loss: 0.7165 - val_mae: 0.5799\n",
            "Epoch 43/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7943 - mae: 0.5683\n",
            "Epoch 43: val_loss improved from 0.71651 to 0.70903, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.7913 - mae: 0.5661 - val_loss: 0.7090 - val_mae: 0.5427\n",
            "Epoch 44/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8913 - mae: 0.6528\n",
            "Epoch 44: val_loss did not improve from 0.70903\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8855 - mae: 0.6459 - val_loss: 0.7446 - val_mae: 0.3909\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7822 - mae: 0.5544\n",
            "Epoch 45: val_loss did not improve from 0.70903\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.7822 - mae: 0.5544 - val_loss: 0.7129 - val_mae: 0.4548\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7772 - mae: 0.5599\n",
            "Epoch 46: val_loss did not improve from 0.70903\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.7772 - mae: 0.5599 - val_loss: 0.8655 - val_mae: 0.5214\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9174 - mae: 0.6710\n",
            "Epoch 47: val_loss did not improve from 0.70903\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9174 - mae: 0.6710 - val_loss: 0.7828 - val_mae: 0.7242\n",
            "Epoch 48/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8152 - mae: 0.5838\n",
            "Epoch 48: val_loss improved from 0.70903 to 0.70177, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8108 - mae: 0.5820 - val_loss: 0.7018 - val_mae: 0.4653\n",
            "Epoch 49/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7540 - mae: 0.5376\n",
            "Epoch 49: val_loss did not improve from 0.70177\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.7566 - mae: 0.5360 - val_loss: 0.7355 - val_mae: 0.6581\n",
            "Epoch 50/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8424 - mae: 0.6136\n",
            "Epoch 50: val_loss did not improve from 0.70177\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8382 - mae: 0.6130 - val_loss: 0.7121 - val_mae: 0.4143\n",
            "Epoch 51/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7696 - mae: 0.5325\n",
            "Epoch 51: val_loss did not improve from 0.70177\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.7691 - mae: 0.5365 - val_loss: 0.7129 - val_mae: 0.4047\n",
            "Epoch 52/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7948 - mae: 0.5653\n",
            "Epoch 52: val_loss did not improve from 0.70177\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7920 - mae: 0.5605 - val_loss: 0.7340 - val_mae: 0.6609\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8073 - mae: 0.5831\n",
            "Epoch 53: val_loss improved from 0.70177 to 0.69581, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8073 - mae: 0.5831 - val_loss: 0.6958 - val_mae: 0.4369\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7708 - mae: 0.5533\n",
            "Epoch 54: val_loss did not improve from 0.69581\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7708 - mae: 0.5533 - val_loss: 0.7225 - val_mae: 0.6449\n",
            "Epoch 55/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8287 - mae: 0.6028\n",
            "Epoch 55: val_loss did not improve from 0.69581\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8245 - mae: 0.6041 - val_loss: 0.8198 - val_mae: 0.5031\n",
            "Epoch 56/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7883 - mae: 0.5611\n",
            "Epoch 56: val_loss did not improve from 0.69581\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.7862 - mae: 0.5594 - val_loss: 0.6986 - val_mae: 0.5996\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7526 - mae: 0.5370\n",
            "Epoch 57: val_loss did not improve from 0.69581\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7526 - mae: 0.5370 - val_loss: 0.7815 - val_mae: 0.7304\n",
            "Epoch 58/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7626 - mae: 0.5507\n",
            "Epoch 58: val_loss improved from 0.69581 to 0.68604, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.7624 - mae: 0.5484 - val_loss: 0.6860 - val_mae: 0.4250\n",
            "Epoch 59/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7520 - mae: 0.5350\n",
            "Epoch 59: val_loss did not improve from 0.68604\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7649 - mae: 0.5436 - val_loss: 0.7860 - val_mae: 0.4771\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9424 - mae: 0.6894\n",
            "Epoch 60: val_loss did not improve from 0.68604\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9424 - mae: 0.6894 - val_loss: 1.0002 - val_mae: 0.9105\n",
            "Epoch 61/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7819 - mae: 0.5768\n",
            "Epoch 61: val_loss improved from 0.68604 to 0.68213, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7871 - mae: 0.5757 - val_loss: 0.6821 - val_mae: 0.4225\n",
            "Epoch 62/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8113 - mae: 0.5958\n",
            "Epoch 62: val_loss did not improve from 0.68213\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8085 - mae: 0.5910 - val_loss: 0.6950 - val_mae: 0.3851\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7805 - mae: 0.5585\n",
            "Epoch 63: val_loss improved from 0.68213 to 0.67638, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.7805 - mae: 0.5585 - val_loss: 0.6764 - val_mae: 0.5639\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7232 - mae: 0.5123\n",
            "Epoch 64: val_loss improved from 0.67638 to 0.66489, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7232 - mae: 0.5123 - val_loss: 0.6649 - val_mae: 0.5149\n",
            "Epoch 65/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7992 - mae: 0.5908\n",
            "Epoch 65: val_loss did not improve from 0.66489\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.7980 - mae: 0.5954 - val_loss: 0.8632 - val_mae: 0.5643\n",
            "Epoch 66/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8564 - mae: 0.6474\n",
            "Epoch 66: val_loss did not improve from 0.66489\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8506 - mae: 0.6430 - val_loss: 0.6678 - val_mae: 0.4332\n",
            "Epoch 67/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7941 - mae: 0.5831\n",
            "Epoch 67: val_loss did not improve from 0.66489\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7977 - mae: 0.5795 - val_loss: 0.8618 - val_mae: 0.8116\n",
            "Epoch 68/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8084 - mae: 0.6085\n",
            "Epoch 68: val_loss did not improve from 0.66489\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8073 - mae: 0.6032 - val_loss: 0.6897 - val_mae: 0.6153\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7237 - mae: 0.5182\n",
            "Epoch 69: val_loss did not improve from 0.66489\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7237 - mae: 0.5182 - val_loss: 0.7555 - val_mae: 0.7117\n",
            "Epoch 70/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8046 - mae: 0.5959\n",
            "Epoch 70: val_loss did not improve from 0.66489\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8091 - mae: 0.6029 - val_loss: 0.6880 - val_mae: 0.3582\n",
            "Epoch 70: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7047502994537354, RMSE:0.8394940495491028, MAE:0.5317232012748718, R2:0.17769785502510116\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_152\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_105 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_112 (Embedding)      (None, 17, 900)      649800      ['input_105[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_156 (Conv1D)            (None, 17, 100)      270100      ['embedding_112[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_157 (Conv1D)            (None, 17, 100)      270100      ['embedding_112[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_158 (Conv1D)            (None, 17, 100)      270100      ['embedding_112[0][0]']          \n",
            "                                                                                                  \n",
            " input_106 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_336 (Glob  (None, 100)         0           ['conv1d_156[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_337 (Glob  (None, 100)         0           ['conv1d_157[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_338 (Glob  (None, 100)         0           ['conv1d_158[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_220 (Dense)              (None, 3)            33          ['input_106[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_52 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_336[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_337[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_338[0][0]'\n",
            "                                                                 , 'dense_220[0][0]']             \n",
            "                                                                                                  \n",
            " dense_221 (Dense)              (None, 1000)         304000      ['concatenate_52[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_176 (Dropout)          (None, 1000)         0           ['dense_221[0][0]']              \n",
            "                                                                                                  \n",
            " dense_222 (Dense)              (None, 500)          500500      ['dropout_176[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_177 (Dropout)          (None, 500)          0           ['dense_222[0][0]']              \n",
            "                                                                                                  \n",
            " dense_223 (Dense)              (None, 100)          50100       ['dropout_177[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_178 (Dropout)          (None, 100)          0           ['dense_223[0][0]']              \n",
            "                                                                                                  \n",
            " dense_224 (Dense)              (None, 1)            101         ['dropout_178[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.3121 - mae: 1.2913\n",
            "Epoch 1: val_loss improved from inf to 0.99148, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 44ms/step - loss: 4.2057 - mae: 1.2790 - val_loss: 0.9915 - val_mae: 0.4469\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9878 - mae: 0.6618\n",
            "Epoch 2: val_loss improved from 0.99148 to 0.91622, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9878 - mae: 0.6618 - val_loss: 0.9162 - val_mae: 0.8131\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9443 - mae: 0.6382\n",
            "Epoch 3: val_loss improved from 0.91622 to 0.90506, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9403 - mae: 0.6354 - val_loss: 0.9051 - val_mae: 0.4082\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9519 - mae: 0.6282\n",
            "Epoch 4: val_loss did not improve from 0.90506\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9519 - mae: 0.6282 - val_loss: 0.9066 - val_mae: 0.8053\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9494 - mae: 0.6464\n",
            "Epoch 5: val_loss improved from 0.90506 to 0.85927, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9494 - mae: 0.6464 - val_loss: 0.8593 - val_mae: 0.4602\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9185 - mae: 0.6173\n",
            "Epoch 6: val_loss improved from 0.85927 to 0.84967, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.9202 - mae: 0.6198 - val_loss: 0.8497 - val_mae: 0.7250\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8660 - mae: 0.6139\n",
            "Epoch 7: val_loss improved from 0.84967 to 0.81865, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8660 - mae: 0.6139 - val_loss: 0.8187 - val_mae: 0.5466\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8725 - mae: 0.6111\n",
            "Epoch 8: val_loss improved from 0.81865 to 0.81175, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8732 - mae: 0.6117 - val_loss: 0.8118 - val_mae: 0.6074\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8540 - mae: 0.6064\n",
            "Epoch 9: val_loss did not improve from 0.81175\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8533 - mae: 0.6098 - val_loss: 0.8456 - val_mae: 0.4590\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8646 - mae: 0.6020\n",
            "Epoch 10: val_loss did not improve from 0.81175\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8694 - mae: 0.6026 - val_loss: 0.8612 - val_mae: 0.7582\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8848 - mae: 0.6189\n",
            "Epoch 11: val_loss did not improve from 0.81175\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8818 - mae: 0.6110 - val_loss: 0.8415 - val_mae: 0.7286\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8451 - mae: 0.6052\n",
            "Epoch 12: val_loss improved from 0.81175 to 0.80082, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8453 - mae: 0.6064 - val_loss: 0.8008 - val_mae: 0.5682\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9104 - mae: 0.6358\n",
            "Epoch 13: val_loss did not improve from 0.80082\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9104 - mae: 0.6358 - val_loss: 0.8363 - val_mae: 0.4501\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8591 - mae: 0.6000\n",
            "Epoch 14: val_loss improved from 0.80082 to 0.79433, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8561 - mae: 0.5979 - val_loss: 0.7943 - val_mae: 0.5808\n",
            "Epoch 15/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8852 - mae: 0.6137\n",
            "Epoch 15: val_loss did not improve from 0.79433\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8881 - mae: 0.6178 - val_loss: 0.8877 - val_mae: 0.8050\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8969 - mae: 0.6295\n",
            "Epoch 16: val_loss did not improve from 0.79433\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8915 - mae: 0.6215 - val_loss: 0.8148 - val_mae: 0.6981\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8274 - mae: 0.5996\n",
            "Epoch 17: val_loss improved from 0.79433 to 0.78845, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8267 - mae: 0.5998 - val_loss: 0.7884 - val_mae: 0.5359\n",
            "Epoch 18/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8500 - mae: 0.6032\n",
            "Epoch 18: val_loss improved from 0.78845 to 0.78114, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8445 - mae: 0.6000 - val_loss: 0.7811 - val_mae: 0.5695\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8940 - mae: 0.6221\n",
            "Epoch 19: val_loss did not improve from 0.78114\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8973 - mae: 0.6160 - val_loss: 0.9005 - val_mae: 0.8263\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8349 - mae: 0.5989\n",
            "Epoch 20: val_loss did not improve from 0.78114\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8347 - mae: 0.6028 - val_loss: 0.8194 - val_mae: 0.4270\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8697 - mae: 0.5939\n",
            "Epoch 21: val_loss improved from 0.78114 to 0.77083, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8677 - mae: 0.5937 - val_loss: 0.7708 - val_mae: 0.5883\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9262 - mae: 0.6415\n",
            "Epoch 22: val_loss did not improve from 0.77083\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9224 - mae: 0.6445 - val_loss: 0.8620 - val_mae: 0.3975\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8217 - mae: 0.5847\n",
            "Epoch 23: val_loss improved from 0.77083 to 0.76319, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8195 - mae: 0.5832 - val_loss: 0.7632 - val_mae: 0.5772\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8186 - mae: 0.5861\n",
            "Epoch 24: val_loss did not improve from 0.76319\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8148 - mae: 0.5840 - val_loss: 0.7641 - val_mae: 0.5154\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8014 - mae: 0.5819\n",
            "Epoch 25: val_loss improved from 0.76319 to 0.75525, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.7972 - mae: 0.5793 - val_loss: 0.7552 - val_mae: 0.5448\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9607 - mae: 0.6836\n",
            "Epoch 26: val_loss did not improve from 0.75525\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9603 - mae: 0.6885 - val_loss: 0.8793 - val_mae: 0.4474\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8226 - mae: 0.5784\n",
            "Epoch 27: val_loss improved from 0.75525 to 0.74726, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8226 - mae: 0.5807 - val_loss: 0.7473 - val_mae: 0.5624\n",
            "Epoch 28/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8268 - mae: 0.5834\n",
            "Epoch 28: val_loss did not improve from 0.74726\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8264 - mae: 0.5877 - val_loss: 0.8121 - val_mae: 0.3761\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8309 - mae: 0.5803\n",
            "Epoch 29: val_loss did not improve from 0.74726\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8320 - mae: 0.5852 - val_loss: 0.7753 - val_mae: 0.4213\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8009 - mae: 0.5753\n",
            "Epoch 30: val_loss improved from 0.74726 to 0.73599, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7966 - mae: 0.5715 - val_loss: 0.7360 - val_mae: 0.5670\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8239 - mae: 0.5886\n",
            "Epoch 31: val_loss did not improve from 0.73599\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8258 - mae: 0.5846 - val_loss: 0.8242 - val_mae: 0.7623\n",
            "Epoch 32/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8090 - mae: 0.5786\n",
            "Epoch 32: val_loss did not improve from 0.73599\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8063 - mae: 0.5802 - val_loss: 0.7906 - val_mae: 0.3778\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9054 - mae: 0.6542\n",
            "Epoch 33: val_loss did not improve from 0.73599\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9054 - mae: 0.6542 - val_loss: 0.8661 - val_mae: 0.4797\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8204 - mae: 0.5839\n",
            "Epoch 34: val_loss did not improve from 0.73599\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8204 - mae: 0.5839 - val_loss: 0.8280 - val_mae: 0.4427\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8262 - mae: 0.5884\n",
            "Epoch 35: val_loss did not improve from 0.73599\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8262 - mae: 0.5884 - val_loss: 0.8042 - val_mae: 0.7436\n",
            "Epoch 36/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8230 - mae: 0.5874\n",
            "Epoch 36: val_loss did not improve from 0.73599\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8244 - mae: 0.5939 - val_loss: 0.8951 - val_mae: 0.5267\n",
            "Epoch 36: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.771304190158844, RMSE:0.8782392740249634, MAE:0.5823045372962952, R2:0.10004279282495943\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_153\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_107 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_113 (Embedding)      (None, 17, 900)      649800      ['input_107[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_159 (Conv1D)            (None, 17, 100)      270100      ['embedding_113[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_160 (Conv1D)            (None, 17, 100)      270100      ['embedding_113[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_161 (Conv1D)            (None, 17, 100)      270100      ['embedding_113[0][0]']          \n",
            "                                                                                                  \n",
            " input_108 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_339 (Glob  (None, 100)         0           ['conv1d_159[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_340 (Glob  (None, 100)         0           ['conv1d_160[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_341 (Glob  (None, 100)         0           ['conv1d_161[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_225 (Dense)              (None, 3)            33          ['input_108[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_53 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_339[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_340[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_341[0][0]'\n",
            "                                                                 , 'dense_225[0][0]']             \n",
            "                                                                                                  \n",
            " dense_226 (Dense)              (None, 1000)         304000      ['concatenate_53[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_179 (Dropout)          (None, 1000)         0           ['dense_226[0][0]']              \n",
            "                                                                                                  \n",
            " dense_227 (Dense)              (None, 500)          500500      ['dropout_179[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_180 (Dropout)          (None, 500)          0           ['dense_227[0][0]']              \n",
            "                                                                                                  \n",
            " dense_228 (Dense)              (None, 100)          50100       ['dropout_180[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_181 (Dropout)          (None, 100)          0           ['dense_228[0][0]']              \n",
            "                                                                                                  \n",
            " dense_229 (Dense)              (None, 1)            101         ['dropout_181[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.6725 - mae: 1.2831\n",
            "Epoch 1: val_loss improved from inf to 0.88621, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 44ms/step - loss: 4.6725 - mae: 1.2831 - val_loss: 0.8862 - val_mae: 0.4393\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8916 - mae: 0.6136\n",
            "Epoch 2: val_loss did not improve from 0.88621\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9019 - mae: 0.6113 - val_loss: 1.0290 - val_mae: 0.9226\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9464 - mae: 0.6428\n",
            "Epoch 3: val_loss improved from 0.88621 to 0.85547, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9516 - mae: 0.6422 - val_loss: 0.8555 - val_mae: 0.4795\n",
            "Epoch 4/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8849 - mae: 0.6137\n",
            "Epoch 4: val_loss improved from 0.85547 to 0.84940, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8871 - mae: 0.6153 - val_loss: 0.8494 - val_mae: 0.7135\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9325 - mae: 0.6243\n",
            "Epoch 5: val_loss improved from 0.84940 to 0.82149, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9292 - mae: 0.6239 - val_loss: 0.8215 - val_mae: 0.5855\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8628 - mae: 0.6126\n",
            "Epoch 6: val_loss improved from 0.82149 to 0.81908, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8607 - mae: 0.6132 - val_loss: 0.8191 - val_mae: 0.5957\n",
            "Epoch 7/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8817 - mae: 0.6125\n",
            "Epoch 7: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8819 - mae: 0.6140 - val_loss: 0.8423 - val_mae: 0.7090\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8777 - mae: 0.6154\n",
            "Epoch 8: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8762 - mae: 0.6122 - val_loss: 0.8337 - val_mae: 0.6927\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8574 - mae: 0.6109\n",
            "Epoch 9: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8574 - mae: 0.6109 - val_loss: 0.8340 - val_mae: 0.6983\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9040 - mae: 0.6191\n",
            "Epoch 10: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8970 - mae: 0.6183 - val_loss: 0.8567 - val_mae: 0.4436\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8524 - mae: 0.6066\n",
            "Epoch 11: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8524 - mae: 0.6066 - val_loss: 0.8697 - val_mae: 0.4204\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8703 - mae: 0.6057\n",
            "Epoch 12: val_loss did not improve from 0.81908\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8703 - mae: 0.6057 - val_loss: 0.8505 - val_mae: 0.4431\n",
            "Epoch 12: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8489818572998047, RMSE:0.921402096748352, MAE:0.6095839738845825, R2:0.009408563706876372\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n",
            "Model: \"model_154\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_109 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_114 (Embedding)      (None, 17, 900)      649800      ['input_109[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_162 (Conv1D)            (None, 17, 100)      270100      ['embedding_114[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_163 (Conv1D)            (None, 17, 100)      270100      ['embedding_114[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_164 (Conv1D)            (None, 17, 100)      270100      ['embedding_114[0][0]']          \n",
            "                                                                                                  \n",
            " input_110 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_342 (Glob  (None, 100)         0           ['conv1d_162[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_343 (Glob  (None, 100)         0           ['conv1d_163[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_344 (Glob  (None, 100)         0           ['conv1d_164[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_230 (Dense)              (None, 3)            33          ['input_110[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_54 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_342[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_343[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_344[0][0]'\n",
            "                                                                 , 'dense_230[0][0]']             \n",
            "                                                                                                  \n",
            " dense_231 (Dense)              (None, 1000)         304000      ['concatenate_54[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_182 (Dropout)          (None, 1000)         0           ['dense_231[0][0]']              \n",
            "                                                                                                  \n",
            " dense_232 (Dense)              (None, 500)          500500      ['dropout_182[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_183 (Dropout)          (None, 500)          0           ['dense_232[0][0]']              \n",
            "                                                                                                  \n",
            " dense_233 (Dense)              (None, 100)          50100       ['dropout_183[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_184 (Dropout)          (None, 100)          0           ['dense_233[0][0]']              \n",
            "                                                                                                  \n",
            " dense_234 (Dense)              (None, 1)            101         ['dropout_184[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.3609 - mae: 1.2601\n",
            "Epoch 1: val_loss improved from inf to 0.90736, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 4s 60ms/step - loss: 4.3609 - mae: 1.2601 - val_loss: 0.9074 - val_mae: 0.4133\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8796 - mae: 0.6100\n",
            "Epoch 2: val_loss improved from 0.90736 to 0.88161, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8796 - mae: 0.6100 - val_loss: 0.8816 - val_mae: 0.7661\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9415 - mae: 0.6480\n",
            "Epoch 3: val_loss improved from 0.88161 to 0.83058, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.9415 - mae: 0.6480 - val_loss: 0.8306 - val_mae: 0.6497\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8727 - mae: 0.6158\n",
            "Epoch 4: val_loss improved from 0.83058 to 0.82452, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8727 - mae: 0.6158 - val_loss: 0.8245 - val_mae: 0.5772\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8772 - mae: 0.6182\n",
            "Epoch 5: val_loss improved from 0.82452 to 0.82276, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8772 - mae: 0.6182 - val_loss: 0.8228 - val_mae: 0.6230\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8831 - mae: 0.6192\n",
            "Epoch 6: val_loss did not improve from 0.82276\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8831 - mae: 0.6192 - val_loss: 0.8570 - val_mae: 0.7348\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9045 - mae: 0.6311\n",
            "Epoch 7: val_loss improved from 0.82276 to 0.81789, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 0.9009 - mae: 0.6275 - val_loss: 0.8179 - val_mae: 0.6120\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8956 - mae: 0.6201\n",
            "Epoch 8: val_loss did not improve from 0.81789\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8956 - mae: 0.6201 - val_loss: 0.8401 - val_mae: 0.7069\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8635 - mae: 0.6148\n",
            "Epoch 9: val_loss did not improve from 0.81789\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8635 - mae: 0.6148 - val_loss: 0.8783 - val_mae: 0.4197\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9462 - mae: 0.6265\n",
            "Epoch 10: val_loss did not improve from 0.81789\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.9462 - mae: 0.6265 - val_loss: 0.8303 - val_mae: 0.6914\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8789 - mae: 0.6083\n",
            "Epoch 11: val_loss did not improve from 0.81789\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8789 - mae: 0.6083 - val_loss: 0.9238 - val_mae: 0.8346\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8955 - mae: 0.6271\n",
            "Epoch 12: val_loss did not improve from 0.81789\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8955 - mae: 0.6271 - val_loss: 0.8568 - val_mae: 0.7507\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8959 - mae: 0.6189\n",
            "Epoch 13: val_loss did not improve from 0.81789\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8959 - mae: 0.6189 - val_loss: 0.8577 - val_mae: 0.7547\n",
            "Epoch 13: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8468841314315796, RMSE:0.9202630519866943, MAE:0.6250134110450745, R2:0.01185616168204262\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372, 0.01185616168204262]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n",
            "Model: \"model_155\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_111 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_115 (Embedding)      (None, 17, 900)      649800      ['input_111[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_165 (Conv1D)            (None, 17, 100)      270100      ['embedding_115[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_166 (Conv1D)            (None, 17, 100)      270100      ['embedding_115[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_167 (Conv1D)            (None, 17, 100)      270100      ['embedding_115[0][0]']          \n",
            "                                                                                                  \n",
            " input_112 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_345 (Glob  (None, 100)         0           ['conv1d_165[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_346 (Glob  (None, 100)         0           ['conv1d_166[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_347 (Glob  (None, 100)         0           ['conv1d_167[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_235 (Dense)              (None, 3)            33          ['input_112[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_55 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_345[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_346[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_347[0][0]'\n",
            "                                                                 , 'dense_235[0][0]']             \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                  \n",
            " dense_236 (Dense)              (None, 1000)         304000      ['concatenate_55[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_185 (Dropout)          (None, 1000)         0           ['dense_236[0][0]']              \n",
            "                                                                                                  \n",
            " dense_237 (Dense)              (None, 500)          500500      ['dropout_185[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_186 (Dropout)          (None, 500)          0           ['dense_237[0][0]']              \n",
            "                                                                                                  \n",
            " dense_238 (Dense)              (None, 100)          50100       ['dropout_186[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_187 (Dropout)          (None, 100)          0           ['dense_238[0][0]']              \n",
            "                                                                                                  \n",
            " dense_239 (Dense)              (None, 1)            101         ['dropout_187[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.9064 - mae: 1.3091\n",
            "Epoch 1: val_loss improved from inf to 0.84645, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 4s 43ms/step - loss: 4.7752 - mae: 1.2867 - val_loss: 0.8465 - val_mae: 0.6810\n",
            "Epoch 2/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8964 - mae: 0.6271\n",
            "Epoch 2: val_loss did not improve from 0.84645\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.9095 - mae: 0.6261 - val_loss: 1.1651 - val_mae: 1.0182\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9679 - mae: 0.6684\n",
            "Epoch 3: val_loss did not improve from 0.84645\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.9679 - mae: 0.6684 - val_loss: 1.0622 - val_mae: 0.9486\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9670 - mae: 0.6444\n",
            "Epoch 4: val_loss did not improve from 0.84645\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.9685 - mae: 0.6510 - val_loss: 0.8843 - val_mae: 0.4324\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8930 - mae: 0.6141\n",
            "Epoch 5: val_loss did not improve from 0.84645\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.8930 - mae: 0.6141 - val_loss: 0.9346 - val_mae: 0.3937\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9756 - mae: 0.6511\n",
            "Epoch 6: val_loss did not improve from 0.84645\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.9756 - mae: 0.6511 - val_loss: 0.9607 - val_mae: 0.8652\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9428 - mae: 0.6431\n",
            "Epoch 7: val_loss did not improve from 0.84645\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.9441 - mae: 0.6500 - val_loss: 0.8947 - val_mae: 0.4108\n",
            "Epoch 7: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8747063875198364, RMSE:0.9352573752403259, MAE:0.6911579370498657, R2:-0.020606800611077558\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372, 0.01185616168204262, -0.020606800611077558]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n",
            "Model: \"model_156\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_113 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_116 (Embedding)      (None, 17, 900)      649800      ['input_113[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_168 (Conv1D)            (None, 17, 100)      270100      ['embedding_116[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_169 (Conv1D)            (None, 17, 100)      270100      ['embedding_116[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_170 (Conv1D)            (None, 17, 100)      270100      ['embedding_116[0][0]']          \n",
            "                                                                                                  \n",
            " input_114 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_348 (Glob  (None, 100)         0           ['conv1d_168[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_349 (Glob  (None, 100)         0           ['conv1d_169[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_350 (Glob  (None, 100)         0           ['conv1d_170[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_240 (Dense)              (None, 3)            33          ['input_114[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_56 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_348[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_349[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_350[0][0]'\n",
            "                                                                 , 'dense_240[0][0]']             \n",
            "                                                                                                  \n",
            " dense_241 (Dense)              (None, 1000)         304000      ['concatenate_56[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_188 (Dropout)          (None, 1000)         0           ['dense_241[0][0]']              \n",
            "                                                                                                  \n",
            " dense_242 (Dense)              (None, 500)          500500      ['dropout_188[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_189 (Dropout)          (None, 500)          0           ['dense_242[0][0]']              \n",
            "                                                                                                  \n",
            " dense_243 (Dense)              (None, 100)          50100       ['dropout_189[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_190 (Dropout)          (None, 100)          0           ['dense_243[0][0]']              \n",
            "                                                                                                  \n",
            " dense_244 (Dense)              (None, 1)            101         ['dropout_190[0][0]']            \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.5347 - mae: 1.2618\n",
            "Epoch 1: val_loss improved from inf to 0.86109, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 5s 60ms/step - loss: 4.5347 - mae: 1.2618 - val_loss: 0.8611 - val_mae: 0.7279\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8720 - mae: 0.6207\n",
            "Epoch 2: val_loss improved from 0.86109 to 0.85527, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 0.8720 - mae: 0.6207 - val_loss: 0.8553 - val_mae: 0.4852\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8883 - mae: 0.6137\n",
            "Epoch 3: val_loss did not improve from 0.85527\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8883 - mae: 0.6137 - val_loss: 0.8705 - val_mae: 0.7506\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8704 - mae: 0.6161\n",
            "Epoch 4: val_loss improved from 0.85527 to 0.83963, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.8704 - mae: 0.6161 - val_loss: 0.8396 - val_mae: 0.6881\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8672 - mae: 0.6109\n",
            "Epoch 5: val_loss did not improve from 0.83963\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8721 - mae: 0.6174 - val_loss: 0.8435 - val_mae: 0.4964\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8711 - mae: 0.6146\n",
            "Epoch 6: val_loss improved from 0.83963 to 0.82044, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 0.8647 - mae: 0.6111 - val_loss: 0.8204 - val_mae: 0.5851\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8606 - mae: 0.6121\n",
            "Epoch 7: val_loss improved from 0.82044 to 0.81903, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 0.8606 - mae: 0.6121 - val_loss: 0.8190 - val_mae: 0.6164\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9171 - mae: 0.6258\n",
            "Epoch 8: val_loss did not improve from 0.81903\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.9171 - mae: 0.6258 - val_loss: 0.8331 - val_mae: 0.6878\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8724 - mae: 0.6108\n",
            "Epoch 9: val_loss improved from 0.81903 to 0.81423, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8732 - mae: 0.6134 - val_loss: 0.8142 - val_mae: 0.5901\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8504 - mae: 0.6086\n",
            "Epoch 10: val_loss did not improve from 0.81423\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8551 - mae: 0.6077 - val_loss: 0.8497 - val_mae: 0.7315\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8690 - mae: 0.6104\n",
            "Epoch 11: val_loss did not improve from 0.81423\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8690 - mae: 0.6104 - val_loss: 0.8604 - val_mae: 0.7525\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8502 - mae: 0.6170\n",
            "Epoch 12: val_loss did not improve from 0.81423\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8487 - mae: 0.6099 - val_loss: 0.8339 - val_mae: 0.7061\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8614 - mae: 0.6116\n",
            "Epoch 13: val_loss did not improve from 0.81423\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8614 - mae: 0.6116 - val_loss: 0.8670 - val_mae: 0.4208\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8667 - mae: 0.6035\n",
            "Epoch 14: val_loss improved from 0.81423 to 0.80442, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8667 - mae: 0.6035 - val_loss: 0.8044 - val_mae: 0.6113\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8726 - mae: 0.6177\n",
            "Epoch 15: val_loss did not improve from 0.80442\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8726 - mae: 0.6177 - val_loss: 0.8906 - val_mae: 0.3858\n",
            "Epoch 16/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8840 - mae: 0.6033\n",
            "Epoch 16: val_loss did not improve from 0.80442\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8802 - mae: 0.6036 - val_loss: 0.8077 - val_mae: 0.5222\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8375 - mae: 0.6025\n",
            "Epoch 17: val_loss improved from 0.80442 to 0.79644, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8353 - mae: 0.6014 - val_loss: 0.7964 - val_mae: 0.5911\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8380 - mae: 0.6068\n",
            "Epoch 18: val_loss did not improve from 0.79644\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8413 - mae: 0.6003 - val_loss: 0.8630 - val_mae: 0.7723\n",
            "Epoch 19/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8588 - mae: 0.6022\n",
            "Epoch 19: val_loss did not improve from 0.79644\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8581 - mae: 0.6039 - val_loss: 0.8011 - val_mae: 0.6540\n",
            "Epoch 20/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8594 - mae: 0.5991\n",
            "Epoch 20: val_loss did not improve from 0.79644\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8599 - mae: 0.6017 - val_loss: 0.7998 - val_mae: 0.5081\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8316 - mae: 0.5988\n",
            "Epoch 21: val_loss improved from 0.79644 to 0.78569, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8283 - mae: 0.5964 - val_loss: 0.7857 - val_mae: 0.5730\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8243 - mae: 0.5951\n",
            "Epoch 22: val_loss did not improve from 0.78569\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8243 - mae: 0.5951 - val_loss: 0.7874 - val_mae: 0.6281\n",
            "Epoch 23/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8367 - mae: 0.6063\n",
            "Epoch 23: val_loss did not improve from 0.78569\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8231 - mae: 0.5995 - val_loss: 0.8980 - val_mae: 0.4200\n",
            "Epoch 24/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8413 - mae: 0.5931\n",
            "Epoch 24: val_loss did not improve from 0.78569\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8420 - mae: 0.5939 - val_loss: 0.9121 - val_mae: 0.4420\n",
            "Epoch 25/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8404 - mae: 0.5923\n",
            "Epoch 25: val_loss did not improve from 0.78569\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8319 - mae: 0.5909 - val_loss: 0.8026 - val_mae: 0.4537\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8263 - mae: 0.5931\n",
            "Epoch 26: val_loss improved from 0.78569 to 0.76993, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8223 - mae: 0.5902 - val_loss: 0.7699 - val_mae: 0.5534\n",
            "Epoch 27/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8411 - mae: 0.6015\n",
            "Epoch 27: val_loss did not improve from 0.76993\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8353 - mae: 0.5950 - val_loss: 0.7746 - val_mae: 0.6358\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8098 - mae: 0.5832\n",
            "Epoch 28: val_loss improved from 0.76993 to 0.76229, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8139 - mae: 0.5860 - val_loss: 0.7623 - val_mae: 0.5579\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8286 - mae: 0.5922\n",
            "Epoch 29: val_loss improved from 0.76229 to 0.75980, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8250 - mae: 0.5910 - val_loss: 0.7598 - val_mae: 0.5385\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8908 - mae: 0.6345\n",
            "Epoch 30: val_loss did not improve from 0.75980\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8854 - mae: 0.6280 - val_loss: 0.8001 - val_mae: 0.4162\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8186 - mae: 0.5778\n",
            "Epoch 31: val_loss did not improve from 0.75980\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8206 - mae: 0.5721 - val_loss: 0.8459 - val_mae: 0.7792\n",
            "Epoch 32/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8636 - mae: 0.6173\n",
            "Epoch 32: val_loss did not improve from 0.75980\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8657 - mae: 0.6116 - val_loss: 0.8376 - val_mae: 0.7711\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8944 - mae: 0.6206\n",
            "Epoch 33: val_loss did not improve from 0.75980\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8957 - mae: 0.6267 - val_loss: 0.8264 - val_mae: 0.3940\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8133 - mae: 0.5731\n",
            "Epoch 34: val_loss improved from 0.75980 to 0.74490, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8111 - mae: 0.5714 - val_loss: 0.7449 - val_mae: 0.5989\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8150 - mae: 0.5833\n",
            "Epoch 35: val_loss did not improve from 0.74490\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8150 - mae: 0.5833 - val_loss: 0.7515 - val_mae: 0.4663\n",
            "Epoch 36/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7937 - mae: 0.5644\n",
            "Epoch 36: val_loss did not improve from 0.74490\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.7927 - mae: 0.5625 - val_loss: 0.7855 - val_mae: 0.7097\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8300 - mae: 0.5859\n",
            "Epoch 37: val_loss did not improve from 0.74490\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8300 - mae: 0.5859 - val_loss: 0.8072 - val_mae: 0.3984\n",
            "Epoch 38/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9064 - mae: 0.6421\n",
            "Epoch 38: val_loss did not improve from 0.74490\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.9061 - mae: 0.6388 - val_loss: 0.8309 - val_mae: 0.7722\n",
            "Epoch 39/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8135 - mae: 0.5832\n",
            "Epoch 39: val_loss improved from 0.74490 to 0.72561, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8095 - mae: 0.5810 - val_loss: 0.7256 - val_mae: 0.5049\n",
            "Epoch 40/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8439 - mae: 0.6030\n",
            "Epoch 40: val_loss did not improve from 0.72561\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8434 - mae: 0.6063 - val_loss: 0.7432 - val_mae: 0.4314\n",
            "Epoch 41/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8259 - mae: 0.5913\n",
            "Epoch 41: val_loss did not improve from 0.72561\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8304 - mae: 0.5869 - val_loss: 0.8296 - val_mae: 0.7735\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8459 - mae: 0.6057\n",
            "Epoch 42: val_loss did not improve from 0.72561\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.8459 - mae: 0.6057 - val_loss: 0.7807 - val_mae: 0.3943\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7889 - mae: 0.5521\n",
            "Epoch 43: val_loss did not improve from 0.72561\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.7889 - mae: 0.5521 - val_loss: 0.9694 - val_mae: 0.8938\n",
            "Epoch 44/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9079 - mae: 0.6540\n",
            "Epoch 44: val_loss improved from 0.72561 to 0.71415, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.9049 - mae: 0.6529 - val_loss: 0.7141 - val_mae: 0.4765\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7977 - mae: 0.5657\n",
            "Epoch 45: val_loss did not improve from 0.71415\n",
            "25/25 [==============================] - 1s 39ms/step - loss: 0.7977 - mae: 0.5657 - val_loss: 0.7276 - val_mae: 0.6286\n",
            "Epoch 46/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8050 - mae: 0.5714\n",
            "Epoch 46: val_loss did not improve from 0.71415\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8040 - mae: 0.5742 - val_loss: 0.7526 - val_mae: 0.3734\n",
            "Epoch 47/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7725 - mae: 0.5450\n",
            "Epoch 47: val_loss improved from 0.71415 to 0.70842, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.7729 - mae: 0.5426 - val_loss: 0.7084 - val_mae: 0.5836\n",
            "Epoch 48/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8526 - mae: 0.6176\n",
            "Epoch 48: val_loss did not improve from 0.70842\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8562 - mae: 0.6239 - val_loss: 0.8208 - val_mae: 0.4782\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8761 - mae: 0.6414\n",
            "Epoch 49: val_loss did not improve from 0.70842\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8761 - mae: 0.6414 - val_loss: 0.7333 - val_mae: 0.3860\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7990 - mae: 0.5784\n",
            "Epoch 50: val_loss did not improve from 0.70842\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.7990 - mae: 0.5784 - val_loss: 0.7452 - val_mae: 0.3806\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7749 - mae: 0.5446\n",
            "Epoch 51: val_loss improved from 0.70842 to 0.70003, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 0.7749 - mae: 0.5446 - val_loss: 0.7000 - val_mae: 0.4491\n",
            "Epoch 52/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7934 - mae: 0.5679\n",
            "Epoch 52: val_loss improved from 0.70003 to 0.69140, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 45ms/step - loss: 0.7877 - mae: 0.5636 - val_loss: 0.6914 - val_mae: 0.4799\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7872 - mae: 0.5667\n",
            "Epoch 53: val_loss did not improve from 0.69140\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.7872 - mae: 0.5667 - val_loss: 0.7030 - val_mae: 0.4227\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8473 - mae: 0.6138\n",
            "Epoch 54: val_loss did not improve from 0.69140\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8473 - mae: 0.6138 - val_loss: 0.6933 - val_mae: 0.5686\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7548 - mae: 0.5372\n",
            "Epoch 55: val_loss improved from 0.69140 to 0.68710, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 0.7548 - mae: 0.5372 - val_loss: 0.6871 - val_mae: 0.4616\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7376 - mae: 0.5213\n",
            "Epoch 56: val_loss did not improve from 0.68710\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.7376 - mae: 0.5213 - val_loss: 0.7024 - val_mae: 0.6065\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8568 - mae: 0.6313\n",
            "Epoch 57: val_loss improved from 0.68710 to 0.68129, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8568 - mae: 0.6313 - val_loss: 0.6813 - val_mae: 0.4736\n",
            "Epoch 58/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7615 - mae: 0.5356\n",
            "Epoch 58: val_loss did not improve from 0.68129\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.7639 - mae: 0.5404 - val_loss: 0.7055 - val_mae: 0.3850\n",
            "Epoch 59/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8488 - mae: 0.6270\n",
            "Epoch 59: val_loss improved from 0.68129 to 0.67616, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 0.8417 - mae: 0.6210 - val_loss: 0.6762 - val_mae: 0.5172\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8819 - mae: 0.6383\n",
            "Epoch 60: val_loss did not improve from 0.67616\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8819 - mae: 0.6383 - val_loss: 0.6902 - val_mae: 0.4148\n",
            "Epoch 61/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7670 - mae: 0.5568\n",
            "Epoch 61: val_loss did not improve from 0.67616\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.7705 - mae: 0.5538 - val_loss: 0.7704 - val_mae: 0.7205\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7670 - mae: 0.5533\n",
            "Epoch 62: val_loss did not improve from 0.67616\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.7670 - mae: 0.5533 - val_loss: 0.8140 - val_mae: 0.7665\n",
            "Epoch 63/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7907 - mae: 0.5755\n",
            "Epoch 63: val_loss did not improve from 0.67616\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 0.7917 - mae: 0.5790 - val_loss: 0.7034 - val_mae: 0.3675\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7480 - mae: 0.5339\n",
            "Epoch 64: val_loss did not improve from 0.67616\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.7480 - mae: 0.5339 - val_loss: 0.7119 - val_mae: 0.6440\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7920 - mae: 0.5842\n",
            "Epoch 65: val_loss did not improve from 0.67616\n",
            "25/25 [==============================] - 1s 40ms/step - loss: 0.7920 - mae: 0.5842 - val_loss: 0.7509 - val_mae: 0.7010\n",
            "Epoch 65: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7173014879226685, RMSE:0.8469365239143372, MAE:0.5345284342765808, R2:0.1630530721057133\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372, 0.01185616168204262, -0.020606800611077558, 0.1630530721057133]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_157\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_115 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_117 (Embedding)      (None, 17, 900)      649800      ['input_115[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_171 (Conv1D)            (None, 17, 100)      270100      ['embedding_117[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_172 (Conv1D)            (None, 17, 100)      270100      ['embedding_117[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_173 (Conv1D)            (None, 17, 100)      270100      ['embedding_117[0][0]']          \n",
            "                                                                                                  \n",
            " input_116 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_351 (Glob  (None, 100)         0           ['conv1d_171[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_352 (Glob  (None, 100)         0           ['conv1d_172[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_353 (Glob  (None, 100)         0           ['conv1d_173[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_245 (Dense)              (None, 3)            33          ['input_116[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_57 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_351[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_352[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_353[0][0]'\n",
            "                                                                 , 'dense_245[0][0]']             \n",
            "                                                                                                  \n",
            " dense_246 (Dense)              (None, 1000)         304000      ['concatenate_57[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_191 (Dropout)          (None, 1000)         0           ['dense_246[0][0]']              \n",
            "                                                                                                  \n",
            " dense_247 (Dense)              (None, 500)          500500      ['dropout_191[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_192 (Dropout)          (None, 500)          0           ['dense_247[0][0]']              \n",
            "                                                                                                  \n",
            " dense_248 (Dense)              (None, 100)          50100       ['dropout_192[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_193 (Dropout)          (None, 100)          0           ['dense_248[0][0]']              \n",
            "                                                                                                  \n",
            " dense_249 (Dense)              (None, 1)            101         ['dropout_193[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 6.7141 - mae: 1.6030\n",
            "Epoch 1: val_loss improved from inf to 1.01340, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 4s 46ms/step - loss: 6.5251 - mae: 1.5784 - val_loss: 1.0134 - val_mae: 0.4675\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9034 - mae: 0.6237\n",
            "Epoch 2: val_loss improved from 1.01340 to 0.85271, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9034 - mae: 0.6237 - val_loss: 0.8527 - val_mae: 0.7078\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9018 - mae: 0.6327\n",
            "Epoch 3: val_loss did not improve from 0.85271\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8990 - mae: 0.6325 - val_loss: 0.8624 - val_mae: 0.4716\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8789 - mae: 0.6091\n",
            "Epoch 4: val_loss did not improve from 0.85271\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8791 - mae: 0.6142 - val_loss: 0.8726 - val_mae: 0.4494\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9099 - mae: 0.6255\n",
            "Epoch 5: val_loss did not improve from 0.85271\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9041 - mae: 0.6258 - val_loss: 0.8782 - val_mae: 0.4367\n",
            "Epoch 6/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8790 - mae: 0.6055\n",
            "Epoch 6: val_loss improved from 0.85271 to 0.83106, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8799 - mae: 0.6089 - val_loss: 0.8311 - val_mae: 0.6715\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8577 - mae: 0.6125\n",
            "Epoch 7: val_loss did not improve from 0.83106\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8610 - mae: 0.6115 - val_loss: 0.8751 - val_mae: 0.7671\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8824 - mae: 0.6145\n",
            "Epoch 8: val_loss improved from 0.83106 to 0.81939, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8824 - mae: 0.6145 - val_loss: 0.8194 - val_mae: 0.5459\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8561 - mae: 0.6094\n",
            "Epoch 9: val_loss improved from 0.81939 to 0.81496, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 50ms/step - loss: 0.8561 - mae: 0.6094 - val_loss: 0.8150 - val_mae: 0.5544\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8636 - mae: 0.6110\n",
            "Epoch 10: val_loss improved from 0.81496 to 0.81211, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.8636 - mae: 0.6110 - val_loss: 0.8121 - val_mae: 0.5544\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9277 - mae: 0.6373\n",
            "Epoch 11: val_loss improved from 0.81211 to 0.80904, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 0.9206 - mae: 0.6303 - val_loss: 0.8090 - val_mae: 0.6260\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8637 - mae: 0.6057\n",
            "Epoch 12: val_loss did not improve from 0.80904\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8637 - mae: 0.6057 - val_loss: 0.8563 - val_mae: 0.7528\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8739 - mae: 0.6156\n",
            "Epoch 13: val_loss did not improve from 0.80904\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8739 - mae: 0.6156 - val_loss: 0.8176 - val_mae: 0.4988\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8808 - mae: 0.6132\n",
            "Epoch 14: val_loss did not improve from 0.80904\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8882 - mae: 0.6091 - val_loss: 0.9861 - val_mae: 0.8972\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9248 - mae: 0.6385\n",
            "Epoch 15: val_loss did not improve from 0.80904\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.9248 - mae: 0.6385 - val_loss: 0.9233 - val_mae: 0.4243\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8591 - mae: 0.5981\n",
            "Epoch 16: val_loss did not improve from 0.80904\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8591 - mae: 0.5981 - val_loss: 0.8153 - val_mae: 0.6907\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8636 - mae: 0.6044\n",
            "Epoch 17: val_loss improved from 0.80904 to 0.79340, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.8612 - mae: 0.6053 - val_loss: 0.7934 - val_mae: 0.5389\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8553 - mae: 0.6035\n",
            "Epoch 18: val_loss did not improve from 0.79340\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8553 - mae: 0.6035 - val_loss: 0.8377 - val_mae: 0.4246\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8585 - mae: 0.5964\n",
            "Epoch 19: val_loss did not improve from 0.79340\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8640 - mae: 0.5919 - val_loss: 0.8896 - val_mae: 0.8118\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8484 - mae: 0.6027\n",
            "Epoch 20: val_loss improved from 0.79340 to 0.79064, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8484 - mae: 0.6027 - val_loss: 0.7906 - val_mae: 0.6505\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8359 - mae: 0.5902\n",
            "Epoch 21: val_loss did not improve from 0.79064\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8396 - mae: 0.5918 - val_loss: 0.8737 - val_mae: 0.7976\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8216 - mae: 0.5942\n",
            "Epoch 22: val_loss improved from 0.79064 to 0.77406, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 40ms/step - loss: 0.8216 - mae: 0.5942 - val_loss: 0.7741 - val_mae: 0.5970\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8312 - mae: 0.5893\n",
            "Epoch 23: val_loss did not improve from 0.77406\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8312 - mae: 0.5893 - val_loss: 0.7775 - val_mae: 0.6349\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8306 - mae: 0.5930\n",
            "Epoch 24: val_loss did not improve from 0.77406\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8306 - mae: 0.5930 - val_loss: 0.8248 - val_mae: 0.4017\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8470 - mae: 0.5869\n",
            "Epoch 25: val_loss improved from 0.77406 to 0.76395, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 49ms/step - loss: 0.8470 - mae: 0.5869 - val_loss: 0.7639 - val_mae: 0.5354\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8252 - mae: 0.5870\n",
            "Epoch 26: val_loss did not improve from 0.76395\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 0.8243 - mae: 0.5875 - val_loss: 0.7679 - val_mae: 0.6345\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8298 - mae: 0.5848\n",
            "Epoch 27: val_loss did not improve from 0.76395\n",
            "25/25 [==============================] - 1s 41ms/step - loss: 0.8298 - mae: 0.5848 - val_loss: 0.7784 - val_mae: 0.4542\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8753 - mae: 0.6248\n",
            "Epoch 28: val_loss improved from 0.76395 to 0.75442, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 51ms/step - loss: 0.8753 - mae: 0.6248 - val_loss: 0.7544 - val_mae: 0.5162\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8336 - mae: 0.5927\n",
            "Epoch 29: val_loss did not improve from 0.75442\n",
            "25/25 [==============================] - 1s 42ms/step - loss: 0.8330 - mae: 0.5872 - val_loss: 0.7785 - val_mae: 0.6827\n",
            "Epoch 30/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8318 - mae: 0.6026\n",
            "Epoch 30: val_loss did not improve from 0.75442\n",
            "25/25 [==============================] - 1s 44ms/step - loss: 0.8342 - mae: 0.6000 - val_loss: 0.8186 - val_mae: 0.7494\n",
            "Epoch 31/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8250 - mae: 0.5817\n",
            "Epoch 31: val_loss improved from 0.75442 to 0.73994, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 55ms/step - loss: 0.8254 - mae: 0.5835 - val_loss: 0.7399 - val_mae: 0.5357\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8216 - mae: 0.5927\n",
            "Epoch 32: val_loss did not improve from 0.73994\n",
            "25/25 [==============================] - 1s 51ms/step - loss: 0.8216 - mae: 0.5927 - val_loss: 0.9337 - val_mae: 0.5293\n",
            "Epoch 33/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8730 - mae: 0.6065\n",
            "Epoch 33: val_loss did not improve from 0.73994\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 0.8736 - mae: 0.6009 - val_loss: 0.8322 - val_mae: 0.7712\n",
            "Epoch 34/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8162 - mae: 0.5799\n",
            "Epoch 34: val_loss did not improve from 0.73994\n",
            "25/25 [==============================] - 1s 50ms/step - loss: 0.8143 - mae: 0.5830 - val_loss: 0.7817 - val_mae: 0.3864\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8024 - mae: 0.5687\n",
            "Epoch 35: val_loss did not improve from 0.73994\n",
            "25/25 [==============================] - 1s 38ms/step - loss: 0.8024 - mae: 0.5687 - val_loss: 0.8020 - val_mae: 0.4008\n",
            "Epoch 36/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8574 - mae: 0.6060\n",
            "Epoch 36: val_loss did not improve from 0.73994\n",
            "25/25 [==============================] - 1s 46ms/step - loss: 0.8554 - mae: 0.6020 - val_loss: 0.7896 - val_mae: 0.7224\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7831 - mae: 0.5630\n",
            "Epoch 37: val_loss improved from 0.73994 to 0.71857, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 59ms/step - loss: 0.7831 - mae: 0.5630 - val_loss: 0.7186 - val_mae: 0.5383\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7833 - mae: 0.5561\n",
            "Epoch 38: val_loss did not improve from 0.71857\n",
            "25/25 [==============================] - 1s 43ms/step - loss: 0.7833 - mae: 0.5561 - val_loss: 0.7375 - val_mae: 0.6377\n",
            "Epoch 39/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8126 - mae: 0.5713\n",
            "Epoch 39: val_loss did not improve from 0.71857\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8104 - mae: 0.5727 - val_loss: 0.7380 - val_mae: 0.4195\n",
            "Epoch 40/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7980 - mae: 0.5707\n",
            "Epoch 40: val_loss did not improve from 0.71857\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.7996 - mae: 0.5662 - val_loss: 0.7714 - val_mae: 0.7039\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8862 - mae: 0.6434\n",
            "Epoch 41: val_loss did not improve from 0.71857\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8862 - mae: 0.6434 - val_loss: 1.0500 - val_mae: 0.6630\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8245 - mae: 0.5850\n",
            "Epoch 42: val_loss did not improve from 0.71857\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.8245 - mae: 0.5850 - val_loss: 0.7844 - val_mae: 0.7237\n",
            "Epoch 43/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8375 - mae: 0.6056\n",
            "Epoch 43: val_loss did not improve from 0.71857\n",
            "25/25 [==============================] - 1s 37ms/step - loss: 0.8363 - mae: 0.6105 - val_loss: 0.8152 - val_mae: 0.4641\n",
            "Epoch 43: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7569121718406677, RMSE:0.8700069785118103, MAE:0.5550954341888428, R2:0.11683536432251918\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372, 0.01185616168204262, -0.020606800611077558, 0.1630530721057133, 0.11683536432251918]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_158\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_117 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_118 (Embedding)      (None, 17, 900)      649800      ['input_117[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_174 (Conv1D)            (None, 17, 100)      270100      ['embedding_118[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_175 (Conv1D)            (None, 17, 100)      270100      ['embedding_118[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_176 (Conv1D)            (None, 17, 100)      270100      ['embedding_118[0][0]']          \n",
            "                                                                                                  \n",
            " input_118 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_354 (Glob  (None, 100)         0           ['conv1d_174[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_355 (Glob  (None, 100)         0           ['conv1d_175[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_356 (Glob  (None, 100)         0           ['conv1d_176[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_250 (Dense)              (None, 3)            33          ['input_118[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_58 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_354[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_355[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_356[0][0]'\n",
            "                                                                 , 'dense_250[0][0]']             \n",
            "                                                                                                  \n",
            " dense_251 (Dense)              (None, 1000)         304000      ['concatenate_58[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_194 (Dropout)          (None, 1000)         0           ['dense_251[0][0]']              \n",
            "                                                                                                  \n",
            " dense_252 (Dense)              (None, 500)          500500      ['dropout_194[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_195 (Dropout)          (None, 500)          0           ['dense_252[0][0]']              \n",
            "                                                                                                  \n",
            " dense_253 (Dense)              (None, 100)          50100       ['dropout_195[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_196 (Dropout)          (None, 100)          0           ['dense_253[0][0]']              \n",
            "                                                                                                  \n",
            " dense_254 (Dense)              (None, 1)            101         ['dropout_196[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 4.2148 - mae: 1.2838\n",
            "Epoch 1: val_loss improved from inf to 1.19919, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 44ms/step - loss: 3.9998 - mae: 1.2546 - val_loss: 1.1992 - val_mae: 0.6299\n",
            "Epoch 2/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 1.0986 - mae: 0.7421\n",
            "Epoch 2: val_loss improved from 1.19919 to 0.97439, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 1.0954 - mae: 0.7399 - val_loss: 0.9744 - val_mae: 0.8787\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9250 - mae: 0.6407\n",
            "Epoch 3: val_loss improved from 0.97439 to 0.81378, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9250 - mae: 0.6407 - val_loss: 0.8138 - val_mae: 0.6294\n",
            "Epoch 4/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9363 - mae: 0.6421\n",
            "Epoch 4: val_loss did not improve from 0.81378\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9429 - mae: 0.6516 - val_loss: 0.8449 - val_mae: 0.4579\n",
            "Epoch 5/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0176 - mae: 0.6924\n",
            "Epoch 5: val_loss did not improve from 0.81378\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 1.0123 - mae: 0.6861 - val_loss: 0.8520 - val_mae: 0.7506\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9828 - mae: 0.6903\n",
            "Epoch 6: val_loss improved from 0.81378 to 0.80594, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9725 - mae: 0.6833 - val_loss: 0.8059 - val_mae: 0.5674\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8608 - mae: 0.6050\n",
            "Epoch 7: val_loss improved from 0.80594 to 0.79404, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8595 - mae: 0.6045 - val_loss: 0.7940 - val_mae: 0.5909\n",
            "Epoch 8/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8744 - mae: 0.6269\n",
            "Epoch 8: val_loss did not improve from 0.79404\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8819 - mae: 0.6302 - val_loss: 0.9357 - val_mae: 0.4315\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.0007 - mae: 0.6850\n",
            "Epoch 9: val_loss did not improve from 0.79404\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 1.0007 - mae: 0.6850 - val_loss: 1.2530 - val_mae: 1.0749\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9086 - mae: 0.6429\n",
            "Epoch 10: val_loss improved from 0.79404 to 0.78337, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9039 - mae: 0.6414 - val_loss: 0.7834 - val_mae: 0.5510\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9180 - mae: 0.6394\n",
            "Epoch 11: val_loss did not improve from 0.78337\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9162 - mae: 0.6366 - val_loss: 0.8106 - val_mae: 0.7067\n",
            "Epoch 12/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0251 - mae: 0.7165\n",
            "Epoch 12: val_loss did not improve from 0.78337\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.0197 - mae: 0.7177 - val_loss: 0.8517 - val_mae: 0.4006\n",
            "Epoch 13/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8863 - mae: 0.6016\n",
            "Epoch 13: val_loss did not improve from 0.78337\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8804 - mae: 0.6036 - val_loss: 0.8083 - val_mae: 0.7100\n",
            "Epoch 14/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9737 - mae: 0.6838\n",
            "Epoch 14: val_loss did not improve from 0.78337\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9723 - mae: 0.6758 - val_loss: 0.8450 - val_mae: 0.7754\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8259 - mae: 0.5950\n",
            "Epoch 15: val_loss improved from 0.78337 to 0.76650, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8259 - mae: 0.5950 - val_loss: 0.7665 - val_mae: 0.6264\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8415 - mae: 0.6026\n",
            "Epoch 16: val_loss did not improve from 0.76650\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8415 - mae: 0.6026 - val_loss: 0.9070 - val_mae: 0.4497\n",
            "Epoch 17/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9752 - mae: 0.6801\n",
            "Epoch 17: val_loss improved from 0.76650 to 0.75961, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9683 - mae: 0.6775 - val_loss: 0.7596 - val_mae: 0.6001\n",
            "Epoch 18/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8391 - mae: 0.6100\n",
            "Epoch 18: val_loss did not improve from 0.75961\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8481 - mae: 0.6075 - val_loss: 0.9764 - val_mae: 0.9074\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8816 - mae: 0.6182\n",
            "Epoch 19: val_loss improved from 0.75961 to 0.75213, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8761 - mae: 0.6169 - val_loss: 0.7521 - val_mae: 0.5142\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.7969 - mae: 0.5851\n",
            "Epoch 20: val_loss improved from 0.75213 to 0.74352, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.7930 - mae: 0.5824 - val_loss: 0.7435 - val_mae: 0.5057\n",
            "Epoch 21/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8019 - mae: 0.5775\n",
            "Epoch 21: val_loss improved from 0.74352 to 0.74101, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8046 - mae: 0.5798 - val_loss: 0.7410 - val_mae: 0.5686\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 1.0174 - mae: 0.7169\n",
            "Epoch 22: val_loss did not improve from 0.74101\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 1.0190 - mae: 0.7114 - val_loss: 0.9178 - val_mae: 0.8584\n",
            "Epoch 23/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9273 - mae: 0.6483\n",
            "Epoch 23: val_loss did not improve from 0.74101\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9292 - mae: 0.6562 - val_loss: 0.9531 - val_mae: 0.5334\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9094 - mae: 0.6465\n",
            "Epoch 24: val_loss did not improve from 0.74101\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9094 - mae: 0.6465 - val_loss: 0.7423 - val_mae: 0.6504\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7818 - mae: 0.5736\n",
            "Epoch 25: val_loss did not improve from 0.74101\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.7818 - mae: 0.5736 - val_loss: 0.7448 - val_mae: 0.6627\n",
            "Epoch 26/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.7960 - mae: 0.5694\n",
            "Epoch 26: val_loss did not improve from 0.74101\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8033 - mae: 0.5763 - val_loss: 0.8539 - val_mae: 0.4459\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9587 - mae: 0.6890\n",
            "Epoch 27: val_loss did not improve from 0.74101\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.9542 - mae: 0.6832 - val_loss: 0.7517 - val_mae: 0.6681\n",
            "Epoch 27: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7743582725524902, RMSE:0.8799762725830078, MAE:0.5828953981399536, R2:0.09647933283335941\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372, 0.01185616168204262, -0.020606800611077558, 0.1630530721057133, 0.11683536432251918, 0.09647933283335941]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_159\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_119 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_119 (Embedding)      (None, 17, 900)      649800      ['input_119[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_177 (Conv1D)            (None, 17, 100)      270100      ['embedding_119[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_178 (Conv1D)            (None, 17, 100)      270100      ['embedding_119[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_179 (Conv1D)            (None, 17, 100)      270100      ['embedding_119[0][0]']          \n",
            "                                                                                                  \n",
            " input_120 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_357 (Glob  (None, 100)         0           ['conv1d_177[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_358 (Glob  (None, 100)         0           ['conv1d_178[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_359 (Glob  (None, 100)         0           ['conv1d_179[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " dense_255 (Dense)              (None, 3)            33          ['input_120[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_59 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_357[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_358[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_359[0][0]'\n",
            "                                                                 , 'dense_255[0][0]']             \n",
            "                                                                                                  \n",
            " dense_256 (Dense)              (None, 1000)         304000      ['concatenate_59[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_197 (Dropout)          (None, 1000)         0           ['dense_256[0][0]']              \n",
            "                                                                                                  \n",
            " dense_257 (Dense)              (None, 500)          500500      ['dropout_197[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_198 (Dropout)          (None, 500)          0           ['dense_257[0][0]']              \n",
            "                                                                                                  \n",
            " dense_258 (Dense)              (None, 100)          50100       ['dropout_198[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_199 (Dropout)          (None, 100)          0           ['dense_258[0][0]']              \n",
            "                                                                                                  \n",
            " dense_259 (Dense)              (None, 1)            101         ['dropout_199[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 4.9574 - mae: 1.3250\n",
            "Epoch 1: val_loss improved from inf to 0.86886, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 44ms/step - loss: 4.6557 - mae: 1.2724 - val_loss: 0.8689 - val_mae: 0.4662\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9251 - mae: 0.6216\n",
            "Epoch 2: val_loss did not improve from 0.86886\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9251 - mae: 0.6216 - val_loss: 0.9514 - val_mae: 0.8523\n",
            "Epoch 3/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9081 - mae: 0.6272\n",
            "Epoch 3: val_loss improved from 0.86886 to 0.85492, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.9103 - mae: 0.6308 - val_loss: 0.8549 - val_mae: 0.4815\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.0155 - mae: 0.6915\n",
            "Epoch 4: val_loss improved from 0.85492 to 0.82391, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 1.0155 - mae: 0.6915 - val_loss: 0.8239 - val_mae: 0.5864\n",
            "Epoch 5/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8630 - mae: 0.6131\n",
            "Epoch 5: val_loss did not improve from 0.82391\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8629 - mae: 0.6131 - val_loss: 0.8311 - val_mae: 0.5295\n",
            "Epoch 6/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8962 - mae: 0.6270\n",
            "Epoch 6: val_loss did not improve from 0.82391\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9074 - mae: 0.6243 - val_loss: 1.0443 - val_mae: 0.9372\n",
            "Epoch 7/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9346 - mae: 0.6546\n",
            "Epoch 7: val_loss did not improve from 0.82391\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9365 - mae: 0.6490 - val_loss: 0.9249 - val_mae: 0.8307\n",
            "Epoch 8/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9454 - mae: 0.6436\n",
            "Epoch 8: val_loss improved from 0.82391 to 0.81480, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.9425 - mae: 0.6425 - val_loss: 0.8148 - val_mae: 0.5951\n",
            "Epoch 9/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8418 - mae: 0.6055\n",
            "Epoch 9: val_loss did not improve from 0.81480\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8486 - mae: 0.6074 - val_loss: 0.8681 - val_mae: 0.7618\n",
            "Epoch 10/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9522 - mae: 0.6421\n",
            "Epoch 10: val_loss did not improve from 0.81480\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.9514 - mae: 0.6460 - val_loss: 0.8223 - val_mae: 0.5162\n",
            "Epoch 11/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.9174 - mae: 0.6254\n",
            "Epoch 11: val_loss did not improve from 0.81480\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.9106 - mae: 0.6179 - val_loss: 0.8263 - val_mae: 0.6884\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9188 - mae: 0.6236\n",
            "Epoch 12: val_loss did not improve from 0.81480\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9188 - mae: 0.6236 - val_loss: 0.8955 - val_mae: 0.3867\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8687 - mae: 0.6056\n",
            "Epoch 13: val_loss improved from 0.81480 to 0.80285, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8687 - mae: 0.6056 - val_loss: 0.8028 - val_mae: 0.5783\n",
            "Epoch 14/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8544 - mae: 0.6074\n",
            "Epoch 14: val_loss did not improve from 0.80285\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8553 - mae: 0.6054 - val_loss: 0.8205 - val_mae: 0.4878\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8409 - mae: 0.6024\n",
            "Epoch 15: val_loss did not improve from 0.80285\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8409 - mae: 0.6024 - val_loss: 0.8280 - val_mae: 0.4642\n",
            "Epoch 16/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.9745 - mae: 0.6599\n",
            "Epoch 16: val_loss improved from 0.80285 to 0.79764, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.9719 - mae: 0.6632 - val_loss: 0.7976 - val_mae: 0.6243\n",
            "Epoch 17/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8442 - mae: 0.6023\n",
            "Epoch 17: val_loss did not improve from 0.79764\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8487 - mae: 0.5997 - val_loss: 0.8414 - val_mae: 0.7423\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8466 - mae: 0.6035\n",
            "Epoch 18: val_loss improved from 0.79764 to 0.79741, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 36ms/step - loss: 0.8466 - mae: 0.6035 - val_loss: 0.7974 - val_mae: 0.5165\n",
            "Epoch 19/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8372 - mae: 0.5931\n",
            "Epoch 19: val_loss did not improve from 0.79741\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8364 - mae: 0.5971 - val_loss: 0.8266 - val_mae: 0.4394\n",
            "Epoch 20/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8995 - mae: 0.6202\n",
            "Epoch 20: val_loss did not improve from 0.79741\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.9000 - mae: 0.6184 - val_loss: 0.8126 - val_mae: 0.7023\n",
            "Epoch 21/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8790 - mae: 0.6136\n",
            "Epoch 21: val_loss did not improve from 0.79741\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.8700 - mae: 0.6123 - val_loss: 0.8700 - val_mae: 0.3860\n",
            "Epoch 22/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8571 - mae: 0.6005\n",
            "Epoch 22: val_loss did not improve from 0.79741\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8581 - mae: 0.5956 - val_loss: 0.8174 - val_mae: 0.7208\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8460 - mae: 0.5970\n",
            "Epoch 23: val_loss did not improve from 0.79741\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8460 - mae: 0.5970 - val_loss: 0.8549 - val_mae: 0.7783\n",
            "Epoch 24/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8964 - mae: 0.6383\n",
            "Epoch 24: val_loss improved from 0.79741 to 0.77301, saving model to ./cnn_model/SG_split/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8973 - mae: 0.6385 - val_loss: 0.7730 - val_mae: 0.6196\n",
            "Epoch 25/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8156 - mae: 0.5853\n",
            "Epoch 25: val_loss did not improve from 0.77301\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.8169 - mae: 0.5885 - val_loss: 0.7835 - val_mae: 0.4725\n",
            "Epoch 26/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8585 - mae: 0.6018\n",
            "Epoch 26: val_loss did not improve from 0.77301\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.8630 - mae: 0.6110 - val_loss: 0.9189 - val_mae: 0.4740\n",
            "Epoch 27/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8765 - mae: 0.6057\n",
            "Epoch 27: val_loss did not improve from 0.77301\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8751 - mae: 0.6010 - val_loss: 0.7731 - val_mae: 0.6533\n",
            "Epoch 28/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8339 - mae: 0.6043\n",
            "Epoch 28: val_loss did not improve from 0.77301\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8464 - mae: 0.6094 - val_loss: 0.9212 - val_mae: 0.4893\n",
            "Epoch 29/100\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 0.8485 - mae: 0.5960\n",
            "Epoch 29: val_loss did not improve from 0.77301\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8466 - mae: 0.5993 - val_loss: 0.7997 - val_mae: 0.4050\n",
            "Epoch 30/100\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 0.8647 - mae: 0.6023\n",
            "Epoch 30: val_loss did not improve from 0.77301\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.8620 - mae: 0.6006 - val_loss: 0.7770 - val_mae: 0.6810\n",
            "Epoch 30: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.8039616942405701, RMSE:0.8966391086578369, MAE:0.6322073340415955, R2:0.061938110133189395\n",
            "[0.015441060128791428, 0.06660595280856729, 0.057687555104453, 0.024239409486910612, -0.02467382782391736, 0.15113259425115744, 0.012020036471809936, 0.18196206547355598, 0.04624251756716846, 0.017468553496777006, 0.04855712997036077, 0.17769785502510116, 0.10004279282495943, 0.009408563706876372, 0.01185616168204262, -0.020606800611077558, 0.1630530721057133, 0.11683536432251918, 0.09647933283335941, 0.061938110133189395]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_160\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_121 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_120 (Embedding)      (None, 17, 900)      649800      ['input_121[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_121 (Embedding)      (None, 17, 900)      649800      ['input_121[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_122 (Embedding)      (None, 17, 900)      649800      ['input_121[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_123 (Embedding)      (None, 17, 900)      649800      ['input_121[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_180 (Conv1D)            (None, 17, 100)      270100      ['embedding_120[0][0]',          \n",
            "                                                                  'embedding_121[0][0]',          \n",
            "                                                                  'embedding_122[0][0]',          \n",
            "                                                                  'embedding_123[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_181 (Conv1D)            (None, 17, 100)      270100      ['embedding_120[0][0]',          \n",
            "                                                                  'embedding_121[0][0]',          \n",
            "                                                                  'embedding_122[0][0]',          \n",
            "                                                                  'embedding_123[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_182 (Conv1D)            (None, 17, 100)      270100      ['embedding_120[0][0]',          \n",
            "                                                                  'embedding_121[0][0]',          \n",
            "                                                                  'embedding_122[0][0]',          \n",
            "                                                                  'embedding_123[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_360 (Glob  (None, 100)         0           ['conv1d_180[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_361 (Glob  (None, 100)         0           ['conv1d_180[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_362 (Glob  (None, 100)         0           ['conv1d_180[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_363 (Glob  (None, 100)         0           ['conv1d_180[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_364 (Glob  (None, 100)         0           ['conv1d_181[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_365 (Glob  (None, 100)         0           ['conv1d_181[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_366 (Glob  (None, 100)         0           ['conv1d_181[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_367 (Glob  (None, 100)         0           ['conv1d_181[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_368 (Glob  (None, 100)         0           ['conv1d_182[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_369 (Glob  (None, 100)         0           ['conv1d_182[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_370 (Glob  (None, 100)         0           ['conv1d_182[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_371 (Glob  (None, 100)         0           ['conv1d_182[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_122 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 100)          0           ['global_max_pooling1d_360[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_361[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_362[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_363[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 100)          0           ['global_max_pooling1d_364[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_365[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_366[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_367[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 100)          0           ['global_max_pooling1d_368[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_369[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_370[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_371[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_260 (Dense)              (None, 1000)         11000       ['input_122[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_60 (Concatenate)   (None, 1300)         0           ['add_60[0][0]',                 \n",
            "                                                                  'add_61[0][0]',                 \n",
            "                                                                  'add_62[0][0]',                 \n",
            "                                                                  'dense_260[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_60[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_200 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_200[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_201 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_201[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_202 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_202[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_203 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_203[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_204 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_261 (Dense)              (None, 1)            3           ['dropout_204[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.9038 - mae: 1.6478\n",
            "Epoch 1: val_loss improved from inf to 0.78903, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 115ms/step - loss: 5.9038 - mae: 1.6478 - val_loss: 0.7890 - val_mae: 0.7186\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7510 - mae: 0.5398\n",
            "Epoch 2: val_loss improved from 0.78903 to 0.59744, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.7510 - mae: 0.5398 - val_loss: 0.5974 - val_mae: 0.5179\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6181 - mae: 0.4674\n",
            "Epoch 3: val_loss improved from 0.59744 to 0.52081, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.6181 - mae: 0.4674 - val_loss: 0.5208 - val_mae: 0.3175\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5131 - mae: 0.3915\n",
            "Epoch 4: val_loss improved from 0.52081 to 0.43069, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.5131 - mae: 0.3915 - val_loss: 0.4307 - val_mae: 0.3328\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4584 - mae: 0.3600\n",
            "Epoch 5: val_loss improved from 0.43069 to 0.42073, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.4584 - mae: 0.3600 - val_loss: 0.4207 - val_mae: 0.4335\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4235 - mae: 0.3491\n",
            "Epoch 6: val_loss improved from 0.42073 to 0.38008, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.4235 - mae: 0.3491 - val_loss: 0.3801 - val_mae: 0.3170\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4397 - mae: 0.3929\n",
            "Epoch 7: val_loss did not improve from 0.38008\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4397 - mae: 0.3929 - val_loss: 0.4611 - val_mae: 0.3922\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4420 - mae: 0.4027\n",
            "Epoch 8: val_loss did not improve from 0.38008\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.4420 - mae: 0.4027 - val_loss: 0.4065 - val_mae: 0.4492\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4095 - mae: 0.3729\n",
            "Epoch 9: val_loss did not improve from 0.38008\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4095 - mae: 0.3729 - val_loss: 0.4121 - val_mae: 0.3425\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4040 - mae: 0.3549\n",
            "Epoch 10: val_loss improved from 0.38008 to 0.35900, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.4040 - mae: 0.3549 - val_loss: 0.3590 - val_mae: 0.3263\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3963 - mae: 0.3620\n",
            "Epoch 11: val_loss did not improve from 0.35900\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3963 - mae: 0.3620 - val_loss: 0.3780 - val_mae: 0.3254\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3994 - mae: 0.3665\n",
            "Epoch 12: val_loss improved from 0.35900 to 0.35307, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.3994 - mae: 0.3665 - val_loss: 0.3531 - val_mae: 0.3289\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4045 - mae: 0.3881\n",
            "Epoch 13: val_loss improved from 0.35307 to 0.35066, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.4045 - mae: 0.3881 - val_loss: 0.3507 - val_mae: 0.2904\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3753 - mae: 0.3369\n",
            "Epoch 14: val_loss did not improve from 0.35066\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3753 - mae: 0.3369 - val_loss: 0.3747 - val_mae: 0.4112\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3704 - mae: 0.3379\n",
            "Epoch 15: val_loss improved from 0.35066 to 0.34767, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3704 - mae: 0.3379 - val_loss: 0.3477 - val_mae: 0.2921\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3762 - mae: 0.3552\n",
            "Epoch 16: val_loss did not improve from 0.34767\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3762 - mae: 0.3552 - val_loss: 0.4160 - val_mae: 0.4089\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3779 - mae: 0.3653\n",
            "Epoch 17: val_loss improved from 0.34767 to 0.34241, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 0.3779 - mae: 0.3653 - val_loss: 0.3424 - val_mae: 0.3051\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3681 - mae: 0.3550\n",
            "Epoch 18: val_loss improved from 0.34241 to 0.33491, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 0.3681 - mae: 0.3550 - val_loss: 0.3349 - val_mae: 0.2799\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3366 - mae: 0.3159\n",
            "Epoch 19: val_loss improved from 0.33491 to 0.30791, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 0.3366 - mae: 0.3159 - val_loss: 0.3079 - val_mae: 0.2864\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3491 - mae: 0.3436\n",
            "Epoch 20: val_loss did not improve from 0.30791\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3491 - mae: 0.3436 - val_loss: 0.3837 - val_mae: 0.4345\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3635 - mae: 0.3732\n",
            "Epoch 21: val_loss did not improve from 0.30791\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3635 - mae: 0.3732 - val_loss: 0.3873 - val_mae: 0.3667\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3456 - mae: 0.3556\n",
            "Epoch 22: val_loss improved from 0.30791 to 0.30342, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.3456 - mae: 0.3556 - val_loss: 0.3034 - val_mae: 0.2509\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3155 - mae: 0.3273\n",
            "Epoch 23: val_loss improved from 0.30342 to 0.30066, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3155 - mae: 0.3273 - val_loss: 0.3007 - val_mae: 0.2780\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3204 - mae: 0.3475\n",
            "Epoch 24: val_loss improved from 0.30066 to 0.29712, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3204 - mae: 0.3475 - val_loss: 0.2971 - val_mae: 0.3077\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3144 - mae: 0.3420\n",
            "Epoch 25: val_loss improved from 0.29712 to 0.28184, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3144 - mae: 0.3420 - val_loss: 0.2818 - val_mae: 0.3086\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2726 - mae: 0.2909\n",
            "Epoch 26: val_loss improved from 0.28184 to 0.24628, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 0.2726 - mae: 0.2909 - val_loss: 0.2463 - val_mae: 0.2431\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2578 - mae: 0.2924\n",
            "Epoch 27: val_loss improved from 0.24628 to 0.22673, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.2578 - mae: 0.2924 - val_loss: 0.2267 - val_mae: 0.2287\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3000 - mae: 0.3550\n",
            "Epoch 28: val_loss did not improve from 0.22673\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3000 - mae: 0.3550 - val_loss: 0.2652 - val_mae: 0.2800\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2291 - mae: 0.2757\n",
            "Epoch 29: val_loss improved from 0.22673 to 0.21845, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 0.2291 - mae: 0.2757 - val_loss: 0.2185 - val_mae: 0.1986\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2506 - mae: 0.3098\n",
            "Epoch 30: val_loss improved from 0.21845 to 0.19914, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.2506 - mae: 0.3098 - val_loss: 0.1991 - val_mae: 0.2278\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2542 - mae: 0.3256\n",
            "Epoch 31: val_loss did not improve from 0.19914\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2542 - mae: 0.3256 - val_loss: 0.2229 - val_mae: 0.3001\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3220 - mae: 0.3908\n",
            "Epoch 32: val_loss improved from 0.19914 to 0.18519, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3220 - mae: 0.3908 - val_loss: 0.1852 - val_mae: 0.2172\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1923 - mae: 0.2528\n",
            "Epoch 33: val_loss did not improve from 0.18519\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.1923 - mae: 0.2528 - val_loss: 0.2007 - val_mae: 0.2519\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1974 - mae: 0.2617\n",
            "Epoch 34: val_loss did not improve from 0.18519\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1974 - mae: 0.2617 - val_loss: 0.2490 - val_mae: 0.3342\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1668 - mae: 0.2315\n",
            "Epoch 35: val_loss did not improve from 0.18519\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1668 - mae: 0.2315 - val_loss: 0.1956 - val_mae: 0.1785\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1995 - mae: 0.2779\n",
            "Epoch 36: val_loss did not improve from 0.18519\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1995 - mae: 0.2779 - val_loss: 0.2048 - val_mae: 0.2915\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1948 - mae: 0.2775\n",
            "Epoch 37: val_loss did not improve from 0.18519\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1948 - mae: 0.2775 - val_loss: 0.2354 - val_mae: 0.3825\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3414 - mae: 0.4199\n",
            "Epoch 38: val_loss did not improve from 0.18519\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3414 - mae: 0.4199 - val_loss: 0.2787 - val_mae: 0.2900\n",
            "Epoch 38: early stopping\n",
            "491/491 [==============================] - 4s 7ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 5s 9ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 5s 9ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 1s 28ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.18010257184505463, RMSE:0.4243849217891693, MAE:0.22913527488708496, R2:0.7898564382883024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7898564382883024]\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_166\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_123 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_124 (Embedding)      (None, 17, 900)      649800      ['input_123[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_125 (Embedding)      (None, 17, 900)      649800      ['input_123[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_126 (Embedding)      (None, 17, 900)      649800      ['input_123[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_127 (Embedding)      (None, 17, 900)      649800      ['input_123[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_183 (Conv1D)            (None, 17, 100)      270100      ['embedding_124[0][0]',          \n",
            "                                                                  'embedding_125[0][0]',          \n",
            "                                                                  'embedding_126[0][0]',          \n",
            "                                                                  'embedding_127[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_184 (Conv1D)            (None, 17, 100)      270100      ['embedding_124[0][0]',          \n",
            "                                                                  'embedding_125[0][0]',          \n",
            "                                                                  'embedding_126[0][0]',          \n",
            "                                                                  'embedding_127[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_185 (Conv1D)            (None, 17, 100)      270100      ['embedding_124[0][0]',          \n",
            "                                                                  'embedding_125[0][0]',          \n",
            "                                                                  'embedding_126[0][0]',          \n",
            "                                                                  'embedding_127[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_372 (Glob  (None, 100)         0           ['conv1d_183[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_373 (Glob  (None, 100)         0           ['conv1d_183[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_374 (Glob  (None, 100)         0           ['conv1d_183[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_375 (Glob  (None, 100)         0           ['conv1d_183[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_376 (Glob  (None, 100)         0           ['conv1d_184[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_377 (Glob  (None, 100)         0           ['conv1d_184[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_378 (Glob  (None, 100)         0           ['conv1d_184[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_379 (Glob  (None, 100)         0           ['conv1d_184[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_380 (Glob  (None, 100)         0           ['conv1d_185[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_381 (Glob  (None, 100)         0           ['conv1d_185[1][0]']             \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_382 (Glob  (None, 100)         0           ['conv1d_185[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_383 (Glob  (None, 100)         0           ['conv1d_185[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_124 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 100)          0           ['global_max_pooling1d_372[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_373[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_374[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_375[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 100)          0           ['global_max_pooling1d_376[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_377[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_378[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_379[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 100)          0           ['global_max_pooling1d_380[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_381[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_382[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_383[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_262 (Dense)              (None, 1000)         11000       ['input_124[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_61 (Concatenate)   (None, 1300)         0           ['add_63[0][0]',                 \n",
            "                                                                  'add_64[0][0]',                 \n",
            "                                                                  'add_65[0][0]',                 \n",
            "                                                                  'dense_262[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_61[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_205 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_205[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_206 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_206[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_207 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_207[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_208 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_208[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_209 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_263 (Dense)              (None, 1)            3           ['dropout_209[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.2376 - mae: 1.5070\n",
            "Epoch 1: val_loss improved from inf to 0.81379, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 10s 147ms/step - loss: 5.2376 - mae: 1.5070 - val_loss: 0.8138 - val_mae: 0.7419\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7682 - mae: 0.5675\n",
            "Epoch 2: val_loss improved from 0.81379 to 0.63441, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.7682 - mae: 0.5675 - val_loss: 0.6344 - val_mae: 0.3521\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6315 - mae: 0.4610\n",
            "Epoch 3: val_loss improved from 0.63441 to 0.51338, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.6315 - mae: 0.4610 - val_loss: 0.5134 - val_mae: 0.4915\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5242 - mae: 0.4026\n",
            "Epoch 4: val_loss improved from 0.51338 to 0.45653, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 0.5242 - mae: 0.4026 - val_loss: 0.4565 - val_mae: 0.4569\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4722 - mae: 0.3789\n",
            "Epoch 5: val_loss improved from 0.45653 to 0.40435, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.4722 - mae: 0.3789 - val_loss: 0.4043 - val_mae: 0.3829\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4275 - mae: 0.3522\n",
            "Epoch 6: val_loss improved from 0.40435 to 0.37868, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 0.4275 - mae: 0.3522 - val_loss: 0.3787 - val_mae: 0.3416\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4208 - mae: 0.3688\n",
            "Epoch 7: val_loss improved from 0.37868 to 0.37389, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 0.4208 - mae: 0.3688 - val_loss: 0.3739 - val_mae: 0.3470\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4094 - mae: 0.3666\n",
            "Epoch 8: val_loss improved from 0.37389 to 0.36395, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.4094 - mae: 0.3666 - val_loss: 0.3640 - val_mae: 0.3229\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4005 - mae: 0.3604\n",
            "Epoch 9: val_loss did not improve from 0.36395\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.4005 - mae: 0.3604 - val_loss: 0.4556 - val_mae: 0.5169\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4084 - mae: 0.3787\n",
            "Epoch 10: val_loss did not improve from 0.36395\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4084 - mae: 0.3787 - val_loss: 0.3945 - val_mae: 0.4400\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3873 - mae: 0.3541\n",
            "Epoch 11: val_loss improved from 0.36395 to 0.35714, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.3873 - mae: 0.3541 - val_loss: 0.3571 - val_mae: 0.2954\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3852 - mae: 0.3567\n",
            "Epoch 12: val_loss did not improve from 0.35714\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3852 - mae: 0.3567 - val_loss: 0.3704 - val_mae: 0.3141\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3646 - mae: 0.3225\n",
            "Epoch 13: val_loss improved from 0.35714 to 0.33747, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 0.3646 - mae: 0.3225 - val_loss: 0.3375 - val_mae: 0.2924\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3765 - mae: 0.3469\n",
            "Epoch 14: val_loss did not improve from 0.33747\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3765 - mae: 0.3469 - val_loss: 0.4056 - val_mae: 0.4703\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3976 - mae: 0.3845\n",
            "Epoch 15: val_loss did not improve from 0.33747\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.3976 - mae: 0.3845 - val_loss: 0.3384 - val_mae: 0.2831\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3515 - mae: 0.3248\n",
            "Epoch 16: val_loss did not improve from 0.33747\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.3515 - mae: 0.3248 - val_loss: 0.3848 - val_mae: 0.4286\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3825 - mae: 0.3767\n",
            "Epoch 17: val_loss improved from 0.33747 to 0.31880, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 0.3825 - mae: 0.3767 - val_loss: 0.3188 - val_mae: 0.2937\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3313 - mae: 0.3050\n",
            "Epoch 18: val_loss did not improve from 0.31880\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.3313 - mae: 0.3050 - val_loss: 0.3412 - val_mae: 0.3877\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3535 - mae: 0.3503\n",
            "Epoch 19: val_loss improved from 0.31880 to 0.29968, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.3535 - mae: 0.3503 - val_loss: 0.2997 - val_mae: 0.3008\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3477 - mae: 0.3521\n",
            "Epoch 20: val_loss improved from 0.29968 to 0.29125, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 0.3477 - mae: 0.3521 - val_loss: 0.2913 - val_mae: 0.2683\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3325 - mae: 0.3398\n",
            "Epoch 21: val_loss did not improve from 0.29125\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.3325 - mae: 0.3398 - val_loss: 0.3952 - val_mae: 0.4063\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3312 - mae: 0.3570\n",
            "Epoch 22: val_loss did not improve from 0.29125\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3312 - mae: 0.3570 - val_loss: 0.3026 - val_mae: 0.3057\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3061 - mae: 0.3197\n",
            "Epoch 23: val_loss did not improve from 0.29125\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 0.3061 - mae: 0.3197 - val_loss: 0.3645 - val_mae: 0.4693\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2903 - mae: 0.3160\n",
            "Epoch 24: val_loss did not improve from 0.29125\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.2903 - mae: 0.3160 - val_loss: 0.3243 - val_mae: 0.3514\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2671 - mae: 0.2818\n",
            "Epoch 25: val_loss improved from 0.29125 to 0.23716, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 0.2671 - mae: 0.2818 - val_loss: 0.2372 - val_mae: 0.2359\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2865 - mae: 0.3265\n",
            "Epoch 26: val_loss did not improve from 0.23716\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.2865 - mae: 0.3265 - val_loss: 0.2972 - val_mae: 0.3227\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2397 - mae: 0.2740\n",
            "Epoch 27: val_loss improved from 0.23716 to 0.23456, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.2397 - mae: 0.2740 - val_loss: 0.2346 - val_mae: 0.3036\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2405 - mae: 0.2952\n",
            "Epoch 28: val_loss did not improve from 0.23456\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.2405 - mae: 0.2952 - val_loss: 0.3921 - val_mae: 0.5323\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2416 - mae: 0.3036\n",
            "Epoch 29: val_loss did not improve from 0.23456\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.2416 - mae: 0.3036 - val_loss: 0.2390 - val_mae: 0.3468\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2377 - mae: 0.3111\n",
            "Epoch 30: val_loss did not improve from 0.23456\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 0.2377 - mae: 0.3111 - val_loss: 0.3272 - val_mae: 0.4384\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2611 - mae: 0.3352\n",
            "Epoch 31: val_loss did not improve from 0.23456\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.2611 - mae: 0.3352 - val_loss: 0.3182 - val_mae: 0.3433\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2447 - mae: 0.3178\n",
            "Epoch 32: val_loss did not improve from 0.23456\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.2447 - mae: 0.3178 - val_loss: 0.3498 - val_mae: 0.4973\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2238 - mae: 0.2953\n",
            "Epoch 33: val_loss improved from 0.23456 to 0.18328, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.2238 - mae: 0.2953 - val_loss: 0.1833 - val_mae: 0.2112\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1794 - mae: 0.2558\n",
            "Epoch 34: val_loss did not improve from 0.18328\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.1794 - mae: 0.2558 - val_loss: 0.1893 - val_mae: 0.3059\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2340 - mae: 0.3458\n",
            "Epoch 35: val_loss improved from 0.18328 to 0.15047, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.2340 - mae: 0.3458 - val_loss: 0.1505 - val_mae: 0.1864\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1325 - mae: 0.2012\n",
            "Epoch 36: val_loss improved from 0.15047 to 0.14269, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.1325 - mae: 0.2012 - val_loss: 0.1427 - val_mae: 0.1966\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1530 - mae: 0.2474\n",
            "Epoch 37: val_loss did not improve from 0.14269\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1530 - mae: 0.2474 - val_loss: 0.1520 - val_mae: 0.1890\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1851 - mae: 0.2981\n",
            "Epoch 38: val_loss did not improve from 0.14269\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.1851 - mae: 0.2981 - val_loss: 0.1868 - val_mae: 0.2691\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1684 - mae: 0.2436\n",
            "Epoch 39: val_loss did not improve from 0.14269\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.1684 - mae: 0.2436 - val_loss: 0.3165 - val_mae: 0.4689\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1590 - mae: 0.2561\n",
            "Epoch 40: val_loss did not improve from 0.14269\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.1590 - mae: 0.2561 - val_loss: 0.1449 - val_mae: 0.2177\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0996 - mae: 0.1834\n",
            "Epoch 41: val_loss improved from 0.14269 to 0.10630, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 0.0996 - mae: 0.1834 - val_loss: 0.1063 - val_mae: 0.2068\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1117 - mae: 0.2104\n",
            "Epoch 42: val_loss did not improve from 0.10630\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.1117 - mae: 0.2104 - val_loss: 0.1581 - val_mae: 0.2190\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1350 - mae: 0.2392\n",
            "Epoch 43: val_loss did not improve from 0.10630\n",
            "25/25 [==============================] - 2s 93ms/step - loss: 0.1350 - mae: 0.2392 - val_loss: 0.3094 - val_mae: 0.3185\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2416 - mae: 0.3314\n",
            "Epoch 44: val_loss did not improve from 0.10630\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 0.2416 - mae: 0.3314 - val_loss: 0.1685 - val_mae: 0.3259\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1339 - mae: 0.2466\n",
            "Epoch 45: val_loss improved from 0.10630 to 0.10536, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.1339 - mae: 0.2466 - val_loss: 0.1054 - val_mae: 0.1860\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0770 - mae: 0.1430\n",
            "Epoch 46: val_loss improved from 0.10536 to 0.08308, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 0.0770 - mae: 0.1430 - val_loss: 0.0831 - val_mae: 0.1375\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0755 - mae: 0.1446\n",
            "Epoch 47: val_loss did not improve from 0.08308\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 0.0755 - mae: 0.1446 - val_loss: 0.0964 - val_mae: 0.1713\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0746 - mae: 0.1416\n",
            "Epoch 48: val_loss did not improve from 0.08308\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.0746 - mae: 0.1416 - val_loss: 0.0865 - val_mae: 0.1539\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0852 - mae: 0.1779\n",
            "Epoch 49: val_loss did not improve from 0.08308\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.0852 - mae: 0.1779 - val_loss: 0.0881 - val_mae: 0.2045\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1120 - mae: 0.2219\n",
            "Epoch 50: val_loss did not improve from 0.08308\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.1120 - mae: 0.2219 - val_loss: 0.0878 - val_mae: 0.1460\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4349 - mae: 0.4376\n",
            "Epoch 51: val_loss did not improve from 0.08308\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.4349 - mae: 0.4376 - val_loss: 0.4530 - val_mae: 0.3092\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3646 - mae: 0.3395\n",
            "Epoch 52: val_loss did not improve from 0.08308\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.3646 - mae: 0.3395 - val_loss: 0.2678 - val_mae: 0.2950\n",
            "Epoch 52: early stopping\n",
            "491/491 [==============================] - 4s 8ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 4s 8ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 4s 7ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 1s 21ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.08964589238166809, RMSE:0.29940924048423767, MAE:0.1447320580482483, R2:0.895401236480248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7898564382883024, 0.895401236480248]\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_172\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_125 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_128 (Embedding)      (None, 17, 900)      649800      ['input_125[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_129 (Embedding)      (None, 17, 900)      649800      ['input_125[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_130 (Embedding)      (None, 17, 900)      649800      ['input_125[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_131 (Embedding)      (None, 17, 900)      649800      ['input_125[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_186 (Conv1D)            (None, 17, 100)      270100      ['embedding_128[0][0]',          \n",
            "                                                                  'embedding_129[0][0]',          \n",
            "                                                                  'embedding_130[0][0]',          \n",
            "                                                                  'embedding_131[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_187 (Conv1D)            (None, 17, 100)      270100      ['embedding_128[0][0]',          \n",
            "                                                                  'embedding_129[0][0]',          \n",
            "                                                                  'embedding_130[0][0]',          \n",
            "                                                                  'embedding_131[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_188 (Conv1D)            (None, 17, 100)      270100      ['embedding_128[0][0]',          \n",
            "                                                                  'embedding_129[0][0]',          \n",
            "                                                                  'embedding_130[0][0]',          \n",
            "                                                                  'embedding_131[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_384 (Glob  (None, 100)         0           ['conv1d_186[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_385 (Glob  (None, 100)         0           ['conv1d_186[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_386 (Glob  (None, 100)         0           ['conv1d_186[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_387 (Glob  (None, 100)         0           ['conv1d_186[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_388 (Glob  (None, 100)         0           ['conv1d_187[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_389 (Glob  (None, 100)         0           ['conv1d_187[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_390 (Glob  (None, 100)         0           ['conv1d_187[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_391 (Glob  (None, 100)         0           ['conv1d_187[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_392 (Glob  (None, 100)         0           ['conv1d_188[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_393 (Glob  (None, 100)         0           ['conv1d_188[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_394 (Glob  (None, 100)         0           ['conv1d_188[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " global_max_pooling1d_395 (Glob  (None, 100)         0           ['conv1d_188[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_126 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 100)          0           ['global_max_pooling1d_384[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_385[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_386[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_387[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 100)          0           ['global_max_pooling1d_388[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_389[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_390[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_391[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 100)          0           ['global_max_pooling1d_392[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_393[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_394[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_395[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_264 (Dense)              (None, 1000)         11000       ['input_126[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_62 (Concatenate)   (None, 1300)         0           ['add_66[0][0]',                 \n",
            "                                                                  'add_67[0][0]',                 \n",
            "                                                                  'add_68[0][0]',                 \n",
            "                                                                  'dense_264[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_62[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_210 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_210[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_211 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_211[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_212 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_212[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_213 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_213[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_214 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_265 (Dense)              (None, 1)            3           ['dropout_214[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 6.7133 - mae: 1.8154\n",
            "Epoch 1: val_loss improved from inf to 0.95245, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 10s 151ms/step - loss: 6.7133 - mae: 1.8154 - val_loss: 0.9525 - val_mae: 0.4864\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7907 - mae: 0.5744\n",
            "Epoch 2: val_loss improved from 0.95245 to 0.60814, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.7907 - mae: 0.5744 - val_loss: 0.6081 - val_mae: 0.4289\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6242 - mae: 0.4667\n",
            "Epoch 3: val_loss improved from 0.60814 to 0.49642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 124ms/step - loss: 0.6242 - mae: 0.4667 - val_loss: 0.4964 - val_mae: 0.3599\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5162 - mae: 0.3889\n",
            "Epoch 4: val_loss improved from 0.49642 to 0.43271, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 0.5162 - mae: 0.3889 - val_loss: 0.4327 - val_mae: 0.3537\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4615 - mae: 0.3643\n",
            "Epoch 5: val_loss improved from 0.43271 to 0.40004, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.4615 - mae: 0.3643 - val_loss: 0.4000 - val_mae: 0.3644\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4293 - mae: 0.3530\n",
            "Epoch 6: val_loss improved from 0.40004 to 0.37905, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.4293 - mae: 0.3530 - val_loss: 0.3791 - val_mae: 0.3228\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4005 - mae: 0.3316\n",
            "Epoch 7: val_loss improved from 0.37905 to 0.36642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.4005 - mae: 0.3316 - val_loss: 0.3664 - val_mae: 0.3104\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3905 - mae: 0.3373\n",
            "Epoch 8: val_loss did not improve from 0.36642\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3905 - mae: 0.3373 - val_loss: 0.3772 - val_mae: 0.3127\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4005 - mae: 0.3627\n",
            "Epoch 9: val_loss did not improve from 0.36642\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.4005 - mae: 0.3627 - val_loss: 0.3686 - val_mae: 0.3390\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3838 - mae: 0.3399\n",
            "Epoch 10: val_loss did not improve from 0.36642\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3838 - mae: 0.3399 - val_loss: 0.3766 - val_mae: 0.4003\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3882 - mae: 0.3520\n",
            "Epoch 11: val_loss improved from 0.36642 to 0.35926, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 136ms/step - loss: 0.3882 - mae: 0.3520 - val_loss: 0.3593 - val_mae: 0.3514\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3903 - mae: 0.3609\n",
            "Epoch 12: val_loss did not improve from 0.35926\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3903 - mae: 0.3609 - val_loss: 0.3696 - val_mae: 0.3856\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3648 - mae: 0.3191\n",
            "Epoch 13: val_loss improved from 0.35926 to 0.34164, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 0.3648 - mae: 0.3191 - val_loss: 0.3416 - val_mae: 0.2980\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3776 - mae: 0.3504\n",
            "Epoch 14: val_loss did not improve from 0.34164\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3776 - mae: 0.3504 - val_loss: 0.3477 - val_mae: 0.2877\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3827 - mae: 0.3627\n",
            "Epoch 15: val_loss improved from 0.34164 to 0.33607, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 0.3827 - mae: 0.3627 - val_loss: 0.3361 - val_mae: 0.3002\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4001 - mae: 0.3958\n",
            "Epoch 16: val_loss did not improve from 0.33607\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.4001 - mae: 0.3958 - val_loss: 0.4595 - val_mae: 0.4468\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3621 - mae: 0.3346\n",
            "Epoch 17: val_loss improved from 0.33607 to 0.33096, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 0.3621 - mae: 0.3346 - val_loss: 0.3310 - val_mae: 0.3093\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3512 - mae: 0.3503\n",
            "Epoch 18: val_loss improved from 0.33096 to 0.32277, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.3512 - mae: 0.3503 - val_loss: 0.3228 - val_mae: 0.2983\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3630 - mae: 0.3628\n",
            "Epoch 19: val_loss did not improve from 0.32277\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3630 - mae: 0.3628 - val_loss: 0.3234 - val_mae: 0.3447\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3218 - mae: 0.3058\n",
            "Epoch 20: val_loss improved from 0.32277 to 0.30383, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.3218 - mae: 0.3058 - val_loss: 0.3038 - val_mae: 0.2882\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3218 - mae: 0.3226\n",
            "Epoch 21: val_loss improved from 0.30383 to 0.30249, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3218 - mae: 0.3226 - val_loss: 0.3025 - val_mae: 0.2521\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3243 - mae: 0.3276\n",
            "Epoch 22: val_loss improved from 0.30249 to 0.29056, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3243 - mae: 0.3276 - val_loss: 0.2906 - val_mae: 0.2804\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3150 - mae: 0.3280\n",
            "Epoch 23: val_loss did not improve from 0.29056\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3150 - mae: 0.3280 - val_loss: 0.4290 - val_mae: 0.4527\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3073 - mae: 0.3224\n",
            "Epoch 24: val_loss improved from 0.29056 to 0.28444, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.3073 - mae: 0.3224 - val_loss: 0.2844 - val_mae: 0.2692\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2573 - mae: 0.2716\n",
            "Epoch 25: val_loss did not improve from 0.28444\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2573 - mae: 0.2716 - val_loss: 0.3429 - val_mae: 0.3561\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3107 - mae: 0.3539\n",
            "Epoch 26: val_loss did not improve from 0.28444\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3107 - mae: 0.3539 - val_loss: 0.3240 - val_mae: 0.3592\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2457 - mae: 0.2747\n",
            "Epoch 27: val_loss improved from 0.28444 to 0.25372, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.2457 - mae: 0.2747 - val_loss: 0.2537 - val_mae: 0.2609\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2409 - mae: 0.2839\n",
            "Epoch 28: val_loss improved from 0.25372 to 0.23297, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.2409 - mae: 0.2839 - val_loss: 0.2330 - val_mae: 0.2095\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2165 - mae: 0.2639\n",
            "Epoch 29: val_loss improved from 0.23297 to 0.23274, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 0.2165 - mae: 0.2639 - val_loss: 0.2327 - val_mae: 0.2557\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2044 - mae: 0.2549\n",
            "Epoch 30: val_loss improved from 0.23274 to 0.18648, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.2044 - mae: 0.2549 - val_loss: 0.1865 - val_mae: 0.2071\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4293 - mae: 0.4592\n",
            "Epoch 31: val_loss did not improve from 0.18648\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.4293 - mae: 0.4592 - val_loss: 0.2713 - val_mae: 0.3140\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2362 - mae: 0.2609\n",
            "Epoch 32: val_loss did not improve from 0.18648\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.2362 - mae: 0.2609 - val_loss: 0.2102 - val_mae: 0.2176\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2032 - mae: 0.2573\n",
            "Epoch 33: val_loss did not improve from 0.18648\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2032 - mae: 0.2573 - val_loss: 0.2247 - val_mae: 0.3089\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1589 - mae: 0.2104\n",
            "Epoch 34: val_loss improved from 0.18648 to 0.17969, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.1589 - mae: 0.2104 - val_loss: 0.1797 - val_mae: 0.2176\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1783 - mae: 0.2549\n",
            "Epoch 35: val_loss did not improve from 0.17969\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.1783 - mae: 0.2549 - val_loss: 0.2100 - val_mae: 0.2319\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2682 - mae: 0.3411\n",
            "Epoch 36: val_loss did not improve from 0.17969\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2682 - mae: 0.3411 - val_loss: 1.1024 - val_mae: 0.9470\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3617 - mae: 0.4160\n",
            "Epoch 37: val_loss improved from 0.17969 to 0.16854, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3617 - mae: 0.4160 - val_loss: 0.1685 - val_mae: 0.1897\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1483 - mae: 0.2111\n",
            "Epoch 38: val_loss did not improve from 0.16854\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.1483 - mae: 0.2111 - val_loss: 0.2142 - val_mae: 0.2880\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1241 - mae: 0.1871\n",
            "Epoch 39: val_loss improved from 0.16854 to 0.13654, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 0.1241 - mae: 0.1871 - val_loss: 0.1365 - val_mae: 0.1872\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1000 - mae: 0.1621\n",
            "Epoch 40: val_loss did not improve from 0.13654\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1000 - mae: 0.1621 - val_loss: 0.1964 - val_mae: 0.3151\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2251 - mae: 0.3242\n",
            "Epoch 41: val_loss did not improve from 0.13654\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2251 - mae: 0.3242 - val_loss: 0.2890 - val_mae: 0.3194\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1636 - mae: 0.2588\n",
            "Epoch 42: val_loss did not improve from 0.13654\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1636 - mae: 0.2588 - val_loss: 0.2961 - val_mae: 0.3837\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1893 - mae: 0.2891\n",
            "Epoch 43: val_loss did not improve from 0.13654\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1893 - mae: 0.2891 - val_loss: 0.3541 - val_mae: 0.4745\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2718 - mae: 0.3652\n",
            "Epoch 44: val_loss did not improve from 0.13654\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2718 - mae: 0.3652 - val_loss: 0.2768 - val_mae: 0.3277\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1471 - mae: 0.2393\n",
            "Epoch 45: val_loss improved from 0.13654 to 0.11361, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1471 - mae: 0.2393 - val_loss: 0.1136 - val_mae: 0.1811\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1021 - mae: 0.1879\n",
            "Epoch 46: val_loss improved from 0.11361 to 0.08875, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1021 - mae: 0.1879 - val_loss: 0.0888 - val_mae: 0.1284\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0829 - mae: 0.1581\n",
            "Epoch 47: val_loss did not improve from 0.08875\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.0829 - mae: 0.1581 - val_loss: 0.1620 - val_mae: 0.2493\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0943 - mae: 0.1863\n",
            "Epoch 48: val_loss did not improve from 0.08875\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.0943 - mae: 0.1863 - val_loss: 0.0921 - val_mae: 0.1355\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0789 - mae: 0.1546\n",
            "Epoch 49: val_loss improved from 0.08875 to 0.08053, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.0789 - mae: 0.1546 - val_loss: 0.0805 - val_mae: 0.1478\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2375 - mae: 0.2537\n",
            "Epoch 50: val_loss did not improve from 0.08053\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2375 - mae: 0.2537 - val_loss: 1.5868 - val_mae: 1.2264\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6233 - mae: 0.5367\n",
            "Epoch 51: val_loss did not improve from 0.08053\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.6233 - mae: 0.5367 - val_loss: 0.3684 - val_mae: 0.2809\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3636 - mae: 0.3369\n",
            "Epoch 52: val_loss did not improve from 0.08053\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3636 - mae: 0.3369 - val_loss: 0.3240 - val_mae: 0.3811\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3001 - mae: 0.3083\n",
            "Epoch 53: val_loss did not improve from 0.08053\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3001 - mae: 0.3083 - val_loss: 0.2647 - val_mae: 0.2656\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2759 - mae: 0.3119\n",
            "Epoch 54: val_loss did not improve from 0.08053\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2759 - mae: 0.3119 - val_loss: 0.2846 - val_mae: 0.3782\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2519 - mae: 0.3042\n",
            "Epoch 55: val_loss did not improve from 0.08053\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2519 - mae: 0.3042 - val_loss: 0.2257 - val_mae: 0.2522\n",
            "Epoch 55: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.09033931791782379, RMSE:0.3005650043487549, MAE:0.15624850988388062, R2:0.8945921456551583\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_178\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_127 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_132 (Embedding)      (None, 17, 900)      649800      ['input_127[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_133 (Embedding)      (None, 17, 900)      649800      ['input_127[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_134 (Embedding)      (None, 17, 900)      649800      ['input_127[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_135 (Embedding)      (None, 17, 900)      649800      ['input_127[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_189 (Conv1D)            (None, 17, 100)      270100      ['embedding_132[0][0]',          \n",
            "                                                                  'embedding_133[0][0]',          \n",
            "                                                                  'embedding_134[0][0]',          \n",
            "                                                                  'embedding_135[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_190 (Conv1D)            (None, 17, 100)      270100      ['embedding_132[0][0]',          \n",
            "                                                                  'embedding_133[0][0]',          \n",
            "                                                                  'embedding_134[0][0]',          \n",
            "                                                                  'embedding_135[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_191 (Conv1D)            (None, 17, 100)      270100      ['embedding_132[0][0]',          \n",
            "                                                                  'embedding_133[0][0]',          \n",
            "                                                                  'embedding_134[0][0]',          \n",
            "                                                                  'embedding_135[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_396 (Glob  (None, 100)         0           ['conv1d_189[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_397 (Glob  (None, 100)         0           ['conv1d_189[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_398 (Glob  (None, 100)         0           ['conv1d_189[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_399 (Glob  (None, 100)         0           ['conv1d_189[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_400 (Glob  (None, 100)         0           ['conv1d_190[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_401 (Glob  (None, 100)         0           ['conv1d_190[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_402 (Glob  (None, 100)         0           ['conv1d_190[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_403 (Glob  (None, 100)         0           ['conv1d_190[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_404 (Glob  (None, 100)         0           ['conv1d_191[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_405 (Glob  (None, 100)         0           ['conv1d_191[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_406 (Glob  (None, 100)         0           ['conv1d_191[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_407 (Glob  (None, 100)         0           ['conv1d_191[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_128 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 100)          0           ['global_max_pooling1d_396[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_397[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_398[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_399[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_70 (Add)                   (None, 100)          0           ['global_max_pooling1d_400[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_401[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_402[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_403[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 100)          0           ['global_max_pooling1d_404[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_405[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_406[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_407[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_266 (Dense)              (None, 1000)         11000       ['input_128[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_63 (Concatenate)   (None, 1300)         0           ['add_69[0][0]',                 \n",
            "                                                                  'add_70[0][0]',                 \n",
            "                                                                  'add_71[0][0]',                 \n",
            "                                                                  'dense_266[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_63[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_215 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_215[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_216 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_216[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_217 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_217[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_218 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_218[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_219 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_267 (Dense)              (None, 1)            3           ['dropout_219[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.9584 - mae: 1.3187\n",
            "Epoch 1: val_loss improved from inf to 0.88071, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 116ms/step - loss: 3.9584 - mae: 1.3187 - val_loss: 0.8807 - val_mae: 0.8325\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7439 - mae: 0.5402\n",
            "Epoch 2: val_loss improved from 0.88071 to 0.59353, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.7439 - mae: 0.5402 - val_loss: 0.5935 - val_mae: 0.4762\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5944 - mae: 0.4479\n",
            "Epoch 3: val_loss improved from 0.59353 to 0.46268, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.5944 - mae: 0.4479 - val_loss: 0.4627 - val_mae: 0.3485\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4886 - mae: 0.3854\n",
            "Epoch 4: val_loss improved from 0.46268 to 0.44846, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 0.4886 - mae: 0.3854 - val_loss: 0.4485 - val_mae: 0.3109\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4394 - mae: 0.3556\n",
            "Epoch 5: val_loss improved from 0.44846 to 0.41135, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.4394 - mae: 0.3556 - val_loss: 0.4113 - val_mae: 0.3129\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4324 - mae: 0.3778\n",
            "Epoch 6: val_loss improved from 0.41135 to 0.37816, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.4324 - mae: 0.3778 - val_loss: 0.3782 - val_mae: 0.2960\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4245 - mae: 0.3874\n",
            "Epoch 7: val_loss did not improve from 0.37816\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.4245 - mae: 0.3874 - val_loss: 0.4947 - val_mae: 0.4416\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4186 - mae: 0.3793\n",
            "Epoch 8: val_loss did not improve from 0.37816\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4186 - mae: 0.3793 - val_loss: 0.3953 - val_mae: 0.4245\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3961 - mae: 0.3476\n",
            "Epoch 9: val_loss improved from 0.37816 to 0.35401, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 0.3961 - mae: 0.3476 - val_loss: 0.3540 - val_mae: 0.2936\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3921 - mae: 0.3421\n",
            "Epoch 10: val_loss did not improve from 0.35401\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3921 - mae: 0.3421 - val_loss: 0.3889 - val_mae: 0.4145\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3921 - mae: 0.3618\n",
            "Epoch 11: val_loss did not improve from 0.35401\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3921 - mae: 0.3618 - val_loss: 0.3588 - val_mae: 0.3030\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3959 - mae: 0.3576\n",
            "Epoch 12: val_loss did not improve from 0.35401\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3959 - mae: 0.3576 - val_loss: 0.3547 - val_mae: 0.2850\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3810 - mae: 0.3429\n",
            "Epoch 13: val_loss improved from 0.35401 to 0.34221, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3810 - mae: 0.3429 - val_loss: 0.3422 - val_mae: 0.2804\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3839 - mae: 0.3597\n",
            "Epoch 14: val_loss did not improve from 0.34221\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3839 - mae: 0.3597 - val_loss: 0.3496 - val_mae: 0.3273\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3622 - mae: 0.3217\n",
            "Epoch 15: val_loss did not improve from 0.34221\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3622 - mae: 0.3217 - val_loss: 0.3469 - val_mae: 0.3256\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3913 - mae: 0.3732\n",
            "Epoch 16: val_loss did not improve from 0.34221\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3913 - mae: 0.3732 - val_loss: 0.3853 - val_mae: 0.4423\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4254 - mae: 0.4399\n",
            "Epoch 17: val_loss did not improve from 0.34221\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4254 - mae: 0.4399 - val_loss: 0.4023 - val_mae: 0.3943\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3740 - mae: 0.3694\n",
            "Epoch 18: val_loss improved from 0.34221 to 0.33732, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 0.3740 - mae: 0.3694 - val_loss: 0.3373 - val_mae: 0.3718\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3470 - mae: 0.3406\n",
            "Epoch 19: val_loss improved from 0.33732 to 0.30912, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3470 - mae: 0.3406 - val_loss: 0.3091 - val_mae: 0.3163\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3648 - mae: 0.3655\n",
            "Epoch 20: val_loss did not improve from 0.30912\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.3648 - mae: 0.3655 - val_loss: 0.3201 - val_mae: 0.3351\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3128 - mae: 0.3026\n",
            "Epoch 21: val_loss improved from 0.30912 to 0.29693, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.3128 - mae: 0.3026 - val_loss: 0.2969 - val_mae: 0.2538\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3230 - mae: 0.3232\n",
            "Epoch 22: val_loss did not improve from 0.29693\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3230 - mae: 0.3232 - val_loss: 0.3691 - val_mae: 0.3254\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3572 - mae: 0.3772\n",
            "Epoch 23: val_loss did not improve from 0.29693\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3572 - mae: 0.3772 - val_loss: 0.3816 - val_mae: 0.4434\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3227 - mae: 0.3409\n",
            "Epoch 24: val_loss improved from 0.29693 to 0.25868, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.3227 - mae: 0.3409 - val_loss: 0.2587 - val_mae: 0.2378\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2613 - mae: 0.2683\n",
            "Epoch 25: val_loss did not improve from 0.25868\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2613 - mae: 0.2683 - val_loss: 0.2802 - val_mae: 0.2765\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2749 - mae: 0.3011\n",
            "Epoch 26: val_loss did not improve from 0.25868\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2749 - mae: 0.3011 - val_loss: 0.2723 - val_mae: 0.3582\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2639 - mae: 0.2939\n",
            "Epoch 27: val_loss improved from 0.25868 to 0.22746, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.2639 - mae: 0.2939 - val_loss: 0.2275 - val_mae: 0.2307\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2339 - mae: 0.2761\n",
            "Epoch 28: val_loss did not improve from 0.22746\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.2339 - mae: 0.2761 - val_loss: 0.3023 - val_mae: 0.3153\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2520 - mae: 0.3048\n",
            "Epoch 29: val_loss did not improve from 0.22746\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2520 - mae: 0.3048 - val_loss: 0.2336 - val_mae: 0.3159\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2397 - mae: 0.2918\n",
            "Epoch 30: val_loss improved from 0.22746 to 0.20046, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.2397 - mae: 0.2918 - val_loss: 0.2005 - val_mae: 0.2349\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2085 - mae: 0.2667\n",
            "Epoch 31: val_loss did not improve from 0.20046\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2085 - mae: 0.2667 - val_loss: 0.2083 - val_mae: 0.2661\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1661 - mae: 0.2209\n",
            "Epoch 32: val_loss did not improve from 0.20046\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1661 - mae: 0.2209 - val_loss: 0.2233 - val_mae: 0.3271\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2556 - mae: 0.3274\n",
            "Epoch 33: val_loss did not improve from 0.20046\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2556 - mae: 0.3274 - val_loss: 0.3458 - val_mae: 0.4390\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1805 - mae: 0.2519\n",
            "Epoch 34: val_loss improved from 0.20046 to 0.15753, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.1805 - mae: 0.2519 - val_loss: 0.1575 - val_mae: 0.2151\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2914 - mae: 0.3726\n",
            "Epoch 35: val_loss did not improve from 0.15753\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2914 - mae: 0.3726 - val_loss: 0.2309 - val_mae: 0.2904\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1468 - mae: 0.2145\n",
            "Epoch 36: val_loss improved from 0.15753 to 0.13941, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.1468 - mae: 0.2145 - val_loss: 0.1394 - val_mae: 0.1835\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1766 - mae: 0.2625\n",
            "Epoch 37: val_loss did not improve from 0.13941\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.1766 - mae: 0.2625 - val_loss: 0.2265 - val_mae: 0.3377\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1296 - mae: 0.1966\n",
            "Epoch 38: val_loss improved from 0.13941 to 0.12644, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.1296 - mae: 0.1966 - val_loss: 0.1264 - val_mae: 0.1746\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1201 - mae: 0.1979\n",
            "Epoch 39: val_loss did not improve from 0.12644\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.1201 - mae: 0.1979 - val_loss: 0.1489 - val_mae: 0.2727\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1176 - mae: 0.1939\n",
            "Epoch 40: val_loss improved from 0.12644 to 0.10299, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.1176 - mae: 0.1939 - val_loss: 0.1030 - val_mae: 0.1382\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0863 - mae: 0.1491\n",
            "Epoch 41: val_loss did not improve from 0.10299\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.0863 - mae: 0.1491 - val_loss: 0.1405 - val_mae: 0.2840\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1706 - mae: 0.2616\n",
            "Epoch 42: val_loss did not improve from 0.10299\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1706 - mae: 0.2616 - val_loss: 0.4603 - val_mae: 0.3943\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3492 - mae: 0.3892\n",
            "Epoch 43: val_loss did not improve from 0.10299\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.3492 - mae: 0.3892 - val_loss: 0.3261 - val_mae: 0.4661\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2187 - mae: 0.2944\n",
            "Epoch 44: val_loss did not improve from 0.10299\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2187 - mae: 0.2944 - val_loss: 0.2459 - val_mae: 0.4109\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1597 - mae: 0.2562\n",
            "Epoch 45: val_loss did not improve from 0.10299\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1597 - mae: 0.2562 - val_loss: 0.1902 - val_mae: 0.2992\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1475 - mae: 0.2626\n",
            "Epoch 46: val_loss did not improve from 0.10299\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1475 - mae: 0.2626 - val_loss: 0.1184 - val_mae: 0.1792\n",
            "Epoch 46: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 23ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.10498622804880142, RMSE:0.3240157961845398, MAE:0.14587081968784332, R2:0.8775021410006084\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_184\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_129 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_136 (Embedding)      (None, 17, 900)      649800      ['input_129[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_137 (Embedding)      (None, 17, 900)      649800      ['input_129[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_138 (Embedding)      (None, 17, 900)      649800      ['input_129[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_139 (Embedding)      (None, 17, 900)      649800      ['input_129[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_192 (Conv1D)            (None, 17, 100)      270100      ['embedding_136[0][0]',          \n",
            "                                                                  'embedding_137[0][0]',          \n",
            "                                                                  'embedding_138[0][0]',          \n",
            "                                                                  'embedding_139[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_193 (Conv1D)            (None, 17, 100)      270100      ['embedding_136[0][0]',          \n",
            "                                                                  'embedding_137[0][0]',          \n",
            "                                                                  'embedding_138[0][0]',          \n",
            "                                                                  'embedding_139[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_194 (Conv1D)            (None, 17, 100)      270100      ['embedding_136[0][0]',          \n",
            "                                                                  'embedding_137[0][0]',          \n",
            "                                                                  'embedding_138[0][0]',          \n",
            "                                                                  'embedding_139[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_408 (Glob  (None, 100)         0           ['conv1d_192[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_409 (Glob  (None, 100)         0           ['conv1d_192[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_410 (Glob  (None, 100)         0           ['conv1d_192[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_411 (Glob  (None, 100)         0           ['conv1d_192[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_412 (Glob  (None, 100)         0           ['conv1d_193[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_413 (Glob  (None, 100)         0           ['conv1d_193[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_414 (Glob  (None, 100)         0           ['conv1d_193[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_415 (Glob  (None, 100)         0           ['conv1d_193[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_416 (Glob  (None, 100)         0           ['conv1d_194[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_417 (Glob  (None, 100)         0           ['conv1d_194[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_418 (Glob  (None, 100)         0           ['conv1d_194[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_419 (Glob  (None, 100)         0           ['conv1d_194[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_130 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_72 (Add)                   (None, 100)          0           ['global_max_pooling1d_408[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_409[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_410[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_411[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_73 (Add)                   (None, 100)          0           ['global_max_pooling1d_412[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_413[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_414[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_415[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_74 (Add)                   (None, 100)          0           ['global_max_pooling1d_416[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_417[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_418[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_419[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_268 (Dense)              (None, 1000)         11000       ['input_130[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_64 (Concatenate)   (None, 1300)         0           ['add_72[0][0]',                 \n",
            "                                                                  'add_73[0][0]',                 \n",
            "                                                                  'add_74[0][0]',                 \n",
            "                                                                  'dense_268[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_64[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_220 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_220[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_221 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_221[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_222 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_222[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_223 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_223[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_224 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_269 (Dense)              (None, 1)            3           ['dropout_224[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.1862 - mae: 6.0274\n",
            "Epoch 1: val_loss improved from inf to 36.87303, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 118ms/step - loss: 37.1862 - mae: 6.0274 - val_loss: 36.8730 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8449 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87303 to 36.57393, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 36.8449 - mae: 5.9990 - val_loss: 36.5739 - val_mae: 5.9788\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5464 - mae: 5.9741\n",
            "Epoch 3: val_loss improved from 36.57393 to 36.27647, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 36.5464 - mae: 5.9741 - val_loss: 36.2765 - val_mae: 5.9538\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2494 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27647 to 35.98103, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 36.2494 - mae: 5.9492 - val_loss: 35.9810 - val_mae: 5.9290\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9542 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98103 to 35.68702, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 35.9542 - mae: 5.9243 - val_loss: 35.6870 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6607 - mae: 5.8995\n",
            "Epoch 6: val_loss improved from 35.68702 to 35.39454, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.6607 - mae: 5.8995 - val_loss: 35.3945 - val_mae: 5.8793\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3687 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39454 to 35.10369, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.3687 - mae: 5.8747 - val_loss: 35.1037 - val_mae: 5.8545\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0781 - mae: 5.8499\n",
            "Epoch 8: val_loss improved from 35.10369 to 34.81449, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 35.0781 - mae: 5.8499 - val_loss: 34.8145 - val_mae: 5.8298\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7893 - mae: 5.8252\n",
            "Epoch 9: val_loss improved from 34.81449 to 34.52660, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.7893 - mae: 5.8252 - val_loss: 34.5266 - val_mae: 5.8050\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5018 - mae: 5.8004\n",
            "Epoch 10: val_loss improved from 34.52660 to 34.24023, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.5018 - mae: 5.8004 - val_loss: 34.2402 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2156 - mae: 5.7757\n",
            "Epoch 11: val_loss improved from 34.24023 to 33.95541, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 34.2156 - mae: 5.7757 - val_loss: 33.9554 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9310 - mae: 5.7510\n",
            "Epoch 12: val_loss improved from 33.95541 to 33.67171, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 33.9310 - mae: 5.7510 - val_loss: 33.6717 - val_mae: 5.7309\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6477 - mae: 5.7264\n",
            "Epoch 13: val_loss improved from 33.67171 to 33.38950, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 100ms/step - loss: 33.6477 - mae: 5.7264 - val_loss: 33.3895 - val_mae: 5.7062\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3659 - mae: 5.7017\n",
            "Epoch 14: val_loss improved from 33.38950 to 33.10855, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.3659 - mae: 5.7017 - val_loss: 33.1086 - val_mae: 5.6816\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0853 - mae: 5.6770\n",
            "Epoch 15: val_loss improved from 33.10855 to 32.82908, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.0853 - mae: 5.6770 - val_loss: 32.8291 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8060 - mae: 5.6524\n",
            "Epoch 16: val_loss improved from 32.82908 to 32.55085, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 32.8060 - mae: 5.6524 - val_loss: 32.5508 - val_mae: 5.6323\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5280 - mae: 5.6277\n",
            "Epoch 17: val_loss improved from 32.55085 to 32.27407, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 32.5280 - mae: 5.6277 - val_loss: 32.2741 - val_mae: 5.6076\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2514 - mae: 5.6031\n",
            "Epoch 18: val_loss improved from 32.27407 to 31.99848, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 32.2514 - mae: 5.6031 - val_loss: 31.9985 - val_mae: 5.5830\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9760 - mae: 5.5785\n",
            "Epoch 19: val_loss improved from 31.99848 to 31.72401, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 31.9760 - mae: 5.5785 - val_loss: 31.7240 - val_mae: 5.5584\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7019 - mae: 5.5539\n",
            "Epoch 20: val_loss improved from 31.72401 to 31.45092, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 106ms/step - loss: 31.7019 - mae: 5.5539 - val_loss: 31.4509 - val_mae: 5.5338\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4291 - mae: 5.5292\n",
            "Epoch 21: val_loss improved from 31.45092 to 31.17896, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 31.4291 - mae: 5.5292 - val_loss: 31.1790 - val_mae: 5.5091\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1575 - mae: 5.5046\n",
            "Epoch 22: val_loss improved from 31.17896 to 30.90847, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 31.1575 - mae: 5.5046 - val_loss: 30.9085 - val_mae: 5.4845\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8872 - mae: 5.4800\n",
            "Epoch 23: val_loss improved from 30.90847 to 30.63917, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.8872 - mae: 5.4800 - val_loss: 30.6392 - val_mae: 5.4599\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6182 - mae: 5.4554\n",
            "Epoch 24: val_loss improved from 30.63917 to 30.37105, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 30.6182 - mae: 5.4554 - val_loss: 30.3711 - val_mae: 5.4353\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3503 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37105 to 30.10433, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 30.3503 - mae: 5.4308 - val_loss: 30.1043 - val_mae: 5.4107\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0838 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10433 to 29.83868, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 30.0838 - mae: 5.4062 - val_loss: 29.8387 - val_mae: 5.3861\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8184 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83868 to 29.57444, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 29.8184 - mae: 5.3816 - val_loss: 29.5744 - val_mae: 5.3615\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5543 - mae: 5.3570\n",
            "Epoch 28: val_loss improved from 29.57444 to 29.31134, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 29.5543 - mae: 5.3570 - val_loss: 29.3113 - val_mae: 5.3369\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2915 - mae: 5.3324\n",
            "Epoch 29: val_loss improved from 29.31134 to 29.04941, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.2915 - mae: 5.3324 - val_loss: 29.0494 - val_mae: 5.3124\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0298 - mae: 5.3078\n",
            "Epoch 30: val_loss improved from 29.04941 to 28.78886, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.0298 - mae: 5.3078 - val_loss: 28.7889 - val_mae: 5.2878\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7694 - mae: 5.2833\n",
            "Epoch 31: val_loss improved from 28.78886 to 28.52948, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 28.7694 - mae: 5.2833 - val_loss: 28.5295 - val_mae: 5.2632\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5102 - mae: 5.2587\n",
            "Epoch 32: val_loss improved from 28.52948 to 28.27128, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 28.5102 - mae: 5.2587 - val_loss: 28.2713 - val_mae: 5.2386\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2524 - mae: 5.2341\n",
            "Epoch 33: val_loss improved from 28.27128 to 28.01419, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 28.2524 - mae: 5.2341 - val_loss: 28.0142 - val_mae: 5.2140\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9955 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01419 to 27.75862, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 27.9955 - mae: 5.2095 - val_loss: 27.7586 - val_mae: 5.1894\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7401 - mae: 5.1849\n",
            "Epoch 35: val_loss improved from 27.75862 to 27.50414, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 27.7401 - mae: 5.1849 - val_loss: 27.5041 - val_mae: 5.1649\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4858 - mae: 5.1604\n",
            "Epoch 36: val_loss improved from 27.50414 to 27.25090, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 27.4858 - mae: 5.1604 - val_loss: 27.2509 - val_mae: 5.1403\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2329 - mae: 5.1358\n",
            "Epoch 37: val_loss improved from 27.25090 to 26.99880, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 27.2329 - mae: 5.1358 - val_loss: 26.9988 - val_mae: 5.1157\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9811 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99880 to 26.74801, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 26.9811 - mae: 5.1112 - val_loss: 26.7480 - val_mae: 5.0911\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7305 - mae: 5.0866\n",
            "Epoch 39: val_loss improved from 26.74801 to 26.49857, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 26.7305 - mae: 5.0866 - val_loss: 26.4986 - val_mae: 5.0666\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4812 - mae: 5.0621\n",
            "Epoch 40: val_loss improved from 26.49857 to 26.25028, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.4812 - mae: 5.0621 - val_loss: 26.2503 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2331 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.25028 to 26.00308, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.2331 - mae: 5.0375 - val_loss: 26.0031 - val_mae: 5.0174\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9863 - mae: 5.0129\n",
            "Epoch 42: val_loss improved from 26.00308 to 25.75710, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 25.9863 - mae: 5.0129 - val_loss: 25.7571 - val_mae: 4.9929\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7406 - mae: 4.9884\n",
            "Epoch 43: val_loss improved from 25.75710 to 25.51246, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 25.7406 - mae: 4.9884 - val_loss: 25.5125 - val_mae: 4.9683\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4962 - mae: 4.9638\n",
            "Epoch 44: val_loss improved from 25.51246 to 25.26915, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 25.4962 - mae: 4.9638 - val_loss: 25.2691 - val_mae: 4.9438\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2530 - mae: 4.9393\n",
            "Epoch 45: val_loss improved from 25.26915 to 25.02695, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 25.2530 - mae: 4.9393 - val_loss: 25.0270 - val_mae: 4.9192\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0111 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02695 to 24.78591, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 25.0111 - mae: 4.9147 - val_loss: 24.7859 - val_mae: 4.8946\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7703 - mae: 4.8901\n",
            "Epoch 47: val_loss improved from 24.78591 to 24.54624, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 24.7703 - mae: 4.8901 - val_loss: 24.5462 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5308 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54624 to 24.30779, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 24.5308 - mae: 4.8656 - val_loss: 24.3078 - val_mae: 4.8456\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2925 - mae: 4.8411\n",
            "Epoch 49: val_loss improved from 24.30779 to 24.07043, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 24.2925 - mae: 4.8411 - val_loss: 24.0704 - val_mae: 4.8210\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0554 - mae: 4.8165\n",
            "Epoch 50: val_loss improved from 24.07043 to 23.83441, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 24.0554 - mae: 4.8165 - val_loss: 23.8344 - val_mae: 4.7965\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8196 - mae: 4.7920\n",
            "Epoch 51: val_loss improved from 23.83441 to 23.59958, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 23.8196 - mae: 4.7920 - val_loss: 23.5996 - val_mae: 4.7719\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5851 - mae: 4.7674\n",
            "Epoch 52: val_loss improved from 23.59958 to 23.36584, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 23.5851 - mae: 4.7674 - val_loss: 23.3658 - val_mae: 4.7474\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3517 - mae: 4.7429\n",
            "Epoch 53: val_loss improved from 23.36584 to 23.13357, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 23.3517 - mae: 4.7429 - val_loss: 23.1336 - val_mae: 4.7228\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1195 - mae: 4.7184\n",
            "Epoch 54: val_loss improved from 23.13357 to 22.90247, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 23.1195 - mae: 4.7184 - val_loss: 22.9025 - val_mae: 4.6983\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8886 - mae: 4.6938\n",
            "Epoch 55: val_loss improved from 22.90247 to 22.67253, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 22.8886 - mae: 4.6938 - val_loss: 22.6725 - val_mae: 4.6738\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6589 - mae: 4.6693\n",
            "Epoch 56: val_loss improved from 22.67253 to 22.44385, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 22.6589 - mae: 4.6693 - val_loss: 22.4438 - val_mae: 4.6492\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4304 - mae: 4.6448\n",
            "Epoch 57: val_loss improved from 22.44385 to 22.21642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.4304 - mae: 4.6448 - val_loss: 22.2164 - val_mae: 4.6247\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2032 - mae: 4.6202\n",
            "Epoch 58: val_loss improved from 22.21642 to 21.99019, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.2032 - mae: 4.6202 - val_loss: 21.9902 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9772 - mae: 4.5957\n",
            "Epoch 59: val_loss improved from 21.99019 to 21.76507, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 21.9772 - mae: 4.5957 - val_loss: 21.7651 - val_mae: 4.5757\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7524 - mae: 4.5712\n",
            "Epoch 60: val_loss improved from 21.76507 to 21.54121, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 21.7524 - mae: 4.5712 - val_loss: 21.5412 - val_mae: 4.5511\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5288 - mae: 4.5466\n",
            "Epoch 61: val_loss improved from 21.54121 to 21.31860, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 21.5288 - mae: 4.5466 - val_loss: 21.3186 - val_mae: 4.5266\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3065 - mae: 4.5221\n",
            "Epoch 62: val_loss improved from 21.31860 to 21.09737, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 21.3065 - mae: 4.5221 - val_loss: 21.0974 - val_mae: 4.5021\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0853 - mae: 4.4976\n",
            "Epoch 63: val_loss improved from 21.09737 to 20.87729, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 21.0853 - mae: 4.4976 - val_loss: 20.8773 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8655 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87729 to 20.65837, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 20.8655 - mae: 4.4731 - val_loss: 20.6584 - val_mae: 4.4531\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6469 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65837 to 20.44058, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.6469 - mae: 4.4486 - val_loss: 20.4406 - val_mae: 4.4286\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4293 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44058 to 20.22416, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 20.4293 - mae: 4.4241 - val_loss: 20.2242 - val_mae: 4.4041\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2131 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22416 to 20.00907, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 20.2131 - mae: 4.3996 - val_loss: 20.0091 - val_mae: 4.3796\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9981 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00907 to 19.79501, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 19.9981 - mae: 4.3751 - val_loss: 19.7950 - val_mae: 4.3551\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7843 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79501 to 19.58222, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.7843 - mae: 4.3506 - val_loss: 19.5822 - val_mae: 4.3306\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5720 - mae: 4.3261\n",
            "Epoch 70: val_loss improved from 19.58222 to 19.37039, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.5720 - mae: 4.3261 - val_loss: 19.3704 - val_mae: 4.3060\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3605 - mae: 4.3016\n",
            "Epoch 71: val_loss improved from 19.37039 to 19.16023, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.3605 - mae: 4.3016 - val_loss: 19.1602 - val_mae: 4.2816\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1504 - mae: 4.2771\n",
            "Epoch 72: val_loss improved from 19.16023 to 18.95128, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 19.1504 - mae: 4.2771 - val_loss: 18.9513 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9416 - mae: 4.2526\n",
            "Epoch 73: val_loss improved from 18.95128 to 18.74337, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 18.9416 - mae: 4.2526 - val_loss: 18.7434 - val_mae: 4.2326\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7339 - mae: 4.2281\n",
            "Epoch 74: val_loss improved from 18.74337 to 18.53674, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 18.7339 - mae: 4.2281 - val_loss: 18.5367 - val_mae: 4.2081\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5274 - mae: 4.2037\n",
            "Epoch 75: val_loss improved from 18.53674 to 18.33140, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.5274 - mae: 4.2037 - val_loss: 18.3314 - val_mae: 4.1837\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3223 - mae: 4.1792\n",
            "Epoch 76: val_loss improved from 18.33140 to 18.12705, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 18.3223 - mae: 4.1792 - val_loss: 18.1270 - val_mae: 4.1592\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1181 - mae: 4.1547\n",
            "Epoch 77: val_loss improved from 18.12705 to 17.92419, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.1181 - mae: 4.1547 - val_loss: 17.9242 - val_mae: 4.1347\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9156 - mae: 4.1302\n",
            "Epoch 78: val_loss improved from 17.92419 to 17.72220, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.9156 - mae: 4.1302 - val_loss: 17.7222 - val_mae: 4.1102\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7140 - mae: 4.1058\n",
            "Epoch 79: val_loss improved from 17.72220 to 17.52169, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 17.7140 - mae: 4.1058 - val_loss: 17.5217 - val_mae: 4.0858\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5137 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52169 to 17.32231, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 17.5137 - mae: 4.0813 - val_loss: 17.3223 - val_mae: 4.0613\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3145 - mae: 4.0568\n",
            "Epoch 81: val_loss improved from 17.32231 to 17.12446, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.3145 - mae: 4.0568 - val_loss: 17.1245 - val_mae: 4.0368\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1167 - mae: 4.0324\n",
            "Epoch 82: val_loss improved from 17.12446 to 16.92755, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 17.1167 - mae: 4.0324 - val_loss: 16.9275 - val_mae: 4.0124\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9201 - mae: 4.0079\n",
            "Epoch 83: val_loss improved from 16.92755 to 16.73176, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.9201 - mae: 4.0079 - val_loss: 16.7318 - val_mae: 3.9879\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7246 - mae: 3.9835\n",
            "Epoch 84: val_loss improved from 16.73176 to 16.53740, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 16.7246 - mae: 3.9835 - val_loss: 16.5374 - val_mae: 3.9635\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5305 - mae: 3.9590\n",
            "Epoch 85: val_loss improved from 16.53740 to 16.34411, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.5305 - mae: 3.9590 - val_loss: 16.3441 - val_mae: 3.9390\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3375 - mae: 3.9346\n",
            "Epoch 86: val_loss improved from 16.34411 to 16.15208, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.3375 - mae: 3.9346 - val_loss: 16.1521 - val_mae: 3.9146\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1457 - mae: 3.9101\n",
            "Epoch 87: val_loss improved from 16.15208 to 15.96144, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 16.1457 - mae: 3.9101 - val_loss: 15.9614 - val_mae: 3.8901\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9552 - mae: 3.8857\n",
            "Epoch 88: val_loss improved from 15.96144 to 15.77196, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 15.9552 - mae: 3.8857 - val_loss: 15.7720 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7659 - mae: 3.8613\n",
            "Epoch 89: val_loss improved from 15.77196 to 15.58370, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 15.7659 - mae: 3.8613 - val_loss: 15.5837 - val_mae: 3.8413\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5779 - mae: 3.8368\n",
            "Epoch 90: val_loss improved from 15.58370 to 15.39637, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 15.5779 - mae: 3.8368 - val_loss: 15.3964 - val_mae: 3.8168\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3908 - mae: 3.8124\n",
            "Epoch 91: val_loss improved from 15.39637 to 15.21069, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 15.3908 - mae: 3.8124 - val_loss: 15.2107 - val_mae: 3.7924\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2053 - mae: 3.7880\n",
            "Epoch 92: val_loss improved from 15.21069 to 15.02594, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.2053 - mae: 3.7880 - val_loss: 15.0259 - val_mae: 3.7680\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0208 - mae: 3.7635\n",
            "Epoch 93: val_loss improved from 15.02594 to 14.84241, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 15.0208 - mae: 3.7635 - val_loss: 14.8424 - val_mae: 3.7435\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8376 - mae: 3.7391\n",
            "Epoch 94: val_loss improved from 14.84241 to 14.66018, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 14.8376 - mae: 3.7391 - val_loss: 14.6602 - val_mae: 3.7191\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6556 - mae: 3.7147\n",
            "Epoch 95: val_loss improved from 14.66018 to 14.47921, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 14.6556 - mae: 3.7147 - val_loss: 14.4792 - val_mae: 3.6947\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4748 - mae: 3.6903\n",
            "Epoch 96: val_loss improved from 14.47921 to 14.29950, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.4748 - mae: 3.6903 - val_loss: 14.2995 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2953 - mae: 3.6659\n",
            "Epoch 97: val_loss improved from 14.29950 to 14.12095, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 14.2953 - mae: 3.6659 - val_loss: 14.1209 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1170 - mae: 3.6415\n",
            "Epoch 98: val_loss improved from 14.12095 to 13.94356, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 14.1170 - mae: 3.6415 - val_loss: 13.9436 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9400 - mae: 3.6170\n",
            "Epoch 99: val_loss improved from 13.94356 to 13.76723, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 13.9400 - mae: 3.6170 - val_loss: 13.7672 - val_mae: 3.5971\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7639 - mae: 3.5927\n",
            "Epoch 100: val_loss improved from 13.76723 to 13.59252, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 13.7639 - mae: 3.5927 - val_loss: 13.5925 - val_mae: 3.5727\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 25ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.747248649597168, RMSE:3.707728147506714, MAE:3.5902934074401855, R2:-15.040280030095737\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_190\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_131 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_140 (Embedding)      (None, 17, 900)      649800      ['input_131[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_141 (Embedding)      (None, 17, 900)      649800      ['input_131[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_142 (Embedding)      (None, 17, 900)      649800      ['input_131[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_143 (Embedding)      (None, 17, 900)      649800      ['input_131[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_195 (Conv1D)            (None, 17, 100)      270100      ['embedding_140[0][0]',          \n",
            "                                                                  'embedding_141[0][0]',          \n",
            "                                                                  'embedding_142[0][0]',          \n",
            "                                                                  'embedding_143[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_196 (Conv1D)            (None, 17, 100)      270100      ['embedding_140[0][0]',          \n",
            "                                                                  'embedding_141[0][0]',          \n",
            "                                                                  'embedding_142[0][0]',          \n",
            "                                                                  'embedding_143[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_197 (Conv1D)            (None, 17, 100)      270100      ['embedding_140[0][0]',          \n",
            "                                                                  'embedding_141[0][0]',          \n",
            "                                                                  'embedding_142[0][0]',          \n",
            "                                                                  'embedding_143[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_420 (Glob  (None, 100)         0           ['conv1d_195[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_421 (Glob  (None, 100)         0           ['conv1d_195[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_422 (Glob  (None, 100)         0           ['conv1d_195[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_423 (Glob  (None, 100)         0           ['conv1d_195[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_424 (Glob  (None, 100)         0           ['conv1d_196[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_425 (Glob  (None, 100)         0           ['conv1d_196[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_426 (Glob  (None, 100)         0           ['conv1d_196[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_427 (Glob  (None, 100)         0           ['conv1d_196[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_428 (Glob  (None, 100)         0           ['conv1d_197[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_429 (Glob  (None, 100)         0           ['conv1d_197[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_430 (Glob  (None, 100)         0           ['conv1d_197[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_431 (Glob  (None, 100)         0           ['conv1d_197[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_132 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_75 (Add)                   (None, 100)          0           ['global_max_pooling1d_420[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_421[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_422[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_423[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_76 (Add)                   (None, 100)          0           ['global_max_pooling1d_424[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_425[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_426[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_427[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_77 (Add)                   (None, 100)          0           ['global_max_pooling1d_428[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_429[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_430[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_431[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_270 (Dense)              (None, 1000)         11000       ['input_132[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_65 (Concatenate)   (None, 1300)         0           ['add_75[0][0]',                 \n",
            "                                                                  'add_76[0][0]',                 \n",
            "                                                                  'add_77[0][0]',                 \n",
            "                                                                  'dense_270[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_65[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_225 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_225[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_226 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_226[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_227 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_227[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_228 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_228[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_229 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_271 (Dense)              (None, 1)            3           ['dropout_229[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.5449 - mae: 1.0146\n",
            "Epoch 1: val_loss improved from inf to 0.68692, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 7s 141ms/step - loss: 2.5449 - mae: 1.0146 - val_loss: 0.6869 - val_mae: 0.6423\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7164 - mae: 0.5367\n",
            "Epoch 2: val_loss improved from 0.68692 to 0.61319, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.7164 - mae: 0.5367 - val_loss: 0.6132 - val_mae: 0.6318\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5717 - mae: 0.4602\n",
            "Epoch 3: val_loss improved from 0.61319 to 0.45772, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.5717 - mae: 0.4602 - val_loss: 0.4577 - val_mae: 0.4718\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5184 - mae: 0.4555\n",
            "Epoch 4: val_loss improved from 0.45772 to 0.41090, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.5184 - mae: 0.4555 - val_loss: 0.4109 - val_mae: 0.3938\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4495 - mae: 0.3957\n",
            "Epoch 5: val_loss did not improve from 0.41090\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4495 - mae: 0.3957 - val_loss: 0.4554 - val_mae: 0.5181\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5178 - mae: 0.4936\n",
            "Epoch 6: val_loss improved from 0.41090 to 0.37685, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.5178 - mae: 0.4936 - val_loss: 0.3769 - val_mae: 0.2940\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4233 - mae: 0.3758\n",
            "Epoch 7: val_loss did not improve from 0.37685\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4233 - mae: 0.3758 - val_loss: 0.3918 - val_mae: 0.3251\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4365 - mae: 0.4089\n",
            "Epoch 8: val_loss did not improve from 0.37685\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4365 - mae: 0.4089 - val_loss: 0.3900 - val_mae: 0.3217\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4218 - mae: 0.3924\n",
            "Epoch 9: val_loss did not improve from 0.37685\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4218 - mae: 0.3924 - val_loss: 0.4063 - val_mae: 0.4621\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3847 - mae: 0.3440\n",
            "Epoch 10: val_loss improved from 0.37685 to 0.36360, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3847 - mae: 0.3440 - val_loss: 0.3636 - val_mae: 0.3206\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4251 - mae: 0.4063\n",
            "Epoch 11: val_loss did not improve from 0.36360\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4251 - mae: 0.4063 - val_loss: 0.3740 - val_mae: 0.3022\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4136 - mae: 0.3991\n",
            "Epoch 12: val_loss improved from 0.36360 to 0.34739, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.4136 - mae: 0.3991 - val_loss: 0.3474 - val_mae: 0.2851\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3934 - mae: 0.3764\n",
            "Epoch 13: val_loss did not improve from 0.34739\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3934 - mae: 0.3764 - val_loss: 0.4529 - val_mae: 0.5160\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4198 - mae: 0.4177\n",
            "Epoch 14: val_loss did not improve from 0.34739\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.4198 - mae: 0.4177 - val_loss: 0.3514 - val_mae: 0.3738\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3814 - mae: 0.3661\n",
            "Epoch 15: val_loss did not improve from 0.34739\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.3814 - mae: 0.3661 - val_loss: 0.3610 - val_mae: 0.4088\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4310 - mae: 0.4528\n",
            "Epoch 16: val_loss did not improve from 0.34739\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.4310 - mae: 0.4528 - val_loss: 0.4583 - val_mae: 0.4495\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3700 - mae: 0.3519\n",
            "Epoch 17: val_loss improved from 0.34739 to 0.32481, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3700 - mae: 0.3519 - val_loss: 0.3248 - val_mae: 0.3176\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3455 - mae: 0.3221\n",
            "Epoch 18: val_loss did not improve from 0.32481\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3455 - mae: 0.3221 - val_loss: 0.3567 - val_mae: 0.4231\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3705 - mae: 0.3710\n",
            "Epoch 19: val_loss did not improve from 0.32481\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.3705 - mae: 0.3710 - val_loss: 0.3473 - val_mae: 0.4016\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3779 - mae: 0.3916\n",
            "Epoch 20: val_loss did not improve from 0.32481\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3779 - mae: 0.3916 - val_loss: 0.3386 - val_mae: 0.3669\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3386 - mae: 0.3375\n",
            "Epoch 21: val_loss did not improve from 0.32481\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.3386 - mae: 0.3375 - val_loss: 0.3367 - val_mae: 0.3900\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3213 - mae: 0.3155\n",
            "Epoch 22: val_loss improved from 0.32481 to 0.29656, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.3213 - mae: 0.3155 - val_loss: 0.2966 - val_mae: 0.2626\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3182 - mae: 0.3253\n",
            "Epoch 23: val_loss improved from 0.29656 to 0.28025, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3182 - mae: 0.3253 - val_loss: 0.2803 - val_mae: 0.2504\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3433 - mae: 0.3739\n",
            "Epoch 24: val_loss improved from 0.28025 to 0.27235, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.3433 - mae: 0.3739 - val_loss: 0.2723 - val_mae: 0.2663\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3119 - mae: 0.3336\n",
            "Epoch 25: val_loss improved from 0.27235 to 0.26402, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3119 - mae: 0.3336 - val_loss: 0.2640 - val_mae: 0.2753\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2695 - mae: 0.2895\n",
            "Epoch 26: val_loss did not improve from 0.26402\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2695 - mae: 0.2895 - val_loss: 0.3511 - val_mae: 0.4038\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2971 - mae: 0.3380\n",
            "Epoch 27: val_loss improved from 0.26402 to 0.24748, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.2971 - mae: 0.3380 - val_loss: 0.2475 - val_mae: 0.2442\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3115 - mae: 0.3440\n",
            "Epoch 28: val_loss did not improve from 0.24748\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3115 - mae: 0.3440 - val_loss: 0.3326 - val_mae: 0.2993\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2539 - mae: 0.2903\n",
            "Epoch 29: val_loss improved from 0.24748 to 0.20101, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.2539 - mae: 0.2903 - val_loss: 0.2010 - val_mae: 0.2007\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2668 - mae: 0.3146\n",
            "Epoch 30: val_loss did not improve from 0.20101\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2668 - mae: 0.3146 - val_loss: 0.2428 - val_mae: 0.2862\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2172 - mae: 0.2798\n",
            "Epoch 31: val_loss did not improve from 0.20101\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2172 - mae: 0.2798 - val_loss: 0.4737 - val_mae: 0.5969\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2333 - mae: 0.3008\n",
            "Epoch 32: val_loss did not improve from 0.20101\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2333 - mae: 0.3008 - val_loss: 0.2682 - val_mae: 0.4329\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1965 - mae: 0.2758\n",
            "Epoch 33: val_loss did not improve from 0.20101\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1965 - mae: 0.2758 - val_loss: 0.2092 - val_mae: 0.2287\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2741 - mae: 0.3474\n",
            "Epoch 34: val_loss improved from 0.20101 to 0.17359, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.2741 - mae: 0.3474 - val_loss: 0.1736 - val_mae: 0.2133\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2041 - mae: 0.2916\n",
            "Epoch 35: val_loss improved from 0.17359 to 0.16063, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 0.2041 - mae: 0.2916 - val_loss: 0.1606 - val_mae: 0.2232\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1697 - mae: 0.2566\n",
            "Epoch 36: val_loss did not improve from 0.16063\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1697 - mae: 0.2566 - val_loss: 0.1847 - val_mae: 0.2895\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1268 - mae: 0.2096\n",
            "Epoch 37: val_loss improved from 0.16063 to 0.10573, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1268 - mae: 0.2096 - val_loss: 0.1057 - val_mae: 0.1487\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0970 - mae: 0.1663\n",
            "Epoch 38: val_loss improved from 0.10573 to 0.09799, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.0970 - mae: 0.1663 - val_loss: 0.0980 - val_mae: 0.1510\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3632 - mae: 0.4030\n",
            "Epoch 39: val_loss did not improve from 0.09799\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3632 - mae: 0.4030 - val_loss: 0.4013 - val_mae: 0.5056\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2600 - mae: 0.3204\n",
            "Epoch 40: val_loss did not improve from 0.09799\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2600 - mae: 0.3204 - val_loss: 0.2315 - val_mae: 0.3600\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1689 - mae: 0.2311\n",
            "Epoch 41: val_loss did not improve from 0.09799\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1689 - mae: 0.2311 - val_loss: 0.1667 - val_mae: 0.2860\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2441 - mae: 0.3457\n",
            "Epoch 42: val_loss did not improve from 0.09799\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2441 - mae: 0.3457 - val_loss: 0.1761 - val_mae: 0.2472\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1536 - mae: 0.2429\n",
            "Epoch 43: val_loss did not improve from 0.09799\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1536 - mae: 0.2429 - val_loss: 0.1631 - val_mae: 0.2922\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1421 - mae: 0.2466\n",
            "Epoch 44: val_loss did not improve from 0.09799\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1421 - mae: 0.2466 - val_loss: 0.1300 - val_mae: 0.2419\n",
            "Epoch 44: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 26ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.09682081639766693, RMSE:0.31116044521331787, MAE:0.1582188904285431, R2:0.8870295410026343\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_196\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_133 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_144 (Embedding)      (None, 17, 900)      649800      ['input_133[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_145 (Embedding)      (None, 17, 900)      649800      ['input_133[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_146 (Embedding)      (None, 17, 900)      649800      ['input_133[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_147 (Embedding)      (None, 17, 900)      649800      ['input_133[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_198 (Conv1D)            (None, 17, 100)      270100      ['embedding_144[0][0]',          \n",
            "                                                                  'embedding_145[0][0]',          \n",
            "                                                                  'embedding_146[0][0]',          \n",
            "                                                                  'embedding_147[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_199 (Conv1D)            (None, 17, 100)      270100      ['embedding_144[0][0]',          \n",
            "                                                                  'embedding_145[0][0]',          \n",
            "                                                                  'embedding_146[0][0]',          \n",
            "                                                                  'embedding_147[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_200 (Conv1D)            (None, 17, 100)      270100      ['embedding_144[0][0]',          \n",
            "                                                                  'embedding_145[0][0]',          \n",
            "                                                                  'embedding_146[0][0]',          \n",
            "                                                                  'embedding_147[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_432 (Glob  (None, 100)         0           ['conv1d_198[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_433 (Glob  (None, 100)         0           ['conv1d_198[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_434 (Glob  (None, 100)         0           ['conv1d_198[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_435 (Glob  (None, 100)         0           ['conv1d_198[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_436 (Glob  (None, 100)         0           ['conv1d_199[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_437 (Glob  (None, 100)         0           ['conv1d_199[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_438 (Glob  (None, 100)         0           ['conv1d_199[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_439 (Glob  (None, 100)         0           ['conv1d_199[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_440 (Glob  (None, 100)         0           ['conv1d_200[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_441 (Glob  (None, 100)         0           ['conv1d_200[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_442 (Glob  (None, 100)         0           ['conv1d_200[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_443 (Glob  (None, 100)         0           ['conv1d_200[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_134 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_78 (Add)                   (None, 100)          0           ['global_max_pooling1d_432[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_433[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_434[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_435[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_79 (Add)                   (None, 100)          0           ['global_max_pooling1d_436[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_437[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_438[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_439[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_80 (Add)                   (None, 100)          0           ['global_max_pooling1d_440[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_441[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_442[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_443[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_272 (Dense)              (None, 1000)         11000       ['input_134[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_66 (Concatenate)   (None, 1300)         0           ['add_78[0][0]',                 \n",
            "                                                                  'add_79[0][0]',                 \n",
            "                                                                  'add_80[0][0]',                 \n",
            "                                                                  'dense_272[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_66[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_230 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_230[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_231 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_231[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_232 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_232[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_233 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_233[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_234 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_273 (Dense)              (None, 1)            3           ['dropout_234[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.4287 - mae: 6.0464\n",
            "Epoch 1: val_loss improved from inf to 36.87366, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 117ms/step - loss: 37.4287 - mae: 6.0464 - val_loss: 36.8737 - val_mae: 6.0038\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8459 - mae: 5.9991\n",
            "Epoch 2: val_loss improved from 36.87366 to 36.57508, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 36.8459 - mae: 5.9991 - val_loss: 36.5751 - val_mae: 5.9789\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5477 - mae: 5.9742\n",
            "Epoch 3: val_loss improved from 36.57508 to 36.27818, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 36.5477 - mae: 5.9742 - val_loss: 36.2782 - val_mae: 5.9540\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2512 - mae: 5.9493\n",
            "Epoch 4: val_loss improved from 36.27818 to 35.98261, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 36.2512 - mae: 5.9493 - val_loss: 35.9826 - val_mae: 5.9291\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9561 - mae: 5.9245\n",
            "Epoch 5: val_loss improved from 35.98261 to 35.68877, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 35.9561 - mae: 5.9245 - val_loss: 35.6888 - val_mae: 5.9043\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6626 - mae: 5.8997\n",
            "Epoch 6: val_loss improved from 35.68877 to 35.39652, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.6626 - mae: 5.8997 - val_loss: 35.3965 - val_mae: 5.8795\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3707 - mae: 5.8749\n",
            "Epoch 7: val_loss improved from 35.39652 to 35.10574, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 35.3707 - mae: 5.8749 - val_loss: 35.1057 - val_mae: 5.8547\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0802 - mae: 5.8501\n",
            "Epoch 8: val_loss improved from 35.10574 to 34.81644, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 35.0802 - mae: 5.8501 - val_loss: 34.8164 - val_mae: 5.8299\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7913 - mae: 5.8253\n",
            "Epoch 9: val_loss improved from 34.81644 to 34.52859, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 34.7913 - mae: 5.8253 - val_loss: 34.5286 - val_mae: 5.8052\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5038 - mae: 5.8006\n",
            "Epoch 10: val_loss improved from 34.52859 to 34.24223, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 34.5038 - mae: 5.8006 - val_loss: 34.2422 - val_mae: 5.7805\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2178 - mae: 5.7759\n",
            "Epoch 11: val_loss improved from 34.24223 to 33.95723, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.2178 - mae: 5.7759 - val_loss: 33.9572 - val_mae: 5.7558\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9331 - mae: 5.7512\n",
            "Epoch 12: val_loss improved from 33.95723 to 33.67374, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.9331 - mae: 5.7512 - val_loss: 33.6737 - val_mae: 5.7311\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6498 - mae: 5.7265\n",
            "Epoch 13: val_loss improved from 33.67374 to 33.39163, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 33.6498 - mae: 5.7265 - val_loss: 33.3916 - val_mae: 5.7064\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3680 - mae: 5.7019\n",
            "Epoch 14: val_loss improved from 33.39163 to 33.11079, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 33.3680 - mae: 5.7019 - val_loss: 33.1108 - val_mae: 5.6818\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0873 - mae: 5.6772\n",
            "Epoch 15: val_loss improved from 33.11079 to 32.83125, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 33.0873 - mae: 5.6772 - val_loss: 32.8312 - val_mae: 5.6571\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8081 - mae: 5.6526\n",
            "Epoch 16: val_loss improved from 32.83125 to 32.55291, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 32.8081 - mae: 5.6526 - val_loss: 32.5529 - val_mae: 5.6325\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5301 - mae: 5.6279\n",
            "Epoch 17: val_loss improved from 32.55291 to 32.27594, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 32.5301 - mae: 5.6279 - val_loss: 32.2759 - val_mae: 5.6078\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2534 - mae: 5.6033\n",
            "Epoch 18: val_loss improved from 32.27594 to 32.00033, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 32.2534 - mae: 5.6033 - val_loss: 32.0003 - val_mae: 5.5832\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9781 - mae: 5.5786\n",
            "Epoch 19: val_loss improved from 32.00033 to 31.72594, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 31.9781 - mae: 5.5786 - val_loss: 31.7259 - val_mae: 5.5586\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7039 - mae: 5.5540\n",
            "Epoch 20: val_loss improved from 31.72594 to 31.45292, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.7039 - mae: 5.5540 - val_loss: 31.4529 - val_mae: 5.5339\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4311 - mae: 5.5294\n",
            "Epoch 21: val_loss improved from 31.45292 to 31.18110, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.4311 - mae: 5.5294 - val_loss: 31.1811 - val_mae: 5.5093\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1596 - mae: 5.5048\n",
            "Epoch 22: val_loss improved from 31.18110 to 30.91052, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.1596 - mae: 5.5048 - val_loss: 30.9105 - val_mae: 5.4847\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8891 - mae: 5.4802\n",
            "Epoch 23: val_loss improved from 30.91052 to 30.64130, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.8891 - mae: 5.4802 - val_loss: 30.6413 - val_mae: 5.4601\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6201 - mae: 5.4556\n",
            "Epoch 24: val_loss improved from 30.64130 to 30.37311, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.6201 - mae: 5.4556 - val_loss: 30.3731 - val_mae: 5.4355\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3522 - mae: 5.4310\n",
            "Epoch 25: val_loss improved from 30.37311 to 30.10634, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.3522 - mae: 5.4310 - val_loss: 30.1063 - val_mae: 5.4109\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0856 - mae: 5.4064\n",
            "Epoch 26: val_loss improved from 30.10634 to 29.84064, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 106ms/step - loss: 30.0856 - mae: 5.4064 - val_loss: 29.8406 - val_mae: 5.3863\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8202 - mae: 5.3818\n",
            "Epoch 27: val_loss improved from 29.84064 to 29.57642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.8202 - mae: 5.3818 - val_loss: 29.5764 - val_mae: 5.3617\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5562 - mae: 5.3572\n",
            "Epoch 28: val_loss improved from 29.57642 to 29.31319, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 29.5562 - mae: 5.3572 - val_loss: 29.3132 - val_mae: 5.3371\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2934 - mae: 5.3326\n",
            "Epoch 29: val_loss improved from 29.31319 to 29.05118, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 29.2934 - mae: 5.3326 - val_loss: 29.0512 - val_mae: 5.3125\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0316 - mae: 5.3080\n",
            "Epoch 30: val_loss improved from 29.05118 to 28.79066, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.0316 - mae: 5.3080 - val_loss: 28.7907 - val_mae: 5.2879\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7713 - mae: 5.2834\n",
            "Epoch 31: val_loss improved from 28.79066 to 28.53121, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.7713 - mae: 5.2834 - val_loss: 28.5312 - val_mae: 5.2634\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5121 - mae: 5.2588\n",
            "Epoch 32: val_loss improved from 28.53121 to 28.27301, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 28.5121 - mae: 5.2588 - val_loss: 28.2730 - val_mae: 5.2388\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2541 - mae: 5.2343\n",
            "Epoch 33: val_loss improved from 28.27301 to 28.01623, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.2541 - mae: 5.2343 - val_loss: 28.0162 - val_mae: 5.2142\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9976 - mae: 5.2097\n",
            "Epoch 34: val_loss improved from 28.01623 to 27.76038, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 27.9976 - mae: 5.2097 - val_loss: 27.7604 - val_mae: 5.1896\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7421 - mae: 5.1851\n",
            "Epoch 35: val_loss improved from 27.76038 to 27.50596, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 27.7421 - mae: 5.1851 - val_loss: 27.5060 - val_mae: 5.1650\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4879 - mae: 5.1605\n",
            "Epoch 36: val_loss improved from 27.50596 to 27.25276, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 27.4879 - mae: 5.1605 - val_loss: 27.2528 - val_mae: 5.1405\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2348 - mae: 5.1360\n",
            "Epoch 37: val_loss improved from 27.25276 to 27.00083, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 27.2348 - mae: 5.1360 - val_loss: 27.0008 - val_mae: 5.1159\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9830 - mae: 5.1114\n",
            "Epoch 38: val_loss improved from 27.00083 to 26.74999, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.9830 - mae: 5.1114 - val_loss: 26.7500 - val_mae: 5.0913\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7324 - mae: 5.0868\n",
            "Epoch 39: val_loss improved from 26.74999 to 26.50044, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.7324 - mae: 5.0868 - val_loss: 26.5004 - val_mae: 5.0668\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4831 - mae: 5.0622\n",
            "Epoch 40: val_loss improved from 26.50044 to 26.25192, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 26.4831 - mae: 5.0622 - val_loss: 26.2519 - val_mae: 5.0422\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2348 - mae: 5.0377\n",
            "Epoch 41: val_loss improved from 26.25192 to 26.00505, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 26.2348 - mae: 5.0377 - val_loss: 26.0051 - val_mae: 5.0176\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9880 - mae: 5.0131\n",
            "Epoch 42: val_loss improved from 26.00505 to 25.75912, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 25.9880 - mae: 5.0131 - val_loss: 25.7591 - val_mae: 4.9931\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7424 - mae: 4.9886\n",
            "Epoch 43: val_loss improved from 25.75912 to 25.51426, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 25.7424 - mae: 4.9886 - val_loss: 25.5143 - val_mae: 4.9685\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4979 - mae: 4.9640\n",
            "Epoch 44: val_loss improved from 25.51426 to 25.27089, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.4979 - mae: 4.9640 - val_loss: 25.2709 - val_mae: 4.9439\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2548 - mae: 4.9394\n",
            "Epoch 45: val_loss improved from 25.27089 to 25.02873, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 25.2548 - mae: 4.9394 - val_loss: 25.0287 - val_mae: 4.9194\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0128 - mae: 4.9149\n",
            "Epoch 46: val_loss improved from 25.02873 to 24.78774, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 25.0128 - mae: 4.9149 - val_loss: 24.7877 - val_mae: 4.8948\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7720 - mae: 4.8903\n",
            "Epoch 47: val_loss improved from 24.78774 to 24.54798, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 24.7720 - mae: 4.8903 - val_loss: 24.5480 - val_mae: 4.8703\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5325 - mae: 4.8658\n",
            "Epoch 48: val_loss improved from 24.54798 to 24.30948, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 24.5325 - mae: 4.8658 - val_loss: 24.3095 - val_mae: 4.8457\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2942 - mae: 4.8412\n",
            "Epoch 49: val_loss improved from 24.30948 to 24.07221, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.2942 - mae: 4.8412 - val_loss: 24.0722 - val_mae: 4.8212\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0572 - mae: 4.8167\n",
            "Epoch 50: val_loss improved from 24.07221 to 23.83603, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.0572 - mae: 4.8167 - val_loss: 23.8360 - val_mae: 4.7966\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8214 - mae: 4.7921\n",
            "Epoch 51: val_loss improved from 23.83603 to 23.60119, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 23.8214 - mae: 4.7921 - val_loss: 23.6012 - val_mae: 4.7721\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5868 - mae: 4.7676\n",
            "Epoch 52: val_loss improved from 23.60119 to 23.36764, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.5868 - mae: 4.7676 - val_loss: 23.3676 - val_mae: 4.7476\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3534 - mae: 4.7431\n",
            "Epoch 53: val_loss improved from 23.36764 to 23.13526, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 23.3534 - mae: 4.7431 - val_loss: 23.1353 - val_mae: 4.7230\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1212 - mae: 4.7185\n",
            "Epoch 54: val_loss improved from 23.13526 to 22.90419, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 23.1212 - mae: 4.7185 - val_loss: 22.9042 - val_mae: 4.6985\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8903 - mae: 4.6940\n",
            "Epoch 55: val_loss improved from 22.90419 to 22.67420, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 22.8903 - mae: 4.6940 - val_loss: 22.6742 - val_mae: 4.6740\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6607 - mae: 4.6695\n",
            "Epoch 56: val_loss improved from 22.67420 to 22.44538, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.6607 - mae: 4.6695 - val_loss: 22.4454 - val_mae: 4.6494\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4323 - mae: 4.6449\n",
            "Epoch 57: val_loss improved from 22.44538 to 22.21777, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 22.4323 - mae: 4.6449 - val_loss: 22.2178 - val_mae: 4.6249\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2048 - mae: 4.6204\n",
            "Epoch 58: val_loss improved from 22.21777 to 21.99194, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 22.2048 - mae: 4.6204 - val_loss: 21.9919 - val_mae: 4.6004\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9789 - mae: 4.5959\n",
            "Epoch 59: val_loss improved from 21.99194 to 21.76682, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.9789 - mae: 4.5959 - val_loss: 21.7668 - val_mae: 4.5759\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7540 - mae: 4.5714\n",
            "Epoch 60: val_loss improved from 21.76682 to 21.54314, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.7540 - mae: 4.5714 - val_loss: 21.5431 - val_mae: 4.5514\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5304 - mae: 4.5469\n",
            "Epoch 61: val_loss improved from 21.54314 to 21.32040, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.5304 - mae: 4.5469 - val_loss: 21.3204 - val_mae: 4.5268\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3081 - mae: 4.5223\n",
            "Epoch 62: val_loss improved from 21.32040 to 21.09894, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.3081 - mae: 4.5223 - val_loss: 21.0989 - val_mae: 4.5023\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0870 - mae: 4.4978\n",
            "Epoch 63: val_loss improved from 21.09894 to 20.87874, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 21.0870 - mae: 4.4978 - val_loss: 20.8787 - val_mae: 4.4778\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8671 - mae: 4.4733\n",
            "Epoch 64: val_loss improved from 20.87874 to 20.65987, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 20.8671 - mae: 4.4733 - val_loss: 20.6599 - val_mae: 4.4533\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6484 - mae: 4.4488\n",
            "Epoch 65: val_loss improved from 20.65987 to 20.44226, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 20.6484 - mae: 4.4488 - val_loss: 20.4423 - val_mae: 4.4288\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4308 - mae: 4.4243\n",
            "Epoch 66: val_loss improved from 20.44226 to 20.22600, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 20.4308 - mae: 4.4243 - val_loss: 20.2260 - val_mae: 4.4043\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2147 - mae: 4.3998\n",
            "Epoch 67: val_loss improved from 20.22600 to 20.01066, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 20.2147 - mae: 4.3998 - val_loss: 20.0107 - val_mae: 4.3798\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9996 - mae: 4.3753\n",
            "Epoch 68: val_loss improved from 20.01066 to 19.79666, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 19.9996 - mae: 4.3753 - val_loss: 19.7967 - val_mae: 4.3553\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7860 - mae: 4.3508\n",
            "Epoch 69: val_loss improved from 19.79666 to 19.58355, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 19.7860 - mae: 4.3508 - val_loss: 19.5836 - val_mae: 4.3307\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5733 - mae: 4.3263\n",
            "Epoch 70: val_loss improved from 19.58355 to 19.37220, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 19.5733 - mae: 4.3263 - val_loss: 19.3722 - val_mae: 4.3063\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3620 - mae: 4.3018\n",
            "Epoch 71: val_loss improved from 19.37220 to 19.16194, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 19.3620 - mae: 4.3018 - val_loss: 19.1619 - val_mae: 4.2818\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1520 - mae: 4.2773\n",
            "Epoch 72: val_loss improved from 19.16194 to 18.95278, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.1520 - mae: 4.2773 - val_loss: 18.9528 - val_mae: 4.2573\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9432 - mae: 4.2528\n",
            "Epoch 73: val_loss improved from 18.95278 to 18.74473, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.9432 - mae: 4.2528 - val_loss: 18.7447 - val_mae: 4.2328\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7354 - mae: 4.2283\n",
            "Epoch 74: val_loss improved from 18.74473 to 18.53825, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 18.7354 - mae: 4.2283 - val_loss: 18.5382 - val_mae: 4.2083\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5289 - mae: 4.2039\n",
            "Epoch 75: val_loss improved from 18.53825 to 18.33287, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.5289 - mae: 4.2039 - val_loss: 18.3329 - val_mae: 4.1838\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3239 - mae: 4.1794\n",
            "Epoch 76: val_loss improved from 18.33287 to 18.12847, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 18.3239 - mae: 4.1794 - val_loss: 18.1285 - val_mae: 4.1593\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1197 - mae: 4.1549\n",
            "Epoch 77: val_loss improved from 18.12847 to 17.92566, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 18.1197 - mae: 4.1549 - val_loss: 17.9257 - val_mae: 4.1349\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9171 - mae: 4.1304\n",
            "Epoch 78: val_loss improved from 17.92566 to 17.72376, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 17.9171 - mae: 4.1304 - val_loss: 17.7238 - val_mae: 4.1104\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7155 - mae: 4.1060\n",
            "Epoch 79: val_loss improved from 17.72376 to 17.52334, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.7155 - mae: 4.1060 - val_loss: 17.5233 - val_mae: 4.0860\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5152 - mae: 4.0815\n",
            "Epoch 80: val_loss improved from 17.52334 to 17.32396, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.5152 - mae: 4.0815 - val_loss: 17.3240 - val_mae: 4.0615\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3161 - mae: 4.0570\n",
            "Epoch 81: val_loss improved from 17.32396 to 17.12592, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 17.3161 - mae: 4.0570 - val_loss: 17.1259 - val_mae: 4.0370\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1182 - mae: 4.0326\n",
            "Epoch 82: val_loss improved from 17.12592 to 16.92895, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.1182 - mae: 4.0326 - val_loss: 16.9289 - val_mae: 4.0126\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9215 - mae: 4.0081\n",
            "Epoch 83: val_loss improved from 16.92895 to 16.73337, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 16.9215 - mae: 4.0081 - val_loss: 16.7334 - val_mae: 3.9881\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7261 - mae: 3.9836\n",
            "Epoch 84: val_loss improved from 16.73337 to 16.53882, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 16.7261 - mae: 3.9836 - val_loss: 16.5388 - val_mae: 3.9636\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5318 - mae: 3.9592\n",
            "Epoch 85: val_loss improved from 16.53882 to 16.34574, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 16.5318 - mae: 3.9592 - val_loss: 16.3457 - val_mae: 3.9392\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3390 - mae: 3.9347\n",
            "Epoch 86: val_loss improved from 16.34574 to 16.15354, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 16.3390 - mae: 3.9347 - val_loss: 16.1535 - val_mae: 3.9147\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1471 - mae: 3.9103\n",
            "Epoch 87: val_loss improved from 16.15354 to 15.96284, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 16.1471 - mae: 3.9103 - val_loss: 15.9628 - val_mae: 3.8903\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9565 - mae: 3.8859\n",
            "Epoch 88: val_loss improved from 15.96284 to 15.77341, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 15.9565 - mae: 3.8859 - val_loss: 15.7734 - val_mae: 3.8659\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7674 - mae: 3.8614\n",
            "Epoch 89: val_loss improved from 15.77341 to 15.58488, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 15.7674 - mae: 3.8614 - val_loss: 15.5849 - val_mae: 3.8414\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5792 - mae: 3.8370\n",
            "Epoch 90: val_loss improved from 15.58488 to 15.39781, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 15.5792 - mae: 3.8370 - val_loss: 15.3978 - val_mae: 3.8170\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3923 - mae: 3.8126\n",
            "Epoch 91: val_loss improved from 15.39781 to 15.21194, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 15.3923 - mae: 3.8126 - val_loss: 15.2119 - val_mae: 3.7926\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2066 - mae: 3.7881\n",
            "Epoch 92: val_loss improved from 15.21194 to 15.02736, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 15.2066 - mae: 3.7881 - val_loss: 15.0274 - val_mae: 3.7682\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0223 - mae: 3.7637\n",
            "Epoch 93: val_loss improved from 15.02736 to 14.84382, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 15.0223 - mae: 3.7637 - val_loss: 14.8438 - val_mae: 3.7437\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8390 - mae: 3.7393\n",
            "Epoch 94: val_loss improved from 14.84382 to 14.66173, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 14.8390 - mae: 3.7393 - val_loss: 14.6617 - val_mae: 3.7193\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6571 - mae: 3.7149\n",
            "Epoch 95: val_loss improved from 14.66173 to 14.48058, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 14.6571 - mae: 3.7149 - val_loss: 14.4806 - val_mae: 3.6949\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4761 - mae: 3.6905\n",
            "Epoch 96: val_loss improved from 14.48058 to 14.30096, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 14.4761 - mae: 3.6905 - val_loss: 14.3010 - val_mae: 3.6705\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2966 - mae: 3.6661\n",
            "Epoch 97: val_loss improved from 14.30096 to 14.12238, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 14.2966 - mae: 3.6661 - val_loss: 14.1224 - val_mae: 3.6461\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1183 - mae: 3.6417\n",
            "Epoch 98: val_loss improved from 14.12238 to 13.94494, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 124ms/step - loss: 14.1183 - mae: 3.6417 - val_loss: 13.9449 - val_mae: 3.6217\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9412 - mae: 3.6172\n",
            "Epoch 99: val_loss improved from 13.94494 to 13.76866, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 13.9412 - mae: 3.6172 - val_loss: 13.7687 - val_mae: 3.5973\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7652 - mae: 3.5929\n",
            "Epoch 100: val_loss improved from 13.76866 to 13.59393, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 132ms/step - loss: 13.7652 - mae: 3.5929 - val_loss: 13.5939 - val_mae: 3.5729\n",
            "491/491 [==============================] - 5s 8ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 4s 8ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 24ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.748661041259766, RMSE:3.707918643951416, MAE:3.590489387512207, R2:-15.04192970700353\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_202\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_135 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_148 (Embedding)      (None, 17, 900)      649800      ['input_135[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_149 (Embedding)      (None, 17, 900)      649800      ['input_135[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_150 (Embedding)      (None, 17, 900)      649800      ['input_135[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_151 (Embedding)      (None, 17, 900)      649800      ['input_135[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_201 (Conv1D)            (None, 17, 100)      270100      ['embedding_148[0][0]',          \n",
            "                                                                  'embedding_149[0][0]',          \n",
            "                                                                  'embedding_150[0][0]',          \n",
            "                                                                  'embedding_151[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_202 (Conv1D)            (None, 17, 100)      270100      ['embedding_148[0][0]',          \n",
            "                                                                  'embedding_149[0][0]',          \n",
            "                                                                  'embedding_150[0][0]',          \n",
            "                                                                  'embedding_151[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_203 (Conv1D)            (None, 17, 100)      270100      ['embedding_148[0][0]',          \n",
            "                                                                  'embedding_149[0][0]',          \n",
            "                                                                  'embedding_150[0][0]',          \n",
            "                                                                  'embedding_151[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_444 (Glob  (None, 100)         0           ['conv1d_201[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_445 (Glob  (None, 100)         0           ['conv1d_201[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_446 (Glob  (None, 100)         0           ['conv1d_201[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_447 (Glob  (None, 100)         0           ['conv1d_201[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_448 (Glob  (None, 100)         0           ['conv1d_202[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_449 (Glob  (None, 100)         0           ['conv1d_202[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_450 (Glob  (None, 100)         0           ['conv1d_202[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_451 (Glob  (None, 100)         0           ['conv1d_202[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_452 (Glob  (None, 100)         0           ['conv1d_203[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_453 (Glob  (None, 100)         0           ['conv1d_203[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_454 (Glob  (None, 100)         0           ['conv1d_203[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_455 (Glob  (None, 100)         0           ['conv1d_203[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_136 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_81 (Add)                   (None, 100)          0           ['global_max_pooling1d_444[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_445[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_446[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_447[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_82 (Add)                   (None, 100)          0           ['global_max_pooling1d_448[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_449[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_450[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_451[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_83 (Add)                   (None, 100)          0           ['global_max_pooling1d_452[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_453[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_454[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_455[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_274 (Dense)              (None, 1000)         11000       ['input_136[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_67 (Concatenate)   (None, 1300)         0           ['add_81[0][0]',                 \n",
            "                                                                  'add_82[0][0]',                 \n",
            "                                                                  'add_83[0][0]',                 \n",
            "                                                                  'dense_274[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_67[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_235 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_235[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_236 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_236[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_237 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_237[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_238 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_238[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_239 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_275 (Dense)              (None, 1)            3           ['dropout_239[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.1451 - mae: 6.0240\n",
            "Epoch 1: val_loss improved from inf to 36.87288, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 116ms/step - loss: 37.1451 - mae: 6.0240 - val_loss: 36.8729 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8448 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87288 to 36.57364, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 36.8448 - mae: 5.9990 - val_loss: 36.5736 - val_mae: 5.9787\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5460 - mae: 5.9741\n",
            "Epoch 3: val_loss improved from 36.57364 to 36.27633, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.5460 - mae: 5.9741 - val_loss: 36.2763 - val_mae: 5.9538\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2492 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27633 to 35.98054, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 36.2492 - mae: 5.9492 - val_loss: 35.9805 - val_mae: 5.9289\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9539 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98054 to 35.68668, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 35.9539 - mae: 5.9243 - val_loss: 35.6867 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6604 - mae: 5.8995\n",
            "Epoch 6: val_loss improved from 35.68668 to 35.39421, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.6604 - mae: 5.8995 - val_loss: 35.3942 - val_mae: 5.8793\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3683 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39421 to 35.10338, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 35.3683 - mae: 5.8747 - val_loss: 35.1034 - val_mae: 5.8545\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0779 - mae: 5.8499\n",
            "Epoch 8: val_loss improved from 35.10338 to 34.81398, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 35.0779 - mae: 5.8499 - val_loss: 34.8140 - val_mae: 5.8297\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7889 - mae: 5.8251\n",
            "Epoch 9: val_loss improved from 34.81398 to 34.52625, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.7889 - mae: 5.8251 - val_loss: 34.5262 - val_mae: 5.8050\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5014 - mae: 5.8004\n",
            "Epoch 10: val_loss improved from 34.52625 to 34.23990, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.5014 - mae: 5.8004 - val_loss: 34.2399 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2154 - mae: 5.7757\n",
            "Epoch 11: val_loss improved from 34.23990 to 33.95485, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.2154 - mae: 5.7757 - val_loss: 33.9549 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9307 - mae: 5.7510\n",
            "Epoch 12: val_loss improved from 33.95485 to 33.67126, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.9307 - mae: 5.7510 - val_loss: 33.6713 - val_mae: 5.7309\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6474 - mae: 5.7263\n",
            "Epoch 13: val_loss improved from 33.67126 to 33.38909, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 33.6474 - mae: 5.7263 - val_loss: 33.3891 - val_mae: 5.7062\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3655 - mae: 5.7017\n",
            "Epoch 14: val_loss improved from 33.38909 to 33.10827, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 33.3655 - mae: 5.7017 - val_loss: 33.1083 - val_mae: 5.6815\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0849 - mae: 5.6770\n",
            "Epoch 15: val_loss improved from 33.10827 to 32.82874, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.0849 - mae: 5.6770 - val_loss: 32.8287 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8057 - mae: 5.6523\n",
            "Epoch 16: val_loss improved from 32.82874 to 32.55040, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 32.8057 - mae: 5.6523 - val_loss: 32.5504 - val_mae: 5.6322\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5277 - mae: 5.6277\n",
            "Epoch 17: val_loss improved from 32.55040 to 32.27354, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 32.5277 - mae: 5.6277 - val_loss: 32.2735 - val_mae: 5.6076\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2511 - mae: 5.6031\n",
            "Epoch 18: val_loss improved from 32.27354 to 31.99789, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 32.2511 - mae: 5.6031 - val_loss: 31.9979 - val_mae: 5.5830\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9756 - mae: 5.5785\n",
            "Epoch 19: val_loss improved from 31.99789 to 31.72375, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.9756 - mae: 5.5785 - val_loss: 31.7237 - val_mae: 5.5584\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7016 - mae: 5.5538\n",
            "Epoch 20: val_loss improved from 31.72375 to 31.45053, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 31.7016 - mae: 5.5538 - val_loss: 31.4505 - val_mae: 5.5337\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4287 - mae: 5.5292\n",
            "Epoch 21: val_loss improved from 31.45053 to 31.17874, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 31.4287 - mae: 5.5292 - val_loss: 31.1787 - val_mae: 5.5091\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1572 - mae: 5.5046\n",
            "Epoch 22: val_loss improved from 31.17874 to 30.90815, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 31.1572 - mae: 5.5046 - val_loss: 30.9082 - val_mae: 5.4845\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8869 - mae: 5.4800\n",
            "Epoch 23: val_loss improved from 30.90815 to 30.63869, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.8869 - mae: 5.4800 - val_loss: 30.6387 - val_mae: 5.4599\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6177 - mae: 5.4554\n",
            "Epoch 24: val_loss improved from 30.63869 to 30.37085, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 30.6177 - mae: 5.4554 - val_loss: 30.3709 - val_mae: 5.4353\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3500 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37085 to 30.10397, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 30.3500 - mae: 5.4308 - val_loss: 30.1040 - val_mae: 5.4107\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0833 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10397 to 29.83835, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 30.0833 - mae: 5.4062 - val_loss: 29.8384 - val_mae: 5.3861\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8181 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83835 to 29.57392, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 29.8181 - mae: 5.3816 - val_loss: 29.5739 - val_mae: 5.3615\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5539 - mae: 5.3570\n",
            "Epoch 28: val_loss improved from 29.57392 to 29.31092, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 29.5539 - mae: 5.3570 - val_loss: 29.3109 - val_mae: 5.3369\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2911 - mae: 5.3324\n",
            "Epoch 29: val_loss improved from 29.31092 to 29.04904, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 29.2911 - mae: 5.3324 - val_loss: 29.0490 - val_mae: 5.3123\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0295 - mae: 5.3078\n",
            "Epoch 30: val_loss improved from 29.04904 to 28.78840, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 29.0295 - mae: 5.3078 - val_loss: 28.7884 - val_mae: 5.2877\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7691 - mae: 5.2832\n",
            "Epoch 31: val_loss improved from 28.78840 to 28.52894, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.7691 - mae: 5.2832 - val_loss: 28.5289 - val_mae: 5.2631\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5099 - mae: 5.2586\n",
            "Epoch 32: val_loss improved from 28.52894 to 28.27093, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 28.5099 - mae: 5.2586 - val_loss: 28.2709 - val_mae: 5.2386\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2520 - mae: 5.2340\n",
            "Epoch 33: val_loss improved from 28.27093 to 28.01394, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 111ms/step - loss: 28.2520 - mae: 5.2340 - val_loss: 28.0139 - val_mae: 5.2140\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9953 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01394 to 27.75830, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 27.9953 - mae: 5.2095 - val_loss: 27.7583 - val_mae: 5.1894\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7398 - mae: 5.1849\n",
            "Epoch 35: val_loss improved from 27.75830 to 27.50382, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 27.7398 - mae: 5.1849 - val_loss: 27.5038 - val_mae: 5.1648\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4856 - mae: 5.1603\n",
            "Epoch 36: val_loss improved from 27.50382 to 27.25055, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 110ms/step - loss: 27.4856 - mae: 5.1603 - val_loss: 27.2505 - val_mae: 5.1403\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2326 - mae: 5.1357\n",
            "Epoch 37: val_loss improved from 27.25055 to 26.99848, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 27.2326 - mae: 5.1357 - val_loss: 26.9985 - val_mae: 5.1157\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9808 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99848 to 26.74759, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 26.9808 - mae: 5.1112 - val_loss: 26.7476 - val_mae: 5.0911\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7302 - mae: 5.0866\n",
            "Epoch 39: val_loss improved from 26.74759 to 26.49819, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 111ms/step - loss: 26.7302 - mae: 5.0866 - val_loss: 26.4982 - val_mae: 5.0665\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4810 - mae: 5.0620\n",
            "Epoch 40: val_loss improved from 26.49819 to 26.24979, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 26.4810 - mae: 5.0620 - val_loss: 26.2498 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2327 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.24979 to 26.00287, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 138ms/step - loss: 26.2327 - mae: 5.0375 - val_loss: 26.0029 - val_mae: 5.0174\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9860 - mae: 5.0129\n",
            "Epoch 42: val_loss improved from 26.00287 to 25.75686, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 125ms/step - loss: 25.9860 - mae: 5.0129 - val_loss: 25.7569 - val_mae: 4.9928\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7402 - mae: 4.9884\n",
            "Epoch 43: val_loss improved from 25.75686 to 25.51232, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 111ms/step - loss: 25.7402 - mae: 4.9884 - val_loss: 25.5123 - val_mae: 4.9683\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4958 - mae: 4.9638\n",
            "Epoch 44: val_loss improved from 25.51232 to 25.26884, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 25.4958 - mae: 4.9638 - val_loss: 25.2688 - val_mae: 4.9437\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2526 - mae: 4.9392\n",
            "Epoch 45: val_loss improved from 25.26884 to 25.02662, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 127ms/step - loss: 25.2526 - mae: 4.9392 - val_loss: 25.0266 - val_mae: 4.9192\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0107 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02662 to 24.78559, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 127ms/step - loss: 25.0107 - mae: 4.9147 - val_loss: 24.7856 - val_mae: 4.8946\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7700 - mae: 4.8901\n",
            "Epoch 47: val_loss improved from 24.78559 to 24.54582, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 24.7700 - mae: 4.8901 - val_loss: 24.5458 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5304 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54582 to 24.30734, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 24.5304 - mae: 4.8656 - val_loss: 24.3073 - val_mae: 4.8455\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2922 - mae: 4.8410\n",
            "Epoch 49: val_loss improved from 24.30734 to 24.07010, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 24.2922 - mae: 4.8410 - val_loss: 24.0701 - val_mae: 4.8210\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0551 - mae: 4.8165\n",
            "Epoch 50: val_loss improved from 24.07010 to 23.83406, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 24.0551 - mae: 4.8165 - val_loss: 23.8341 - val_mae: 4.7964\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8193 - mae: 4.7919\n",
            "Epoch 51: val_loss improved from 23.83406 to 23.59926, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 122ms/step - loss: 23.8193 - mae: 4.7919 - val_loss: 23.5993 - val_mae: 4.7719\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5848 - mae: 4.7674\n",
            "Epoch 52: val_loss improved from 23.59926 to 23.36560, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 23.5848 - mae: 4.7674 - val_loss: 23.3656 - val_mae: 4.7473\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3514 - mae: 4.7428\n",
            "Epoch 53: val_loss improved from 23.36560 to 23.13321, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 23.3514 - mae: 4.7428 - val_loss: 23.1332 - val_mae: 4.7228\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1192 - mae: 4.7183\n",
            "Epoch 54: val_loss improved from 23.13321 to 22.90211, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 23.1192 - mae: 4.7183 - val_loss: 22.9021 - val_mae: 4.6983\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8883 - mae: 4.6938\n",
            "Epoch 55: val_loss improved from 22.90211 to 22.67218, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.8883 - mae: 4.6938 - val_loss: 22.6722 - val_mae: 4.6737\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6585 - mae: 4.6693\n",
            "Epoch 56: val_loss improved from 22.67218 to 22.44350, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 22.6585 - mae: 4.6693 - val_loss: 22.4435 - val_mae: 4.6492\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4301 - mae: 4.6447\n",
            "Epoch 57: val_loss improved from 22.44350 to 22.21594, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 22.4301 - mae: 4.6447 - val_loss: 22.2159 - val_mae: 4.6247\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2028 - mae: 4.6202\n",
            "Epoch 58: val_loss improved from 22.21594 to 21.98986, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 22.2028 - mae: 4.6202 - val_loss: 21.9899 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9769 - mae: 4.5957\n",
            "Epoch 59: val_loss improved from 21.98986 to 21.76469, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 21.9769 - mae: 4.5957 - val_loss: 21.7647 - val_mae: 4.5756\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7520 - mae: 4.5711\n",
            "Epoch 60: val_loss improved from 21.76469 to 21.54095, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 137ms/step - loss: 21.7520 - mae: 4.5711 - val_loss: 21.5410 - val_mae: 4.5511\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5285 - mae: 4.5466\n",
            "Epoch 61: val_loss improved from 21.54095 to 21.31834, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 131ms/step - loss: 21.5285 - mae: 4.5466 - val_loss: 21.3183 - val_mae: 4.5266\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3061 - mae: 4.5221\n",
            "Epoch 62: val_loss improved from 21.31834 to 21.09699, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 135ms/step - loss: 21.3061 - mae: 4.5221 - val_loss: 21.0970 - val_mae: 4.5021\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0850 - mae: 4.4976\n",
            "Epoch 63: val_loss improved from 21.09699 to 20.87686, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 21.0850 - mae: 4.4976 - val_loss: 20.8769 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8652 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87686 to 20.65797, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 20.8652 - mae: 4.4731 - val_loss: 20.6580 - val_mae: 4.4530\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6465 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65797 to 20.44042, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 20.6465 - mae: 4.4486 - val_loss: 20.4404 - val_mae: 4.4286\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4290 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44042 to 20.22399, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 20.4290 - mae: 4.4241 - val_loss: 20.2240 - val_mae: 4.4040\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2129 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22399 to 20.00860, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 20.2129 - mae: 4.3996 - val_loss: 20.0086 - val_mae: 4.3795\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9978 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00860 to 19.79466, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 125ms/step - loss: 19.9978 - mae: 4.3751 - val_loss: 19.7947 - val_mae: 4.3550\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7841 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79466 to 19.58184, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 19.7841 - mae: 4.3506 - val_loss: 19.5818 - val_mae: 4.3305\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5715 - mae: 4.3261\n",
            "Epoch 70: val_loss improved from 19.58184 to 19.37037, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 19.5715 - mae: 4.3261 - val_loss: 19.3704 - val_mae: 4.3060\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3603 - mae: 4.3016\n",
            "Epoch 71: val_loss improved from 19.37037 to 19.15995, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 127ms/step - loss: 19.3603 - mae: 4.3016 - val_loss: 19.1599 - val_mae: 4.2815\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1501 - mae: 4.2771\n",
            "Epoch 72: val_loss improved from 19.15995 to 18.95100, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 19.1501 - mae: 4.2771 - val_loss: 18.9510 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9413 - mae: 4.2526\n",
            "Epoch 73: val_loss improved from 18.95100 to 18.74300, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 18.9413 - mae: 4.2526 - val_loss: 18.7430 - val_mae: 4.2326\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7336 - mae: 4.2281\n",
            "Epoch 74: val_loss improved from 18.74300 to 18.53633, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 18.7336 - mae: 4.2281 - val_loss: 18.5363 - val_mae: 4.2081\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5272 - mae: 4.2036\n",
            "Epoch 75: val_loss improved from 18.53633 to 18.33089, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 18.5272 - mae: 4.2036 - val_loss: 18.3309 - val_mae: 4.1836\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3220 - mae: 4.1791\n",
            "Epoch 76: val_loss improved from 18.33089 to 18.12677, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 18.3220 - mae: 4.1791 - val_loss: 18.1268 - val_mae: 4.1591\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1180 - mae: 4.1547\n",
            "Epoch 77: val_loss improved from 18.12677 to 17.92384, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 18.1180 - mae: 4.1547 - val_loss: 17.9238 - val_mae: 4.1347\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9152 - mae: 4.1302\n",
            "Epoch 78: val_loss improved from 17.92384 to 17.72208, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 127ms/step - loss: 17.9152 - mae: 4.1302 - val_loss: 17.7221 - val_mae: 4.1102\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7137 - mae: 4.1057\n",
            "Epoch 79: val_loss improved from 17.72208 to 17.52145, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 124ms/step - loss: 17.7137 - mae: 4.1057 - val_loss: 17.5215 - val_mae: 4.0857\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5134 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52145 to 17.32214, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 17.5134 - mae: 4.0813 - val_loss: 17.3221 - val_mae: 4.0613\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3142 - mae: 4.0568\n",
            "Epoch 81: val_loss improved from 17.32214 to 17.12423, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 17.3142 - mae: 4.0568 - val_loss: 17.1242 - val_mae: 4.0368\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1165 - mae: 4.0323\n",
            "Epoch 82: val_loss improved from 17.12423 to 16.92713, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 17.1165 - mae: 4.0323 - val_loss: 16.9271 - val_mae: 4.0123\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9198 - mae: 4.0079\n",
            "Epoch 83: val_loss improved from 16.92713 to 16.73151, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 16.9198 - mae: 4.0079 - val_loss: 16.7315 - val_mae: 3.9879\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7244 - mae: 3.9834\n",
            "Epoch 84: val_loss improved from 16.73151 to 16.53713, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 16.7244 - mae: 3.9834 - val_loss: 16.5371 - val_mae: 3.9634\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5303 - mae: 3.9590\n",
            "Epoch 85: val_loss improved from 16.53713 to 16.34386, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 129ms/step - loss: 16.5303 - mae: 3.9590 - val_loss: 16.3439 - val_mae: 3.9390\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3372 - mae: 3.9345\n",
            "Epoch 86: val_loss improved from 16.34386 to 16.15197, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 125ms/step - loss: 16.3372 - mae: 3.9345 - val_loss: 16.1520 - val_mae: 3.9145\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1455 - mae: 3.9101\n",
            "Epoch 87: val_loss improved from 16.15197 to 15.96119, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 16.1455 - mae: 3.9101 - val_loss: 15.9612 - val_mae: 3.8901\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9549 - mae: 3.8856\n",
            "Epoch 88: val_loss improved from 15.96119 to 15.77164, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 133ms/step - loss: 15.9549 - mae: 3.8856 - val_loss: 15.7716 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7656 - mae: 3.8612\n",
            "Epoch 89: val_loss improved from 15.77164 to 15.58334, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 15.7656 - mae: 3.8612 - val_loss: 15.5833 - val_mae: 3.8412\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5775 - mae: 3.8368\n",
            "Epoch 90: val_loss improved from 15.58334 to 15.39631, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 15.5775 - mae: 3.8368 - val_loss: 15.3963 - val_mae: 3.8168\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3907 - mae: 3.8123\n",
            "Epoch 91: val_loss improved from 15.39631 to 15.21025, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 111ms/step - loss: 15.3907 - mae: 3.8123 - val_loss: 15.2102 - val_mae: 3.7923\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2049 - mae: 3.7879\n",
            "Epoch 92: val_loss improved from 15.21025 to 15.02579, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 15.2049 - mae: 3.7879 - val_loss: 15.0258 - val_mae: 3.7680\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0207 - mae: 3.7635\n",
            "Epoch 93: val_loss improved from 15.02579 to 14.84212, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 15.0207 - mae: 3.7635 - val_loss: 14.8421 - val_mae: 3.7435\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8373 - mae: 3.7391\n",
            "Epoch 94: val_loss improved from 14.84212 to 14.66003, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 133ms/step - loss: 14.8373 - mae: 3.7391 - val_loss: 14.6600 - val_mae: 3.7191\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6553 - mae: 3.7147\n",
            "Epoch 95: val_loss improved from 14.66003 to 14.47905, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 14.6553 - mae: 3.7147 - val_loss: 14.4790 - val_mae: 3.6947\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4746 - mae: 3.6902\n",
            "Epoch 96: val_loss improved from 14.47905 to 14.29919, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 14.4746 - mae: 3.6902 - val_loss: 14.2992 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2950 - mae: 3.6658\n",
            "Epoch 97: val_loss improved from 14.29919 to 14.12063, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 14.2950 - mae: 3.6658 - val_loss: 14.1206 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1168 - mae: 3.6414\n",
            "Epoch 98: val_loss improved from 14.12063 to 13.94328, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 14.1168 - mae: 3.6414 - val_loss: 13.9433 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9396 - mae: 3.6170\n",
            "Epoch 99: val_loss improved from 13.94328 to 13.76722, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 13.9396 - mae: 3.6170 - val_loss: 13.7672 - val_mae: 3.5971\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7638 - mae: 3.5926\n",
            "Epoch 100: val_loss improved from 13.76722 to 13.59229, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 13.7638 - mae: 3.5926 - val_loss: 13.5923 - val_mae: 3.5727\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 6ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 24ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.74702262878418, RMSE:3.707697868347168, MAE:3.5902609825134277, R2:-15.04001659674605\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_208\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_137 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_152 (Embedding)      (None, 17, 900)      649800      ['input_137[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_153 (Embedding)      (None, 17, 900)      649800      ['input_137[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_154 (Embedding)      (None, 17, 900)      649800      ['input_137[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_155 (Embedding)      (None, 17, 900)      649800      ['input_137[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_204 (Conv1D)            (None, 17, 100)      270100      ['embedding_152[0][0]',          \n",
            "                                                                  'embedding_153[0][0]',          \n",
            "                                                                  'embedding_154[0][0]',          \n",
            "                                                                  'embedding_155[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_205 (Conv1D)            (None, 17, 100)      270100      ['embedding_152[0][0]',          \n",
            "                                                                  'embedding_153[0][0]',          \n",
            "                                                                  'embedding_154[0][0]',          \n",
            "                                                                  'embedding_155[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_206 (Conv1D)            (None, 17, 100)      270100      ['embedding_152[0][0]',          \n",
            "                                                                  'embedding_153[0][0]',          \n",
            "                                                                  'embedding_154[0][0]',          \n",
            "                                                                  'embedding_155[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_456 (Glob  (None, 100)         0           ['conv1d_204[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_457 (Glob  (None, 100)         0           ['conv1d_204[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_458 (Glob  (None, 100)         0           ['conv1d_204[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_459 (Glob  (None, 100)         0           ['conv1d_204[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_460 (Glob  (None, 100)         0           ['conv1d_205[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_461 (Glob  (None, 100)         0           ['conv1d_205[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_462 (Glob  (None, 100)         0           ['conv1d_205[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_463 (Glob  (None, 100)         0           ['conv1d_205[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_464 (Glob  (None, 100)         0           ['conv1d_206[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_465 (Glob  (None, 100)         0           ['conv1d_206[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_466 (Glob  (None, 100)         0           ['conv1d_206[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_467 (Glob  (None, 100)         0           ['conv1d_206[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_138 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_84 (Add)                   (None, 100)          0           ['global_max_pooling1d_456[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_457[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_458[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_459[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_85 (Add)                   (None, 100)          0           ['global_max_pooling1d_460[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_461[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_462[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_463[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_86 (Add)                   (None, 100)          0           ['global_max_pooling1d_464[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_465[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_466[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_467[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_276 (Dense)              (None, 1000)         11000       ['input_138[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_68 (Concatenate)   (None, 1300)         0           ['add_84[0][0]',                 \n",
            "                                                                  'add_85[0][0]',                 \n",
            "                                                                  'add_86[0][0]',                 \n",
            "                                                                  'dense_276[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_68[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_240 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_240[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_241 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_241[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_242 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_242[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_243 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_243[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_244 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_277 (Dense)              (None, 1)            3           ['dropout_244[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.8117 - mae: 1.1760\n",
            "Epoch 1: val_loss improved from inf to 0.67207, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 120ms/step - loss: 2.8117 - mae: 1.1760 - val_loss: 0.6721 - val_mae: 0.5354\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7161 - mae: 0.5204\n",
            "Epoch 2: val_loss improved from 0.67207 to 0.56187, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.7161 - mae: 0.5204 - val_loss: 0.5619 - val_mae: 0.3877\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5838 - mae: 0.4345\n",
            "Epoch 3: val_loss improved from 0.56187 to 0.45696, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 0.5838 - mae: 0.4345 - val_loss: 0.4570 - val_mae: 0.3507\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5067 - mae: 0.4100\n",
            "Epoch 4: val_loss improved from 0.45696 to 0.44533, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.5067 - mae: 0.4100 - val_loss: 0.4453 - val_mae: 0.3079\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4623 - mae: 0.3857\n",
            "Epoch 5: val_loss improved from 0.44533 to 0.40427, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.4623 - mae: 0.3857 - val_loss: 0.4043 - val_mae: 0.3144\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4503 - mae: 0.4042\n",
            "Epoch 6: val_loss did not improve from 0.40427\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4503 - mae: 0.4042 - val_loss: 0.4096 - val_mae: 0.3462\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4051 - mae: 0.3565\n",
            "Epoch 7: val_loss did not improve from 0.40427\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.4051 - mae: 0.3565 - val_loss: 0.4155 - val_mae: 0.4622\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4961 - mae: 0.4788\n",
            "Epoch 8: val_loss improved from 0.40427 to 0.36261, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.4961 - mae: 0.4788 - val_loss: 0.3626 - val_mae: 0.2850\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4173 - mae: 0.3925\n",
            "Epoch 9: val_loss did not improve from 0.36261\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.4173 - mae: 0.3925 - val_loss: 0.3634 - val_mae: 0.3055\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4184 - mae: 0.3901\n",
            "Epoch 10: val_loss did not improve from 0.36261\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.4184 - mae: 0.3901 - val_loss: 0.3715 - val_mae: 0.3913\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3925 - mae: 0.3705\n",
            "Epoch 11: val_loss did not improve from 0.36261\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3925 - mae: 0.3705 - val_loss: 0.3634 - val_mae: 0.3660\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4239 - mae: 0.4124\n",
            "Epoch 12: val_loss improved from 0.36261 to 0.35222, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.4239 - mae: 0.4124 - val_loss: 0.3522 - val_mae: 0.2908\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3891 - mae: 0.3627\n",
            "Epoch 13: val_loss did not improve from 0.35222\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3891 - mae: 0.3627 - val_loss: 0.4331 - val_mae: 0.4086\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3646 - mae: 0.3317\n",
            "Epoch 14: val_loss improved from 0.35222 to 0.34587, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 0.3646 - mae: 0.3317 - val_loss: 0.3459 - val_mae: 0.3585\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3525 - mae: 0.3199\n",
            "Epoch 15: val_loss did not improve from 0.34587\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3525 - mae: 0.3199 - val_loss: 0.3742 - val_mae: 0.3392\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3553 - mae: 0.3433\n",
            "Epoch 16: val_loss improved from 0.34587 to 0.32285, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3553 - mae: 0.3433 - val_loss: 0.3229 - val_mae: 0.2796\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3911 - mae: 0.3990\n",
            "Epoch 17: val_loss did not improve from 0.32285\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3911 - mae: 0.3990 - val_loss: 0.4946 - val_mae: 0.5878\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4035 - mae: 0.4119\n",
            "Epoch 18: val_loss did not improve from 0.32285\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.4035 - mae: 0.4119 - val_loss: 0.3724 - val_mae: 0.4087\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3369 - mae: 0.3188\n",
            "Epoch 19: val_loss did not improve from 0.32285\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3369 - mae: 0.3188 - val_loss: 0.3411 - val_mae: 0.4003\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3348 - mae: 0.3352\n",
            "Epoch 20: val_loss did not improve from 0.32285\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.3348 - mae: 0.3352 - val_loss: 0.3624 - val_mae: 0.4430\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3362 - mae: 0.3404\n",
            "Epoch 21: val_loss did not improve from 0.32285\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3362 - mae: 0.3404 - val_loss: 0.4306 - val_mae: 0.5189\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3496 - mae: 0.3724\n",
            "Epoch 22: val_loss did not improve from 0.32285\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3496 - mae: 0.3724 - val_loss: 0.3668 - val_mae: 0.3820\n",
            "Epoch 22: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 23ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.3369697034358978, RMSE:0.5804908871650696, MAE:0.2974435091018677, R2:0.606824009099239\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_214\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_139 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_156 (Embedding)      (None, 17, 900)      649800      ['input_139[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_157 (Embedding)      (None, 17, 900)      649800      ['input_139[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_158 (Embedding)      (None, 17, 900)      649800      ['input_139[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_159 (Embedding)      (None, 17, 900)      649800      ['input_139[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_207 (Conv1D)            (None, 17, 100)      270100      ['embedding_156[0][0]',          \n",
            "                                                                  'embedding_157[0][0]',          \n",
            "                                                                  'embedding_158[0][0]',          \n",
            "                                                                  'embedding_159[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_208 (Conv1D)            (None, 17, 100)      270100      ['embedding_156[0][0]',          \n",
            "                                                                  'embedding_157[0][0]',          \n",
            "                                                                  'embedding_158[0][0]',          \n",
            "                                                                  'embedding_159[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_209 (Conv1D)            (None, 17, 100)      270100      ['embedding_156[0][0]',          \n",
            "                                                                  'embedding_157[0][0]',          \n",
            "                                                                  'embedding_158[0][0]',          \n",
            "                                                                  'embedding_159[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_468 (Glob  (None, 100)         0           ['conv1d_207[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_469 (Glob  (None, 100)         0           ['conv1d_207[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_470 (Glob  (None, 100)         0           ['conv1d_207[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_471 (Glob  (None, 100)         0           ['conv1d_207[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_472 (Glob  (None, 100)         0           ['conv1d_208[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_473 (Glob  (None, 100)         0           ['conv1d_208[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_474 (Glob  (None, 100)         0           ['conv1d_208[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_475 (Glob  (None, 100)         0           ['conv1d_208[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_476 (Glob  (None, 100)         0           ['conv1d_209[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_477 (Glob  (None, 100)         0           ['conv1d_209[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_478 (Glob  (None, 100)         0           ['conv1d_209[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_479 (Glob  (None, 100)         0           ['conv1d_209[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_140 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_87 (Add)                   (None, 100)          0           ['global_max_pooling1d_468[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_469[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_470[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_471[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_88 (Add)                   (None, 100)          0           ['global_max_pooling1d_472[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_473[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_474[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_475[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_89 (Add)                   (None, 100)          0           ['global_max_pooling1d_476[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_477[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_478[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_479[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_278 (Dense)              (None, 1000)         11000       ['input_140[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_69 (Concatenate)   (None, 1300)         0           ['add_87[0][0]',                 \n",
            "                                                                  'add_88[0][0]',                 \n",
            "                                                                  'add_89[0][0]',                 \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                  'dense_278[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_69[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_245 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_245[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_246 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_246[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_247 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_247[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_248 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_248[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_249 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_279 (Dense)              (None, 1)            3           ['dropout_249[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.6338 - mae: 1.2324\n",
            "Epoch 1: val_loss improved from inf to 0.77563, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 7s 120ms/step - loss: 3.6338 - mae: 1.2324 - val_loss: 0.7756 - val_mae: 0.4272\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6547 - mae: 0.4803\n",
            "Epoch 2: val_loss improved from 0.77563 to 0.48258, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.6547 - mae: 0.4803 - val_loss: 0.4826 - val_mae: 0.3173\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5016 - mae: 0.4071\n",
            "Epoch 3: val_loss improved from 0.48258 to 0.46492, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.5016 - mae: 0.4071 - val_loss: 0.4649 - val_mae: 0.3684\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4659 - mae: 0.4115\n",
            "Epoch 4: val_loss improved from 0.46492 to 0.38227, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.4659 - mae: 0.4115 - val_loss: 0.3823 - val_mae: 0.3410\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4077 - mae: 0.3479\n",
            "Epoch 5: val_loss improved from 0.38227 to 0.36576, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 0.4077 - mae: 0.3479 - val_loss: 0.3658 - val_mae: 0.3030\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4043 - mae: 0.3550\n",
            "Epoch 6: val_loss did not improve from 0.36576\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4043 - mae: 0.3550 - val_loss: 0.3725 - val_mae: 0.3654\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3957 - mae: 0.3549\n",
            "Epoch 7: val_loss did not improve from 0.36576\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.3957 - mae: 0.3549 - val_loss: 0.3989 - val_mae: 0.3329\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4488 - mae: 0.4281\n",
            "Epoch 8: val_loss improved from 0.36576 to 0.36245, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.4488 - mae: 0.4281 - val_loss: 0.3625 - val_mae: 0.2832\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4044 - mae: 0.3775\n",
            "Epoch 9: val_loss did not improve from 0.36245\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.4044 - mae: 0.3775 - val_loss: 0.3634 - val_mae: 0.3555\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4033 - mae: 0.3784\n",
            "Epoch 10: val_loss did not improve from 0.36245\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.4033 - mae: 0.3784 - val_loss: 0.3986 - val_mae: 0.3567\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3876 - mae: 0.3595\n",
            "Epoch 11: val_loss did not improve from 0.36245\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3876 - mae: 0.3595 - val_loss: 0.3750 - val_mae: 0.4003\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3738 - mae: 0.3390\n",
            "Epoch 12: val_loss improved from 0.36245 to 0.33868, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 0.3738 - mae: 0.3390 - val_loss: 0.3387 - val_mae: 0.2864\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3656 - mae: 0.3328\n",
            "Epoch 13: val_loss improved from 0.33868 to 0.33782, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3656 - mae: 0.3328 - val_loss: 0.3378 - val_mae: 0.2746\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3607 - mae: 0.3352\n",
            "Epoch 14: val_loss did not improve from 0.33782\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.3607 - mae: 0.3352 - val_loss: 0.3538 - val_mae: 0.3384\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3561 - mae: 0.3413\n",
            "Epoch 15: val_loss did not improve from 0.33782\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.3561 - mae: 0.3413 - val_loss: 0.3396 - val_mae: 0.2843\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3611 - mae: 0.3510\n",
            "Epoch 16: val_loss improved from 0.33782 to 0.31614, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3611 - mae: 0.3510 - val_loss: 0.3161 - val_mae: 0.2753\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3454 - mae: 0.3434\n",
            "Epoch 17: val_loss did not improve from 0.31614\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3454 - mae: 0.3434 - val_loss: 0.3181 - val_mae: 0.2869\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3353 - mae: 0.3363\n",
            "Epoch 18: val_loss did not improve from 0.31614\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3353 - mae: 0.3363 - val_loss: 0.3317 - val_mae: 0.3939\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3251 - mae: 0.3322\n",
            "Epoch 19: val_loss improved from 0.31614 to 0.29299, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3251 - mae: 0.3322 - val_loss: 0.2930 - val_mae: 0.2839\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3220 - mae: 0.3442\n",
            "Epoch 20: val_loss improved from 0.29299 to 0.28788, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3220 - mae: 0.3442 - val_loss: 0.2879 - val_mae: 0.2874\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3089 - mae: 0.3281\n",
            "Epoch 21: val_loss did not improve from 0.28788\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.3089 - mae: 0.3281 - val_loss: 0.3467 - val_mae: 0.3805\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4003 - mae: 0.4264\n",
            "Epoch 22: val_loss did not improve from 0.28788\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4003 - mae: 0.4264 - val_loss: 0.4086 - val_mae: 0.4215\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3433 - mae: 0.3765\n",
            "Epoch 23: val_loss improved from 0.28788 to 0.26742, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3433 - mae: 0.3765 - val_loss: 0.2674 - val_mae: 0.3016\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2758 - mae: 0.2984\n",
            "Epoch 24: val_loss improved from 0.26742 to 0.23988, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.2758 - mae: 0.2984 - val_loss: 0.2399 - val_mae: 0.2329\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2602 - mae: 0.2980\n",
            "Epoch 25: val_loss did not improve from 0.23988\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.2602 - mae: 0.2980 - val_loss: 0.2636 - val_mae: 0.3590\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2537 - mae: 0.3047\n",
            "Epoch 26: val_loss did not improve from 0.23988\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2537 - mae: 0.3047 - val_loss: 0.2629 - val_mae: 0.3575\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2356 - mae: 0.2884\n",
            "Epoch 27: val_loss did not improve from 0.23988\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.2356 - mae: 0.2884 - val_loss: 0.3069 - val_mae: 0.4584\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2125 - mae: 0.2717\n",
            "Epoch 28: val_loss did not improve from 0.23988\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2125 - mae: 0.2717 - val_loss: 0.3579 - val_mae: 0.4136\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2716 - mae: 0.3488\n",
            "Epoch 29: val_loss improved from 0.23988 to 0.18753, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 0.2716 - mae: 0.3488 - val_loss: 0.1875 - val_mae: 0.2561\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1813 - mae: 0.2560\n",
            "Epoch 30: val_loss improved from 0.18753 to 0.15135, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1813 - mae: 0.2560 - val_loss: 0.1513 - val_mae: 0.1810\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1534 - mae: 0.2148\n",
            "Epoch 31: val_loss did not improve from 0.15135\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.1534 - mae: 0.2148 - val_loss: 0.1889 - val_mae: 0.3257\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2691 - mae: 0.3472\n",
            "Epoch 32: val_loss did not improve from 0.15135\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2691 - mae: 0.3472 - val_loss: 0.1547 - val_mae: 0.2563\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2378 - mae: 0.3373\n",
            "Epoch 33: val_loss did not improve from 0.15135\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.2378 - mae: 0.3373 - val_loss: 0.1514 - val_mae: 0.2106\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1498 - mae: 0.2255\n",
            "Epoch 34: val_loss improved from 0.15135 to 0.12987, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.1498 - mae: 0.2255 - val_loss: 0.1299 - val_mae: 0.1564\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1056 - mae: 0.1809\n",
            "Epoch 35: val_loss improved from 0.12987 to 0.11642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.1056 - mae: 0.1809 - val_loss: 0.1164 - val_mae: 0.1901\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1049 - mae: 0.1900\n",
            "Epoch 36: val_loss improved from 0.11642 to 0.10448, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.1049 - mae: 0.1900 - val_loss: 0.1045 - val_mae: 0.1398\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1094 - mae: 0.2078\n",
            "Epoch 37: val_loss did not improve from 0.10448\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.1094 - mae: 0.2078 - val_loss: 0.1124 - val_mae: 0.2469\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2354 - mae: 0.3043\n",
            "Epoch 38: val_loss did not improve from 0.10448\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2354 - mae: 0.3043 - val_loss: 0.2496 - val_mae: 0.3418\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1838 - mae: 0.2746\n",
            "Epoch 39: val_loss did not improve from 0.10448\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.1838 - mae: 0.2746 - val_loss: 0.1171 - val_mae: 0.1629\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1581 - mae: 0.2720\n",
            "Epoch 40: val_loss did not improve from 0.10448\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1581 - mae: 0.2720 - val_loss: 0.1966 - val_mae: 0.3656\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1644 - mae: 0.2920\n",
            "Epoch 41: val_loss did not improve from 0.10448\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.1644 - mae: 0.2920 - val_loss: 0.1244 - val_mae: 0.2330\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1239 - mae: 0.2248\n",
            "Epoch 42: val_loss did not improve from 0.10448\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.1239 - mae: 0.2248 - val_loss: 0.1489 - val_mae: 0.1584\n",
            "Epoch 42: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.10046449303627014, RMSE:0.3169613480567932, MAE:0.14720970392227173, R2:0.8827780960464037\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_220\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_141 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_160 (Embedding)      (None, 17, 900)      649800      ['input_141[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_161 (Embedding)      (None, 17, 900)      649800      ['input_141[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_162 (Embedding)      (None, 17, 900)      649800      ['input_141[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_163 (Embedding)      (None, 17, 900)      649800      ['input_141[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_210 (Conv1D)            (None, 17, 100)      270100      ['embedding_160[0][0]',          \n",
            "                                                                  'embedding_161[0][0]',          \n",
            "                                                                  'embedding_162[0][0]',          \n",
            "                                                                  'embedding_163[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_211 (Conv1D)            (None, 17, 100)      270100      ['embedding_160[0][0]',          \n",
            "                                                                  'embedding_161[0][0]',          \n",
            "                                                                  'embedding_162[0][0]',          \n",
            "                                                                  'embedding_163[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_212 (Conv1D)            (None, 17, 100)      270100      ['embedding_160[0][0]',          \n",
            "                                                                  'embedding_161[0][0]',          \n",
            "                                                                  'embedding_162[0][0]',          \n",
            "                                                                  'embedding_163[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_480 (Glob  (None, 100)         0           ['conv1d_210[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_481 (Glob  (None, 100)         0           ['conv1d_210[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_482 (Glob  (None, 100)         0           ['conv1d_210[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_483 (Glob  (None, 100)         0           ['conv1d_210[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_484 (Glob  (None, 100)         0           ['conv1d_211[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_485 (Glob  (None, 100)         0           ['conv1d_211[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_486 (Glob  (None, 100)         0           ['conv1d_211[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_487 (Glob  (None, 100)         0           ['conv1d_211[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_488 (Glob  (None, 100)         0           ['conv1d_212[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_489 (Glob  (None, 100)         0           ['conv1d_212[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_490 (Glob  (None, 100)         0           ['conv1d_212[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_491 (Glob  (None, 100)         0           ['conv1d_212[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_142 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_90 (Add)                   (None, 100)          0           ['global_max_pooling1d_480[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_481[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_482[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_483[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_91 (Add)                   (None, 100)          0           ['global_max_pooling1d_484[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_485[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_486[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_487[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_92 (Add)                   (None, 100)          0           ['global_max_pooling1d_488[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_489[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_490[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_491[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_280 (Dense)              (None, 1000)         11000       ['input_142[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_70 (Concatenate)   (None, 1300)         0           ['add_90[0][0]',                 \n",
            "                                                                  'add_91[0][0]',                 \n",
            "                                                                  'add_92[0][0]',                 \n",
            "                                                                  'dense_280[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_70[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_250 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_250[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_251 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_251[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_252 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_252[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_253 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_253[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_254 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_281 (Dense)              (None, 1)            3           ['dropout_254[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.1455 - mae: 6.0240\n",
            "Epoch 1: val_loss improved from inf to 36.87311, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 118ms/step - loss: 37.1455 - mae: 6.0240 - val_loss: 36.8731 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8451 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87311 to 36.57419, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.8451 - mae: 5.9990 - val_loss: 36.5742 - val_mae: 5.9788\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5466 - mae: 5.9741\n",
            "Epoch 3: val_loss improved from 36.57419 to 36.27676, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 36.5466 - mae: 5.9741 - val_loss: 36.2768 - val_mae: 5.9539\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2496 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27676 to 35.98111, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 36.2496 - mae: 5.9492 - val_loss: 35.9811 - val_mae: 5.9290\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9543 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98111 to 35.68708, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 35.9543 - mae: 5.9243 - val_loss: 35.6871 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6607 - mae: 5.8995\n",
            "Epoch 6: val_loss improved from 35.68708 to 35.39456, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 35.6607 - mae: 5.8995 - val_loss: 35.3946 - val_mae: 5.8793\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3687 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39456 to 35.10368, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 35.3687 - mae: 5.8747 - val_loss: 35.1037 - val_mae: 5.8545\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0782 - mae: 5.8499\n",
            "Epoch 8: val_loss improved from 35.10368 to 34.81441, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 35.0782 - mae: 5.8499 - val_loss: 34.8144 - val_mae: 5.8298\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7892 - mae: 5.8252\n",
            "Epoch 9: val_loss improved from 34.81441 to 34.52670, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 34.7892 - mae: 5.8252 - val_loss: 34.5267 - val_mae: 5.8050\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5018 - mae: 5.8004\n",
            "Epoch 10: val_loss improved from 34.52670 to 34.24023, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.5018 - mae: 5.8004 - val_loss: 34.2402 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2156 - mae: 5.7757\n",
            "Epoch 11: val_loss improved from 34.24023 to 33.95536, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.2156 - mae: 5.7757 - val_loss: 33.9554 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9310 - mae: 5.7510\n",
            "Epoch 12: val_loss improved from 33.95536 to 33.67166, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.9310 - mae: 5.7510 - val_loss: 33.6717 - val_mae: 5.7309\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6478 - mae: 5.7263\n",
            "Epoch 13: val_loss improved from 33.67166 to 33.38938, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.6478 - mae: 5.7263 - val_loss: 33.3894 - val_mae: 5.7062\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3658 - mae: 5.7017\n",
            "Epoch 14: val_loss improved from 33.38938 to 33.10863, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.3658 - mae: 5.7017 - val_loss: 33.1086 - val_mae: 5.6816\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0853 - mae: 5.6770\n",
            "Epoch 15: val_loss improved from 33.10863 to 32.82895, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.0853 - mae: 5.6770 - val_loss: 32.8289 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8060 - mae: 5.6524\n",
            "Epoch 16: val_loss improved from 32.82895 to 32.55079, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 32.8060 - mae: 5.6524 - val_loss: 32.5508 - val_mae: 5.6323\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5281 - mae: 5.6277\n",
            "Epoch 17: val_loss improved from 32.55079 to 32.27385, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 32.5281 - mae: 5.6277 - val_loss: 32.2738 - val_mae: 5.6076\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2513 - mae: 5.6031\n",
            "Epoch 18: val_loss improved from 32.27385 to 31.99836, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 32.2513 - mae: 5.6031 - val_loss: 31.9984 - val_mae: 5.5830\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9760 - mae: 5.5785\n",
            "Epoch 19: val_loss improved from 31.99836 to 31.72394, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.9760 - mae: 5.5785 - val_loss: 31.7239 - val_mae: 5.5584\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7019 - mae: 5.5538\n",
            "Epoch 20: val_loss improved from 31.72394 to 31.45085, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.7019 - mae: 5.5538 - val_loss: 31.4508 - val_mae: 5.5338\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4290 - mae: 5.5292\n",
            "Epoch 21: val_loss improved from 31.45085 to 31.17912, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.4290 - mae: 5.5292 - val_loss: 31.1791 - val_mae: 5.5092\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1575 - mae: 5.5046\n",
            "Epoch 22: val_loss improved from 31.17912 to 30.90857, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 31.1575 - mae: 5.5046 - val_loss: 30.9086 - val_mae: 5.4845\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8872 - mae: 5.4800\n",
            "Epoch 23: val_loss improved from 30.90857 to 30.63912, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 30.8872 - mae: 5.4800 - val_loss: 30.6391 - val_mae: 5.4599\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6181 - mae: 5.4554\n",
            "Epoch 24: val_loss improved from 30.63912 to 30.37110, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 30.6181 - mae: 5.4554 - val_loss: 30.3711 - val_mae: 5.4353\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3503 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37110 to 30.10427, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.3503 - mae: 5.4308 - val_loss: 30.1043 - val_mae: 5.4107\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0837 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10427 to 29.83870, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 30.0837 - mae: 5.4062 - val_loss: 29.8387 - val_mae: 5.3861\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8183 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83870 to 29.57451, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.8183 - mae: 5.3816 - val_loss: 29.5745 - val_mae: 5.3615\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5542 - mae: 5.3570\n",
            "Epoch 28: val_loss improved from 29.57451 to 29.31134, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.5542 - mae: 5.3570 - val_loss: 29.3113 - val_mae: 5.3369\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2913 - mae: 5.3324\n",
            "Epoch 29: val_loss improved from 29.31134 to 29.04953, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 29.2913 - mae: 5.3324 - val_loss: 29.0495 - val_mae: 5.3124\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0298 - mae: 5.3078\n",
            "Epoch 30: val_loss improved from 29.04953 to 28.78868, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.0298 - mae: 5.3078 - val_loss: 28.7887 - val_mae: 5.2878\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7694 - mae: 5.2832\n",
            "Epoch 31: val_loss improved from 28.78868 to 28.52920, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 28.7694 - mae: 5.2832 - val_loss: 28.5292 - val_mae: 5.2632\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5101 - mae: 5.2587\n",
            "Epoch 32: val_loss improved from 28.52920 to 28.27124, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 28.5101 - mae: 5.2587 - val_loss: 28.2712 - val_mae: 5.2386\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2523 - mae: 5.2341\n",
            "Epoch 33: val_loss improved from 28.27124 to 28.01424, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 28.2523 - mae: 5.2341 - val_loss: 28.0142 - val_mae: 5.2140\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9955 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01424 to 27.75856, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 27.9955 - mae: 5.2095 - val_loss: 27.7586 - val_mae: 5.1894\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7401 - mae: 5.1849\n",
            "Epoch 35: val_loss improved from 27.75856 to 27.50410, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.7401 - mae: 5.1849 - val_loss: 27.5041 - val_mae: 5.1649\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4858 - mae: 5.1603\n",
            "Epoch 36: val_loss improved from 27.50410 to 27.25082, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 27.4858 - mae: 5.1603 - val_loss: 27.2508 - val_mae: 5.1403\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2328 - mae: 5.1358\n",
            "Epoch 37: val_loss improved from 27.25082 to 26.99881, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 27.2328 - mae: 5.1358 - val_loss: 26.9988 - val_mae: 5.1157\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9810 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99881 to 26.74804, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.9810 - mae: 5.1112 - val_loss: 26.7480 - val_mae: 5.0911\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7305 - mae: 5.0866\n",
            "Epoch 39: val_loss improved from 26.74804 to 26.49845, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.7305 - mae: 5.0866 - val_loss: 26.4985 - val_mae: 5.0666\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4812 - mae: 5.0621\n",
            "Epoch 40: val_loss improved from 26.49845 to 26.24996, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 26.4812 - mae: 5.0621 - val_loss: 26.2500 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2330 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.24996 to 26.00308, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.2330 - mae: 5.0375 - val_loss: 26.0031 - val_mae: 5.0174\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9862 - mae: 5.0129\n",
            "Epoch 42: val_loss improved from 26.00308 to 25.75716, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.9862 - mae: 5.0129 - val_loss: 25.7572 - val_mae: 4.9929\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7405 - mae: 4.9884\n",
            "Epoch 43: val_loss improved from 25.75716 to 25.51262, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.7405 - mae: 4.9884 - val_loss: 25.5126 - val_mae: 4.9683\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4961 - mae: 4.9638\n",
            "Epoch 44: val_loss improved from 25.51262 to 25.26912, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.4961 - mae: 4.9638 - val_loss: 25.2691 - val_mae: 4.9438\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2530 - mae: 4.9392\n",
            "Epoch 45: val_loss improved from 25.26912 to 25.02679, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.2530 - mae: 4.9392 - val_loss: 25.0268 - val_mae: 4.9192\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0110 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02679 to 24.78591, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 25.0110 - mae: 4.9147 - val_loss: 24.7859 - val_mae: 4.8946\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7702 - mae: 4.8901\n",
            "Epoch 47: val_loss improved from 24.78591 to 24.54621, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.7702 - mae: 4.8901 - val_loss: 24.5462 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5308 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54621 to 24.30766, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 24.5308 - mae: 4.8656 - val_loss: 24.3077 - val_mae: 4.8455\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2924 - mae: 4.8411\n",
            "Epoch 49: val_loss improved from 24.30766 to 24.07038, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.2924 - mae: 4.8411 - val_loss: 24.0704 - val_mae: 4.8210\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0554 - mae: 4.8165\n",
            "Epoch 50: val_loss improved from 24.07038 to 23.83424, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 24.0554 - mae: 4.8165 - val_loss: 23.8342 - val_mae: 4.7964\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8195 - mae: 4.7920\n",
            "Epoch 51: val_loss improved from 23.83424 to 23.59952, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 23.8195 - mae: 4.7920 - val_loss: 23.5995 - val_mae: 4.7719\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5849 - mae: 4.7674\n",
            "Epoch 52: val_loss improved from 23.59952 to 23.36600, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 23.5849 - mae: 4.7674 - val_loss: 23.3660 - val_mae: 4.7474\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3516 - mae: 4.7429\n",
            "Epoch 53: val_loss improved from 23.36600 to 23.13355, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 23.3516 - mae: 4.7429 - val_loss: 23.1336 - val_mae: 4.7228\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1194 - mae: 4.7184\n",
            "Epoch 54: val_loss improved from 23.13355 to 22.90245, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 23.1194 - mae: 4.7184 - val_loss: 22.9025 - val_mae: 4.6983\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8886 - mae: 4.6938\n",
            "Epoch 55: val_loss improved from 22.90245 to 22.67234, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 22.8886 - mae: 4.6938 - val_loss: 22.6723 - val_mae: 4.6738\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6588 - mae: 4.6693\n",
            "Epoch 56: val_loss improved from 22.67234 to 22.44377, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 22.6588 - mae: 4.6693 - val_loss: 22.4438 - val_mae: 4.6492\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4305 - mae: 4.6447\n",
            "Epoch 57: val_loss improved from 22.44377 to 22.21618, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.4305 - mae: 4.6447 - val_loss: 22.2162 - val_mae: 4.6247\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2032 - mae: 4.6202\n",
            "Epoch 58: val_loss improved from 22.21618 to 21.98997, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 22.2032 - mae: 4.6202 - val_loss: 21.9900 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9771 - mae: 4.5957\n",
            "Epoch 59: val_loss improved from 21.98997 to 21.76520, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.9771 - mae: 4.5957 - val_loss: 21.7652 - val_mae: 4.5757\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7524 - mae: 4.5712\n",
            "Epoch 60: val_loss improved from 21.76520 to 21.54123, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 21.7524 - mae: 4.5712 - val_loss: 21.5412 - val_mae: 4.5511\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5288 - mae: 4.5467\n",
            "Epoch 61: val_loss improved from 21.54123 to 21.31874, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 21.5288 - mae: 4.5467 - val_loss: 21.3187 - val_mae: 4.5266\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3064 - mae: 4.5221\n",
            "Epoch 62: val_loss improved from 21.31874 to 21.09741, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 21.3064 - mae: 4.5221 - val_loss: 21.0974 - val_mae: 4.5021\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0854 - mae: 4.4976\n",
            "Epoch 63: val_loss improved from 21.09741 to 20.87709, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.0854 - mae: 4.4976 - val_loss: 20.8771 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8654 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87709 to 20.65830, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 20.8654 - mae: 4.4731 - val_loss: 20.6583 - val_mae: 4.4531\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6467 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65830 to 20.44068, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 20.6467 - mae: 4.4486 - val_loss: 20.4407 - val_mae: 4.4286\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4293 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44068 to 20.22427, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 20.4293 - mae: 4.4241 - val_loss: 20.2243 - val_mae: 4.4041\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2131 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22427 to 20.00893, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 20.2131 - mae: 4.3996 - val_loss: 20.0089 - val_mae: 4.3796\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9981 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00893 to 19.79483, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 19.9981 - mae: 4.3751 - val_loss: 19.7948 - val_mae: 4.3551\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7843 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79483 to 19.58217, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.7843 - mae: 4.3506 - val_loss: 19.5822 - val_mae: 4.3306\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5716 - mae: 4.3261\n",
            "Epoch 70: val_loss improved from 19.58217 to 19.37082, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 19.5716 - mae: 4.3261 - val_loss: 19.3708 - val_mae: 4.3061\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3605 - mae: 4.3016\n",
            "Epoch 71: val_loss improved from 19.37082 to 19.16021, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 19.3605 - mae: 4.3016 - val_loss: 19.1602 - val_mae: 4.2816\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1503 - mae: 4.2771\n",
            "Epoch 72: val_loss improved from 19.16021 to 18.95109, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 19.1503 - mae: 4.2771 - val_loss: 18.9511 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9415 - mae: 4.2526\n",
            "Epoch 73: val_loss improved from 18.95109 to 18.74324, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.9415 - mae: 4.2526 - val_loss: 18.7432 - val_mae: 4.2326\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7338 - mae: 4.2281\n",
            "Epoch 74: val_loss improved from 18.74324 to 18.53659, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 18.7338 - mae: 4.2281 - val_loss: 18.5366 - val_mae: 4.2081\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5274 - mae: 4.2037\n",
            "Epoch 75: val_loss improved from 18.53659 to 18.33126, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.5274 - mae: 4.2037 - val_loss: 18.3313 - val_mae: 4.1836\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3222 - mae: 4.1792\n",
            "Epoch 76: val_loss improved from 18.33126 to 18.12695, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.3222 - mae: 4.1792 - val_loss: 18.1270 - val_mae: 4.1592\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1182 - mae: 4.1547\n",
            "Epoch 77: val_loss improved from 18.12695 to 17.92406, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 18.1182 - mae: 4.1547 - val_loss: 17.9241 - val_mae: 4.1347\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9155 - mae: 4.1302\n",
            "Epoch 78: val_loss improved from 17.92406 to 17.72224, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 17.9155 - mae: 4.1302 - val_loss: 17.7222 - val_mae: 4.1102\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7139 - mae: 4.1058\n",
            "Epoch 79: val_loss improved from 17.72224 to 17.52168, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.7139 - mae: 4.1058 - val_loss: 17.5217 - val_mae: 4.0857\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5136 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52168 to 17.32243, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.5136 - mae: 4.0813 - val_loss: 17.3224 - val_mae: 4.0613\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3144 - mae: 4.0568\n",
            "Epoch 81: val_loss improved from 17.32243 to 17.12447, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 17.3144 - mae: 4.0568 - val_loss: 17.1245 - val_mae: 4.0368\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1167 - mae: 4.0324\n",
            "Epoch 82: val_loss improved from 17.12447 to 16.92738, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.1167 - mae: 4.0324 - val_loss: 16.9274 - val_mae: 4.0124\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9200 - mae: 4.0079\n",
            "Epoch 83: val_loss improved from 16.92738 to 16.73171, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 16.9200 - mae: 4.0079 - val_loss: 16.7317 - val_mae: 3.9879\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7245 - mae: 3.9835\n",
            "Epoch 84: val_loss improved from 16.73171 to 16.53731, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.7245 - mae: 3.9835 - val_loss: 16.5373 - val_mae: 3.9635\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5304 - mae: 3.9590\n",
            "Epoch 85: val_loss improved from 16.53731 to 16.34409, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.5304 - mae: 3.9590 - val_loss: 16.3441 - val_mae: 3.9390\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3374 - mae: 3.9346\n",
            "Epoch 86: val_loss improved from 16.34409 to 16.15214, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.3374 - mae: 3.9346 - val_loss: 16.1521 - val_mae: 3.9146\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1457 - mae: 3.9101\n",
            "Epoch 87: val_loss improved from 16.15214 to 15.96131, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.1457 - mae: 3.9101 - val_loss: 15.9613 - val_mae: 3.8901\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9551 - mae: 3.8857\n",
            "Epoch 88: val_loss improved from 15.96131 to 15.77188, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.9551 - mae: 3.8857 - val_loss: 15.7719 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7659 - mae: 3.8612\n",
            "Epoch 89: val_loss improved from 15.77188 to 15.58340, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 15.7659 - mae: 3.8612 - val_loss: 15.5834 - val_mae: 3.8412\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5777 - mae: 3.8368\n",
            "Epoch 90: val_loss improved from 15.58340 to 15.39645, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 15.5777 - mae: 3.8368 - val_loss: 15.3964 - val_mae: 3.8168\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3909 - mae: 3.8124\n",
            "Epoch 91: val_loss improved from 15.39645 to 15.21054, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.3909 - mae: 3.8124 - val_loss: 15.2105 - val_mae: 3.7924\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2053 - mae: 3.7879\n",
            "Epoch 92: val_loss improved from 15.21054 to 15.02579, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.2053 - mae: 3.7879 - val_loss: 15.0258 - val_mae: 3.7680\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0209 - mae: 3.7635\n",
            "Epoch 93: val_loss improved from 15.02579 to 14.84231, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.0209 - mae: 3.7635 - val_loss: 14.8423 - val_mae: 3.7435\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8376 - mae: 3.7391\n",
            "Epoch 94: val_loss improved from 14.84231 to 14.66018, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 14.8376 - mae: 3.7391 - val_loss: 14.6602 - val_mae: 3.7191\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6556 - mae: 3.7147\n",
            "Epoch 95: val_loss improved from 14.66018 to 14.47931, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 14.6556 - mae: 3.7147 - val_loss: 14.4793 - val_mae: 3.6947\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4748 - mae: 3.6903\n",
            "Epoch 96: val_loss improved from 14.47931 to 14.29956, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 14.4748 - mae: 3.6903 - val_loss: 14.2996 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2953 - mae: 3.6659\n",
            "Epoch 97: val_loss improved from 14.29956 to 14.12089, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 14.2953 - mae: 3.6659 - val_loss: 14.1209 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1169 - mae: 3.6415\n",
            "Epoch 98: val_loss improved from 14.12089 to 13.94360, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 14.1169 - mae: 3.6415 - val_loss: 13.9436 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9399 - mae: 3.6171\n",
            "Epoch 99: val_loss improved from 13.94360 to 13.76727, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 13.9399 - mae: 3.6171 - val_loss: 13.7673 - val_mae: 3.5971\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7639 - mae: 3.5927\n",
            "Epoch 100: val_loss improved from 13.76727 to 13.59252, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 13.7639 - mae: 3.5927 - val_loss: 13.5925 - val_mae: 3.5727\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.747248649597168, RMSE:3.707728147506714, MAE:3.5902934074401855, R2:-15.040280030095737\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_226\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_143 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_164 (Embedding)      (None, 17, 900)      649800      ['input_143[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_165 (Embedding)      (None, 17, 900)      649800      ['input_143[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_166 (Embedding)      (None, 17, 900)      649800      ['input_143[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_167 (Embedding)      (None, 17, 900)      649800      ['input_143[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_213 (Conv1D)            (None, 17, 100)      270100      ['embedding_164[0][0]',          \n",
            "                                                                  'embedding_165[0][0]',          \n",
            "                                                                  'embedding_166[0][0]',          \n",
            "                                                                  'embedding_167[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_214 (Conv1D)            (None, 17, 100)      270100      ['embedding_164[0][0]',          \n",
            "                                                                  'embedding_165[0][0]',          \n",
            "                                                                  'embedding_166[0][0]',          \n",
            "                                                                  'embedding_167[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_215 (Conv1D)            (None, 17, 100)      270100      ['embedding_164[0][0]',          \n",
            "                                                                  'embedding_165[0][0]',          \n",
            "                                                                  'embedding_166[0][0]',          \n",
            "                                                                  'embedding_167[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_492 (Glob  (None, 100)         0           ['conv1d_213[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_493 (Glob  (None, 100)         0           ['conv1d_213[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_494 (Glob  (None, 100)         0           ['conv1d_213[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_495 (Glob  (None, 100)         0           ['conv1d_213[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_496 (Glob  (None, 100)         0           ['conv1d_214[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_497 (Glob  (None, 100)         0           ['conv1d_214[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_498 (Glob  (None, 100)         0           ['conv1d_214[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_499 (Glob  (None, 100)         0           ['conv1d_214[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_500 (Glob  (None, 100)         0           ['conv1d_215[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_501 (Glob  (None, 100)         0           ['conv1d_215[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_502 (Glob  (None, 100)         0           ['conv1d_215[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_503 (Glob  (None, 100)         0           ['conv1d_215[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_144 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_93 (Add)                   (None, 100)          0           ['global_max_pooling1d_492[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_493[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_494[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_495[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_94 (Add)                   (None, 100)          0           ['global_max_pooling1d_496[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_497[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_498[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_499[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_95 (Add)                   (None, 100)          0           ['global_max_pooling1d_500[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_501[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_502[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_503[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_282 (Dense)              (None, 1000)         11000       ['input_144[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_71 (Concatenate)   (None, 1300)         0           ['add_93[0][0]',                 \n",
            "                                                                  'add_94[0][0]',                 \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                  'add_95[0][0]',                 \n",
            "                                                                  'dense_282[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_71[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_255 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_255[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_256 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_256[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_257 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_257[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_258 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_258[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_259 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_283 (Dense)              (None, 1)            3           ['dropout_259[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.1537 - mae: 6.0247\n",
            "Epoch 1: val_loss improved from inf to 36.87293, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 112ms/step - loss: 37.1537 - mae: 6.0247 - val_loss: 36.8729 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8448 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87293 to 36.57383, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.8448 - mae: 5.9990 - val_loss: 36.5738 - val_mae: 5.9788\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5463 - mae: 5.9741\n",
            "Epoch 3: val_loss improved from 36.57383 to 36.27637, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 36.5463 - mae: 5.9741 - val_loss: 36.2764 - val_mae: 5.9538\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2492 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27637 to 35.98086, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 36.2492 - mae: 5.9492 - val_loss: 35.9809 - val_mae: 5.9290\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9541 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98086 to 35.68678, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.9541 - mae: 5.9243 - val_loss: 35.6868 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6604 - mae: 5.8995\n",
            "Epoch 6: val_loss improved from 35.68678 to 35.39437, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.6604 - mae: 5.8995 - val_loss: 35.3944 - val_mae: 5.8793\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3684 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39437 to 35.10352, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 35.3684 - mae: 5.8747 - val_loss: 35.1035 - val_mae: 5.8545\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0781 - mae: 5.8499\n",
            "Epoch 8: val_loss improved from 35.10352 to 34.81409, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 35.0781 - mae: 5.8499 - val_loss: 34.8141 - val_mae: 5.8297\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7890 - mae: 5.8252\n",
            "Epoch 9: val_loss improved from 34.81409 to 34.52642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.7890 - mae: 5.8252 - val_loss: 34.5264 - val_mae: 5.8050\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5015 - mae: 5.8004\n",
            "Epoch 10: val_loss improved from 34.52642 to 34.24004, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.5015 - mae: 5.8004 - val_loss: 34.2400 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2154 - mae: 5.7757\n",
            "Epoch 11: val_loss improved from 34.24004 to 33.95511, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.2154 - mae: 5.7757 - val_loss: 33.9551 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9307 - mae: 5.7510\n",
            "Epoch 12: val_loss improved from 33.95511 to 33.67152, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.9307 - mae: 5.7510 - val_loss: 33.6715 - val_mae: 5.7309\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6475 - mae: 5.7263\n",
            "Epoch 13: val_loss improved from 33.67152 to 33.38923, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 33.6475 - mae: 5.7263 - val_loss: 33.3892 - val_mae: 5.7062\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3656 - mae: 5.7017\n",
            "Epoch 14: val_loss improved from 33.38923 to 33.10828, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.3656 - mae: 5.7017 - val_loss: 33.1083 - val_mae: 5.6815\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0849 - mae: 5.6770\n",
            "Epoch 15: val_loss improved from 33.10828 to 32.82887, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 33.0849 - mae: 5.6770 - val_loss: 32.8289 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8058 - mae: 5.6524\n",
            "Epoch 16: val_loss improved from 32.82887 to 32.55061, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 32.8058 - mae: 5.6524 - val_loss: 32.5506 - val_mae: 5.6323\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5279 - mae: 5.6277\n",
            "Epoch 17: val_loss improved from 32.55061 to 32.27362, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 32.5279 - mae: 5.6277 - val_loss: 32.2736 - val_mae: 5.6076\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2511 - mae: 5.6031\n",
            "Epoch 18: val_loss improved from 32.27362 to 31.99816, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 32.2511 - mae: 5.6031 - val_loss: 31.9982 - val_mae: 5.5830\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9759 - mae: 5.5785\n",
            "Epoch 19: val_loss improved from 31.99816 to 31.72371, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.9759 - mae: 5.5785 - val_loss: 31.7237 - val_mae: 5.5584\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7017 - mae: 5.5538\n",
            "Epoch 20: val_loss improved from 31.72371 to 31.45067, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 31.7017 - mae: 5.5538 - val_loss: 31.4507 - val_mae: 5.5337\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4290 - mae: 5.5292\n",
            "Epoch 21: val_loss improved from 31.45067 to 31.17875, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.4290 - mae: 5.5292 - val_loss: 31.1788 - val_mae: 5.5091\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1573 - mae: 5.5046\n",
            "Epoch 22: val_loss improved from 31.17875 to 30.90831, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 31.1573 - mae: 5.5046 - val_loss: 30.9083 - val_mae: 5.4845\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8871 - mae: 5.4800\n",
            "Epoch 23: val_loss improved from 30.90831 to 30.63894, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 30.8871 - mae: 5.4800 - val_loss: 30.6389 - val_mae: 5.4599\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6179 - mae: 5.4554\n",
            "Epoch 24: val_loss improved from 30.63894 to 30.37101, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 30.6179 - mae: 5.4554 - val_loss: 30.3710 - val_mae: 5.4353\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3501 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37101 to 30.10415, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.3501 - mae: 5.4308 - val_loss: 30.1041 - val_mae: 5.4107\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0836 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10415 to 29.83848, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.0836 - mae: 5.4062 - val_loss: 29.8385 - val_mae: 5.3861\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8181 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83848 to 29.57425, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 29.8181 - mae: 5.3816 - val_loss: 29.5743 - val_mae: 5.3615\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5540 - mae: 5.3570\n",
            "Epoch 28: val_loss improved from 29.57425 to 29.31117, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 29.5540 - mae: 5.3570 - val_loss: 29.3112 - val_mae: 5.3369\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2913 - mae: 5.3324\n",
            "Epoch 29: val_loss improved from 29.31117 to 29.04913, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 29.2913 - mae: 5.3324 - val_loss: 29.0491 - val_mae: 5.3123\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0296 - mae: 5.3078\n",
            "Epoch 30: val_loss improved from 29.04913 to 28.78853, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 29.0296 - mae: 5.3078 - val_loss: 28.7885 - val_mae: 5.2877\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7691 - mae: 5.2832\n",
            "Epoch 31: val_loss improved from 28.78853 to 28.52933, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.7691 - mae: 5.2832 - val_loss: 28.5293 - val_mae: 5.2632\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5100 - mae: 5.2587\n",
            "Epoch 32: val_loss improved from 28.52933 to 28.27112, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 28.5100 - mae: 5.2587 - val_loss: 28.2711 - val_mae: 5.2386\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2521 - mae: 5.2341\n",
            "Epoch 33: val_loss improved from 28.27112 to 28.01405, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 28.2521 - mae: 5.2341 - val_loss: 28.0141 - val_mae: 5.2140\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9954 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01405 to 27.75837, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.9954 - mae: 5.2095 - val_loss: 27.7584 - val_mae: 5.1894\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7399 - mae: 5.1849\n",
            "Epoch 35: val_loss improved from 27.75837 to 27.50396, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 27.7399 - mae: 5.1849 - val_loss: 27.5040 - val_mae: 5.1648\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4858 - mae: 5.1603\n",
            "Epoch 36: val_loss improved from 27.50396 to 27.25049, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.4858 - mae: 5.1603 - val_loss: 27.2505 - val_mae: 5.1402\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2326 - mae: 5.1358\n",
            "Epoch 37: val_loss improved from 27.25049 to 26.99881, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 27.2326 - mae: 5.1358 - val_loss: 26.9988 - val_mae: 5.1157\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9809 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99881 to 26.74794, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 26.9809 - mae: 5.1112 - val_loss: 26.7479 - val_mae: 5.0911\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7303 - mae: 5.0866\n",
            "Epoch 39: val_loss improved from 26.74794 to 26.49831, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.7303 - mae: 5.0866 - val_loss: 26.4983 - val_mae: 5.0666\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4810 - mae: 5.0620\n",
            "Epoch 40: val_loss improved from 26.49831 to 26.24993, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 26.4810 - mae: 5.0620 - val_loss: 26.2499 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2328 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.24993 to 26.00289, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.2328 - mae: 5.0375 - val_loss: 26.0029 - val_mae: 5.0174\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9860 - mae: 5.0129\n",
            "Epoch 42: val_loss improved from 26.00289 to 25.75700, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 25.9860 - mae: 5.0129 - val_loss: 25.7570 - val_mae: 4.9929\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7403 - mae: 4.9884\n",
            "Epoch 43: val_loss improved from 25.75700 to 25.51230, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.7403 - mae: 4.9884 - val_loss: 25.5123 - val_mae: 4.9683\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4959 - mae: 4.9638\n",
            "Epoch 44: val_loss improved from 25.51230 to 25.26889, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.4959 - mae: 4.9638 - val_loss: 25.2689 - val_mae: 4.9437\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2528 - mae: 4.9392\n",
            "Epoch 45: val_loss improved from 25.26889 to 25.02668, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 25.2528 - mae: 4.9392 - val_loss: 25.0267 - val_mae: 4.9192\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0109 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02668 to 24.78570, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 25.0109 - mae: 4.9147 - val_loss: 24.7857 - val_mae: 4.8946\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7701 - mae: 4.8901\n",
            "Epoch 47: val_loss improved from 24.78570 to 24.54606, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.7701 - mae: 4.8901 - val_loss: 24.5461 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5306 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54606 to 24.30754, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 24.5306 - mae: 4.8656 - val_loss: 24.3075 - val_mae: 4.8455\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2924 - mae: 4.8410\n",
            "Epoch 49: val_loss improved from 24.30754 to 24.07020, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.2924 - mae: 4.8410 - val_loss: 24.0702 - val_mae: 4.8210\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0553 - mae: 4.8165\n",
            "Epoch 50: val_loss improved from 24.07020 to 23.83421, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 24.0553 - mae: 4.8165 - val_loss: 23.8342 - val_mae: 4.7964\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8194 - mae: 4.7919\n",
            "Epoch 51: val_loss improved from 23.83421 to 23.59943, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 23.8194 - mae: 4.7919 - val_loss: 23.5994 - val_mae: 4.7719\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5848 - mae: 4.7674\n",
            "Epoch 52: val_loss improved from 23.59943 to 23.36577, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 23.5848 - mae: 4.7674 - val_loss: 23.3658 - val_mae: 4.7474\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3515 - mae: 4.7429\n",
            "Epoch 53: val_loss improved from 23.36577 to 23.13331, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.3515 - mae: 4.7429 - val_loss: 23.1333 - val_mae: 4.7228\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1194 - mae: 4.7183\n",
            "Epoch 54: val_loss improved from 23.13331 to 22.90220, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.1194 - mae: 4.7183 - val_loss: 22.9022 - val_mae: 4.6983\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8884 - mae: 4.6938\n",
            "Epoch 55: val_loss improved from 22.90220 to 22.67240, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.8884 - mae: 4.6938 - val_loss: 22.6724 - val_mae: 4.6738\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6588 - mae: 4.6693\n",
            "Epoch 56: val_loss improved from 22.67240 to 22.44366, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 22.6588 - mae: 4.6693 - val_loss: 22.4437 - val_mae: 4.6492\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4302 - mae: 4.6447\n",
            "Epoch 57: val_loss improved from 22.44366 to 22.21624, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 22.4302 - mae: 4.6447 - val_loss: 22.2162 - val_mae: 4.6247\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2031 - mae: 4.6202\n",
            "Epoch 58: val_loss improved from 22.21624 to 21.98980, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.2031 - mae: 4.6202 - val_loss: 21.9898 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9770 - mae: 4.5957\n",
            "Epoch 59: val_loss improved from 21.98980 to 21.76486, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.9770 - mae: 4.5957 - val_loss: 21.7649 - val_mae: 4.5756\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7521 - mae: 4.5712\n",
            "Epoch 60: val_loss improved from 21.76486 to 21.54118, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.7521 - mae: 4.5712 - val_loss: 21.5412 - val_mae: 4.5511\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5285 - mae: 4.5467\n",
            "Epoch 61: val_loss improved from 21.54118 to 21.31869, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 21.5285 - mae: 4.5467 - val_loss: 21.3187 - val_mae: 4.5266\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3064 - mae: 4.5221\n",
            "Epoch 62: val_loss improved from 21.31869 to 21.09706, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 21.3064 - mae: 4.5221 - val_loss: 21.0971 - val_mae: 4.5021\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0851 - mae: 4.4976\n",
            "Epoch 63: val_loss improved from 21.09706 to 20.87712, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.0851 - mae: 4.4976 - val_loss: 20.8771 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8653 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87712 to 20.65806, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 20.8653 - mae: 4.4731 - val_loss: 20.6581 - val_mae: 4.4531\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6466 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65806 to 20.44042, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.6466 - mae: 4.4486 - val_loss: 20.4404 - val_mae: 4.4286\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4292 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44042 to 20.22404, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 20.4292 - mae: 4.4241 - val_loss: 20.2240 - val_mae: 4.4041\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2130 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22404 to 20.00884, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.2130 - mae: 4.3996 - val_loss: 20.0088 - val_mae: 4.3796\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9979 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00884 to 19.79491, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.9979 - mae: 4.3751 - val_loss: 19.7949 - val_mae: 4.3551\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7842 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79491 to 19.58208, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 19.7842 - mae: 4.3506 - val_loss: 19.5821 - val_mae: 4.3306\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5716 - mae: 4.3261\n",
            "Epoch 70: val_loss improved from 19.58208 to 19.37049, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 19.5716 - mae: 4.3261 - val_loss: 19.3705 - val_mae: 4.3061\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3603 - mae: 4.3016\n",
            "Epoch 71: val_loss improved from 19.37049 to 19.16008, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 19.3603 - mae: 4.3016 - val_loss: 19.1601 - val_mae: 4.2816\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1502 - mae: 4.2771\n",
            "Epoch 72: val_loss improved from 19.16008 to 18.95100, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.1502 - mae: 4.2771 - val_loss: 18.9510 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9414 - mae: 4.2526\n",
            "Epoch 73: val_loss improved from 18.95100 to 18.74310, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.9414 - mae: 4.2526 - val_loss: 18.7431 - val_mae: 4.2326\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7337 - mae: 4.2281\n",
            "Epoch 74: val_loss improved from 18.74310 to 18.53651, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.7337 - mae: 4.2281 - val_loss: 18.5365 - val_mae: 4.2081\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5273 - mae: 4.2036\n",
            "Epoch 75: val_loss improved from 18.53651 to 18.33106, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.5273 - mae: 4.2036 - val_loss: 18.3311 - val_mae: 4.1836\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3221 - mae: 4.1792\n",
            "Epoch 76: val_loss improved from 18.33106 to 18.12675, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 18.3221 - mae: 4.1792 - val_loss: 18.1268 - val_mae: 4.1591\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1181 - mae: 4.1547\n",
            "Epoch 77: val_loss improved from 18.12675 to 17.92389, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.1181 - mae: 4.1547 - val_loss: 17.9239 - val_mae: 4.1347\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9154 - mae: 4.1302\n",
            "Epoch 78: val_loss improved from 17.92389 to 17.72214, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 17.9154 - mae: 4.1302 - val_loss: 17.7221 - val_mae: 4.1102\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7138 - mae: 4.1058\n",
            "Epoch 79: val_loss improved from 17.72214 to 17.52169, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.7138 - mae: 4.1058 - val_loss: 17.5217 - val_mae: 4.0858\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5136 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52169 to 17.32224, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 17.5136 - mae: 4.0813 - val_loss: 17.3222 - val_mae: 4.0613\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3144 - mae: 4.0568\n",
            "Epoch 81: val_loss improved from 17.32224 to 17.12427, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.3144 - mae: 4.0568 - val_loss: 17.1243 - val_mae: 4.0368\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1166 - mae: 4.0324\n",
            "Epoch 82: val_loss improved from 17.12427 to 16.92729, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 17.1166 - mae: 4.0324 - val_loss: 16.9273 - val_mae: 4.0123\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9198 - mae: 4.0079\n",
            "Epoch 83: val_loss improved from 16.92729 to 16.73178, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.9198 - mae: 4.0079 - val_loss: 16.7318 - val_mae: 3.9879\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7246 - mae: 3.9834\n",
            "Epoch 84: val_loss improved from 16.73178 to 16.53718, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.7246 - mae: 3.9834 - val_loss: 16.5372 - val_mae: 3.9634\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5303 - mae: 3.9590\n",
            "Epoch 85: val_loss improved from 16.53718 to 16.34394, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.5303 - mae: 3.9590 - val_loss: 16.3439 - val_mae: 3.9390\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3373 - mae: 3.9345\n",
            "Epoch 86: val_loss improved from 16.34394 to 16.15199, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.3373 - mae: 3.9345 - val_loss: 16.1520 - val_mae: 3.9145\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1456 - mae: 3.9101\n",
            "Epoch 87: val_loss improved from 16.15199 to 15.96133, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 16.1456 - mae: 3.9101 - val_loss: 15.9613 - val_mae: 3.8901\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9551 - mae: 3.8857\n",
            "Epoch 88: val_loss improved from 15.96133 to 15.77174, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 15.9551 - mae: 3.8857 - val_loss: 15.7717 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7657 - mae: 3.8612\n",
            "Epoch 89: val_loss improved from 15.77174 to 15.58351, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 15.7657 - mae: 3.8612 - val_loss: 15.5835 - val_mae: 3.8412\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5777 - mae: 3.8368\n",
            "Epoch 90: val_loss improved from 15.58351 to 15.39633, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 15.5777 - mae: 3.8368 - val_loss: 15.3963 - val_mae: 3.8168\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3908 - mae: 3.8124\n",
            "Epoch 91: val_loss improved from 15.39633 to 15.21049, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.3908 - mae: 3.8124 - val_loss: 15.2105 - val_mae: 3.7924\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2052 - mae: 3.7879\n",
            "Epoch 92: val_loss improved from 15.21049 to 15.02580, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 15.2052 - mae: 3.7879 - val_loss: 15.0258 - val_mae: 3.7680\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0206 - mae: 3.7635\n",
            "Epoch 93: val_loss improved from 15.02580 to 14.84253, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 15.0206 - mae: 3.7635 - val_loss: 14.8425 - val_mae: 3.7436\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8375 - mae: 3.7391\n",
            "Epoch 94: val_loss improved from 14.84253 to 14.66020, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 14.8375 - mae: 3.7391 - val_loss: 14.6602 - val_mae: 3.7191\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6555 - mae: 3.7147\n",
            "Epoch 95: val_loss improved from 14.66020 to 14.47904, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 14.6555 - mae: 3.7147 - val_loss: 14.4790 - val_mae: 3.6947\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4747 - mae: 3.6903\n",
            "Epoch 96: val_loss improved from 14.47904 to 14.29930, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 14.4747 - mae: 3.6903 - val_loss: 14.2993 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2952 - mae: 3.6659\n",
            "Epoch 97: val_loss improved from 14.29930 to 14.12072, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 14.2952 - mae: 3.6659 - val_loss: 14.1207 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1168 - mae: 3.6415\n",
            "Epoch 98: val_loss improved from 14.12072 to 13.94352, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 14.1168 - mae: 3.6415 - val_loss: 13.9435 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9397 - mae: 3.6171\n",
            "Epoch 99: val_loss improved from 13.94352 to 13.76731, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 13.9397 - mae: 3.6171 - val_loss: 13.7673 - val_mae: 3.5971\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7639 - mae: 3.5926\n",
            "Epoch 100: val_loss improved from 13.76731 to 13.59231, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 13.7639 - mae: 3.5926 - val_loss: 13.5923 - val_mae: 3.5727\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 21ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.747042655944824, RMSE:3.707700490951538, MAE:3.590263843536377, R2:-15.040040183923402\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_232\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_145 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_168 (Embedding)      (None, 17, 900)      649800      ['input_145[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_169 (Embedding)      (None, 17, 900)      649800      ['input_145[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_170 (Embedding)      (None, 17, 900)      649800      ['input_145[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_171 (Embedding)      (None, 17, 900)      649800      ['input_145[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_216 (Conv1D)            (None, 17, 100)      270100      ['embedding_168[0][0]',          \n",
            "                                                                  'embedding_169[0][0]',          \n",
            "                                                                  'embedding_170[0][0]',          \n",
            "                                                                  'embedding_171[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_217 (Conv1D)            (None, 17, 100)      270100      ['embedding_168[0][0]',          \n",
            "                                                                  'embedding_169[0][0]',          \n",
            "                                                                  'embedding_170[0][0]',          \n",
            "                                                                  'embedding_171[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_218 (Conv1D)            (None, 17, 100)      270100      ['embedding_168[0][0]',          \n",
            "                                                                  'embedding_169[0][0]',          \n",
            "                                                                  'embedding_170[0][0]',          \n",
            "                                                                  'embedding_171[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_504 (Glob  (None, 100)         0           ['conv1d_216[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_505 (Glob  (None, 100)         0           ['conv1d_216[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_506 (Glob  (None, 100)         0           ['conv1d_216[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_507 (Glob  (None, 100)         0           ['conv1d_216[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_508 (Glob  (None, 100)         0           ['conv1d_217[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_509 (Glob  (None, 100)         0           ['conv1d_217[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_510 (Glob  (None, 100)         0           ['conv1d_217[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_511 (Glob  (None, 100)         0           ['conv1d_217[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_512 (Glob  (None, 100)         0           ['conv1d_218[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_513 (Glob  (None, 100)         0           ['conv1d_218[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_514 (Glob  (None, 100)         0           ['conv1d_218[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_515 (Glob  (None, 100)         0           ['conv1d_218[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_146 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_96 (Add)                   (None, 100)          0           ['global_max_pooling1d_504[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_505[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_506[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_507[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_97 (Add)                   (None, 100)          0           ['global_max_pooling1d_508[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_509[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_510[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_511[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_98 (Add)                   (None, 100)          0           ['global_max_pooling1d_512[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_513[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_514[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_515[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_284 (Dense)              (None, 1000)         11000       ['input_146[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_72 (Concatenate)   (None, 1300)         0           ['add_96[0][0]',                 \n",
            "                                                                  'add_97[0][0]',                 \n",
            "                                                                  'add_98[0][0]',                 \n",
            "                                                                  'dense_284[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_72[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_260 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_260[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_261 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_261[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_262 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_262[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_263 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_263[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_264 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_285 (Dense)              (None, 1)            3           ['dropout_264[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 3.1437 - mae: 1.0128\n",
            "Epoch 1: val_loss improved from inf to 0.66392, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 112ms/step - loss: 3.1437 - mae: 1.0128 - val_loss: 0.6639 - val_mae: 0.3933\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6069 - mae: 0.4637\n",
            "Epoch 2: val_loss improved from 0.66392 to 0.49554, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.6069 - mae: 0.4637 - val_loss: 0.4955 - val_mae: 0.5299\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5480 - mae: 0.4888\n",
            "Epoch 3: val_loss improved from 0.49554 to 0.42222, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.5480 - mae: 0.4888 - val_loss: 0.4222 - val_mae: 0.3180\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4662 - mae: 0.4127\n",
            "Epoch 4: val_loss improved from 0.42222 to 0.38367, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.4662 - mae: 0.4127 - val_loss: 0.3837 - val_mae: 0.3090\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4093 - mae: 0.3534\n",
            "Epoch 5: val_loss improved from 0.38367 to 0.37234, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.4093 - mae: 0.3534 - val_loss: 0.3723 - val_mae: 0.3202\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4014 - mae: 0.3507\n",
            "Epoch 6: val_loss did not improve from 0.37234\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.4014 - mae: 0.3507 - val_loss: 0.3744 - val_mae: 0.3179\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4142 - mae: 0.3804\n",
            "Epoch 7: val_loss improved from 0.37234 to 0.35801, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 0.4142 - mae: 0.3804 - val_loss: 0.3580 - val_mae: 0.3189\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3864 - mae: 0.3445\n",
            "Epoch 8: val_loss did not improve from 0.35801\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3864 - mae: 0.3445 - val_loss: 0.3645 - val_mae: 0.3381\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3846 - mae: 0.3478\n",
            "Epoch 9: val_loss did not improve from 0.35801\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.3846 - mae: 0.3478 - val_loss: 0.3827 - val_mae: 0.3404\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3949 - mae: 0.3611\n",
            "Epoch 10: val_loss did not improve from 0.35801\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3949 - mae: 0.3611 - val_loss: 0.3749 - val_mae: 0.4142\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4407 - mae: 0.4269\n",
            "Epoch 11: val_loss did not improve from 0.35801\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4407 - mae: 0.4269 - val_loss: 0.3884 - val_mae: 0.3550\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3881 - mae: 0.3720\n",
            "Epoch 12: val_loss did not improve from 0.35801\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3881 - mae: 0.3720 - val_loss: 0.3963 - val_mae: 0.3490\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3678 - mae: 0.3399\n",
            "Epoch 13: val_loss improved from 0.35801 to 0.34381, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.3678 - mae: 0.3399 - val_loss: 0.3438 - val_mae: 0.3707\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3584 - mae: 0.3358\n",
            "Epoch 14: val_loss did not improve from 0.34381\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3584 - mae: 0.3358 - val_loss: 0.3927 - val_mae: 0.3519\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3651 - mae: 0.3524\n",
            "Epoch 15: val_loss improved from 0.34381 to 0.33294, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 0.3651 - mae: 0.3524 - val_loss: 0.3329 - val_mae: 0.2747\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3776 - mae: 0.3778\n",
            "Epoch 16: val_loss improved from 0.33294 to 0.31050, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3776 - mae: 0.3778 - val_loss: 0.3105 - val_mae: 0.2998\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3335 - mae: 0.3237\n",
            "Epoch 17: val_loss did not improve from 0.31050\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.3335 - mae: 0.3237 - val_loss: 0.3776 - val_mae: 0.3707\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3713 - mae: 0.3867\n",
            "Epoch 18: val_loss did not improve from 0.31050\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.3713 - mae: 0.3867 - val_loss: 0.3331 - val_mae: 0.3048\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3267 - mae: 0.3363\n",
            "Epoch 19: val_loss improved from 0.31050 to 0.29957, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.3267 - mae: 0.3363 - val_loss: 0.2996 - val_mae: 0.3385\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3104 - mae: 0.3262\n",
            "Epoch 20: val_loss did not improve from 0.29957\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3104 - mae: 0.3262 - val_loss: 0.3164 - val_mae: 0.2750\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2825 - mae: 0.3019\n",
            "Epoch 21: val_loss improved from 0.29957 to 0.26870, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.2825 - mae: 0.3019 - val_loss: 0.2687 - val_mae: 0.3015\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2954 - mae: 0.3367\n",
            "Epoch 22: val_loss did not improve from 0.26870\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.2954 - mae: 0.3367 - val_loss: 0.4007 - val_mae: 0.4512\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2638 - mae: 0.3041\n",
            "Epoch 23: val_loss improved from 0.26870 to 0.21653, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.2638 - mae: 0.3041 - val_loss: 0.2165 - val_mae: 0.2489\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2190 - mae: 0.2598\n",
            "Epoch 24: val_loss did not improve from 0.21653\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.2190 - mae: 0.2598 - val_loss: 0.2165 - val_mae: 0.2933\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2203 - mae: 0.2841\n",
            "Epoch 25: val_loss did not improve from 0.21653\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2203 - mae: 0.2841 - val_loss: 0.2979 - val_mae: 0.4217\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2982 - mae: 0.3731\n",
            "Epoch 26: val_loss improved from 0.21653 to 0.20868, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.2982 - mae: 0.3731 - val_loss: 0.2087 - val_mae: 0.2034\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2663 - mae: 0.3367\n",
            "Epoch 27: val_loss did not improve from 0.20868\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2663 - mae: 0.3367 - val_loss: 0.2653 - val_mae: 0.3117\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1975 - mae: 0.2697\n",
            "Epoch 28: val_loss improved from 0.20868 to 0.16066, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.1975 - mae: 0.2697 - val_loss: 0.1607 - val_mae: 0.2153\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1679 - mae: 0.2390\n",
            "Epoch 29: val_loss improved from 0.16066 to 0.15503, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.1679 - mae: 0.2390 - val_loss: 0.1550 - val_mae: 0.2309\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1405 - mae: 0.2143\n",
            "Epoch 30: val_loss improved from 0.15503 to 0.13263, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.1405 - mae: 0.2143 - val_loss: 0.1326 - val_mae: 0.1879\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2210 - mae: 0.3185\n",
            "Epoch 31: val_loss did not improve from 0.13263\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2210 - mae: 0.3185 - val_loss: 0.2129 - val_mae: 0.1823\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3685 - mae: 0.4100\n",
            "Epoch 32: val_loss did not improve from 0.13263\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.3685 - mae: 0.4100 - val_loss: 0.4731 - val_mae: 0.4949\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5015 - mae: 0.4852\n",
            "Epoch 33: val_loss did not improve from 0.13263\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.5015 - mae: 0.4852 - val_loss: 0.3461 - val_mae: 0.3722\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2872 - mae: 0.3185\n",
            "Epoch 34: val_loss did not improve from 0.13263\n",
            "25/25 [==============================] - 2s 83ms/step - loss: 0.2872 - mae: 0.3185 - val_loss: 0.2389 - val_mae: 0.2595\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2399 - mae: 0.2842\n",
            "Epoch 35: val_loss did not improve from 0.13263\n",
            "25/25 [==============================] - 2s 83ms/step - loss: 0.2399 - mae: 0.2842 - val_loss: 0.2360 - val_mae: 0.3261\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1850 - mae: 0.2365\n",
            "Epoch 36: val_loss did not improve from 0.13263\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.1850 - mae: 0.2365 - val_loss: 0.2117 - val_mae: 0.3401\n",
            "Epoch 36: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 23ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.1257973164319992, RMSE:0.3546791672706604, MAE:0.19622497260570526, R2:0.8532197817438554\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_238\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_147 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_172 (Embedding)      (None, 17, 900)      649800      ['input_147[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_173 (Embedding)      (None, 17, 900)      649800      ['input_147[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_174 (Embedding)      (None, 17, 900)      649800      ['input_147[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_175 (Embedding)      (None, 17, 900)      649800      ['input_147[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_219 (Conv1D)            (None, 17, 100)      270100      ['embedding_172[0][0]',          \n",
            "                                                                  'embedding_173[0][0]',          \n",
            "                                                                  'embedding_174[0][0]',          \n",
            "                                                                  'embedding_175[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_220 (Conv1D)            (None, 17, 100)      270100      ['embedding_172[0][0]',          \n",
            "                                                                  'embedding_173[0][0]',          \n",
            "                                                                  'embedding_174[0][0]',          \n",
            "                                                                  'embedding_175[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_221 (Conv1D)            (None, 17, 100)      270100      ['embedding_172[0][0]',          \n",
            "                                                                  'embedding_173[0][0]',          \n",
            "                                                                  'embedding_174[0][0]',          \n",
            "                                                                  'embedding_175[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_516 (Glob  (None, 100)         0           ['conv1d_219[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_517 (Glob  (None, 100)         0           ['conv1d_219[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_518 (Glob  (None, 100)         0           ['conv1d_219[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_519 (Glob  (None, 100)         0           ['conv1d_219[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_520 (Glob  (None, 100)         0           ['conv1d_220[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_521 (Glob  (None, 100)         0           ['conv1d_220[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_522 (Glob  (None, 100)         0           ['conv1d_220[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_523 (Glob  (None, 100)         0           ['conv1d_220[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_524 (Glob  (None, 100)         0           ['conv1d_221[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_525 (Glob  (None, 100)         0           ['conv1d_221[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_526 (Glob  (None, 100)         0           ['conv1d_221[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_527 (Glob  (None, 100)         0           ['conv1d_221[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_148 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_99 (Add)                   (None, 100)          0           ['global_max_pooling1d_516[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_517[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_518[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_519[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_100 (Add)                  (None, 100)          0           ['global_max_pooling1d_520[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_521[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_522[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_523[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_101 (Add)                  (None, 100)          0           ['global_max_pooling1d_524[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_525[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_526[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_527[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_286 (Dense)              (None, 1000)         11000       ['input_148[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_73 (Concatenate)   (None, 1300)         0           ['add_99[0][0]',                 \n",
            "                                                                  'add_100[0][0]',                \n",
            "                                                                  'add_101[0][0]',                \n",
            "                                                                  'dense_286[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_73[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_265 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_265[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_266 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_266[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_267 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_267[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_268 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_268[0][0]']            \n",
            "                                                                                                  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " dropout_269 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_287 (Dense)              (None, 1)            3           ['dropout_269[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.7174 - mae: 1.4831\n",
            "Epoch 1: val_loss improved from inf to 0.76700, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 117ms/step - loss: 4.7174 - mae: 1.4831 - val_loss: 0.7670 - val_mae: 0.7130\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7217 - mae: 0.5207\n",
            "Epoch 2: val_loss improved from 0.76700 to 0.56947, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.7217 - mae: 0.5207 - val_loss: 0.5695 - val_mae: 0.4170\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5625 - mae: 0.4271\n",
            "Epoch 3: val_loss improved from 0.56947 to 0.45760, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.5625 - mae: 0.4271 - val_loss: 0.4576 - val_mae: 0.4666\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4918 - mae: 0.4104\n",
            "Epoch 4: val_loss improved from 0.45760 to 0.44567, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.4918 - mae: 0.4104 - val_loss: 0.4457 - val_mae: 0.4949\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4507 - mae: 0.3843\n",
            "Epoch 5: val_loss improved from 0.44567 to 0.38172, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.4507 - mae: 0.3843 - val_loss: 0.3817 - val_mae: 0.3339\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4418 - mae: 0.3989\n",
            "Epoch 6: val_loss did not improve from 0.38172\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4418 - mae: 0.3989 - val_loss: 0.5089 - val_mae: 0.4643\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4276 - mae: 0.3775\n",
            "Epoch 7: val_loss improved from 0.38172 to 0.36521, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.4276 - mae: 0.3775 - val_loss: 0.3652 - val_mae: 0.3144\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3969 - mae: 0.3392\n",
            "Epoch 8: val_loss did not improve from 0.36521\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3969 - mae: 0.3392 - val_loss: 0.3710 - val_mae: 0.3620\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4309 - mae: 0.3969\n",
            "Epoch 9: val_loss did not improve from 0.36521\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4309 - mae: 0.3969 - val_loss: 0.5419 - val_mae: 0.4780\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4208 - mae: 0.3821\n",
            "Epoch 10: val_loss did not improve from 0.36521\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4208 - mae: 0.3821 - val_loss: 0.3665 - val_mae: 0.3614\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3760 - mae: 0.3201\n",
            "Epoch 11: val_loss did not improve from 0.36521\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.3760 - mae: 0.3201 - val_loss: 0.3755 - val_mae: 0.3173\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4115 - mae: 0.3902\n",
            "Epoch 12: val_loss did not improve from 0.36521\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.4115 - mae: 0.3902 - val_loss: 0.3823 - val_mae: 0.4153\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4177 - mae: 0.3915\n",
            "Epoch 13: val_loss did not improve from 0.36521\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.4177 - mae: 0.3915 - val_loss: 0.4096 - val_mae: 0.3849\n",
            "Epoch 13: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.3878222107887268, RMSE:0.6227537393569946, MAE:0.33168962597846985, R2:0.5474893126544897\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_244\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_149 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_176 (Embedding)      (None, 17, 900)      649800      ['input_149[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_177 (Embedding)      (None, 17, 900)      649800      ['input_149[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_178 (Embedding)      (None, 17, 900)      649800      ['input_149[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_179 (Embedding)      (None, 17, 900)      649800      ['input_149[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_222 (Conv1D)            (None, 17, 100)      270100      ['embedding_176[0][0]',          \n",
            "                                                                  'embedding_177[0][0]',          \n",
            "                                                                  'embedding_178[0][0]',          \n",
            "                                                                  'embedding_179[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_223 (Conv1D)            (None, 17, 100)      270100      ['embedding_176[0][0]',          \n",
            "                                                                  'embedding_177[0][0]',          \n",
            "                                                                  'embedding_178[0][0]',          \n",
            "                                                                  'embedding_179[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_224 (Conv1D)            (None, 17, 100)      270100      ['embedding_176[0][0]',          \n",
            "                                                                  'embedding_177[0][0]',          \n",
            "                                                                  'embedding_178[0][0]',          \n",
            "                                                                  'embedding_179[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_528 (Glob  (None, 100)         0           ['conv1d_222[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_529 (Glob  (None, 100)         0           ['conv1d_222[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_530 (Glob  (None, 100)         0           ['conv1d_222[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_531 (Glob  (None, 100)         0           ['conv1d_222[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_532 (Glob  (None, 100)         0           ['conv1d_223[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_533 (Glob  (None, 100)         0           ['conv1d_223[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_534 (Glob  (None, 100)         0           ['conv1d_223[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_535 (Glob  (None, 100)         0           ['conv1d_223[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_536 (Glob  (None, 100)         0           ['conv1d_224[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_537 (Glob  (None, 100)         0           ['conv1d_224[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_538 (Glob  (None, 100)         0           ['conv1d_224[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_539 (Glob  (None, 100)         0           ['conv1d_224[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_150 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_102 (Add)                  (None, 100)          0           ['global_max_pooling1d_528[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_529[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_530[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_531[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_103 (Add)                  (None, 100)          0           ['global_max_pooling1d_532[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_533[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_534[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_535[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_104 (Add)                  (None, 100)          0           ['global_max_pooling1d_536[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_537[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_538[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_539[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_288 (Dense)              (None, 1000)         11000       ['input_150[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_74 (Concatenate)   (None, 1300)         0           ['add_102[0][0]',                \n",
            "                                                                  'add_103[0][0]',                \n",
            "                                                                  'add_104[0][0]',                \n",
            "                                                                  'dense_288[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_74[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_270 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_270[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_271 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_271[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_272 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_272[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_273 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_273[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_274 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " dense_289 (Dense)              (None, 1)            3           ['dropout_274[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 39.5116 - mae: 6.1761\n",
            "Epoch 1: val_loss improved from inf to 36.88076, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 113ms/step - loss: 39.5116 - mae: 6.1761 - val_loss: 36.8808 - val_mae: 6.0044\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8553 - mae: 5.9999\n",
            "Epoch 2: val_loss improved from 36.88076 to 36.58644, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.8553 - mae: 5.9999 - val_loss: 36.5864 - val_mae: 5.9798\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5603 - mae: 5.9753\n",
            "Epoch 3: val_loss improved from 36.58644 to 36.29185, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 36.5603 - mae: 5.9753 - val_loss: 36.2919 - val_mae: 5.9551\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2656 - mae: 5.9505\n",
            "Epoch 4: val_loss improved from 36.29185 to 35.99777, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.2656 - mae: 5.9505 - val_loss: 35.9978 - val_mae: 5.9304\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9717 - mae: 5.9258\n",
            "Epoch 5: val_loss improved from 35.99777 to 35.70476, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 35.9717 - mae: 5.9258 - val_loss: 35.7048 - val_mae: 5.9056\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6789 - mae: 5.9010\n",
            "Epoch 6: val_loss improved from 35.70476 to 35.41322, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.6789 - mae: 5.9010 - val_loss: 35.4132 - val_mae: 5.8809\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3876 - mae: 5.8763\n",
            "Epoch 7: val_loss improved from 35.41322 to 35.12283, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 35.3876 - mae: 5.8763 - val_loss: 35.1228 - val_mae: 5.8562\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0975 - mae: 5.8516\n",
            "Epoch 8: val_loss improved from 35.12283 to 34.83385, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 35.0975 - mae: 5.8516 - val_loss: 34.8338 - val_mae: 5.8314\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.8089 - mae: 5.8268\n",
            "Epoch 9: val_loss improved from 34.83385 to 34.54625, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.8089 - mae: 5.8268 - val_loss: 34.5462 - val_mae: 5.8067\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5215 - mae: 5.8021\n",
            "Epoch 10: val_loss improved from 34.54625 to 34.26013, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.5215 - mae: 5.8021 - val_loss: 34.2601 - val_mae: 5.7820\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2356 - mae: 5.7775\n",
            "Epoch 11: val_loss improved from 34.26013 to 33.97511, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.2356 - mae: 5.7775 - val_loss: 33.9751 - val_mae: 5.7573\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9510 - mae: 5.7528\n",
            "Epoch 12: val_loss improved from 33.97511 to 33.69168, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.9510 - mae: 5.7528 - val_loss: 33.6917 - val_mae: 5.7327\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6678 - mae: 5.7281\n",
            "Epoch 13: val_loss improved from 33.69168 to 33.40946, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.6678 - mae: 5.7281 - val_loss: 33.4095 - val_mae: 5.7080\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3860 - mae: 5.7034\n",
            "Epoch 14: val_loss improved from 33.40946 to 33.12840, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.3860 - mae: 5.7034 - val_loss: 33.1284 - val_mae: 5.6833\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.1053 - mae: 5.6788\n",
            "Epoch 15: val_loss improved from 33.12840 to 32.84899, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 106ms/step - loss: 33.1053 - mae: 5.6788 - val_loss: 32.8490 - val_mae: 5.6587\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8260 - mae: 5.6541\n",
            "Epoch 16: val_loss improved from 32.84899 to 32.57082, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 32.8260 - mae: 5.6541 - val_loss: 32.5708 - val_mae: 5.6340\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5480 - mae: 5.6295\n",
            "Epoch 17: val_loss improved from 32.57082 to 32.29388, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 32.5480 - mae: 5.6295 - val_loss: 32.2939 - val_mae: 5.6094\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2713 - mae: 5.6049\n",
            "Epoch 18: val_loss improved from 32.29388 to 32.01819, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 32.2713 - mae: 5.6049 - val_loss: 32.0182 - val_mae: 5.5848\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9958 - mae: 5.5802\n",
            "Epoch 19: val_loss improved from 32.01819 to 31.74370, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 31.9958 - mae: 5.5802 - val_loss: 31.7437 - val_mae: 5.5602\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7216 - mae: 5.5556\n",
            "Epoch 20: val_loss improved from 31.74370 to 31.47061, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.7216 - mae: 5.5556 - val_loss: 31.4706 - val_mae: 5.5355\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4487 - mae: 5.5310\n",
            "Epoch 21: val_loss improved from 31.47061 to 31.19869, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 31.4487 - mae: 5.5310 - val_loss: 31.1987 - val_mae: 5.5109\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1771 - mae: 5.5064\n",
            "Epoch 22: val_loss improved from 31.19869 to 30.92796, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 31.1771 - mae: 5.5064 - val_loss: 30.9280 - val_mae: 5.4863\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.9067 - mae: 5.4818\n",
            "Epoch 23: val_loss improved from 30.92796 to 30.65852, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.9067 - mae: 5.4818 - val_loss: 30.6585 - val_mae: 5.4617\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6376 - mae: 5.4572\n",
            "Epoch 24: val_loss improved from 30.65852 to 30.39041, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.6376 - mae: 5.4572 - val_loss: 30.3904 - val_mae: 5.4371\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3697 - mae: 5.4326\n",
            "Epoch 25: val_loss improved from 30.39041 to 30.12350, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 30.3697 - mae: 5.4326 - val_loss: 30.1235 - val_mae: 5.4125\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.1031 - mae: 5.4080\n",
            "Epoch 26: val_loss improved from 30.12350 to 29.85775, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.1031 - mae: 5.4080 - val_loss: 29.8578 - val_mae: 5.3879\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8375 - mae: 5.3834\n",
            "Epoch 27: val_loss improved from 29.85775 to 29.59372, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 29.8375 - mae: 5.3834 - val_loss: 29.5937 - val_mae: 5.3633\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5735 - mae: 5.3588\n",
            "Epoch 28: val_loss improved from 29.59372 to 29.33035, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 29.5735 - mae: 5.3588 - val_loss: 29.3303 - val_mae: 5.3387\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.3105 - mae: 5.3342\n",
            "Epoch 29: val_loss improved from 29.33035 to 29.06831, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 29.3105 - mae: 5.3342 - val_loss: 29.0683 - val_mae: 5.3141\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0489 - mae: 5.3096\n",
            "Epoch 30: val_loss improved from 29.06831 to 28.80749, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.0489 - mae: 5.3096 - val_loss: 28.8075 - val_mae: 5.2895\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7883 - mae: 5.2850\n",
            "Epoch 31: val_loss improved from 28.80749 to 28.54818, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 28.7883 - mae: 5.2850 - val_loss: 28.5482 - val_mae: 5.2650\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5291 - mae: 5.2605\n",
            "Epoch 32: val_loss improved from 28.54818 to 28.29001, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 28.5291 - mae: 5.2605 - val_loss: 28.2900 - val_mae: 5.2404\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2711 - mae: 5.2359\n",
            "Epoch 33: val_loss improved from 28.29001 to 28.03295, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 28.2711 - mae: 5.2359 - val_loss: 28.0330 - val_mae: 5.2158\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.0142 - mae: 5.2113\n",
            "Epoch 34: val_loss improved from 28.03295 to 27.77718, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.0142 - mae: 5.2113 - val_loss: 27.7772 - val_mae: 5.1912\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7587 - mae: 5.1867\n",
            "Epoch 35: val_loss improved from 27.77718 to 27.52256, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 27.7587 - mae: 5.1867 - val_loss: 27.5226 - val_mae: 5.1666\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.5043 - mae: 5.1621\n",
            "Epoch 36: val_loss improved from 27.52256 to 27.26930, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 27.5043 - mae: 5.1621 - val_loss: 27.2693 - val_mae: 5.1421\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2512 - mae: 5.1376\n",
            "Epoch 37: val_loss improved from 27.26930 to 27.01723, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 27.2512 - mae: 5.1376 - val_loss: 27.0172 - val_mae: 5.1175\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9993 - mae: 5.1130\n",
            "Epoch 38: val_loss improved from 27.01723 to 26.76633, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.9993 - mae: 5.1130 - val_loss: 26.7663 - val_mae: 5.0929\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7487 - mae: 5.0884\n",
            "Epoch 39: val_loss improved from 26.76633 to 26.51661, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 26.7487 - mae: 5.0884 - val_loss: 26.5166 - val_mae: 5.0684\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4992 - mae: 5.0639\n",
            "Epoch 40: val_loss improved from 26.51661 to 26.26820, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.4992 - mae: 5.0639 - val_loss: 26.2682 - val_mae: 5.0438\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2511 - mae: 5.0393\n",
            "Epoch 41: val_loss improved from 26.26820 to 26.02083, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.2511 - mae: 5.0393 - val_loss: 26.0208 - val_mae: 5.0192\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.0040 - mae: 5.0147\n",
            "Epoch 42: val_loss improved from 26.02083 to 25.77508, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.0040 - mae: 5.0147 - val_loss: 25.7751 - val_mae: 4.9947\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7583 - mae: 4.9902\n",
            "Epoch 43: val_loss improved from 25.77508 to 25.53030, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 25.7583 - mae: 4.9902 - val_loss: 25.5303 - val_mae: 4.9701\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.5139 - mae: 4.9656\n",
            "Epoch 44: val_loss improved from 25.53030 to 25.28661, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 25.5139 - mae: 4.9656 - val_loss: 25.2866 - val_mae: 4.9455\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2705 - mae: 4.9410\n",
            "Epoch 45: val_loss improved from 25.28661 to 25.04452, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 25.2705 - mae: 4.9410 - val_loss: 25.0445 - val_mae: 4.9210\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0286 - mae: 4.9165\n",
            "Epoch 46: val_loss improved from 25.04452 to 24.80337, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.0286 - mae: 4.9165 - val_loss: 24.8034 - val_mae: 4.8964\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7876 - mae: 4.8919\n",
            "Epoch 47: val_loss improved from 24.80337 to 24.56371, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 24.7876 - mae: 4.8919 - val_loss: 24.5637 - val_mae: 4.8719\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5481 - mae: 4.8674\n",
            "Epoch 48: val_loss improved from 24.56371 to 24.32492, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 24.5481 - mae: 4.8674 - val_loss: 24.3249 - val_mae: 4.8473\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.3096 - mae: 4.8428\n",
            "Epoch 49: val_loss improved from 24.32492 to 24.08763, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 24.3096 - mae: 4.8428 - val_loss: 24.0876 - val_mae: 4.8228\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0725 - mae: 4.8183\n",
            "Epoch 50: val_loss improved from 24.08763 to 23.85142, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 106ms/step - loss: 24.0725 - mae: 4.8183 - val_loss: 23.8514 - val_mae: 4.7982\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8366 - mae: 4.7937\n",
            "Epoch 51: val_loss improved from 23.85142 to 23.61642, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 23.8366 - mae: 4.7937 - val_loss: 23.6164 - val_mae: 4.7737\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.6019 - mae: 4.7692\n",
            "Epoch 52: val_loss improved from 23.61642 to 23.38283, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 123ms/step - loss: 23.6019 - mae: 4.7692 - val_loss: 23.3828 - val_mae: 4.7492\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3685 - mae: 4.7447\n",
            "Epoch 53: val_loss improved from 23.38283 to 23.15030, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 135ms/step - loss: 23.3685 - mae: 4.7447 - val_loss: 23.1503 - val_mae: 4.7246\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1363 - mae: 4.7201\n",
            "Epoch 54: val_loss improved from 23.15030 to 22.91900, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 23.1363 - mae: 4.7201 - val_loss: 22.9190 - val_mae: 4.7001\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.9053 - mae: 4.6956\n",
            "Epoch 55: val_loss improved from 22.91900 to 22.68907, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 22.9053 - mae: 4.6956 - val_loss: 22.6891 - val_mae: 4.6755\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6755 - mae: 4.6711\n",
            "Epoch 56: val_loss improved from 22.68907 to 22.46034, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 22.6755 - mae: 4.6711 - val_loss: 22.4603 - val_mae: 4.6510\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4470 - mae: 4.6465\n",
            "Epoch 57: val_loss improved from 22.46034 to 22.23267, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 22.4470 - mae: 4.6465 - val_loss: 22.2327 - val_mae: 4.6265\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2196 - mae: 4.6220\n",
            "Epoch 58: val_loss improved from 22.23267 to 22.00650, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 22.2196 - mae: 4.6220 - val_loss: 22.0065 - val_mae: 4.6020\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9936 - mae: 4.5975\n",
            "Epoch 59: val_loss improved from 22.00650 to 21.78131, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 21.9936 - mae: 4.5975 - val_loss: 21.7813 - val_mae: 4.5774\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7686 - mae: 4.5730\n",
            "Epoch 60: val_loss improved from 21.78131 to 21.55749, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 21.7686 - mae: 4.5730 - val_loss: 21.5575 - val_mae: 4.5529\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5451 - mae: 4.5484\n",
            "Epoch 61: val_loss improved from 21.55749 to 21.33469, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 21.5451 - mae: 4.5484 - val_loss: 21.3347 - val_mae: 4.5284\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3226 - mae: 4.5239\n",
            "Epoch 62: val_loss improved from 21.33469 to 21.11335, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.3226 - mae: 4.5239 - val_loss: 21.1134 - val_mae: 4.5039\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.1014 - mae: 4.4994\n",
            "Epoch 63: val_loss improved from 21.11335 to 20.89313, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.1014 - mae: 4.4994 - val_loss: 20.8931 - val_mae: 4.4794\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8814 - mae: 4.4749\n",
            "Epoch 64: val_loss improved from 20.89313 to 20.67422, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 20.8814 - mae: 4.4749 - val_loss: 20.6742 - val_mae: 4.4549\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6627 - mae: 4.4504\n",
            "Epoch 65: val_loss improved from 20.67422 to 20.45636, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.6627 - mae: 4.4504 - val_loss: 20.4564 - val_mae: 4.4304\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4451 - mae: 4.4259\n",
            "Epoch 66: val_loss improved from 20.45636 to 20.23985, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 20.4451 - mae: 4.4259 - val_loss: 20.2399 - val_mae: 4.4059\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2288 - mae: 4.4014\n",
            "Epoch 67: val_loss improved from 20.23985 to 20.02446, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.2288 - mae: 4.4014 - val_loss: 20.0245 - val_mae: 4.3813\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.0136 - mae: 4.3769\n",
            "Epoch 68: val_loss improved from 20.02446 to 19.81045, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 20.0136 - mae: 4.3769 - val_loss: 19.8104 - val_mae: 4.3568\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7998 - mae: 4.3524\n",
            "Epoch 69: val_loss improved from 19.81045 to 19.59763, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.7998 - mae: 4.3524 - val_loss: 19.5976 - val_mae: 4.3324\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5871 - mae: 4.3279\n",
            "Epoch 70: val_loss improved from 19.59763 to 19.38599, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.5871 - mae: 4.3279 - val_loss: 19.3860 - val_mae: 4.3079\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3757 - mae: 4.3034\n",
            "Epoch 71: val_loss improved from 19.38599 to 19.17551, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.3757 - mae: 4.3034 - val_loss: 19.1755 - val_mae: 4.2834\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1656 - mae: 4.2789\n",
            "Epoch 72: val_loss improved from 19.17551 to 18.96630, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.1656 - mae: 4.2789 - val_loss: 18.9663 - val_mae: 4.2589\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9566 - mae: 4.2544\n",
            "Epoch 73: val_loss improved from 18.96630 to 18.75832, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.9566 - mae: 4.2544 - val_loss: 18.7583 - val_mae: 4.2344\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7489 - mae: 4.2299\n",
            "Epoch 74: val_loss improved from 18.75832 to 18.55147, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 18.7489 - mae: 4.2299 - val_loss: 18.5515 - val_mae: 4.2099\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5422 - mae: 4.2054\n",
            "Epoch 75: val_loss improved from 18.55147 to 18.34624, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.5422 - mae: 4.2054 - val_loss: 18.3462 - val_mae: 4.1854\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3371 - mae: 4.1810\n",
            "Epoch 76: val_loss improved from 18.34624 to 18.14176, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 18.3371 - mae: 4.1810 - val_loss: 18.1418 - val_mae: 4.1609\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1330 - mae: 4.1565\n",
            "Epoch 77: val_loss improved from 18.14176 to 17.93866, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.1330 - mae: 4.1565 - val_loss: 17.9387 - val_mae: 4.1365\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9302 - mae: 4.1320\n",
            "Epoch 78: val_loss improved from 17.93866 to 17.73674, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 17.9302 - mae: 4.1320 - val_loss: 17.7367 - val_mae: 4.1120\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7285 - mae: 4.1075\n",
            "Epoch 79: val_loss improved from 17.73674 to 17.53626, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.7285 - mae: 4.1075 - val_loss: 17.5363 - val_mae: 4.0875\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5282 - mae: 4.0831\n",
            "Epoch 80: val_loss improved from 17.53626 to 17.33681, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 17.5282 - mae: 4.0831 - val_loss: 17.3368 - val_mae: 4.0631\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3291 - mae: 4.0586\n",
            "Epoch 81: val_loss improved from 17.33681 to 17.13864, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.3291 - mae: 4.0586 - val_loss: 17.1386 - val_mae: 4.0386\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1311 - mae: 4.0342\n",
            "Epoch 82: val_loss improved from 17.13864 to 16.94184, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 17.1311 - mae: 4.0342 - val_loss: 16.9418 - val_mae: 4.0142\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9344 - mae: 4.0097\n",
            "Epoch 83: val_loss improved from 16.94184 to 16.74601, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.9344 - mae: 4.0097 - val_loss: 16.7460 - val_mae: 3.9897\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7387 - mae: 3.9853\n",
            "Epoch 84: val_loss improved from 16.74601 to 16.55160, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 16.7387 - mae: 3.9853 - val_loss: 16.5516 - val_mae: 3.9653\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5445 - mae: 3.9608\n",
            "Epoch 85: val_loss improved from 16.55160 to 16.35819, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.5445 - mae: 3.9608 - val_loss: 16.3582 - val_mae: 3.9408\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3514 - mae: 3.9363\n",
            "Epoch 86: val_loss improved from 16.35819 to 16.16615, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 16.3514 - mae: 3.9363 - val_loss: 16.1661 - val_mae: 3.9164\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1596 - mae: 3.9119\n",
            "Epoch 87: val_loss improved from 16.16615 to 15.97521, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.1596 - mae: 3.9119 - val_loss: 15.9752 - val_mae: 3.8919\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9689 - mae: 3.8874\n",
            "Epoch 88: val_loss improved from 15.97521 to 15.78557, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.9689 - mae: 3.8874 - val_loss: 15.7856 - val_mae: 3.8675\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7795 - mae: 3.8630\n",
            "Epoch 89: val_loss improved from 15.78557 to 15.59717, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.7795 - mae: 3.8630 - val_loss: 15.5972 - val_mae: 3.8430\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5914 - mae: 3.8386\n",
            "Epoch 90: val_loss improved from 15.59717 to 15.40993, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.5914 - mae: 3.8386 - val_loss: 15.4099 - val_mae: 3.8186\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.4044 - mae: 3.8141\n",
            "Epoch 91: val_loss improved from 15.40993 to 15.22402, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.4044 - mae: 3.8141 - val_loss: 15.2240 - val_mae: 3.7942\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2187 - mae: 3.7897\n",
            "Epoch 92: val_loss improved from 15.22402 to 15.03924, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 15.2187 - mae: 3.7897 - val_loss: 15.0392 - val_mae: 3.7697\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0342 - mae: 3.7653\n",
            "Epoch 93: val_loss improved from 15.03924 to 14.85569, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.0342 - mae: 3.7653 - val_loss: 14.8557 - val_mae: 3.7453\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8508 - mae: 3.7409\n",
            "Epoch 94: val_loss improved from 14.85569 to 14.67349, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 14.8508 - mae: 3.7409 - val_loss: 14.6735 - val_mae: 3.7209\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6688 - mae: 3.7165\n",
            "Epoch 95: val_loss improved from 14.67349 to 14.49232, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.6688 - mae: 3.7165 - val_loss: 14.4923 - val_mae: 3.6965\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4879 - mae: 3.6921\n",
            "Epoch 96: val_loss improved from 14.49232 to 14.31244, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 14.4879 - mae: 3.6921 - val_loss: 14.3124 - val_mae: 3.6721\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.3083 - mae: 3.6676\n",
            "Epoch 97: val_loss improved from 14.31244 to 14.13380, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 14.3083 - mae: 3.6676 - val_loss: 14.1338 - val_mae: 3.6477\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1298 - mae: 3.6433\n",
            "Epoch 98: val_loss improved from 14.13380 to 13.95649, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 14.1298 - mae: 3.6433 - val_loss: 13.9565 - val_mae: 3.6233\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9527 - mae: 3.6188\n",
            "Epoch 99: val_loss improved from 13.95649 to 13.78017, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 13.9527 - mae: 3.6188 - val_loss: 13.7802 - val_mae: 3.5989\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7767 - mae: 3.5944\n",
            "Epoch 100: val_loss improved from 13.78017 to 13.60521, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 13.7767 - mae: 3.5944 - val_loss: 13.6052 - val_mae: 3.5745\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.759998321533203, RMSE:3.709447145462036, MAE:3.5920679569244385, R2:-15.055156886640493\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897, -15.055156886640493]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_250\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_151 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_180 (Embedding)      (None, 17, 900)      649800      ['input_151[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_181 (Embedding)      (None, 17, 900)      649800      ['input_151[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_182 (Embedding)      (None, 17, 900)      649800      ['input_151[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_183 (Embedding)      (None, 17, 900)      649800      ['input_151[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_225 (Conv1D)            (None, 17, 100)      270100      ['embedding_180[0][0]',          \n",
            "                                                                  'embedding_181[0][0]',          \n",
            "                                                                  'embedding_182[0][0]',          \n",
            "                                                                  'embedding_183[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_226 (Conv1D)            (None, 17, 100)      270100      ['embedding_180[0][0]',          \n",
            "                                                                  'embedding_181[0][0]',          \n",
            "                                                                  'embedding_182[0][0]',          \n",
            "                                                                  'embedding_183[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_227 (Conv1D)            (None, 17, 100)      270100      ['embedding_180[0][0]',          \n",
            "                                                                  'embedding_181[0][0]',          \n",
            "                                                                  'embedding_182[0][0]',          \n",
            "                                                                  'embedding_183[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_540 (Glob  (None, 100)         0           ['conv1d_225[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_541 (Glob  (None, 100)         0           ['conv1d_225[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_542 (Glob  (None, 100)         0           ['conv1d_225[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_543 (Glob  (None, 100)         0           ['conv1d_225[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_544 (Glob  (None, 100)         0           ['conv1d_226[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_545 (Glob  (None, 100)         0           ['conv1d_226[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_546 (Glob  (None, 100)         0           ['conv1d_226[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_547 (Glob  (None, 100)         0           ['conv1d_226[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_548 (Glob  (None, 100)         0           ['conv1d_227[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_549 (Glob  (None, 100)         0           ['conv1d_227[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_550 (Glob  (None, 100)         0           ['conv1d_227[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_551 (Glob  (None, 100)         0           ['conv1d_227[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_152 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_105 (Add)                  (None, 100)          0           ['global_max_pooling1d_540[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_541[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_542[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_543[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_106 (Add)                  (None, 100)          0           ['global_max_pooling1d_544[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_545[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_546[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_547[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_107 (Add)                  (None, 100)          0           ['global_max_pooling1d_548[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_549[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_550[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_551[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_290 (Dense)              (None, 1000)         11000       ['input_152[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_75 (Concatenate)   (None, 1300)         0           ['add_105[0][0]',                \n",
            "                                                                  'add_106[0][0]',                \n",
            "                                                                  'add_107[0][0]',                \n",
            "                                                                  'dense_290[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_75[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_275 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_275[0][0]']            \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                  \n",
            " dropout_276 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_276[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_277 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_277[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_278 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_278[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_279 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_291 (Dense)              (None, 1)            3           ['dropout_279[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.2416 - mae: 6.0319\n",
            "Epoch 1: val_loss improved from inf to 36.87320, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 117ms/step - loss: 37.2416 - mae: 6.0319 - val_loss: 36.8732 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8451 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87320 to 36.57418, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.8451 - mae: 5.9990 - val_loss: 36.5742 - val_mae: 5.9788\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5466 - mae: 5.9741\n",
            "Epoch 3: val_loss improved from 36.57418 to 36.27697, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 36.5466 - mae: 5.9741 - val_loss: 36.2770 - val_mae: 5.9539\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2499 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27697 to 35.98129, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 36.2499 - mae: 5.9492 - val_loss: 35.9813 - val_mae: 5.9290\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9547 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98129 to 35.68731, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 35.9547 - mae: 5.9243 - val_loss: 35.6873 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6611 - mae: 5.8995\n",
            "Epoch 6: val_loss improved from 35.68731 to 35.39511, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 35.6611 - mae: 5.8995 - val_loss: 35.3951 - val_mae: 5.8794\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3691 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39511 to 35.10427, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.3691 - mae: 5.8747 - val_loss: 35.1043 - val_mae: 5.8546\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0787 - mae: 5.8500\n",
            "Epoch 8: val_loss improved from 35.10427 to 34.81483, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 35.0787 - mae: 5.8500 - val_loss: 34.8148 - val_mae: 5.8298\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7897 - mae: 5.8252\n",
            "Epoch 9: val_loss improved from 34.81483 to 34.52712, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 34.7897 - mae: 5.8252 - val_loss: 34.5271 - val_mae: 5.8051\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5023 - mae: 5.8005\n",
            "Epoch 10: val_loss improved from 34.52712 to 34.24065, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 34.5023 - mae: 5.8005 - val_loss: 34.2406 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2161 - mae: 5.7758\n",
            "Epoch 11: val_loss improved from 34.24065 to 33.95575, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 34.2161 - mae: 5.7758 - val_loss: 33.9558 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9315 - mae: 5.7511\n",
            "Epoch 12: val_loss improved from 33.95575 to 33.67218, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.9315 - mae: 5.7511 - val_loss: 33.6722 - val_mae: 5.7310\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6482 - mae: 5.7264\n",
            "Epoch 13: val_loss improved from 33.67218 to 33.39004, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 33.6482 - mae: 5.7264 - val_loss: 33.3900 - val_mae: 5.7063\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3664 - mae: 5.7017\n",
            "Epoch 14: val_loss improved from 33.39004 to 33.10903, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.3664 - mae: 5.7017 - val_loss: 33.1090 - val_mae: 5.6816\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0859 - mae: 5.6771\n",
            "Epoch 15: val_loss improved from 33.10903 to 32.82940, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.0859 - mae: 5.6771 - val_loss: 32.8294 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8064 - mae: 5.6524\n",
            "Epoch 16: val_loss improved from 32.82940 to 32.55146, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 32.8064 - mae: 5.6524 - val_loss: 32.5515 - val_mae: 5.6323\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5286 - mae: 5.6278\n",
            "Epoch 17: val_loss improved from 32.55146 to 32.27461, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 32.5286 - mae: 5.6278 - val_loss: 32.2746 - val_mae: 5.6077\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2519 - mae: 5.6032\n",
            "Epoch 18: val_loss improved from 32.27461 to 31.99897, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 32.2519 - mae: 5.6032 - val_loss: 31.9990 - val_mae: 5.5831\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9766 - mae: 5.5785\n",
            "Epoch 19: val_loss improved from 31.99897 to 31.72451, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 31.9766 - mae: 5.5785 - val_loss: 31.7245 - val_mae: 5.5584\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7025 - mae: 5.5539\n",
            "Epoch 20: val_loss improved from 31.72451 to 31.45131, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 31.7025 - mae: 5.5539 - val_loss: 31.4513 - val_mae: 5.5338\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4295 - mae: 5.5293\n",
            "Epoch 21: val_loss improved from 31.45131 to 31.17960, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.4295 - mae: 5.5293 - val_loss: 31.1796 - val_mae: 5.5092\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1581 - mae: 5.5047\n",
            "Epoch 22: val_loss improved from 31.17960 to 30.90891, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 108ms/step - loss: 31.1581 - mae: 5.5047 - val_loss: 30.9089 - val_mae: 5.4846\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8877 - mae: 5.4801\n",
            "Epoch 23: val_loss improved from 30.90891 to 30.63964, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.8877 - mae: 5.4801 - val_loss: 30.6396 - val_mae: 5.4600\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6186 - mae: 5.4555\n",
            "Epoch 24: val_loss improved from 30.63964 to 30.37168, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 30.6186 - mae: 5.4555 - val_loss: 30.3717 - val_mae: 5.4354\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3509 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37168 to 30.10477, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 30.3509 - mae: 5.4308 - val_loss: 30.1048 - val_mae: 5.4108\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0843 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10477 to 29.83908, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.0843 - mae: 5.4062 - val_loss: 29.8391 - val_mae: 5.3862\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8189 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83908 to 29.57488, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 29.8189 - mae: 5.3816 - val_loss: 29.5749 - val_mae: 5.3616\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5548 - mae: 5.3571\n",
            "Epoch 28: val_loss improved from 29.57488 to 29.31182, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 29.5548 - mae: 5.3571 - val_loss: 29.3118 - val_mae: 5.3370\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2920 - mae: 5.3325\n",
            "Epoch 29: val_loss improved from 29.31182 to 29.04993, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.2920 - mae: 5.3325 - val_loss: 29.0499 - val_mae: 5.3124\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0304 - mae: 5.3079\n",
            "Epoch 30: val_loss improved from 29.04993 to 28.78930, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 29.0304 - mae: 5.3079 - val_loss: 28.7893 - val_mae: 5.2878\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7699 - mae: 5.2833\n",
            "Epoch 31: val_loss improved from 28.78930 to 28.52998, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 28.7699 - mae: 5.2833 - val_loss: 28.5300 - val_mae: 5.2632\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5108 - mae: 5.2587\n",
            "Epoch 32: val_loss improved from 28.52998 to 28.27180, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 28.5108 - mae: 5.2587 - val_loss: 28.2718 - val_mae: 5.2387\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2528 - mae: 5.2341\n",
            "Epoch 33: val_loss improved from 28.27180 to 28.01479, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.2528 - mae: 5.2341 - val_loss: 28.0148 - val_mae: 5.2141\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9961 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01479 to 27.75904, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.9961 - mae: 5.2095 - val_loss: 27.7590 - val_mae: 5.1895\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7406 - mae: 5.1850\n",
            "Epoch 35: val_loss improved from 27.75904 to 27.50456, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.7406 - mae: 5.1850 - val_loss: 27.5046 - val_mae: 5.1649\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4863 - mae: 5.1604\n",
            "Epoch 36: val_loss improved from 27.50456 to 27.25142, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.4863 - mae: 5.1604 - val_loss: 27.2514 - val_mae: 5.1403\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2334 - mae: 5.1358\n",
            "Epoch 37: val_loss improved from 27.25142 to 26.99925, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 27.2334 - mae: 5.1358 - val_loss: 26.9992 - val_mae: 5.1158\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9815 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99925 to 26.74859, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.9815 - mae: 5.1112 - val_loss: 26.7486 - val_mae: 5.0912\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7309 - mae: 5.0867\n",
            "Epoch 39: val_loss improved from 26.74859 to 26.49910, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.7309 - mae: 5.0867 - val_loss: 26.4991 - val_mae: 5.0666\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4817 - mae: 5.0621\n",
            "Epoch 40: val_loss improved from 26.49910 to 26.25060, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.4817 - mae: 5.0621 - val_loss: 26.2506 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2335 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.25060 to 26.00360, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.2335 - mae: 5.0375 - val_loss: 26.0036 - val_mae: 5.0175\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9866 - mae: 5.0130\n",
            "Epoch 42: val_loss improved from 26.00360 to 25.75768, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 25.9866 - mae: 5.0130 - val_loss: 25.7577 - val_mae: 4.9929\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7411 - mae: 4.9884\n",
            "Epoch 43: val_loss improved from 25.75768 to 25.51284, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.7411 - mae: 4.9884 - val_loss: 25.5128 - val_mae: 4.9684\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4966 - mae: 4.9639\n",
            "Epoch 44: val_loss improved from 25.51284 to 25.26957, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.4966 - mae: 4.9639 - val_loss: 25.2696 - val_mae: 4.9438\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2534 - mae: 4.9393\n",
            "Epoch 45: val_loss improved from 25.26957 to 25.02742, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.2534 - mae: 4.9393 - val_loss: 25.0274 - val_mae: 4.9193\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0116 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02742 to 24.78626, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.0116 - mae: 4.9147 - val_loss: 24.7863 - val_mae: 4.8947\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7707 - mae: 4.8902\n",
            "Epoch 47: val_loss improved from 24.78626 to 24.54666, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.7707 - mae: 4.8902 - val_loss: 24.5467 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5312 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54666 to 24.30819, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.5312 - mae: 4.8656 - val_loss: 24.3082 - val_mae: 4.8456\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2930 - mae: 4.8411\n",
            "Epoch 49: val_loss improved from 24.30819 to 24.07088, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.2930 - mae: 4.8411 - val_loss: 24.0709 - val_mae: 4.8211\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0559 - mae: 4.8166\n",
            "Epoch 50: val_loss improved from 24.07088 to 23.83479, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.0559 - mae: 4.8166 - val_loss: 23.8348 - val_mae: 4.7965\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8200 - mae: 4.7920\n",
            "Epoch 51: val_loss improved from 23.83479 to 23.60014, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 23.8200 - mae: 4.7920 - val_loss: 23.6001 - val_mae: 4.7720\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5855 - mae: 4.7675\n",
            "Epoch 52: val_loss improved from 23.60014 to 23.36644, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 23.5855 - mae: 4.7675 - val_loss: 23.3664 - val_mae: 4.7474\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3521 - mae: 4.7429\n",
            "Epoch 53: val_loss improved from 23.36644 to 23.13391, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.3521 - mae: 4.7429 - val_loss: 23.1339 - val_mae: 4.7229\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1198 - mae: 4.7184\n",
            "Epoch 54: val_loss improved from 23.13391 to 22.90292, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 23.1198 - mae: 4.7184 - val_loss: 22.9029 - val_mae: 4.6984\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8890 - mae: 4.6939\n",
            "Epoch 55: val_loss improved from 22.90292 to 22.67296, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.8890 - mae: 4.6939 - val_loss: 22.6730 - val_mae: 4.6738\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6592 - mae: 4.6693\n",
            "Epoch 56: val_loss improved from 22.67296 to 22.44436, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 22.6592 - mae: 4.6693 - val_loss: 22.4444 - val_mae: 4.6493\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4308 - mae: 4.6448\n",
            "Epoch 57: val_loss improved from 22.44436 to 22.21677, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.4308 - mae: 4.6448 - val_loss: 22.2168 - val_mae: 4.6248\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2036 - mae: 4.6203\n",
            "Epoch 58: val_loss improved from 22.21677 to 21.99047, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 22.2036 - mae: 4.6203 - val_loss: 21.9905 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9775 - mae: 4.5958\n",
            "Epoch 59: val_loss improved from 21.99047 to 21.76554, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.9775 - mae: 4.5958 - val_loss: 21.7655 - val_mae: 4.5757\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7527 - mae: 4.5712\n",
            "Epoch 60: val_loss improved from 21.76554 to 21.54175, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 21.7527 - mae: 4.5712 - val_loss: 21.5418 - val_mae: 4.5512\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5292 - mae: 4.5467\n",
            "Epoch 61: val_loss improved from 21.54175 to 21.31902, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.5292 - mae: 4.5467 - val_loss: 21.3190 - val_mae: 4.5267\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3067 - mae: 4.5222\n",
            "Epoch 62: val_loss improved from 21.31902 to 21.09774, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.3067 - mae: 4.5222 - val_loss: 21.0977 - val_mae: 4.5022\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0857 - mae: 4.4977\n",
            "Epoch 63: val_loss improved from 21.09774 to 20.87757, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.0857 - mae: 4.4977 - val_loss: 20.8776 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8658 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87757 to 20.65862, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 20.8658 - mae: 4.4731 - val_loss: 20.6586 - val_mae: 4.4531\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6472 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65862 to 20.44089, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 20.6472 - mae: 4.4486 - val_loss: 20.4409 - val_mae: 4.4286\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4297 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44089 to 20.22461, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 20.4297 - mae: 4.4241 - val_loss: 20.2246 - val_mae: 4.4041\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2136 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22461 to 20.00935, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 20.2136 - mae: 4.3996 - val_loss: 20.0093 - val_mae: 4.3796\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9986 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00935 to 19.79530, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 19.9986 - mae: 4.3751 - val_loss: 19.7953 - val_mae: 4.3551\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7848 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79530 to 19.58265, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.7848 - mae: 4.3506 - val_loss: 19.5826 - val_mae: 4.3306\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5722 - mae: 4.3262\n",
            "Epoch 70: val_loss improved from 19.58265 to 19.37116, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 19.5722 - mae: 4.3262 - val_loss: 19.3712 - val_mae: 4.3061\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3610 - mae: 4.3017\n",
            "Epoch 71: val_loss improved from 19.37116 to 19.16076, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.3610 - mae: 4.3017 - val_loss: 19.1608 - val_mae: 4.2816\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1509 - mae: 4.2772\n",
            "Epoch 72: val_loss improved from 19.16076 to 18.95161, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.1509 - mae: 4.2772 - val_loss: 18.9516 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9419 - mae: 4.2527\n",
            "Epoch 73: val_loss improved from 18.95161 to 18.74381, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.9419 - mae: 4.2527 - val_loss: 18.7438 - val_mae: 4.2327\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7343 - mae: 4.2282\n",
            "Epoch 74: val_loss improved from 18.74381 to 18.53712, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.7343 - mae: 4.2282 - val_loss: 18.5371 - val_mae: 4.2082\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5280 - mae: 4.2037\n",
            "Epoch 75: val_loss improved from 18.53712 to 18.33147, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.5280 - mae: 4.2037 - val_loss: 18.3315 - val_mae: 4.1837\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3226 - mae: 4.1792\n",
            "Epoch 76: val_loss improved from 18.33147 to 18.12750, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.3226 - mae: 4.1792 - val_loss: 18.1275 - val_mae: 4.1592\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1187 - mae: 4.1548\n",
            "Epoch 77: val_loss improved from 18.12750 to 17.92455, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.1187 - mae: 4.1548 - val_loss: 17.9246 - val_mae: 4.1348\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9160 - mae: 4.1303\n",
            "Epoch 78: val_loss improved from 17.92455 to 17.72268, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.9160 - mae: 4.1303 - val_loss: 17.7227 - val_mae: 4.1103\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7144 - mae: 4.1058\n",
            "Epoch 79: val_loss improved from 17.72268 to 17.52205, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.7144 - mae: 4.1058 - val_loss: 17.5221 - val_mae: 4.0858\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5140 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52205 to 17.32289, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.5140 - mae: 4.0813 - val_loss: 17.3229 - val_mae: 4.0613\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3149 - mae: 4.0569\n",
            "Epoch 81: val_loss improved from 17.32289 to 17.12481, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.3149 - mae: 4.0569 - val_loss: 17.1248 - val_mae: 4.0369\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1172 - mae: 4.0324\n",
            "Epoch 82: val_loss improved from 17.12481 to 16.92773, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.1172 - mae: 4.0324 - val_loss: 16.9277 - val_mae: 4.0124\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9204 - mae: 4.0080\n",
            "Epoch 83: val_loss improved from 16.92773 to 16.73225, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.9204 - mae: 4.0080 - val_loss: 16.7323 - val_mae: 3.9880\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7250 - mae: 3.9835\n",
            "Epoch 84: val_loss improved from 16.73225 to 16.53777, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 16.7250 - mae: 3.9835 - val_loss: 16.5378 - val_mae: 3.9635\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5308 - mae: 3.9591\n",
            "Epoch 85: val_loss improved from 16.53777 to 16.34459, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 16.5308 - mae: 3.9591 - val_loss: 16.3446 - val_mae: 3.9391\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3380 - mae: 3.9346\n",
            "Epoch 86: val_loss improved from 16.34459 to 16.15242, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.3380 - mae: 3.9346 - val_loss: 16.1524 - val_mae: 3.9146\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1461 - mae: 3.9102\n",
            "Epoch 87: val_loss improved from 16.15242 to 15.96187, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.1461 - mae: 3.9102 - val_loss: 15.9619 - val_mae: 3.8902\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9556 - mae: 3.8857\n",
            "Epoch 88: val_loss improved from 15.96187 to 15.77227, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 15.9556 - mae: 3.8857 - val_loss: 15.7723 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7663 - mae: 3.8613\n",
            "Epoch 89: val_loss improved from 15.77227 to 15.58400, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.7663 - mae: 3.8613 - val_loss: 15.5840 - val_mae: 3.8413\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5782 - mae: 3.8369\n",
            "Epoch 90: val_loss improved from 15.58400 to 15.39682, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.5782 - mae: 3.8369 - val_loss: 15.3968 - val_mae: 3.8169\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3913 - mae: 3.8124\n",
            "Epoch 91: val_loss improved from 15.39682 to 15.21097, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.3913 - mae: 3.8124 - val_loss: 15.2110 - val_mae: 3.7924\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2057 - mae: 3.7880\n",
            "Epoch 92: val_loss improved from 15.21097 to 15.02631, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.2057 - mae: 3.7880 - val_loss: 15.0263 - val_mae: 3.7680\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0212 - mae: 3.7636\n",
            "Epoch 93: val_loss improved from 15.02631 to 14.84287, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 15.0212 - mae: 3.7636 - val_loss: 14.8429 - val_mae: 3.7436\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8380 - mae: 3.7392\n",
            "Epoch 94: val_loss improved from 14.84287 to 14.66067, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 14.8380 - mae: 3.7392 - val_loss: 14.6607 - val_mae: 3.7192\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6559 - mae: 3.7148\n",
            "Epoch 95: val_loss improved from 14.66067 to 14.47971, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 14.6559 - mae: 3.7148 - val_loss: 14.4797 - val_mae: 3.6948\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4753 - mae: 3.6903\n",
            "Epoch 96: val_loss improved from 14.47971 to 14.29977, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 14.4753 - mae: 3.6903 - val_loss: 14.2998 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2956 - mae: 3.6659\n",
            "Epoch 97: val_loss improved from 14.29977 to 14.12126, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 14.2956 - mae: 3.6659 - val_loss: 14.1213 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1174 - mae: 3.6415\n",
            "Epoch 98: val_loss improved from 14.12126 to 13.94382, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 14.1174 - mae: 3.6415 - val_loss: 13.9438 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9402 - mae: 3.6171\n",
            "Epoch 99: val_loss improved from 13.94382 to 13.76787, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 13.9402 - mae: 3.6171 - val_loss: 13.7679 - val_mae: 3.5972\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7643 - mae: 3.5927\n",
            "Epoch 100: val_loss improved from 13.76787 to 13.59299, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 107ms/step - loss: 13.7643 - mae: 3.5927 - val_loss: 13.5930 - val_mae: 3.5728\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.747720718383789, RMSE:3.707791805267334, MAE:3.5903584957122803, R2:-15.040831191761544\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897, -15.055156886640493, -15.040831191761544]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_256\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_153 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_184 (Embedding)      (None, 17, 900)      649800      ['input_153[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_185 (Embedding)      (None, 17, 900)      649800      ['input_153[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_186 (Embedding)      (None, 17, 900)      649800      ['input_153[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_187 (Embedding)      (None, 17, 900)      649800      ['input_153[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_228 (Conv1D)            (None, 17, 100)      270100      ['embedding_184[0][0]',          \n",
            "                                                                  'embedding_185[0][0]',          \n",
            "                                                                  'embedding_186[0][0]',          \n",
            "                                                                  'embedding_187[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_229 (Conv1D)            (None, 17, 100)      270100      ['embedding_184[0][0]',          \n",
            "                                                                  'embedding_185[0][0]',          \n",
            "                                                                  'embedding_186[0][0]',          \n",
            "                                                                  'embedding_187[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_230 (Conv1D)            (None, 17, 100)      270100      ['embedding_184[0][0]',          \n",
            "                                                                  'embedding_185[0][0]',          \n",
            "                                                                  'embedding_186[0][0]',          \n",
            "                                                                  'embedding_187[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_552 (Glob  (None, 100)         0           ['conv1d_228[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_553 (Glob  (None, 100)         0           ['conv1d_228[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_554 (Glob  (None, 100)         0           ['conv1d_228[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_555 (Glob  (None, 100)         0           ['conv1d_228[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_556 (Glob  (None, 100)         0           ['conv1d_229[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_557 (Glob  (None, 100)         0           ['conv1d_229[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_558 (Glob  (None, 100)         0           ['conv1d_229[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_559 (Glob  (None, 100)         0           ['conv1d_229[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_560 (Glob  (None, 100)         0           ['conv1d_230[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_561 (Glob  (None, 100)         0           ['conv1d_230[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_562 (Glob  (None, 100)         0           ['conv1d_230[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_563 (Glob  (None, 100)         0           ['conv1d_230[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_154 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_108 (Add)                  (None, 100)          0           ['global_max_pooling1d_552[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_553[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_554[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_555[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_109 (Add)                  (None, 100)          0           ['global_max_pooling1d_556[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_557[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_558[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_559[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_110 (Add)                  (None, 100)          0           ['global_max_pooling1d_560[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_561[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_562[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_563[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_292 (Dense)              (None, 1000)         11000       ['input_154[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_76 (Concatenate)   (None, 1300)         0           ['add_108[0][0]',                \n",
            "                                                                  'add_109[0][0]',                \n",
            "                                                                  'add_110[0][0]',                \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                  'dense_292[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_76[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_280 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_280[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_281 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_281[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_282 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_282[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_283 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_283[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_284 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_293 (Dense)              (None, 1)            3           ['dropout_284[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.1450 - mae: 6.0240\n",
            "Epoch 1: val_loss improved from inf to 36.87269, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 7s 117ms/step - loss: 37.1450 - mae: 6.0240 - val_loss: 36.8727 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8446 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87269 to 36.57359, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.8446 - mae: 5.9990 - val_loss: 36.5736 - val_mae: 5.9787\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5461 - mae: 5.9740\n",
            "Epoch 3: val_loss improved from 36.57359 to 36.27616, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.5461 - mae: 5.9740 - val_loss: 36.2762 - val_mae: 5.9538\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2490 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27616 to 35.98064, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 36.2490 - mae: 5.9492 - val_loss: 35.9806 - val_mae: 5.9289\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9538 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98064 to 35.68663, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.9538 - mae: 5.9243 - val_loss: 35.6866 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6603 - mae: 5.8994\n",
            "Epoch 6: val_loss improved from 35.68663 to 35.39398, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 35.6603 - mae: 5.8994 - val_loss: 35.3940 - val_mae: 5.8793\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3681 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39398 to 35.10333, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.3681 - mae: 5.8747 - val_loss: 35.1033 - val_mae: 5.8545\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0778 - mae: 5.8499\n",
            "Epoch 8: val_loss improved from 35.10333 to 34.81387, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 35.0778 - mae: 5.8499 - val_loss: 34.8139 - val_mae: 5.8297\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7887 - mae: 5.8251\n",
            "Epoch 9: val_loss improved from 34.81387 to 34.52617, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.7887 - mae: 5.8251 - val_loss: 34.5262 - val_mae: 5.8050\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5013 - mae: 5.8004\n",
            "Epoch 10: val_loss improved from 34.52617 to 34.23979, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 34.5013 - mae: 5.8004 - val_loss: 34.2398 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2152 - mae: 5.7757\n",
            "Epoch 11: val_loss improved from 34.23979 to 33.95474, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.2152 - mae: 5.7757 - val_loss: 33.9547 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9306 - mae: 5.7510\n",
            "Epoch 12: val_loss improved from 33.95474 to 33.67118, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.9306 - mae: 5.7510 - val_loss: 33.6712 - val_mae: 5.7309\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6474 - mae: 5.7263\n",
            "Epoch 13: val_loss improved from 33.67118 to 33.38895, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 33.6474 - mae: 5.7263 - val_loss: 33.3890 - val_mae: 5.7062\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3654 - mae: 5.7016\n",
            "Epoch 14: val_loss improved from 33.38895 to 33.10824, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.3654 - mae: 5.7016 - val_loss: 33.1082 - val_mae: 5.6815\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0849 - mae: 5.6770\n",
            "Epoch 15: val_loss improved from 33.10824 to 32.82867, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.0849 - mae: 5.6770 - val_loss: 32.8287 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8056 - mae: 5.6523\n",
            "Epoch 16: val_loss improved from 32.82867 to 32.55048, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 32.8056 - mae: 5.6523 - val_loss: 32.5505 - val_mae: 5.6322\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5277 - mae: 5.6277\n",
            "Epoch 17: val_loss improved from 32.55048 to 32.27343, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 32.5277 - mae: 5.6277 - val_loss: 32.2734 - val_mae: 5.6076\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2510 - mae: 5.6031\n",
            "Epoch 18: val_loss improved from 32.27343 to 31.99794, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 32.2510 - mae: 5.6031 - val_loss: 31.9979 - val_mae: 5.5830\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9756 - mae: 5.5784\n",
            "Epoch 19: val_loss improved from 31.99794 to 31.72359, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 31.9756 - mae: 5.5784 - val_loss: 31.7236 - val_mae: 5.5583\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7015 - mae: 5.5538\n",
            "Epoch 20: val_loss improved from 31.72359 to 31.45047, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.7015 - mae: 5.5538 - val_loss: 31.4505 - val_mae: 5.5337\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4286 - mae: 5.5292\n",
            "Epoch 21: val_loss improved from 31.45047 to 31.17871, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.4286 - mae: 5.5292 - val_loss: 31.1787 - val_mae: 5.5091\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1572 - mae: 5.5046\n",
            "Epoch 22: val_loss improved from 31.17871 to 30.90795, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 31.1572 - mae: 5.5046 - val_loss: 30.9079 - val_mae: 5.4845\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8868 - mae: 5.4800\n",
            "Epoch 23: val_loss improved from 30.90795 to 30.63879, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 30.8868 - mae: 5.4800 - val_loss: 30.6388 - val_mae: 5.4599\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6177 - mae: 5.4554\n",
            "Epoch 24: val_loss improved from 30.63879 to 30.37082, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.6177 - mae: 5.4554 - val_loss: 30.3708 - val_mae: 5.4353\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3499 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37082 to 30.10400, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 30.3499 - mae: 5.4308 - val_loss: 30.1040 - val_mae: 5.4107\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0834 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10400 to 29.83826, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 30.0834 - mae: 5.4062 - val_loss: 29.8383 - val_mae: 5.3861\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8179 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83826 to 29.57412, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.8179 - mae: 5.3816 - val_loss: 29.5741 - val_mae: 5.3615\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5540 - mae: 5.3570\n",
            "Epoch 28: val_loss improved from 29.57412 to 29.31082, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 29.5540 - mae: 5.3570 - val_loss: 29.3108 - val_mae: 5.3369\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2911 - mae: 5.3324\n",
            "Epoch 29: val_loss improved from 29.31082 to 29.04902, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.2911 - mae: 5.3324 - val_loss: 29.0490 - val_mae: 5.3123\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0294 - mae: 5.3078\n",
            "Epoch 30: val_loss improved from 29.04902 to 28.78843, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 29.0294 - mae: 5.3078 - val_loss: 28.7884 - val_mae: 5.2877\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7690 - mae: 5.2832\n",
            "Epoch 31: val_loss improved from 28.78843 to 28.52906, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 28.7690 - mae: 5.2832 - val_loss: 28.5291 - val_mae: 5.2631\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5098 - mae: 5.2586\n",
            "Epoch 32: val_loss improved from 28.52906 to 28.27091, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.5098 - mae: 5.2586 - val_loss: 28.2709 - val_mae: 5.2386\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2520 - mae: 5.2340\n",
            "Epoch 33: val_loss improved from 28.27091 to 28.01378, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.2520 - mae: 5.2340 - val_loss: 28.0138 - val_mae: 5.2140\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9952 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01378 to 27.75818, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 27.9952 - mae: 5.2095 - val_loss: 27.7582 - val_mae: 5.1894\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7398 - mae: 5.1849\n",
            "Epoch 35: val_loss improved from 27.75818 to 27.50373, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.7398 - mae: 5.1849 - val_loss: 27.5037 - val_mae: 5.1648\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4856 - mae: 5.1603\n",
            "Epoch 36: val_loss improved from 27.50373 to 27.25051, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.4856 - mae: 5.1603 - val_loss: 27.2505 - val_mae: 5.1403\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2325 - mae: 5.1357\n",
            "Epoch 37: val_loss improved from 27.25051 to 26.99857, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.2325 - mae: 5.1357 - val_loss: 26.9986 - val_mae: 5.1157\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9808 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99857 to 26.74770, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.9808 - mae: 5.1112 - val_loss: 26.7477 - val_mae: 5.0911\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7302 - mae: 5.0866\n",
            "Epoch 39: val_loss improved from 26.74770 to 26.49810, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 26.7302 - mae: 5.0866 - val_loss: 26.4981 - val_mae: 5.0665\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4808 - mae: 5.0620\n",
            "Epoch 40: val_loss improved from 26.49810 to 26.24978, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 26.4808 - mae: 5.0620 - val_loss: 26.2498 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2326 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.24978 to 26.00281, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 26.2326 - mae: 5.0375 - val_loss: 26.0028 - val_mae: 5.0174\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9859 - mae: 5.0129\n",
            "Epoch 42: val_loss improved from 26.00281 to 25.75679, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.9859 - mae: 5.0129 - val_loss: 25.7568 - val_mae: 4.9928\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7402 - mae: 4.9883\n",
            "Epoch 43: val_loss improved from 25.75679 to 25.51211, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.7402 - mae: 4.9883 - val_loss: 25.5121 - val_mae: 4.9683\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4957 - mae: 4.9638\n",
            "Epoch 44: val_loss improved from 25.51211 to 25.26874, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 25.4957 - mae: 4.9638 - val_loss: 25.2687 - val_mae: 4.9437\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2526 - mae: 4.9392\n",
            "Epoch 45: val_loss improved from 25.26874 to 25.02649, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 25.2526 - mae: 4.9392 - val_loss: 25.0265 - val_mae: 4.9192\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0107 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02649 to 24.78549, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.0107 - mae: 4.9147 - val_loss: 24.7855 - val_mae: 4.8946\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7699 - mae: 4.8901\n",
            "Epoch 47: val_loss improved from 24.78549 to 24.54592, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 24.7699 - mae: 4.8901 - val_loss: 24.5459 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5304 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54592 to 24.30739, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.5304 - mae: 4.8656 - val_loss: 24.3074 - val_mae: 4.8455\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2921 - mae: 4.8410\n",
            "Epoch 49: val_loss improved from 24.30739 to 24.07005, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.2921 - mae: 4.8410 - val_loss: 24.0701 - val_mae: 4.8210\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0550 - mae: 4.8165\n",
            "Epoch 50: val_loss improved from 24.07005 to 23.83414, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.0550 - mae: 4.8165 - val_loss: 23.8341 - val_mae: 4.7964\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8193 - mae: 4.7919\n",
            "Epoch 51: val_loss improved from 23.83414 to 23.59913, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 23.8193 - mae: 4.7919 - val_loss: 23.5991 - val_mae: 4.7719\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5845 - mae: 4.7674\n",
            "Epoch 52: val_loss improved from 23.59913 to 23.36565, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 23.5845 - mae: 4.7674 - val_loss: 23.3657 - val_mae: 4.7473\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3513 - mae: 4.7428\n",
            "Epoch 53: val_loss improved from 23.36565 to 23.13314, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 23.3513 - mae: 4.7428 - val_loss: 23.1331 - val_mae: 4.7228\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1191 - mae: 4.7183\n",
            "Epoch 54: val_loss improved from 23.13314 to 22.90208, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 23.1191 - mae: 4.7183 - val_loss: 22.9021 - val_mae: 4.6983\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8882 - mae: 4.6938\n",
            "Epoch 55: val_loss improved from 22.90208 to 22.67210, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.8882 - mae: 4.6938 - val_loss: 22.6721 - val_mae: 4.6737\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6585 - mae: 4.6692\n",
            "Epoch 56: val_loss improved from 22.67210 to 22.44339, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 22.6585 - mae: 4.6692 - val_loss: 22.4434 - val_mae: 4.6492\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4301 - mae: 4.6447\n",
            "Epoch 57: val_loss improved from 22.44339 to 22.21584, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 22.4301 - mae: 4.6447 - val_loss: 22.2158 - val_mae: 4.6247\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2028 - mae: 4.6202\n",
            "Epoch 58: val_loss improved from 22.21584 to 21.98977, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.2028 - mae: 4.6202 - val_loss: 21.9898 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9768 - mae: 4.5957\n",
            "Epoch 59: val_loss improved from 21.98977 to 21.76476, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.9768 - mae: 4.5957 - val_loss: 21.7648 - val_mae: 4.5756\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7521 - mae: 4.5711\n",
            "Epoch 60: val_loss improved from 21.76476 to 21.54090, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 21.7521 - mae: 4.5711 - val_loss: 21.5409 - val_mae: 4.5511\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5285 - mae: 4.5466\n",
            "Epoch 61: val_loss improved from 21.54090 to 21.31836, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.5285 - mae: 4.5466 - val_loss: 21.3184 - val_mae: 4.5266\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3061 - mae: 4.5221\n",
            "Epoch 62: val_loss improved from 21.31836 to 21.09710, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.3061 - mae: 4.5221 - val_loss: 21.0971 - val_mae: 4.5021\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0850 - mae: 4.4976\n",
            "Epoch 63: val_loss improved from 21.09710 to 20.87685, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 21.0850 - mae: 4.4976 - val_loss: 20.8769 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8651 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87685 to 20.65802, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 20.8651 - mae: 4.4731 - val_loss: 20.6580 - val_mae: 4.4531\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6464 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65802 to 20.44036, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.6464 - mae: 4.4486 - val_loss: 20.4404 - val_mae: 4.4285\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4290 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44036 to 20.22385, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 20.4290 - mae: 4.4241 - val_loss: 20.2239 - val_mae: 4.4040\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2127 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22385 to 20.00859, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 20.2127 - mae: 4.3996 - val_loss: 20.0086 - val_mae: 4.3795\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9976 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00859 to 19.79481, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 19.9976 - mae: 4.3751 - val_loss: 19.7948 - val_mae: 4.3551\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7839 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79481 to 19.58197, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.7839 - mae: 4.3506 - val_loss: 19.5820 - val_mae: 4.3305\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5716 - mae: 4.3260\n",
            "Epoch 70: val_loss improved from 19.58197 to 19.37006, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 19.5716 - mae: 4.3260 - val_loss: 19.3701 - val_mae: 4.3060\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3601 - mae: 4.3015\n",
            "Epoch 71: val_loss improved from 19.37006 to 19.15982, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.3601 - mae: 4.3015 - val_loss: 19.1598 - val_mae: 4.2815\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1500 - mae: 4.2771\n",
            "Epoch 72: val_loss improved from 19.15982 to 18.95089, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 19.1500 - mae: 4.2771 - val_loss: 18.9509 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9411 - mae: 4.2526\n",
            "Epoch 73: val_loss improved from 18.95089 to 18.74310, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.9411 - mae: 4.2526 - val_loss: 18.7431 - val_mae: 4.2326\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7335 - mae: 4.2281\n",
            "Epoch 74: val_loss improved from 18.74310 to 18.53643, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.7335 - mae: 4.2281 - val_loss: 18.5364 - val_mae: 4.2081\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5270 - mae: 4.2036\n",
            "Epoch 75: val_loss improved from 18.53643 to 18.33092, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.5270 - mae: 4.2036 - val_loss: 18.3309 - val_mae: 4.1836\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3218 - mae: 4.1791\n",
            "Epoch 76: val_loss improved from 18.33092 to 18.12670, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 18.3218 - mae: 4.1791 - val_loss: 18.1267 - val_mae: 4.1591\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1179 - mae: 4.1547\n",
            "Epoch 77: val_loss improved from 18.12670 to 17.92361, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 18.1179 - mae: 4.1547 - val_loss: 17.9236 - val_mae: 4.1346\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9151 - mae: 4.1302\n",
            "Epoch 78: val_loss improved from 17.92361 to 17.72185, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 17.9151 - mae: 4.1302 - val_loss: 17.7218 - val_mae: 4.1102\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7137 - mae: 4.1057\n",
            "Epoch 79: val_loss improved from 17.72185 to 17.52131, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 17.7137 - mae: 4.1057 - val_loss: 17.5213 - val_mae: 4.0857\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5133 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52131 to 17.32207, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.5133 - mae: 4.0813 - val_loss: 17.3221 - val_mae: 4.0612\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3143 - mae: 4.0568\n",
            "Epoch 81: val_loss improved from 17.32207 to 17.12404, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.3143 - mae: 4.0568 - val_loss: 17.1240 - val_mae: 4.0368\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1165 - mae: 4.0323\n",
            "Epoch 82: val_loss improved from 17.12404 to 16.92705, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 17.1165 - mae: 4.0323 - val_loss: 16.9271 - val_mae: 4.0123\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9198 - mae: 4.0079\n",
            "Epoch 83: val_loss improved from 16.92705 to 16.73152, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 16.9198 - mae: 4.0079 - val_loss: 16.7315 - val_mae: 3.9879\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7244 - mae: 3.9834\n",
            "Epoch 84: val_loss improved from 16.73152 to 16.53705, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.7244 - mae: 3.9834 - val_loss: 16.5370 - val_mae: 3.9634\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5301 - mae: 3.9590\n",
            "Epoch 85: val_loss improved from 16.53705 to 16.34394, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.5301 - mae: 3.9590 - val_loss: 16.3439 - val_mae: 3.9390\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3373 - mae: 3.9345\n",
            "Epoch 86: val_loss improved from 16.34394 to 16.15183, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.3373 - mae: 3.9345 - val_loss: 16.1518 - val_mae: 3.9145\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1454 - mae: 3.9101\n",
            "Epoch 87: val_loss improved from 16.15183 to 15.96119, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.1454 - mae: 3.9101 - val_loss: 15.9612 - val_mae: 3.8901\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9549 - mae: 3.8857\n",
            "Epoch 88: val_loss improved from 15.96119 to 15.77172, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.9549 - mae: 3.8857 - val_loss: 15.7717 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7657 - mae: 3.8612\n",
            "Epoch 89: val_loss improved from 15.77172 to 15.58322, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 15.7657 - mae: 3.8612 - val_loss: 15.5832 - val_mae: 3.8412\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5776 - mae: 3.8368\n",
            "Epoch 90: val_loss improved from 15.58322 to 15.39608, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.5776 - mae: 3.8368 - val_loss: 15.3961 - val_mae: 3.8168\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3905 - mae: 3.8124\n",
            "Epoch 91: val_loss improved from 15.39608 to 15.21045, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 15.3905 - mae: 3.8124 - val_loss: 15.2104 - val_mae: 3.7924\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2050 - mae: 3.7879\n",
            "Epoch 92: val_loss improved from 15.21045 to 15.02563, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.2050 - mae: 3.7879 - val_loss: 15.0256 - val_mae: 3.7679\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0206 - mae: 3.7635\n",
            "Epoch 93: val_loss improved from 15.02563 to 14.84218, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 15.0206 - mae: 3.7635 - val_loss: 14.8422 - val_mae: 3.7435\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8374 - mae: 3.7391\n",
            "Epoch 94: val_loss improved from 14.84218 to 14.65998, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 14.8374 - mae: 3.7391 - val_loss: 14.6600 - val_mae: 3.7191\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6554 - mae: 3.7147\n",
            "Epoch 95: val_loss improved from 14.65998 to 14.47901, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.6554 - mae: 3.7147 - val_loss: 14.4790 - val_mae: 3.6947\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4747 - mae: 3.6902\n",
            "Epoch 96: val_loss improved from 14.47901 to 14.29917, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 14.4747 - mae: 3.6902 - val_loss: 14.2992 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2951 - mae: 3.6658\n",
            "Epoch 97: val_loss improved from 14.29917 to 14.12068, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 14.2951 - mae: 3.6658 - val_loss: 14.1207 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1168 - mae: 3.6414\n",
            "Epoch 98: val_loss improved from 14.12068 to 13.94333, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 14.1168 - mae: 3.6414 - val_loss: 13.9433 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9397 - mae: 3.6170\n",
            "Epoch 99: val_loss improved from 13.94333 to 13.76716, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 13.9397 - mae: 3.6170 - val_loss: 13.7672 - val_mae: 3.5971\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7638 - mae: 3.5926\n",
            "Epoch 100: val_loss improved from 13.76716 to 13.59233, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 13.7638 - mae: 3.5926 - val_loss: 13.5923 - val_mae: 3.5727\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.747060775756836, RMSE:3.707702875137329, MAE:3.5902669429779053, R2:-15.04006260303602\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897, -15.055156886640493, -15.040831191761544, -15.04006260303602]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_262\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_155 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_188 (Embedding)      (None, 17, 900)      649800      ['input_155[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_189 (Embedding)      (None, 17, 900)      649800      ['input_155[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_190 (Embedding)      (None, 17, 900)      649800      ['input_155[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_191 (Embedding)      (None, 17, 900)      649800      ['input_155[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_231 (Conv1D)            (None, 17, 100)      270100      ['embedding_188[0][0]',          \n",
            "                                                                  'embedding_189[0][0]',          \n",
            "                                                                  'embedding_190[0][0]',          \n",
            "                                                                  'embedding_191[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_232 (Conv1D)            (None, 17, 100)      270100      ['embedding_188[0][0]',          \n",
            "                                                                  'embedding_189[0][0]',          \n",
            "                                                                  'embedding_190[0][0]',          \n",
            "                                                                  'embedding_191[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_233 (Conv1D)            (None, 17, 100)      270100      ['embedding_188[0][0]',          \n",
            "                                                                  'embedding_189[0][0]',          \n",
            "                                                                  'embedding_190[0][0]',          \n",
            "                                                                  'embedding_191[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_564 (Glob  (None, 100)         0           ['conv1d_231[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_565 (Glob  (None, 100)         0           ['conv1d_231[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_566 (Glob  (None, 100)         0           ['conv1d_231[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_567 (Glob  (None, 100)         0           ['conv1d_231[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_568 (Glob  (None, 100)         0           ['conv1d_232[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_569 (Glob  (None, 100)         0           ['conv1d_232[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_570 (Glob  (None, 100)         0           ['conv1d_232[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_571 (Glob  (None, 100)         0           ['conv1d_232[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_572 (Glob  (None, 100)         0           ['conv1d_233[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_573 (Glob  (None, 100)         0           ['conv1d_233[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_574 (Glob  (None, 100)         0           ['conv1d_233[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_575 (Glob  (None, 100)         0           ['conv1d_233[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_156 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_111 (Add)                  (None, 100)          0           ['global_max_pooling1d_564[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_565[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_566[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_567[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_112 (Add)                  (None, 100)          0           ['global_max_pooling1d_568[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_569[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_570[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_571[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_113 (Add)                  (None, 100)          0           ['global_max_pooling1d_572[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_573[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_574[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_575[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_294 (Dense)              (None, 1000)         11000       ['input_156[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_77 (Concatenate)   (None, 1300)         0           ['add_111[0][0]',                \n",
            "                                                                  'add_112[0][0]',                \n",
            "                                                                  'add_113[0][0]',                \n",
            "                                                                  'dense_294[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_77[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_285 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_285[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_286 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_286[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_287 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_287[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_288 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_288[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_289 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_295 (Dense)              (None, 1)            3           ['dropout_289[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.3918 - mae: 6.0436\n",
            "Epoch 1: val_loss improved from inf to 36.87358, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 117ms/step - loss: 37.3918 - mae: 6.0436 - val_loss: 36.8736 - val_mae: 6.0038\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8459 - mae: 5.9991\n",
            "Epoch 2: val_loss improved from 36.87358 to 36.57494, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 36.8459 - mae: 5.9991 - val_loss: 36.5749 - val_mae: 5.9788\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5477 - mae: 5.9742\n",
            "Epoch 3: val_loss improved from 36.57494 to 36.27791, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 36.5477 - mae: 5.9742 - val_loss: 36.2779 - val_mae: 5.9540\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2509 - mae: 5.9493\n",
            "Epoch 4: val_loss improved from 36.27791 to 35.98254, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 36.2509 - mae: 5.9493 - val_loss: 35.9825 - val_mae: 5.9291\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9559 - mae: 5.9245\n",
            "Epoch 5: val_loss improved from 35.98254 to 35.68857, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.9559 - mae: 5.9245 - val_loss: 35.6886 - val_mae: 5.9043\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6623 - mae: 5.8996\n",
            "Epoch 6: val_loss improved from 35.68857 to 35.39640, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 35.6623 - mae: 5.8996 - val_loss: 35.3964 - val_mae: 5.8795\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3705 - mae: 5.8748\n",
            "Epoch 7: val_loss improved from 35.39640 to 35.10539, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 35.3705 - mae: 5.8748 - val_loss: 35.1054 - val_mae: 5.8547\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0799 - mae: 5.8501\n",
            "Epoch 8: val_loss improved from 35.10539 to 34.81627, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 35.0799 - mae: 5.8501 - val_loss: 34.8163 - val_mae: 5.8299\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7910 - mae: 5.8253\n",
            "Epoch 9: val_loss improved from 34.81627 to 34.52832, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.7910 - mae: 5.8253 - val_loss: 34.5283 - val_mae: 5.8052\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5036 - mae: 5.8006\n",
            "Epoch 10: val_loss improved from 34.52832 to 34.24189, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.5036 - mae: 5.8006 - val_loss: 34.2419 - val_mae: 5.7804\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2175 - mae: 5.7759\n",
            "Epoch 11: val_loss improved from 34.24189 to 33.95707, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.2175 - mae: 5.7759 - val_loss: 33.9571 - val_mae: 5.7558\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9329 - mae: 5.7512\n",
            "Epoch 12: val_loss improved from 33.95707 to 33.67352, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 33.9329 - mae: 5.7512 - val_loss: 33.6735 - val_mae: 5.7311\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6496 - mae: 5.7265\n",
            "Epoch 13: val_loss improved from 33.67352 to 33.39132, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 33.6496 - mae: 5.7265 - val_loss: 33.3913 - val_mae: 5.7064\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3677 - mae: 5.7018\n",
            "Epoch 14: val_loss improved from 33.39132 to 33.11041, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 33.3677 - mae: 5.7018 - val_loss: 33.1104 - val_mae: 5.6817\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0872 - mae: 5.6772\n",
            "Epoch 15: val_loss improved from 33.11041 to 32.83074, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 33.0872 - mae: 5.6772 - val_loss: 32.8307 - val_mae: 5.6571\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8077 - mae: 5.6525\n",
            "Epoch 16: val_loss improved from 32.83074 to 32.55275, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 32.8077 - mae: 5.6525 - val_loss: 32.5527 - val_mae: 5.6324\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5299 - mae: 5.6279\n",
            "Epoch 17: val_loss improved from 32.55275 to 32.27580, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 32.5299 - mae: 5.6279 - val_loss: 32.2758 - val_mae: 5.6078\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2533 - mae: 5.6033\n",
            "Epoch 18: val_loss improved from 32.27580 to 31.99999, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 32.2533 - mae: 5.6033 - val_loss: 32.0000 - val_mae: 5.5832\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9777 - mae: 5.5786\n",
            "Epoch 19: val_loss improved from 31.99999 to 31.72589, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.9777 - mae: 5.5786 - val_loss: 31.7259 - val_mae: 5.5586\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7037 - mae: 5.5540\n",
            "Epoch 20: val_loss improved from 31.72589 to 31.45273, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 31.7037 - mae: 5.5540 - val_loss: 31.4527 - val_mae: 5.5339\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4309 - mae: 5.5294\n",
            "Epoch 21: val_loss improved from 31.45273 to 31.18079, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 109ms/step - loss: 31.4309 - mae: 5.5294 - val_loss: 31.1808 - val_mae: 5.5093\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1593 - mae: 5.5048\n",
            "Epoch 22: val_loss improved from 31.18079 to 30.91020, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 31.1593 - mae: 5.5048 - val_loss: 30.9102 - val_mae: 5.4847\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8889 - mae: 5.4802\n",
            "Epoch 23: val_loss improved from 30.91020 to 30.64097, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 30.8889 - mae: 5.4802 - val_loss: 30.6410 - val_mae: 5.4601\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6199 - mae: 5.4556\n",
            "Epoch 24: val_loss improved from 30.64097 to 30.37269, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 30.6199 - mae: 5.4556 - val_loss: 30.3727 - val_mae: 5.4355\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3521 - mae: 5.4309\n",
            "Epoch 25: val_loss improved from 30.37269 to 30.10594, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 30.3521 - mae: 5.4309 - val_loss: 30.1059 - val_mae: 5.4109\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0855 - mae: 5.4064\n",
            "Epoch 26: val_loss improved from 30.10594 to 29.84045, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.0855 - mae: 5.4064 - val_loss: 29.8405 - val_mae: 5.3863\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8202 - mae: 5.3818\n",
            "Epoch 27: val_loss improved from 29.84045 to 29.57613, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 29.8202 - mae: 5.3818 - val_loss: 29.5761 - val_mae: 5.3617\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5562 - mae: 5.3572\n",
            "Epoch 28: val_loss improved from 29.57613 to 29.31292, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.5562 - mae: 5.3572 - val_loss: 29.3129 - val_mae: 5.3371\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2932 - mae: 5.3326\n",
            "Epoch 29: val_loss improved from 29.31292 to 29.05119, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 29.2932 - mae: 5.3326 - val_loss: 29.0512 - val_mae: 5.3125\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0315 - mae: 5.3080\n",
            "Epoch 30: val_loss improved from 29.05119 to 28.79063, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 29.0315 - mae: 5.3080 - val_loss: 28.7906 - val_mae: 5.2879\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7712 - mae: 5.2834\n",
            "Epoch 31: val_loss improved from 28.79063 to 28.53116, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.7712 - mae: 5.2834 - val_loss: 28.5312 - val_mae: 5.2633\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5120 - mae: 5.2588\n",
            "Epoch 32: val_loss improved from 28.53116 to 28.27293, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.5120 - mae: 5.2588 - val_loss: 28.2729 - val_mae: 5.2388\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2540 - mae: 5.2342\n",
            "Epoch 33: val_loss improved from 28.27293 to 28.01599, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.2540 - mae: 5.2342 - val_loss: 28.0160 - val_mae: 5.2142\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9973 - mae: 5.2097\n",
            "Epoch 34: val_loss improved from 28.01599 to 27.76026, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.9973 - mae: 5.2097 - val_loss: 27.7603 - val_mae: 5.1896\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7418 - mae: 5.1851\n",
            "Epoch 35: val_loss improved from 27.76026 to 27.50578, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 27.7418 - mae: 5.1851 - val_loss: 27.5058 - val_mae: 5.1650\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4875 - mae: 5.1605\n",
            "Epoch 36: val_loss improved from 27.50578 to 27.25266, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 27.4875 - mae: 5.1605 - val_loss: 27.2527 - val_mae: 5.1405\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2346 - mae: 5.1359\n",
            "Epoch 37: val_loss improved from 27.25266 to 27.00049, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.2346 - mae: 5.1359 - val_loss: 27.0005 - val_mae: 5.1159\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9828 - mae: 5.1114\n",
            "Epoch 38: val_loss improved from 27.00049 to 26.74965, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 26.9828 - mae: 5.1114 - val_loss: 26.7496 - val_mae: 5.0913\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7322 - mae: 5.0868\n",
            "Epoch 39: val_loss improved from 26.74965 to 26.50007, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 26.7322 - mae: 5.0868 - val_loss: 26.5001 - val_mae: 5.0667\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4828 - mae: 5.0622\n",
            "Epoch 40: val_loss improved from 26.50007 to 26.25188, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 26.4828 - mae: 5.0622 - val_loss: 26.2519 - val_mae: 5.0422\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2348 - mae: 5.0377\n",
            "Epoch 41: val_loss improved from 26.25188 to 26.00467, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 26.2348 - mae: 5.0377 - val_loss: 26.0047 - val_mae: 5.0176\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9879 - mae: 5.0131\n",
            "Epoch 42: val_loss improved from 26.00467 to 25.75869, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.9879 - mae: 5.0131 - val_loss: 25.7587 - val_mae: 4.9930\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7421 - mae: 4.9886\n",
            "Epoch 43: val_loss improved from 25.75869 to 25.51435, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.7421 - mae: 4.9886 - val_loss: 25.5143 - val_mae: 4.9685\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4979 - mae: 4.9640\n",
            "Epoch 44: val_loss improved from 25.51435 to 25.27068, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 25.4979 - mae: 4.9640 - val_loss: 25.2707 - val_mae: 4.9439\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2545 - mae: 4.9394\n",
            "Epoch 45: val_loss improved from 25.27068 to 25.02856, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.2545 - mae: 4.9394 - val_loss: 25.0286 - val_mae: 4.9194\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0126 - mae: 4.9149\n",
            "Epoch 46: val_loss improved from 25.02856 to 24.78761, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 25.0126 - mae: 4.9149 - val_loss: 24.7876 - val_mae: 4.8948\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7718 - mae: 4.8903\n",
            "Epoch 47: val_loss improved from 24.78761 to 24.54781, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.7718 - mae: 4.8903 - val_loss: 24.5478 - val_mae: 4.8703\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5323 - mae: 4.8658\n",
            "Epoch 48: val_loss improved from 24.54781 to 24.30921, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.5323 - mae: 4.8658 - val_loss: 24.3092 - val_mae: 4.8457\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2940 - mae: 4.8412\n",
            "Epoch 49: val_loss improved from 24.30921 to 24.07193, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 24.2940 - mae: 4.8412 - val_loss: 24.0719 - val_mae: 4.8212\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0569 - mae: 4.8167\n",
            "Epoch 50: val_loss improved from 24.07193 to 23.83599, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 106ms/step - loss: 24.0569 - mae: 4.8167 - val_loss: 23.8360 - val_mae: 4.7966\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8211 - mae: 4.7921\n",
            "Epoch 51: val_loss improved from 23.83599 to 23.60114, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 23.8211 - mae: 4.7921 - val_loss: 23.6011 - val_mae: 4.7721\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5866 - mae: 4.7676\n",
            "Epoch 52: val_loss improved from 23.60114 to 23.36742, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 23.5866 - mae: 4.7676 - val_loss: 23.3674 - val_mae: 4.7475\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3531 - mae: 4.7430\n",
            "Epoch 53: val_loss improved from 23.36742 to 23.13510, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.3531 - mae: 4.7430 - val_loss: 23.1351 - val_mae: 4.7230\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1210 - mae: 4.7185\n",
            "Epoch 54: val_loss improved from 23.13510 to 22.90388, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 23.1210 - mae: 4.7185 - val_loss: 22.9039 - val_mae: 4.6985\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8901 - mae: 4.6940\n",
            "Epoch 55: val_loss improved from 22.90388 to 22.67391, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.8901 - mae: 4.6940 - val_loss: 22.6739 - val_mae: 4.6739\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6604 - mae: 4.6694\n",
            "Epoch 56: val_loss improved from 22.67391 to 22.44516, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.6604 - mae: 4.6694 - val_loss: 22.4452 - val_mae: 4.6494\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4319 - mae: 4.6449\n",
            "Epoch 57: val_loss improved from 22.44516 to 22.21778, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 22.4319 - mae: 4.6449 - val_loss: 22.2178 - val_mae: 4.6249\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2046 - mae: 4.6204\n",
            "Epoch 58: val_loss improved from 22.21778 to 21.99166, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 22.2046 - mae: 4.6204 - val_loss: 21.9917 - val_mae: 4.6004\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9787 - mae: 4.5959\n",
            "Epoch 59: val_loss improved from 21.99166 to 21.76650, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.9787 - mae: 4.5959 - val_loss: 21.7665 - val_mae: 4.5758\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7538 - mae: 4.5713\n",
            "Epoch 60: val_loss improved from 21.76650 to 21.54269, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.7538 - mae: 4.5713 - val_loss: 21.5427 - val_mae: 4.5513\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5302 - mae: 4.5468\n",
            "Epoch 61: val_loss improved from 21.54269 to 21.32014, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 21.5302 - mae: 4.5468 - val_loss: 21.3201 - val_mae: 4.5268\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3079 - mae: 4.5223\n",
            "Epoch 62: val_loss improved from 21.32014 to 21.09876, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 21.3079 - mae: 4.5223 - val_loss: 21.0988 - val_mae: 4.5023\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0868 - mae: 4.4978\n",
            "Epoch 63: val_loss improved from 21.09876 to 20.87855, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 21.0868 - mae: 4.4978 - val_loss: 20.8786 - val_mae: 4.4777\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8669 - mae: 4.4733\n",
            "Epoch 64: val_loss improved from 20.87855 to 20.65972, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 20.8669 - mae: 4.4733 - val_loss: 20.6597 - val_mae: 4.4532\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6482 - mae: 4.4488\n",
            "Epoch 65: val_loss improved from 20.65972 to 20.44214, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 20.6482 - mae: 4.4488 - val_loss: 20.4421 - val_mae: 4.4287\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4307 - mae: 4.4243\n",
            "Epoch 66: val_loss improved from 20.44214 to 20.22561, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.4307 - mae: 4.4243 - val_loss: 20.2256 - val_mae: 4.4042\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2145 - mae: 4.3998\n",
            "Epoch 67: val_loss improved from 20.22561 to 20.01037, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.2145 - mae: 4.3998 - val_loss: 20.0104 - val_mae: 4.3797\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9995 - mae: 4.3753\n",
            "Epoch 68: val_loss improved from 20.01037 to 19.79638, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 19.9995 - mae: 4.3753 - val_loss: 19.7964 - val_mae: 4.3552\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7858 - mae: 4.3507\n",
            "Epoch 69: val_loss improved from 19.79638 to 19.58342, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 19.7858 - mae: 4.3507 - val_loss: 19.5834 - val_mae: 4.3307\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5732 - mae: 4.3262\n",
            "Epoch 70: val_loss improved from 19.58342 to 19.37195, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 19.5732 - mae: 4.3262 - val_loss: 19.3720 - val_mae: 4.3062\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3620 - mae: 4.3018\n",
            "Epoch 71: val_loss improved from 19.37195 to 19.16162, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 19.3620 - mae: 4.3018 - val_loss: 19.1616 - val_mae: 4.2817\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1518 - mae: 4.2773\n",
            "Epoch 72: val_loss improved from 19.16162 to 18.95263, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 19.1518 - mae: 4.2773 - val_loss: 18.9526 - val_mae: 4.2573\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9430 - mae: 4.2528\n",
            "Epoch 73: val_loss improved from 18.95263 to 18.74472, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 18.9430 - mae: 4.2528 - val_loss: 18.7447 - val_mae: 4.2328\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7352 - mae: 4.2283\n",
            "Epoch 74: val_loss improved from 18.74472 to 18.53811, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.7352 - mae: 4.2283 - val_loss: 18.5381 - val_mae: 4.2083\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5289 - mae: 4.2038\n",
            "Epoch 75: val_loss improved from 18.53811 to 18.33260, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 18.5289 - mae: 4.2038 - val_loss: 18.3326 - val_mae: 4.1838\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3237 - mae: 4.1793\n",
            "Epoch 76: val_loss improved from 18.33260 to 18.12840, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.3237 - mae: 4.1793 - val_loss: 18.1284 - val_mae: 4.1593\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1196 - mae: 4.1549\n",
            "Epoch 77: val_loss improved from 18.12840 to 17.92543, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 18.1196 - mae: 4.1549 - val_loss: 17.9254 - val_mae: 4.1349\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9169 - mae: 4.1304\n",
            "Epoch 78: val_loss improved from 17.92543 to 17.72365, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 17.9169 - mae: 4.1304 - val_loss: 17.7237 - val_mae: 4.1104\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7154 - mae: 4.1059\n",
            "Epoch 79: val_loss improved from 17.72365 to 17.52295, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 17.7154 - mae: 4.1059 - val_loss: 17.5229 - val_mae: 4.0859\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5150 - mae: 4.0815\n",
            "Epoch 80: val_loss improved from 17.52295 to 17.32380, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.5150 - mae: 4.0815 - val_loss: 17.3238 - val_mae: 4.0615\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3160 - mae: 4.0570\n",
            "Epoch 81: val_loss improved from 17.32380 to 17.12557, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 17.3160 - mae: 4.0570 - val_loss: 17.1256 - val_mae: 4.0370\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1180 - mae: 4.0325\n",
            "Epoch 82: val_loss improved from 17.12557 to 16.92888, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 17.1180 - mae: 4.0325 - val_loss: 16.9289 - val_mae: 4.0125\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9214 - mae: 4.0081\n",
            "Epoch 83: val_loss improved from 16.92888 to 16.73315, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 16.9214 - mae: 4.0081 - val_loss: 16.7331 - val_mae: 3.9881\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7260 - mae: 3.9836\n",
            "Epoch 84: val_loss improved from 16.73315 to 16.53864, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.7260 - mae: 3.9836 - val_loss: 16.5386 - val_mae: 3.9636\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5317 - mae: 3.9592\n",
            "Epoch 85: val_loss improved from 16.53864 to 16.34548, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 16.5317 - mae: 3.9592 - val_loss: 16.3455 - val_mae: 3.9392\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3388 - mae: 3.9347\n",
            "Epoch 86: val_loss improved from 16.34548 to 16.15343, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 16.3388 - mae: 3.9347 - val_loss: 16.1534 - val_mae: 3.9147\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1470 - mae: 3.9103\n",
            "Epoch 87: val_loss improved from 16.15343 to 15.96266, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 16.1470 - mae: 3.9103 - val_loss: 15.9627 - val_mae: 3.8903\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9564 - mae: 3.8858\n",
            "Epoch 88: val_loss improved from 15.96266 to 15.77321, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 15.9564 - mae: 3.8858 - val_loss: 15.7732 - val_mae: 3.8659\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7671 - mae: 3.8614\n",
            "Epoch 89: val_loss improved from 15.77321 to 15.58485, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.7671 - mae: 3.8614 - val_loss: 15.5848 - val_mae: 3.8414\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5790 - mae: 3.8370\n",
            "Epoch 90: val_loss improved from 15.58485 to 15.39765, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.5790 - mae: 3.8370 - val_loss: 15.3977 - val_mae: 3.8170\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3921 - mae: 3.8125\n",
            "Epoch 91: val_loss improved from 15.39765 to 15.21178, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 15.3921 - mae: 3.8125 - val_loss: 15.2118 - val_mae: 3.7926\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2065 - mae: 3.7881\n",
            "Epoch 92: val_loss improved from 15.21178 to 15.02707, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.2065 - mae: 3.7881 - val_loss: 15.0271 - val_mae: 3.7681\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0220 - mae: 3.7637\n",
            "Epoch 93: val_loss improved from 15.02707 to 14.84375, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.0220 - mae: 3.7637 - val_loss: 14.8438 - val_mae: 3.7437\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8388 - mae: 3.7393\n",
            "Epoch 94: val_loss improved from 14.84375 to 14.66149, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.8388 - mae: 3.7393 - val_loss: 14.6615 - val_mae: 3.7193\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6568 - mae: 3.7149\n",
            "Epoch 95: val_loss improved from 14.66149 to 14.48041, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.6568 - mae: 3.7149 - val_loss: 14.4804 - val_mae: 3.6949\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4760 - mae: 3.6904\n",
            "Epoch 96: val_loss improved from 14.48041 to 14.30061, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.4760 - mae: 3.6904 - val_loss: 14.3006 - val_mae: 3.6705\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2964 - mae: 3.6660\n",
            "Epoch 97: val_loss improved from 14.30061 to 14.12210, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.2964 - mae: 3.6660 - val_loss: 14.1221 - val_mae: 3.6461\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1181 - mae: 3.6416\n",
            "Epoch 98: val_loss improved from 14.12210 to 13.94474, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 14.1181 - mae: 3.6416 - val_loss: 13.9447 - val_mae: 3.6217\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9410 - mae: 3.6172\n",
            "Epoch 99: val_loss improved from 13.94474 to 13.76851, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 13.9410 - mae: 3.6172 - val_loss: 13.7685 - val_mae: 3.5972\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7651 - mae: 3.5928\n",
            "Epoch 100: val_loss improved from 13.76851 to 13.59358, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 13.7651 - mae: 3.5928 - val_loss: 13.5936 - val_mae: 3.5728\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.748308181762695, RMSE:3.707871198654175, MAE:3.5904407501220703, R2:-15.04151861559031\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897, -15.055156886640493, -15.040831191761544, -15.04006260303602, -15.04151861559031]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_268\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_157 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_192 (Embedding)      (None, 17, 900)      649800      ['input_157[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_193 (Embedding)      (None, 17, 900)      649800      ['input_157[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_194 (Embedding)      (None, 17, 900)      649800      ['input_157[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_195 (Embedding)      (None, 17, 900)      649800      ['input_157[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_234 (Conv1D)            (None, 17, 100)      270100      ['embedding_192[0][0]',          \n",
            "                                                                  'embedding_193[0][0]',          \n",
            "                                                                  'embedding_194[0][0]',          \n",
            "                                                                  'embedding_195[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_235 (Conv1D)            (None, 17, 100)      270100      ['embedding_192[0][0]',          \n",
            "                                                                  'embedding_193[0][0]',          \n",
            "                                                                  'embedding_194[0][0]',          \n",
            "                                                                  'embedding_195[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_236 (Conv1D)            (None, 17, 100)      270100      ['embedding_192[0][0]',          \n",
            "                                                                  'embedding_193[0][0]',          \n",
            "                                                                  'embedding_194[0][0]',          \n",
            "                                                                  'embedding_195[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_576 (Glob  (None, 100)         0           ['conv1d_234[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_577 (Glob  (None, 100)         0           ['conv1d_234[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_578 (Glob  (None, 100)         0           ['conv1d_234[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_579 (Glob  (None, 100)         0           ['conv1d_234[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_580 (Glob  (None, 100)         0           ['conv1d_235[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_581 (Glob  (None, 100)         0           ['conv1d_235[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_582 (Glob  (None, 100)         0           ['conv1d_235[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_583 (Glob  (None, 100)         0           ['conv1d_235[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_584 (Glob  (None, 100)         0           ['conv1d_236[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_585 (Glob  (None, 100)         0           ['conv1d_236[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_586 (Glob  (None, 100)         0           ['conv1d_236[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_587 (Glob  (None, 100)         0           ['conv1d_236[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_158 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_114 (Add)                  (None, 100)          0           ['global_max_pooling1d_576[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_577[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_578[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_579[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_115 (Add)                  (None, 100)          0           ['global_max_pooling1d_580[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_581[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_582[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_583[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_116 (Add)                  (None, 100)          0           ['global_max_pooling1d_584[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_585[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_586[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_587[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_296 (Dense)              (None, 1000)         11000       ['input_158[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_78 (Concatenate)   (None, 1300)         0           ['add_114[0][0]',                \n",
            "                                                                  'add_115[0][0]',                \n",
            "                                                                  'add_116[0][0]',                \n",
            "                                                                  'dense_296[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_78[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_290 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_290[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_291 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_291[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_292 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_292[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_293 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_293[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_294 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_297 (Dense)              (None, 1)            3           ['dropout_294[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 37.2029 - mae: 6.0287\n",
            "Epoch 1: val_loss improved from inf to 36.87318, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 6s 117ms/step - loss: 37.2029 - mae: 6.0287 - val_loss: 36.8732 - val_mae: 6.0037\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.8452 - mae: 5.9990\n",
            "Epoch 2: val_loss improved from 36.87318 to 36.57418, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 36.8452 - mae: 5.9990 - val_loss: 36.5742 - val_mae: 5.9788\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.5465 - mae: 5.9741\n",
            "Epoch 3: val_loss improved from 36.57418 to 36.27692, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 36.5465 - mae: 5.9741 - val_loss: 36.2769 - val_mae: 5.9539\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 36.2496 - mae: 5.9492\n",
            "Epoch 4: val_loss improved from 36.27692 to 35.98129, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 36.2496 - mae: 5.9492 - val_loss: 35.9813 - val_mae: 5.9290\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.9545 - mae: 5.9243\n",
            "Epoch 5: val_loss improved from 35.98129 to 35.68721, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 35.9545 - mae: 5.9243 - val_loss: 35.6872 - val_mae: 5.9041\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.6609 - mae: 5.8995\n",
            "Epoch 6: val_loss improved from 35.68721 to 35.39472, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 35.6609 - mae: 5.8995 - val_loss: 35.3947 - val_mae: 5.8793\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.3689 - mae: 5.8747\n",
            "Epoch 7: val_loss improved from 35.39472 to 35.10393, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 35.3689 - mae: 5.8747 - val_loss: 35.1039 - val_mae: 5.8545\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 35.0785 - mae: 5.8499\n",
            "Epoch 8: val_loss improved from 35.10393 to 34.81458, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 35.0785 - mae: 5.8499 - val_loss: 34.8146 - val_mae: 5.8298\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.7895 - mae: 5.8252\n",
            "Epoch 9: val_loss improved from 34.81458 to 34.52687, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.7895 - mae: 5.8252 - val_loss: 34.5269 - val_mae: 5.8050\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.5020 - mae: 5.8005\n",
            "Epoch 10: val_loss improved from 34.52687 to 34.24058, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 34.5020 - mae: 5.8005 - val_loss: 34.2406 - val_mae: 5.7803\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 34.2160 - mae: 5.7758\n",
            "Epoch 11: val_loss improved from 34.24058 to 33.95550, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 34.2160 - mae: 5.7758 - val_loss: 33.9555 - val_mae: 5.7556\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.9313 - mae: 5.7511\n",
            "Epoch 12: val_loss improved from 33.95550 to 33.67189, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 101ms/step - loss: 33.9313 - mae: 5.7511 - val_loss: 33.6719 - val_mae: 5.7309\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.6480 - mae: 5.7264\n",
            "Epoch 13: val_loss improved from 33.67189 to 33.38974, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 33.6480 - mae: 5.7264 - val_loss: 33.3897 - val_mae: 5.7063\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.3662 - mae: 5.7017\n",
            "Epoch 14: val_loss improved from 33.38974 to 33.10882, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 33.3662 - mae: 5.7017 - val_loss: 33.1088 - val_mae: 5.6816\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 33.0856 - mae: 5.6770\n",
            "Epoch 15: val_loss improved from 33.10882 to 32.82937, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 33.0856 - mae: 5.6770 - val_loss: 32.8294 - val_mae: 5.6569\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.8063 - mae: 5.6524\n",
            "Epoch 16: val_loss improved from 32.82937 to 32.55117, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 32.8063 - mae: 5.6524 - val_loss: 32.5512 - val_mae: 5.6323\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.5283 - mae: 5.6278\n",
            "Epoch 17: val_loss improved from 32.55117 to 32.27444, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 32.5283 - mae: 5.6278 - val_loss: 32.2744 - val_mae: 5.6077\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 32.2517 - mae: 5.6031\n",
            "Epoch 18: val_loss improved from 32.27444 to 31.99867, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 32.2517 - mae: 5.6031 - val_loss: 31.9987 - val_mae: 5.5830\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.9763 - mae: 5.5785\n",
            "Epoch 19: val_loss improved from 31.99867 to 31.72420, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 31.9763 - mae: 5.5785 - val_loss: 31.7242 - val_mae: 5.5584\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.7021 - mae: 5.5539\n",
            "Epoch 20: val_loss improved from 31.72420 to 31.45126, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.7021 - mae: 5.5539 - val_loss: 31.4513 - val_mae: 5.5338\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.4294 - mae: 5.5293\n",
            "Epoch 21: val_loss improved from 31.45126 to 31.17926, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 31.4294 - mae: 5.5293 - val_loss: 31.1793 - val_mae: 5.5092\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 31.1577 - mae: 5.5046\n",
            "Epoch 22: val_loss improved from 31.17926 to 30.90872, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 31.1577 - mae: 5.5046 - val_loss: 30.9087 - val_mae: 5.4846\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.8875 - mae: 5.4800\n",
            "Epoch 23: val_loss improved from 30.90872 to 30.63939, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.8875 - mae: 5.4800 - val_loss: 30.6394 - val_mae: 5.4599\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.6183 - mae: 5.4554\n",
            "Epoch 24: val_loss improved from 30.63939 to 30.37153, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 30.6183 - mae: 5.4554 - val_loss: 30.3715 - val_mae: 5.4354\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.3506 - mae: 5.4308\n",
            "Epoch 25: val_loss improved from 30.37153 to 30.10458, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 30.3506 - mae: 5.4308 - val_loss: 30.1046 - val_mae: 5.4108\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 30.0841 - mae: 5.4062\n",
            "Epoch 26: val_loss improved from 30.10458 to 29.83891, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 30.0841 - mae: 5.4062 - val_loss: 29.8389 - val_mae: 5.3861\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.8186 - mae: 5.3816\n",
            "Epoch 27: val_loss improved from 29.83891 to 29.57470, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 29.8186 - mae: 5.3816 - val_loss: 29.5747 - val_mae: 5.3616\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.5546 - mae: 5.3570\n",
            "Epoch 28: val_loss improved from 29.57470 to 29.31158, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 29.5546 - mae: 5.3570 - val_loss: 29.3116 - val_mae: 5.3370\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.2917 - mae: 5.3325\n",
            "Epoch 29: val_loss improved from 29.31158 to 29.04971, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.2917 - mae: 5.3325 - val_loss: 29.0497 - val_mae: 5.3124\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 29.0301 - mae: 5.3079\n",
            "Epoch 30: val_loss improved from 29.04971 to 28.78913, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 29.0301 - mae: 5.3079 - val_loss: 28.7891 - val_mae: 5.2878\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.7696 - mae: 5.2833\n",
            "Epoch 31: val_loss improved from 28.78913 to 28.52973, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.7696 - mae: 5.2833 - val_loss: 28.5297 - val_mae: 5.2632\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.5104 - mae: 5.2587\n",
            "Epoch 32: val_loss improved from 28.52973 to 28.27154, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 28.5104 - mae: 5.2587 - val_loss: 28.2715 - val_mae: 5.2386\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 28.2525 - mae: 5.2341\n",
            "Epoch 33: val_loss improved from 28.27154 to 28.01449, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 28.2525 - mae: 5.2341 - val_loss: 28.0145 - val_mae: 5.2140\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.9958 - mae: 5.2095\n",
            "Epoch 34: val_loss improved from 28.01449 to 27.75873, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 27.9958 - mae: 5.2095 - val_loss: 27.7587 - val_mae: 5.1895\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.7403 - mae: 5.1849\n",
            "Epoch 35: val_loss improved from 27.75873 to 27.50436, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 27.7403 - mae: 5.1849 - val_loss: 27.5044 - val_mae: 5.1649\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.4861 - mae: 5.1604\n",
            "Epoch 36: val_loss improved from 27.50436 to 27.25106, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 27.4861 - mae: 5.1604 - val_loss: 27.2511 - val_mae: 5.1403\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 27.2331 - mae: 5.1358\n",
            "Epoch 37: val_loss improved from 27.25106 to 26.99907, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 27.2331 - mae: 5.1358 - val_loss: 26.9991 - val_mae: 5.1157\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.9814 - mae: 5.1112\n",
            "Epoch 38: val_loss improved from 26.99907 to 26.74822, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.9814 - mae: 5.1112 - val_loss: 26.7482 - val_mae: 5.0912\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.7308 - mae: 5.0867\n",
            "Epoch 39: val_loss improved from 26.74822 to 26.49869, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.7308 - mae: 5.0867 - val_loss: 26.4987 - val_mae: 5.0666\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.4814 - mae: 5.0621\n",
            "Epoch 40: val_loss improved from 26.49869 to 26.25052, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 26.4814 - mae: 5.0621 - val_loss: 26.2505 - val_mae: 5.0420\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 26.2333 - mae: 5.0375\n",
            "Epoch 41: val_loss improved from 26.25052 to 26.00343, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 26.2333 - mae: 5.0375 - val_loss: 26.0034 - val_mae: 5.0175\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.9865 - mae: 5.0130\n",
            "Epoch 42: val_loss improved from 26.00343 to 25.75740, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.9865 - mae: 5.0130 - val_loss: 25.7574 - val_mae: 4.9929\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.7409 - mae: 4.9884\n",
            "Epoch 43: val_loss improved from 25.75740 to 25.51269, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.7409 - mae: 4.9884 - val_loss: 25.5127 - val_mae: 4.9683\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.4964 - mae: 4.9638\n",
            "Epoch 44: val_loss improved from 25.51269 to 25.26941, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 25.4964 - mae: 4.9638 - val_loss: 25.2694 - val_mae: 4.9438\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.2532 - mae: 4.9393\n",
            "Epoch 45: val_loss improved from 25.26941 to 25.02722, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.2532 - mae: 4.9393 - val_loss: 25.0272 - val_mae: 4.9192\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 25.0112 - mae: 4.9147\n",
            "Epoch 46: val_loss improved from 25.02722 to 24.78624, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 25.0112 - mae: 4.9147 - val_loss: 24.7862 - val_mae: 4.8947\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.7706 - mae: 4.8902\n",
            "Epoch 47: val_loss improved from 24.78624 to 24.54634, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 24.7706 - mae: 4.8902 - val_loss: 24.5463 - val_mae: 4.8701\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.5310 - mae: 4.8656\n",
            "Epoch 48: val_loss improved from 24.54634 to 24.30799, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 24.5310 - mae: 4.8656 - val_loss: 24.3080 - val_mae: 4.8456\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.2928 - mae: 4.8411\n",
            "Epoch 49: val_loss improved from 24.30799 to 24.07060, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 24.2928 - mae: 4.8411 - val_loss: 24.0706 - val_mae: 4.8210\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 24.0556 - mae: 4.8165\n",
            "Epoch 50: val_loss improved from 24.07060 to 23.83475, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 24.0556 - mae: 4.8165 - val_loss: 23.8348 - val_mae: 4.7965\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.8199 - mae: 4.7920\n",
            "Epoch 51: val_loss improved from 23.83475 to 23.59974, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 23.8199 - mae: 4.7920 - val_loss: 23.5997 - val_mae: 4.7719\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.5852 - mae: 4.7675\n",
            "Epoch 52: val_loss improved from 23.59974 to 23.36627, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 94ms/step - loss: 23.5852 - mae: 4.7675 - val_loss: 23.3663 - val_mae: 4.7474\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.3519 - mae: 4.7429\n",
            "Epoch 53: val_loss improved from 23.36627 to 23.13383, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.3519 - mae: 4.7429 - val_loss: 23.1338 - val_mae: 4.7229\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 23.1197 - mae: 4.7184\n",
            "Epoch 54: val_loss improved from 23.13383 to 22.90258, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 23.1197 - mae: 4.7184 - val_loss: 22.9026 - val_mae: 4.6983\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.8888 - mae: 4.6938\n",
            "Epoch 55: val_loss improved from 22.90258 to 22.67268, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.8888 - mae: 4.6938 - val_loss: 22.6727 - val_mae: 4.6738\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.6591 - mae: 4.6693\n",
            "Epoch 56: val_loss improved from 22.67268 to 22.44404, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 22.6591 - mae: 4.6693 - val_loss: 22.4440 - val_mae: 4.6493\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.4308 - mae: 4.6448\n",
            "Epoch 57: val_loss improved from 22.44404 to 22.21633, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.4308 - mae: 4.6448 - val_loss: 22.2163 - val_mae: 4.6247\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 22.2033 - mae: 4.6202\n",
            "Epoch 58: val_loss improved from 22.21633 to 21.99039, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 22.2033 - mae: 4.6202 - val_loss: 21.9904 - val_mae: 4.6002\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.9773 - mae: 4.5957\n",
            "Epoch 59: val_loss improved from 21.99039 to 21.76540, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.9773 - mae: 4.5957 - val_loss: 21.7654 - val_mae: 4.5757\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.7525 - mae: 4.5712\n",
            "Epoch 60: val_loss improved from 21.76540 to 21.54162, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 21.7525 - mae: 4.5712 - val_loss: 21.5416 - val_mae: 4.5512\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.5291 - mae: 4.5467\n",
            "Epoch 61: val_loss improved from 21.54162 to 21.31880, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 21.5291 - mae: 4.5467 - val_loss: 21.3188 - val_mae: 4.5266\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.3067 - mae: 4.5222\n",
            "Epoch 62: val_loss improved from 21.31880 to 21.09742, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.3067 - mae: 4.5222 - val_loss: 21.0974 - val_mae: 4.5021\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 21.0856 - mae: 4.4976\n",
            "Epoch 63: val_loss improved from 21.09742 to 20.87743, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 21.0856 - mae: 4.4976 - val_loss: 20.8774 - val_mae: 4.4776\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.8657 - mae: 4.4731\n",
            "Epoch 64: val_loss improved from 20.87743 to 20.65850, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.8657 - mae: 4.4731 - val_loss: 20.6585 - val_mae: 4.4531\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.6470 - mae: 4.4486\n",
            "Epoch 65: val_loss improved from 20.65850 to 20.44090, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.6470 - mae: 4.4486 - val_loss: 20.4409 - val_mae: 4.4286\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.4295 - mae: 4.4241\n",
            "Epoch 66: val_loss improved from 20.44090 to 20.22470, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.4295 - mae: 4.4241 - val_loss: 20.2247 - val_mae: 4.4041\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 20.2134 - mae: 4.3996\n",
            "Epoch 67: val_loss improved from 20.22470 to 20.00920, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 20.2134 - mae: 4.3996 - val_loss: 20.0092 - val_mae: 4.3796\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.9983 - mae: 4.3751\n",
            "Epoch 68: val_loss improved from 20.00920 to 19.79519, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 19.9983 - mae: 4.3751 - val_loss: 19.7952 - val_mae: 4.3551\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.7845 - mae: 4.3506\n",
            "Epoch 69: val_loss improved from 19.79519 to 19.58251, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.7845 - mae: 4.3506 - val_loss: 19.5825 - val_mae: 4.3306\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.5719 - mae: 4.3261\n",
            "Epoch 70: val_loss improved from 19.58251 to 19.37096, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 19.5719 - mae: 4.3261 - val_loss: 19.3710 - val_mae: 4.3061\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.3608 - mae: 4.3016\n",
            "Epoch 71: val_loss improved from 19.37096 to 19.16042, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 19.3608 - mae: 4.3016 - val_loss: 19.1604 - val_mae: 4.2816\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 19.1505 - mae: 4.2771\n",
            "Epoch 72: val_loss improved from 19.16042 to 18.95144, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 105ms/step - loss: 19.1505 - mae: 4.2771 - val_loss: 18.9514 - val_mae: 4.2571\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.9417 - mae: 4.2527\n",
            "Epoch 73: val_loss improved from 18.95144 to 18.74361, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 18.9417 - mae: 4.2527 - val_loss: 18.7436 - val_mae: 4.2326\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.7341 - mae: 4.2282\n",
            "Epoch 74: val_loss improved from 18.74361 to 18.53688, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 18.7341 - mae: 4.2282 - val_loss: 18.5369 - val_mae: 4.2082\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.5277 - mae: 4.2037\n",
            "Epoch 75: val_loss improved from 18.53688 to 18.33135, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.5277 - mae: 4.2037 - val_loss: 18.3314 - val_mae: 4.1837\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.3224 - mae: 4.1792\n",
            "Epoch 76: val_loss improved from 18.33135 to 18.12728, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 18.3224 - mae: 4.1792 - val_loss: 18.1273 - val_mae: 4.1592\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 18.1185 - mae: 4.1547\n",
            "Epoch 77: val_loss improved from 18.12728 to 17.92425, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 18.1185 - mae: 4.1547 - val_loss: 17.9242 - val_mae: 4.1347\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.9157 - mae: 4.1303\n",
            "Epoch 78: val_loss improved from 17.92425 to 17.72259, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 17.9157 - mae: 4.1303 - val_loss: 17.7226 - val_mae: 4.1103\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.7143 - mae: 4.1058\n",
            "Epoch 79: val_loss improved from 17.72259 to 17.52188, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 17.7143 - mae: 4.1058 - val_loss: 17.5219 - val_mae: 4.0858\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.5138 - mae: 4.0813\n",
            "Epoch 80: val_loss improved from 17.52188 to 17.32272, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 17.5138 - mae: 4.0813 - val_loss: 17.3227 - val_mae: 4.0613\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.3148 - mae: 4.0569\n",
            "Epoch 81: val_loss improved from 17.32272 to 17.12455, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 17.3148 - mae: 4.0569 - val_loss: 17.1246 - val_mae: 4.0369\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 17.1169 - mae: 4.0324\n",
            "Epoch 82: val_loss improved from 17.12455 to 16.92773, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 17.1169 - mae: 4.0324 - val_loss: 16.9277 - val_mae: 4.0124\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.9204 - mae: 4.0079\n",
            "Epoch 83: val_loss improved from 16.92773 to 16.73199, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 16.9204 - mae: 4.0079 - val_loss: 16.7320 - val_mae: 3.9879\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.7249 - mae: 3.9835\n",
            "Epoch 84: val_loss improved from 16.73199 to 16.53772, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 16.7249 - mae: 3.9835 - val_loss: 16.5377 - val_mae: 3.9635\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.5308 - mae: 3.9590\n",
            "Epoch 85: val_loss improved from 16.53772 to 16.34441, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.5308 - mae: 3.9590 - val_loss: 16.3444 - val_mae: 3.9390\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.3378 - mae: 3.9346\n",
            "Epoch 86: val_loss improved from 16.34441 to 16.15244, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.3378 - mae: 3.9346 - val_loss: 16.1524 - val_mae: 3.9146\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 16.1461 - mae: 3.9101\n",
            "Epoch 87: val_loss improved from 16.15244 to 15.96161, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 16.1461 - mae: 3.9101 - val_loss: 15.9616 - val_mae: 3.8902\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.9554 - mae: 3.8857\n",
            "Epoch 88: val_loss improved from 15.96161 to 15.77232, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 15.9554 - mae: 3.8857 - val_loss: 15.7723 - val_mae: 3.8657\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.7662 - mae: 3.8613\n",
            "Epoch 89: val_loss improved from 15.77232 to 15.58381, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 15.7662 - mae: 3.8613 - val_loss: 15.5838 - val_mae: 3.8413\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.5781 - mae: 3.8368\n",
            "Epoch 90: val_loss improved from 15.58381 to 15.39659, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.5781 - mae: 3.8368 - val_loss: 15.3966 - val_mae: 3.8168\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.3911 - mae: 3.8124\n",
            "Epoch 91: val_loss improved from 15.39659 to 15.21085, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 99ms/step - loss: 15.3911 - mae: 3.8124 - val_loss: 15.2108 - val_mae: 3.7924\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.2057 - mae: 3.7880\n",
            "Epoch 92: val_loss improved from 15.21085 to 15.02604, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 15.2057 - mae: 3.7880 - val_loss: 15.0260 - val_mae: 3.7680\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 15.0211 - mae: 3.7636\n",
            "Epoch 93: val_loss improved from 15.02604 to 14.84280, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 15.0211 - mae: 3.7636 - val_loss: 14.8428 - val_mae: 3.7436\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.8378 - mae: 3.7392\n",
            "Epoch 94: val_loss improved from 14.84280 to 14.66065, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 103ms/step - loss: 14.8378 - mae: 3.7392 - val_loss: 14.6607 - val_mae: 3.7192\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.6560 - mae: 3.7147\n",
            "Epoch 95: val_loss improved from 14.66065 to 14.47942, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 14.6560 - mae: 3.7147 - val_loss: 14.4794 - val_mae: 3.6947\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.4750 - mae: 3.6903\n",
            "Epoch 96: val_loss improved from 14.47942 to 14.29975, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 104ms/step - loss: 14.4750 - mae: 3.6903 - val_loss: 14.2997 - val_mae: 3.6703\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.2955 - mae: 3.6659\n",
            "Epoch 97: val_loss improved from 14.29975 to 14.12115, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.2955 - mae: 3.6659 - val_loss: 14.1211 - val_mae: 3.6459\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 14.1172 - mae: 3.6415\n",
            "Epoch 98: val_loss improved from 14.12115 to 13.94385, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 14.1172 - mae: 3.6415 - val_loss: 13.9439 - val_mae: 3.6215\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.9401 - mae: 3.6171\n",
            "Epoch 99: val_loss improved from 13.94385 to 13.76768, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 13.9401 - mae: 3.6171 - val_loss: 13.7677 - val_mae: 3.5971\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 13.7642 - mae: 3.5927\n",
            "Epoch 100: val_loss improved from 13.76768 to 13.59280, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 13.7642 - mae: 3.5927 - val_loss: 13.5928 - val_mae: 3.5727\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:13.747532844543457, RMSE:3.707766532897949, MAE:3.590332269668579, R2:-15.040613770375487\n",
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897, -15.055156886640493, -15.040831191761544, -15.04006260303602, -15.04151861559031, -15.040613770375487]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_274\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_159 (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_196 (Embedding)      (None, 17, 900)      649800      ['input_159[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_197 (Embedding)      (None, 17, 900)      649800      ['input_159[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_198 (Embedding)      (None, 17, 900)      649800      ['input_159[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_199 (Embedding)      (None, 17, 900)      649800      ['input_159[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_237 (Conv1D)            (None, 17, 100)      270100      ['embedding_196[0][0]',          \n",
            "                                                                  'embedding_197[0][0]',          \n",
            "                                                                  'embedding_198[0][0]',          \n",
            "                                                                  'embedding_199[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_238 (Conv1D)            (None, 17, 100)      270100      ['embedding_196[0][0]',          \n",
            "                                                                  'embedding_197[0][0]',          \n",
            "                                                                  'embedding_198[0][0]',          \n",
            "                                                                  'embedding_199[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_239 (Conv1D)            (None, 17, 100)      270100      ['embedding_196[0][0]',          \n",
            "                                                                  'embedding_197[0][0]',          \n",
            "                                                                  'embedding_198[0][0]',          \n",
            "                                                                  'embedding_199[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d_588 (Glob  (None, 100)         0           ['conv1d_237[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_589 (Glob  (None, 100)         0           ['conv1d_237[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_590 (Glob  (None, 100)         0           ['conv1d_237[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_591 (Glob  (None, 100)         0           ['conv1d_237[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_592 (Glob  (None, 100)         0           ['conv1d_238[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_593 (Glob  (None, 100)         0           ['conv1d_238[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_594 (Glob  (None, 100)         0           ['conv1d_238[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_595 (Glob  (None, 100)         0           ['conv1d_238[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_596 (Glob  (None, 100)         0           ['conv1d_239[0][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_597 (Glob  (None, 100)         0           ['conv1d_239[1][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_598 (Glob  (None, 100)         0           ['conv1d_239[2][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_599 (Glob  (None, 100)         0           ['conv1d_239[3][0]']             \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_160 (InputLayer)         [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_117 (Add)                  (None, 100)          0           ['global_max_pooling1d_588[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_589[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_590[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_591[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_118 (Add)                  (None, 100)          0           ['global_max_pooling1d_592[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_593[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_594[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_595[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_119 (Add)                  (None, 100)          0           ['global_max_pooling1d_596[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_597[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_598[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_599[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_298 (Dense)              (None, 1000)         11000       ['input_160[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_79 (Concatenate)   (None, 1300)         0           ['add_117[0][0]',                \n",
            "                                                                  'add_118[0][0]',                \n",
            "                                                                  'add_119[0][0]',                \n",
            "                                                                  'dense_298[0][0]']              \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_79[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_295 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_295[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_296 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_296[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_297 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_297[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_298 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_298[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_299 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_299 (Dense)              (None, 1)            3           ['dropout_299[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 5.1181 - mae: 1.5556\n",
            "Epoch 1: val_loss improved from inf to 0.78736, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 7s 117ms/step - loss: 5.1181 - mae: 1.5556 - val_loss: 0.7874 - val_mae: 0.7417\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7248 - mae: 0.5276\n",
            "Epoch 2: val_loss improved from 0.78736 to 0.57421, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.7248 - mae: 0.5276 - val_loss: 0.5742 - val_mae: 0.4749\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5708 - mae: 0.4281\n",
            "Epoch 3: val_loss improved from 0.57421 to 0.46686, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.5708 - mae: 0.4281 - val_loss: 0.4669 - val_mae: 0.2911\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4804 - mae: 0.3719\n",
            "Epoch 4: val_loss improved from 0.46686 to 0.41652, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.4804 - mae: 0.3719 - val_loss: 0.4165 - val_mae: 0.3596\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4407 - mae: 0.3511\n",
            "Epoch 5: val_loss improved from 0.41652 to 0.39705, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.4407 - mae: 0.3511 - val_loss: 0.3971 - val_mae: 0.2977\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4205 - mae: 0.3421\n",
            "Epoch 6: val_loss did not improve from 0.39705\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.4205 - mae: 0.3421 - val_loss: 0.4359 - val_mae: 0.4836\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4310 - mae: 0.3868\n",
            "Epoch 7: val_loss did not improve from 0.39705\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.4310 - mae: 0.3868 - val_loss: 0.4159 - val_mae: 0.4346\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.4020 - mae: 0.3376\n",
            "Epoch 8: val_loss improved from 0.39705 to 0.37629, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.4020 - mae: 0.3376 - val_loss: 0.3763 - val_mae: 0.2959\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3931 - mae: 0.3313\n",
            "Epoch 9: val_loss did not improve from 0.37629\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3931 - mae: 0.3313 - val_loss: 0.4168 - val_mae: 0.4514\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3995 - mae: 0.3454\n",
            "Epoch 10: val_loss did not improve from 0.37629\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3995 - mae: 0.3454 - val_loss: 0.3799 - val_mae: 0.3795\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3888 - mae: 0.3384\n",
            "Epoch 11: val_loss improved from 0.37629 to 0.37534, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.3888 - mae: 0.3384 - val_loss: 0.3753 - val_mae: 0.3566\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3998 - mae: 0.3646\n",
            "Epoch 12: val_loss improved from 0.37534 to 0.36934, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3998 - mae: 0.3646 - val_loss: 0.3693 - val_mae: 0.2849\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3843 - mae: 0.3339\n",
            "Epoch 13: val_loss did not improve from 0.36934\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.3843 - mae: 0.3339 - val_loss: 0.3720 - val_mae: 0.3621\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3755 - mae: 0.3219\n",
            "Epoch 14: val_loss did not improve from 0.36934\n",
            "25/25 [==============================] - 2s 87ms/step - loss: 0.3755 - mae: 0.3219 - val_loss: 0.3788 - val_mae: 0.3336\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3689 - mae: 0.3165\n",
            "Epoch 15: val_loss improved from 0.36934 to 0.36082, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.3689 - mae: 0.3165 - val_loss: 0.3608 - val_mae: 0.3105\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3708 - mae: 0.3316\n",
            "Epoch 16: val_loss improved from 0.36082 to 0.34516, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 100ms/step - loss: 0.3708 - mae: 0.3316 - val_loss: 0.3452 - val_mae: 0.2914\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3564 - mae: 0.3089\n",
            "Epoch 17: val_loss did not improve from 0.34516\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.3564 - mae: 0.3089 - val_loss: 0.3824 - val_mae: 0.4180\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3586 - mae: 0.3330\n",
            "Epoch 18: val_loss did not improve from 0.34516\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3586 - mae: 0.3330 - val_loss: 0.3489 - val_mae: 0.3205\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3449 - mae: 0.3161\n",
            "Epoch 19: val_loss improved from 0.34516 to 0.34029, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3449 - mae: 0.3161 - val_loss: 0.3403 - val_mae: 0.2711\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3773 - mae: 0.3803\n",
            "Epoch 20: val_loss did not improve from 0.34029\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.3773 - mae: 0.3803 - val_loss: 0.3891 - val_mae: 0.3766\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3848 - mae: 0.3862\n",
            "Epoch 21: val_loss improved from 0.34029 to 0.31560, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3848 - mae: 0.3862 - val_loss: 0.3156 - val_mae: 0.2853\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3229 - mae: 0.3051\n",
            "Epoch 22: val_loss did not improve from 0.31560\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3229 - mae: 0.3051 - val_loss: 0.3170 - val_mae: 0.2791\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3175 - mae: 0.3116\n",
            "Epoch 23: val_loss did not improve from 0.31560\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3175 - mae: 0.3116 - val_loss: 0.3337 - val_mae: 0.3335\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3109 - mae: 0.3162\n",
            "Epoch 24: val_loss improved from 0.31560 to 0.30427, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.3109 - mae: 0.3162 - val_loss: 0.3043 - val_mae: 0.2791\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2890 - mae: 0.2922\n",
            "Epoch 25: val_loss did not improve from 0.30427\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2890 - mae: 0.2922 - val_loss: 0.3278 - val_mae: 0.3958\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.3049 - mae: 0.3396\n",
            "Epoch 26: val_loss did not improve from 0.30427\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.3049 - mae: 0.3396 - val_loss: 0.3260 - val_mae: 0.3277\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2753 - mae: 0.3004\n",
            "Epoch 27: val_loss improved from 0.30427 to 0.24284, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.2753 - mae: 0.3004 - val_loss: 0.2428 - val_mae: 0.2436\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2557 - mae: 0.2852\n",
            "Epoch 28: val_loss did not improve from 0.24284\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2557 - mae: 0.2852 - val_loss: 0.3586 - val_mae: 0.3648\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2456 - mae: 0.2814\n",
            "Epoch 29: val_loss improved from 0.24284 to 0.22981, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.2456 - mae: 0.2814 - val_loss: 0.2298 - val_mae: 0.2388\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2320 - mae: 0.2776\n",
            "Epoch 30: val_loss did not improve from 0.22981\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2320 - mae: 0.2776 - val_loss: 0.3711 - val_mae: 0.4104\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2622 - mae: 0.3198\n",
            "Epoch 31: val_loss improved from 0.22981 to 0.21474, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.2622 - mae: 0.3198 - val_loss: 0.2147 - val_mae: 0.2603\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1822 - mae: 0.2220\n",
            "Epoch 32: val_loss improved from 0.21474 to 0.19339, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 102ms/step - loss: 0.1822 - mae: 0.2220 - val_loss: 0.1934 - val_mae: 0.2315\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1693 - mae: 0.2172\n",
            "Epoch 33: val_loss did not improve from 0.19339\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1693 - mae: 0.2172 - val_loss: 0.2130 - val_mae: 0.2961\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2651 - mae: 0.3414\n",
            "Epoch 34: val_loss did not improve from 0.19339\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.2651 - mae: 0.3414 - val_loss: 0.2155 - val_mae: 0.3050\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1885 - mae: 0.2657\n",
            "Epoch 35: val_loss improved from 0.19339 to 0.17586, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 96ms/step - loss: 0.1885 - mae: 0.2657 - val_loss: 0.1759 - val_mae: 0.2428\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1541 - mae: 0.2182\n",
            "Epoch 36: val_loss improved from 0.17586 to 0.15986, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 3s 101ms/step - loss: 0.1541 - mae: 0.2182 - val_loss: 0.1599 - val_mae: 0.1846\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1607 - mae: 0.2427\n",
            "Epoch 37: val_loss improved from 0.15986 to 0.15545, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1607 - mae: 0.2427 - val_loss: 0.1554 - val_mae: 0.1879\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1371 - mae: 0.2060\n",
            "Epoch 38: val_loss did not improve from 0.15545\n",
            "25/25 [==============================] - 2s 88ms/step - loss: 0.1371 - mae: 0.2060 - val_loss: 0.2819 - val_mae: 0.3826\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1656 - mae: 0.2612\n",
            "Epoch 39: val_loss did not improve from 0.15545\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1656 - mae: 0.2612 - val_loss: 0.2341 - val_mae: 0.3054\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2345 - mae: 0.3392\n",
            "Epoch 40: val_loss did not improve from 0.15545\n",
            "25/25 [==============================] - 2s 89ms/step - loss: 0.2345 - mae: 0.3392 - val_loss: 0.1604 - val_mae: 0.1809\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1619 - mae: 0.2471\n",
            "Epoch 41: val_loss did not improve from 0.15545\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.1619 - mae: 0.2471 - val_loss: 0.2339 - val_mae: 0.3149\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1323 - mae: 0.2171\n",
            "Epoch 42: val_loss improved from 0.15545 to 0.13939, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1323 - mae: 0.2171 - val_loss: 0.1394 - val_mae: 0.1923\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1106 - mae: 0.1791\n",
            "Epoch 43: val_loss improved from 0.13939 to 0.12836, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.1106 - mae: 0.1791 - val_loss: 0.1284 - val_mae: 0.1992\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1510 - mae: 0.2547\n",
            "Epoch 44: val_loss did not improve from 0.12836\n",
            "25/25 [==============================] - 2s 84ms/step - loss: 0.1510 - mae: 0.2547 - val_loss: 0.1296 - val_mae: 0.1623\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1135 - mae: 0.1902\n",
            "Epoch 45: val_loss improved from 0.12836 to 0.12236, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 95ms/step - loss: 0.1135 - mae: 0.1902 - val_loss: 0.1224 - val_mae: 0.1670\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1493 - mae: 0.2558\n",
            "Epoch 46: val_loss improved from 0.12236 to 0.12199, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 98ms/step - loss: 0.1493 - mae: 0.2558 - val_loss: 0.1220 - val_mae: 0.1670\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1551 - mae: 0.2637\n",
            "Epoch 47: val_loss did not improve from 0.12199\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1551 - mae: 0.2637 - val_loss: 0.1836 - val_mae: 0.2778\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1200 - mae: 0.2109\n",
            "Epoch 48: val_loss did not improve from 0.12199\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1200 - mae: 0.2109 - val_loss: 0.1749 - val_mae: 0.3325\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2706 - mae: 0.3783\n",
            "Epoch 49: val_loss did not improve from 0.12199\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2706 - mae: 0.3783 - val_loss: 0.1564 - val_mae: 0.2142\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1170 - mae: 0.2032\n",
            "Epoch 50: val_loss improved from 0.12199 to 0.11638, saving model to ./cnn_model/MG_merge/tmp/word2vec.model.hdf5\n",
            "25/25 [==============================] - 2s 97ms/step - loss: 0.1170 - mae: 0.2032 - val_loss: 0.1164 - val_mae: 0.1665\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1012 - mae: 0.1898\n",
            "Epoch 51: val_loss did not improve from 0.11638\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.1012 - mae: 0.1898 - val_loss: 0.1273 - val_mae: 0.2461\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0993 - mae: 0.1882\n",
            "Epoch 52: val_loss did not improve from 0.11638\n",
            "25/25 [==============================] - 2s 90ms/step - loss: 0.0993 - mae: 0.1882 - val_loss: 0.1166 - val_mae: 0.1511\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0912 - mae: 0.1762\n",
            "Epoch 53: val_loss did not improve from 0.11638\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.0912 - mae: 0.1762 - val_loss: 0.1219 - val_mae: 0.1877\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0925 - mae: 0.1754\n",
            "Epoch 54: val_loss did not improve from 0.11638\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.0925 - mae: 0.1754 - val_loss: 0.1831 - val_mae: 0.2985\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0965 - mae: 0.1910\n",
            "Epoch 55: val_loss did not improve from 0.11638\n",
            "25/25 [==============================] - 2s 85ms/step - loss: 0.0965 - mae: 0.1910 - val_loss: 0.1631 - val_mae: 0.2406\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.2280 - mae: 0.3390\n",
            "Epoch 56: val_loss did not improve from 0.11638\n",
            "25/25 [==============================] - 2s 86ms/step - loss: 0.2280 - mae: 0.3390 - val_loss: 0.2790 - val_mae: 0.4713\n",
            "Epoch 56: early stopping\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 3s 5ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "4/4 [==============================] - 0s 22ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.11209631711244583, RMSE:0.3348078727722168, MAE:0.17271152138710022, R2:0.8692060884236601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:689: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7898564382883024, 0.895401236480248, 0.8945921456551583, 0.8775021410006084, -15.040280030095737, 0.8870295410026343, -15.04192970700353, -15.04001659674605, 0.606824009099239, 0.8827780960464037, -15.040280030095737, -15.040040183923402, 0.8532197817438554, 0.5474893126544897, -15.055156886640493, -15.040831191761544, -15.04006260303602, -15.04151861559031, -15.040613770375487, 0.8692060884236601]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_SG_merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYWDmcgK2KNI",
        "outputId": "5e80b333-1ca1-4f09-854c-6cf6a3a1f604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8404397759662624,\n",
              " 0.8403280665778942,\n",
              " 0.8355004636743163,\n",
              " 0.8208336965200509,\n",
              " 0.8426558181999703,\n",
              " 0.8380181065462868,\n",
              " 0.8445559758494802,\n",
              " 0.8451903626131092,\n",
              " 0.8014731286273536,\n",
              " 0.8316815533481572,\n",
              " 0.8318390389158271,\n",
              " 0.8509652318431318,\n",
              " 0.8252615713353465,\n",
              " 0.8370728042445453,\n",
              " 0.798863135421634,\n",
              " 0.8065425692152892,\n",
              " 0.8532220419550329,\n",
              " 0.8381169469272299,\n",
              " 0.8087596296790454,\n",
              " 0.8464527605056376]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_SG_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnBrfhr12Nja",
        "outputId": "edb0989f-345c-4ed1-8a09-b0c3697f719a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.015441060128791428,\n",
              " 0.06660595280856729,\n",
              " 0.057687555104453,\n",
              " 0.024239409486910612,\n",
              " -0.02467382782391736,\n",
              " 0.15113259425115744,\n",
              " 0.012020036471809936,\n",
              " 0.18196206547355598,\n",
              " 0.04624251756716846,\n",
              " 0.017468553496777006,\n",
              " 0.04855712997036077,\n",
              " 0.17769785502510116,\n",
              " 0.10004279282495943,\n",
              " 0.009408563706876372,\n",
              " 0.01185616168204262,\n",
              " -0.020606800611077558,\n",
              " 0.1630530721057133,\n",
              " 0.11683536432251918,\n",
              " 0.09647933283335941,\n",
              " 0.061938110133189395]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_MG_merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeVW82MY2UyP",
        "outputId": "8682d998-a2f6-47c8-c6bc-b256ce415c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7898564382883024,\n",
              " 0.895401236480248,\n",
              " 0.8945921456551583,\n",
              " 0.8775021410006084,\n",
              " -15.040280030095737,\n",
              " 0.8870295410026343,\n",
              " -15.04192970700353,\n",
              " -15.04001659674605,\n",
              " 0.606824009099239,\n",
              " 0.8827780960464037,\n",
              " -15.040280030095737,\n",
              " -15.040040183923402,\n",
              " 0.8532197817438554,\n",
              " 0.5474893126544897,\n",
              " -15.055156886640493,\n",
              " -15.040831191761544,\n",
              " -15.04006260303602,\n",
              " -15.04151861559031,\n",
              " -15.040613770375487,\n",
              " 0.8692060884236601]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.1原卷积神经网络模型预测(2)**"
      ],
      "metadata": {
        "id": "4k8U4OIdXo6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Cyxp1gWpaoWg",
        "outputId": "95256409-7230-442f-a308-637d7f7c6934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'weighted_11:26_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_SG_merge=[]\n",
        "seed = 1234\n",
        "\n",
        "def test_single_channel_merge_model(model, modelpath,X_test,demographics_test,y_test, index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    print(r2)\n",
        "    r2_SG_merge.append(r2)\n",
        "    name = 'SG_merge%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "r2_SG_split=[]\n",
        "def test_single_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test,  demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    r2_SG_split.append(r2)\n",
        "    name = 'SG_merge%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "r2_MG_merge=[]\n",
        "def test_multi_channel_split_model(model, modelpath, X1_test,  demographics_test, y_test,index):\n",
        "    print('Testing model...') \n",
        "    model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "    y_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "    print(\"y_pred:{}\".format(y_pred.shape))\n",
        "    r2,rmse = evaluation(y_test, y_pred)\n",
        "    r2_MG_merge.append(r2)\n",
        "    name = 'MG_split%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "for i in range(1):\n",
        "  modelpath = 'SG_merge' + path\n",
        "  model = single_channel_merge_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_single_channel_merge_model(model, modelpath, X_train, demographics_train, y_train)\n",
        "  print('testing on the testing datasets.....')\n",
        "  print(\"modelpath:\",modelpath)\n",
        "  test_single_channel_merge_model(model, modelpath, X_test, demographics_test, y_test, 3)\n",
        "  print('testing on the filtered testing datasets.....')\n",
        "  print(r2_SG_merge)\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "  modelpath = 'SG_split' + path\n",
        "  model = single_channel_split_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_single_channel_split_model(model, modelpath, X1_train, demographics_train, y_train)\n",
        "  print(f,'testing on the testing datasets.....')\n",
        "  test_single_channel_split_model(model, modelpath, X1_test,  demographics_test,y_test,3)\n",
        "  print('testing on the filtered testing datasets.....')\n",
        "  print(r2_SG_split)\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "  modelpath = 'MG_merge' + path\n",
        "  model= multi_channel_split_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_multi_channel_split_model(model, modelpath, X1_train, demographics_train, y_train)\n",
        "  extract_patientvec(model, modelpath, disease, demographics)\n",
        "  print(f,'testing on the testing datasets.....')\n",
        "  test_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
        "  print(r2_MG_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze8-yZkzXwCl",
        "outputId": "d41b50b5-4566-426d-ab85-63cf22162486"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n",
            "Model: \"model_43\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_31 (Embedding)       (None, 17, 900)      649800      ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 17, 100)      270100      ['embedding_31[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 17, 100)      270100      ['embedding_31[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 17, 100)      270100      ['embedding_31[0][0]']           \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_93 (Globa  (None, 100)         0           ['conv1d_39[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_94 (Globa  (None, 100)         0           ['conv1d_40[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_95 (Globa  (None, 100)         0           ['conv1d_41[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 3)            33          ['input_28[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_93[0][0]',\n",
            "                                                                  'global_max_pooling1d_94[0][0]',\n",
            "                                                                  'global_max_pooling1d_95[0][0]',\n",
            "                                                                  'dense_51[0][0]']               \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 500)          152000      ['concatenate_13[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 500)          0           ['dense_52[0][0]']               \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 100)          50100       ['dropout_47[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 100)          0           ['dense_53[0][0]']               \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 1)            101         ['dropout_48[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 99/100 [============================>.] - ETA: 0s - loss: 5.9327 - mae: 1.6349\n",
            "Epoch 1: val_loss improved from inf to 0.84720, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 12ms/step - loss: 5.9214 - mae: 1.6328 - val_loss: 0.8472 - val_mae: 0.6172\n",
            "Epoch 2/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.8654 - mae: 0.6256\n",
            "Epoch 2: val_loss improved from 0.84720 to 0.75428, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8644 - mae: 0.6249 - val_loss: 0.7543 - val_mae: 0.5834\n",
            "Epoch 3/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7836 - mae: 0.5997\n",
            "Epoch 3: val_loss improved from 0.75428 to 0.68525, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7836 - mae: 0.5988 - val_loss: 0.6853 - val_mae: 0.5445\n",
            "Epoch 4/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7186 - mae: 0.5748\n",
            "Epoch 4: val_loss improved from 0.68525 to 0.62978, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7177 - mae: 0.5740 - val_loss: 0.6298 - val_mae: 0.5167\n",
            "Epoch 5/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6661 - mae: 0.5544\n",
            "Epoch 5: val_loss improved from 0.62978 to 0.58525, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.6675 - mae: 0.5543 - val_loss: 0.5852 - val_mae: 0.5072\n",
            "Epoch 6/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6331 - mae: 0.5421\n",
            "Epoch 6: val_loss improved from 0.58525 to 0.54850, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.6279 - mae: 0.5399 - val_loss: 0.5485 - val_mae: 0.4767\n",
            "Epoch 7/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.5921 - mae: 0.5242\n",
            "Epoch 7: val_loss improved from 0.54850 to 0.51846, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5922 - mae: 0.5244 - val_loss: 0.5185 - val_mae: 0.4622\n",
            "Epoch 8/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.5629 - mae: 0.5108\n",
            "Epoch 8: val_loss improved from 0.51846 to 0.48819, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.5629 - mae: 0.5115 - val_loss: 0.4882 - val_mae: 0.4751\n",
            "Epoch 9/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.5306 - mae: 0.5022\n",
            "Epoch 9: val_loss improved from 0.48819 to 0.46090, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5305 - mae: 0.5021 - val_loss: 0.4609 - val_mae: 0.4597\n",
            "Epoch 10/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.5094 - mae: 0.4935\n",
            "Epoch 10: val_loss improved from 0.46090 to 0.43777, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5044 - mae: 0.4913 - val_loss: 0.4378 - val_mae: 0.4417\n",
            "Epoch 11/200\n",
            " 94/100 [===========================>..] - ETA: 0s - loss: 0.4826 - mae: 0.4792\n",
            "Epoch 11: val_loss improved from 0.43777 to 0.41666, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4809 - mae: 0.4778 - val_loss: 0.4167 - val_mae: 0.4325\n",
            "Epoch 12/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.4583 - mae: 0.4673\n",
            "Epoch 12: val_loss improved from 0.41666 to 0.39331, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4584 - mae: 0.4673 - val_loss: 0.3933 - val_mae: 0.4276\n",
            "Epoch 13/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4372 - mae: 0.4578\n",
            "Epoch 13: val_loss improved from 0.39331 to 0.37375, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4378 - mae: 0.4581 - val_loss: 0.3738 - val_mae: 0.4167\n",
            "Epoch 14/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.4161 - mae: 0.4471\n",
            "Epoch 14: val_loss improved from 0.37375 to 0.35696, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.4199 - mae: 0.4484 - val_loss: 0.3570 - val_mae: 0.4056\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4032 - mae: 0.4380\n",
            "Epoch 15: val_loss improved from 0.35696 to 0.33815, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4032 - mae: 0.4380 - val_loss: 0.3382 - val_mae: 0.3961\n",
            "Epoch 16/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.3894 - mae: 0.4301\n",
            "Epoch 16: val_loss improved from 0.33815 to 0.32260, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3879 - mae: 0.4298 - val_loss: 0.3226 - val_mae: 0.3864\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3711 - mae: 0.4214\n",
            "Epoch 17: val_loss improved from 0.32260 to 0.30617, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3711 - mae: 0.4214 - val_loss: 0.3062 - val_mae: 0.3775\n",
            "Epoch 18/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.3497 - mae: 0.4090\n",
            "Epoch 18: val_loss improved from 0.30617 to 0.29171, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3490 - mae: 0.4088 - val_loss: 0.2917 - val_mae: 0.3706\n",
            "Epoch 19/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.3382 - mae: 0.4028\n",
            "Epoch 19: val_loss improved from 0.29171 to 0.27716, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3396 - mae: 0.4030 - val_loss: 0.2772 - val_mae: 0.3591\n",
            "Epoch 20/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.3237 - mae: 0.3916\n",
            "Epoch 20: val_loss improved from 0.27716 to 0.26337, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3245 - mae: 0.3918 - val_loss: 0.2634 - val_mae: 0.3501\n",
            "Epoch 21/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.3073 - mae: 0.3817\n",
            "Epoch 21: val_loss improved from 0.26337 to 0.25018, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3073 - mae: 0.3820 - val_loss: 0.2502 - val_mae: 0.3412\n",
            "Epoch 22/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2957 - mae: 0.3751\n",
            "Epoch 22: val_loss improved from 0.25018 to 0.23830, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2956 - mae: 0.3751 - val_loss: 0.2383 - val_mae: 0.3315\n",
            "Epoch 23/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.2835 - mae: 0.3662\n",
            "Epoch 23: val_loss improved from 0.23830 to 0.22568, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.2828 - mae: 0.3658 - val_loss: 0.2257 - val_mae: 0.3216\n",
            "Epoch 24/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.2665 - mae: 0.3537\n",
            "Epoch 24: val_loss improved from 0.22568 to 0.21507, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2674 - mae: 0.3542 - val_loss: 0.2151 - val_mae: 0.3132\n",
            "Epoch 25/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.2585 - mae: 0.3478\n",
            "Epoch 25: val_loss improved from 0.21507 to 0.20808, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2579 - mae: 0.3476 - val_loss: 0.2081 - val_mae: 0.3070\n",
            "Epoch 26/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2466 - mae: 0.3386\n",
            "Epoch 26: val_loss improved from 0.20808 to 0.19533, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2454 - mae: 0.3384 - val_loss: 0.1953 - val_mae: 0.2946\n",
            "Epoch 27/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2375 - mae: 0.3312\n",
            "Epoch 27: val_loss improved from 0.19533 to 0.18585, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2366 - mae: 0.3308 - val_loss: 0.1859 - val_mae: 0.2872\n",
            "Epoch 28/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2317 - mae: 0.3260\n",
            "Epoch 28: val_loss improved from 0.18585 to 0.17798, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2320 - mae: 0.3262 - val_loss: 0.1780 - val_mae: 0.2794\n",
            "Epoch 29/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.2180 - mae: 0.3171\n",
            "Epoch 29: val_loss improved from 0.17798 to 0.17096, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2196 - mae: 0.3179 - val_loss: 0.1710 - val_mae: 0.2724\n",
            "Epoch 30/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2138 - mae: 0.3123\n",
            "Epoch 30: val_loss improved from 0.17096 to 0.16503, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2130 - mae: 0.3118 - val_loss: 0.1650 - val_mae: 0.2669\n",
            "Epoch 31/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.2107 - mae: 0.3083\n",
            "Epoch 31: val_loss improved from 0.16503 to 0.15857, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.2114 - mae: 0.3088 - val_loss: 0.1586 - val_mae: 0.2601\n",
            "Epoch 32/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1996 - mae: 0.3010\n",
            "Epoch 32: val_loss improved from 0.15857 to 0.15344, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.2001 - mae: 0.3009 - val_loss: 0.1534 - val_mae: 0.2547\n",
            "Epoch 33/200\n",
            " 94/100 [===========================>..] - ETA: 0s - loss: 0.1979 - mae: 0.2973\n",
            "Epoch 33: val_loss improved from 0.15344 to 0.14929, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1992 - mae: 0.2979 - val_loss: 0.1493 - val_mae: 0.2505\n",
            "Epoch 34/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1935 - mae: 0.2933\n",
            "Epoch 34: val_loss improved from 0.14929 to 0.14562, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1938 - mae: 0.2934 - val_loss: 0.1456 - val_mae: 0.2474\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1897 - mae: 0.2905\n",
            "Epoch 35: val_loss improved from 0.14562 to 0.14150, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1897 - mae: 0.2905 - val_loss: 0.1415 - val_mae: 0.2423\n",
            "Epoch 36/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1812 - mae: 0.2835\n",
            "Epoch 36: val_loss improved from 0.14150 to 0.13775, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1815 - mae: 0.2833 - val_loss: 0.1377 - val_mae: 0.2373\n",
            "Epoch 37/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1822 - mae: 0.2840\n",
            "Epoch 37: val_loss improved from 0.13775 to 0.13739, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1825 - mae: 0.2838 - val_loss: 0.1374 - val_mae: 0.2404\n",
            "Epoch 38/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1797 - mae: 0.2808\n",
            "Epoch 38: val_loss improved from 0.13739 to 0.13248, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1783 - mae: 0.2799 - val_loss: 0.1325 - val_mae: 0.2317\n",
            "Epoch 39/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.2761\n",
            "Epoch 39: val_loss improved from 0.13248 to 0.12989, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1736 - mae: 0.2757 - val_loss: 0.1299 - val_mae: 0.2276\n",
            "Epoch 40/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.2744\n",
            "Epoch 40: val_loss improved from 0.12989 to 0.12773, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1723 - mae: 0.2739 - val_loss: 0.1277 - val_mae: 0.2250\n",
            "Epoch 41/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1708 - mae: 0.2726\n",
            "Epoch 41: val_loss improved from 0.12773 to 0.12616, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1705 - mae: 0.2731 - val_loss: 0.1262 - val_mae: 0.2238\n",
            "Epoch 42/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1673 - mae: 0.2688\n",
            "Epoch 42: val_loss improved from 0.12616 to 0.12429, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1677 - mae: 0.2683 - val_loss: 0.1243 - val_mae: 0.2206\n",
            "Epoch 43/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1683 - mae: 0.2695\n",
            "Epoch 43: val_loss improved from 0.12429 to 0.12356, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1668 - mae: 0.2693 - val_loss: 0.1236 - val_mae: 0.2211\n",
            "Epoch 44/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1607 - mae: 0.2636\n",
            "Epoch 44: val_loss improved from 0.12356 to 0.12181, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1626 - mae: 0.2649 - val_loss: 0.1218 - val_mae: 0.2180\n",
            "Epoch 45/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1621 - mae: 0.2651\n",
            "Epoch 45: val_loss improved from 0.12181 to 0.12037, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1630 - mae: 0.2652 - val_loss: 0.1204 - val_mae: 0.2155\n",
            "Epoch 46/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1639 - mae: 0.2653\n",
            "Epoch 46: val_loss improved from 0.12037 to 0.11933, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1632 - mae: 0.2649 - val_loss: 0.1193 - val_mae: 0.2144\n",
            "Epoch 47/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1609 - mae: 0.2633\n",
            "Epoch 47: val_loss improved from 0.11933 to 0.11922, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1607 - mae: 0.2632 - val_loss: 0.1192 - val_mae: 0.2156\n",
            "Epoch 48/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1607 - mae: 0.2605\n",
            "Epoch 48: val_loss improved from 0.11922 to 0.11762, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1606 - mae: 0.2605 - val_loss: 0.1176 - val_mae: 0.2124\n",
            "Epoch 49/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1576 - mae: 0.2595\n",
            "Epoch 49: val_loss did not improve from 0.11762\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1573 - mae: 0.2592 - val_loss: 0.1191 - val_mae: 0.2174\n",
            "Epoch 50/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1524 - mae: 0.2559\n",
            "Epoch 50: val_loss improved from 0.11762 to 0.11595, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1552 - mae: 0.2570 - val_loss: 0.1159 - val_mae: 0.2090\n",
            "Epoch 51/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1563 - mae: 0.2586\n",
            "Epoch 51: val_loss improved from 0.11595 to 0.11501, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1573 - mae: 0.2591 - val_loss: 0.1150 - val_mae: 0.2076\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1528 - mae: 0.2549\n",
            "Epoch 52: val_loss did not improve from 0.11501\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1528 - mae: 0.2549 - val_loss: 0.1150 - val_mae: 0.2086\n",
            "Epoch 53/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1537 - mae: 0.2549\n",
            "Epoch 53: val_loss improved from 0.11501 to 0.11372, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1539 - mae: 0.2551 - val_loss: 0.1137 - val_mae: 0.2055\n",
            "Epoch 54/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1539 - mae: 0.2565\n",
            "Epoch 54: val_loss did not improve from 0.11372\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1539 - mae: 0.2565 - val_loss: 0.1142 - val_mae: 0.2076\n",
            "Epoch 55/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1536 - mae: 0.2536\n",
            "Epoch 55: val_loss improved from 0.11372 to 0.11284, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1534 - mae: 0.2536 - val_loss: 0.1128 - val_mae: 0.2043\n",
            "Epoch 56/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1538 - mae: 0.2539\n",
            "Epoch 56: val_loss improved from 0.11284 to 0.11228, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1522 - mae: 0.2528 - val_loss: 0.1123 - val_mae: 0.2032\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1504 - mae: 0.2518\n",
            "Epoch 57: val_loss improved from 0.11228 to 0.11192, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1504 - mae: 0.2518 - val_loss: 0.1119 - val_mae: 0.2026\n",
            "Epoch 58/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1491 - mae: 0.2501\n",
            "Epoch 58: val_loss improved from 0.11192 to 0.11117, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1504 - mae: 0.2508 - val_loss: 0.1112 - val_mae: 0.2012\n",
            "Epoch 59/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1476 - mae: 0.2498\n",
            "Epoch 59: val_loss did not improve from 0.11117\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1496 - mae: 0.2506 - val_loss: 0.1114 - val_mae: 0.2025\n",
            "Epoch 60/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1483 - mae: 0.2493\n",
            "Epoch 60: val_loss improved from 0.11117 to 0.11057, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1489 - mae: 0.2490 - val_loss: 0.1106 - val_mae: 0.2006\n",
            "Epoch 61/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1476 - mae: 0.2494\n",
            "Epoch 61: val_loss improved from 0.11057 to 0.11029, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1486 - mae: 0.2501 - val_loss: 0.1103 - val_mae: 0.2003\n",
            "Epoch 62/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1477 - mae: 0.2477\n",
            "Epoch 62: val_loss improved from 0.11029 to 0.11027, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1480 - mae: 0.2479 - val_loss: 0.1103 - val_mae: 0.2005\n",
            "Epoch 63/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1477 - mae: 0.2483\n",
            "Epoch 63: val_loss improved from 0.11027 to 0.10935, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1475 - mae: 0.2484 - val_loss: 0.1094 - val_mae: 0.1983\n",
            "Epoch 64/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1476 - mae: 0.2496\n",
            "Epoch 64: val_loss improved from 0.10935 to 0.10892, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1471 - mae: 0.2491 - val_loss: 0.1089 - val_mae: 0.1975\n",
            "Epoch 65/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1482 - mae: 0.2468\n",
            "Epoch 65: val_loss improved from 0.10892 to 0.10848, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1480 - mae: 0.2467 - val_loss: 0.1085 - val_mae: 0.1968\n",
            "Epoch 66/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1492 - mae: 0.2477\n",
            "Epoch 66: val_loss improved from 0.10848 to 0.10841, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1481 - mae: 0.2475 - val_loss: 0.1084 - val_mae: 0.1970\n",
            "Epoch 67/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1474 - mae: 0.2470\n",
            "Epoch 67: val_loss did not improve from 0.10841\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1462 - mae: 0.2463 - val_loss: 0.1087 - val_mae: 0.1981\n",
            "Epoch 68/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1463 - mae: 0.2456\n",
            "Epoch 68: val_loss did not improve from 0.10841\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1452 - mae: 0.2452 - val_loss: 0.1085 - val_mae: 0.1977\n",
            "Epoch 69/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1455 - mae: 0.2437\n",
            "Epoch 69: val_loss improved from 0.10841 to 0.10750, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1455 - mae: 0.2437 - val_loss: 0.1075 - val_mae: 0.1950\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1450 - mae: 0.2427\n",
            "Epoch 70: val_loss improved from 0.10750 to 0.10721, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1450 - mae: 0.2427 - val_loss: 0.1072 - val_mae: 0.1945\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1424 - mae: 0.2428\n",
            "Epoch 71: val_loss did not improve from 0.10721\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1424 - mae: 0.2428 - val_loss: 0.1074 - val_mae: 0.1952\n",
            "Epoch 72/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1405 - mae: 0.2401\n",
            "Epoch 72: val_loss improved from 0.10721 to 0.10669, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1417 - mae: 0.2409 - val_loss: 0.1067 - val_mae: 0.1936\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1388 - mae: 0.2392\n",
            "Epoch 73: val_loss improved from 0.10669 to 0.10632, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1388 - mae: 0.2392 - val_loss: 0.1063 - val_mae: 0.1929\n",
            "Epoch 74/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1401 - mae: 0.2422\n",
            "Epoch 74: val_loss did not improve from 0.10632\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1422 - mae: 0.2432 - val_loss: 0.1072 - val_mae: 0.1954\n",
            "Epoch 75/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1411 - mae: 0.2411\n",
            "Epoch 75: val_loss improved from 0.10632 to 0.10608, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1410 - mae: 0.2414 - val_loss: 0.1061 - val_mae: 0.1925\n",
            "Epoch 76/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1388 - mae: 0.2395\n",
            "Epoch 76: val_loss improved from 0.10608 to 0.10569, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1396 - mae: 0.2400 - val_loss: 0.1057 - val_mae: 0.1918\n",
            "Epoch 77/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1433 - mae: 0.2409\n",
            "Epoch 77: val_loss improved from 0.10569 to 0.10546, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1412 - mae: 0.2399 - val_loss: 0.1055 - val_mae: 0.1913\n",
            "Epoch 78/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1411 - mae: 0.2403\n",
            "Epoch 78: val_loss did not improve from 0.10546\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1407 - mae: 0.2400 - val_loss: 0.1055 - val_mae: 0.1916\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1398 - mae: 0.2392\n",
            "Epoch 79: val_loss improved from 0.10546 to 0.10516, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1398 - mae: 0.2392 - val_loss: 0.1052 - val_mae: 0.1910\n",
            "Epoch 80/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1423 - mae: 0.2415\n",
            "Epoch 80: val_loss improved from 0.10516 to 0.10479, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1430 - mae: 0.2422 - val_loss: 0.1048 - val_mae: 0.1899\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1380 - mae: 0.2384\n",
            "Epoch 81: val_loss improved from 0.10479 to 0.10477, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1380 - mae: 0.2384 - val_loss: 0.1048 - val_mae: 0.1903\n",
            "Epoch 82/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1396 - mae: 0.2394\n",
            "Epoch 82: val_loss did not improve from 0.10477\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1393 - mae: 0.2391 - val_loss: 0.1052 - val_mae: 0.1919\n",
            "Epoch 83/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1421 - mae: 0.2407\n",
            "Epoch 83: val_loss did not improve from 0.10477\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1409 - mae: 0.2401 - val_loss: 0.1051 - val_mae: 0.1922\n",
            "Epoch 84/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1355 - mae: 0.2361\n",
            "Epoch 84: val_loss improved from 0.10477 to 0.10446, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1355 - mae: 0.2360 - val_loss: 0.1045 - val_mae: 0.1902\n",
            "Epoch 85/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1384 - mae: 0.2360\n",
            "Epoch 85: val_loss improved from 0.10446 to 0.10404, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1381 - mae: 0.2358 - val_loss: 0.1040 - val_mae: 0.1890\n",
            "Epoch 86/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1362 - mae: 0.2355\n",
            "Epoch 86: val_loss improved from 0.10404 to 0.10369, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1363 - mae: 0.2357 - val_loss: 0.1037 - val_mae: 0.1881\n",
            "Epoch 87/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1367 - mae: 0.2361\n",
            "Epoch 87: val_loss improved from 0.10369 to 0.10360, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1376 - mae: 0.2368 - val_loss: 0.1036 - val_mae: 0.1879\n",
            "Epoch 88/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1384 - mae: 0.2397\n",
            "Epoch 88: val_loss did not improve from 0.10360\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1391 - mae: 0.2398 - val_loss: 0.1053 - val_mae: 0.1930\n",
            "Epoch 89/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1314 - mae: 0.2337\n",
            "Epoch 89: val_loss improved from 0.10360 to 0.10331, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1355 - mae: 0.2351 - val_loss: 0.1033 - val_mae: 0.1873\n",
            "Epoch 90/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1352 - mae: 0.2346\n",
            "Epoch 90: val_loss improved from 0.10331 to 0.10316, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1344 - mae: 0.2340 - val_loss: 0.1032 - val_mae: 0.1871\n",
            "Epoch 91/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1350 - mae: 0.2342\n",
            "Epoch 91: val_loss improved from 0.10316 to 0.10303, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1357 - mae: 0.2348 - val_loss: 0.1030 - val_mae: 0.1868\n",
            "Epoch 92/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1356 - mae: 0.2354\n",
            "Epoch 92: val_loss improved from 0.10303 to 0.10301, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1355 - mae: 0.2354 - val_loss: 0.1030 - val_mae: 0.1869\n",
            "Epoch 93/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1346 - mae: 0.2332\n",
            "Epoch 93: val_loss did not improve from 0.10301\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1344 - mae: 0.2327 - val_loss: 0.1032 - val_mae: 0.1879\n",
            "Epoch 94/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1328 - mae: 0.2330\n",
            "Epoch 94: val_loss did not improve from 0.10301\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1326 - mae: 0.2328 - val_loss: 0.1033 - val_mae: 0.1886\n",
            "Epoch 95/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1339 - mae: 0.2325\n",
            "Epoch 95: val_loss improved from 0.10301 to 0.10252, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1347 - mae: 0.2330 - val_loss: 0.1025 - val_mae: 0.1860\n",
            "Epoch 96/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1343 - mae: 0.2329\n",
            "Epoch 96: val_loss improved from 0.10252 to 0.10233, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1335 - mae: 0.2325 - val_loss: 0.1023 - val_mae: 0.1856\n",
            "Epoch 97/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1320 - mae: 0.2325\n",
            "Epoch 97: val_loss did not improve from 0.10233\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1319 - mae: 0.2325 - val_loss: 0.1029 - val_mae: 0.1878\n",
            "Epoch 98/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1321 - mae: 0.2313\n",
            "Epoch 98: val_loss improved from 0.10233 to 0.10214, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1330 - mae: 0.2313 - val_loss: 0.1021 - val_mae: 0.1855\n",
            "Epoch 99/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1342 - mae: 0.2328\n",
            "Epoch 99: val_loss did not improve from 0.10214\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1335 - mae: 0.2327 - val_loss: 0.1029 - val_mae: 0.1882\n",
            "Epoch 100/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1333 - mae: 0.2321\n",
            "Epoch 100: val_loss improved from 0.10214 to 0.10212, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1325 - mae: 0.2315 - val_loss: 0.1021 - val_mae: 0.1860\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1320 - mae: 0.2313\n",
            "Epoch 101: val_loss improved from 0.10212 to 0.10172, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1320 - mae: 0.2313 - val_loss: 0.1017 - val_mae: 0.1847\n",
            "Epoch 102/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1325 - mae: 0.2313\n",
            "Epoch 102: val_loss improved from 0.10172 to 0.10154, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1333 - mae: 0.2318 - val_loss: 0.1015 - val_mae: 0.1844\n",
            "Epoch 103/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1328 - mae: 0.2308\n",
            "Epoch 103: val_loss did not improve from 0.10154\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1318 - mae: 0.2302 - val_loss: 0.1018 - val_mae: 0.1855\n",
            "Epoch 104/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1293 - mae: 0.2289\n",
            "Epoch 104: val_loss improved from 0.10154 to 0.10136, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1291 - mae: 0.2289 - val_loss: 0.1014 - val_mae: 0.1841\n",
            "Epoch 105/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1315 - mae: 0.2320\n",
            "Epoch 105: val_loss improved from 0.10136 to 0.10128, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1310 - mae: 0.2315 - val_loss: 0.1013 - val_mae: 0.1838\n",
            "Epoch 106/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1311 - mae: 0.2306\n",
            "Epoch 106: val_loss improved from 0.10128 to 0.10118, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1317 - mae: 0.2308 - val_loss: 0.1012 - val_mae: 0.1837\n",
            "Epoch 107/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1332 - mae: 0.2318\n",
            "Epoch 107: val_loss did not improve from 0.10118\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1331 - mae: 0.2317 - val_loss: 0.1012 - val_mae: 0.1842\n",
            "Epoch 108/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1307 - mae: 0.2311\n",
            "Epoch 108: val_loss improved from 0.10118 to 0.10098, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1303 - mae: 0.2309 - val_loss: 0.1010 - val_mae: 0.1836\n",
            "Epoch 109/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1316 - mae: 0.2301\n",
            "Epoch 109: val_loss did not improve from 0.10098\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1313 - mae: 0.2301 - val_loss: 0.1010 - val_mae: 0.1837\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1307 - mae: 0.2299\n",
            "Epoch 110: val_loss improved from 0.10098 to 0.10096, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1307 - mae: 0.2299 - val_loss: 0.1010 - val_mae: 0.1837\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1307 - mae: 0.2313\n",
            "Epoch 111: val_loss did not improve from 0.10096\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1307 - mae: 0.2313 - val_loss: 0.1011 - val_mae: 0.1844\n",
            "Epoch 112/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1298 - mae: 0.2279\n",
            "Epoch 112: val_loss improved from 0.10096 to 0.10059, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1293 - mae: 0.2279 - val_loss: 0.1006 - val_mae: 0.1829\n",
            "Epoch 113/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1311 - mae: 0.2303\n",
            "Epoch 113: val_loss did not improve from 0.10059\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1312 - mae: 0.2304 - val_loss: 0.1019 - val_mae: 0.1875\n",
            "Epoch 114/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1300 - mae: 0.2294\n",
            "Epoch 114: val_loss did not improve from 0.10059\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1303 - mae: 0.2297 - val_loss: 0.1010 - val_mae: 0.1848\n",
            "Epoch 115/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1303 - mae: 0.2302\n",
            "Epoch 115: val_loss improved from 0.10059 to 0.10019, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1304 - mae: 0.2302 - val_loss: 0.1002 - val_mae: 0.1821\n",
            "Epoch 116/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1266 - mae: 0.2256\n",
            "Epoch 116: val_loss did not improve from 0.10019\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1261 - mae: 0.2255 - val_loss: 0.1011 - val_mae: 0.1847\n",
            "Epoch 117/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1282 - mae: 0.2261\n",
            "Epoch 117: val_loss did not improve from 0.10019\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1278 - mae: 0.2258 - val_loss: 0.1007 - val_mae: 0.1838\n",
            "Epoch 118/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1291 - mae: 0.2283\n",
            "Epoch 118: val_loss improved from 0.10019 to 0.09992, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1285 - mae: 0.2279 - val_loss: 0.0999 - val_mae: 0.1815\n",
            "Epoch 119/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1271 - mae: 0.2269\n",
            "Epoch 119: val_loss did not improve from 0.09992\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1266 - mae: 0.2267 - val_loss: 0.1007 - val_mae: 0.1838\n",
            "Epoch 120/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1283 - mae: 0.2270\n",
            "Epoch 120: val_loss improved from 0.09992 to 0.09977, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1279 - mae: 0.2269 - val_loss: 0.0998 - val_mae: 0.1812\n",
            "Epoch 121/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1253 - mae: 0.2241\n",
            "Epoch 121: val_loss did not improve from 0.09977\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1252 - mae: 0.2242 - val_loss: 0.0999 - val_mae: 0.1820\n",
            "Epoch 122/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1281 - mae: 0.2277\n",
            "Epoch 122: val_loss improved from 0.09977 to 0.09965, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1285 - mae: 0.2275 - val_loss: 0.0997 - val_mae: 0.1811\n",
            "Epoch 123/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1245 - mae: 0.2245\n",
            "Epoch 123: val_loss did not improve from 0.09965\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1250 - mae: 0.2249 - val_loss: 0.0998 - val_mae: 0.1816\n",
            "Epoch 124/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1265 - mae: 0.2258\n",
            "Epoch 124: val_loss did not improve from 0.09965\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1265 - mae: 0.2259 - val_loss: 0.0999 - val_mae: 0.1822\n",
            "Epoch 125/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1258 - mae: 0.2249\n",
            "Epoch 125: val_loss did not improve from 0.09965\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1263 - mae: 0.2252 - val_loss: 0.0997 - val_mae: 0.1811\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1270 - mae: 0.2247\n",
            "Epoch 126: val_loss improved from 0.09965 to 0.09946, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1270 - mae: 0.2247 - val_loss: 0.0995 - val_mae: 0.1806\n",
            "Epoch 127/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1273 - mae: 0.2257\n",
            "Epoch 127: val_loss improved from 0.09946 to 0.09945, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1263 - mae: 0.2250 - val_loss: 0.0994 - val_mae: 0.1808\n",
            "Epoch 128/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1271 - mae: 0.2269\n",
            "Epoch 128: val_loss did not improve from 0.09945\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1270 - mae: 0.2268 - val_loss: 0.1001 - val_mae: 0.1834\n",
            "Epoch 129/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1269 - mae: 0.2256\n",
            "Epoch 129: val_loss improved from 0.09945 to 0.09919, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1264 - mae: 0.2254 - val_loss: 0.0992 - val_mae: 0.1803\n",
            "Epoch 130/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1260 - mae: 0.2259\n",
            "Epoch 130: val_loss did not improve from 0.09919\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1259 - mae: 0.2261 - val_loss: 0.1004 - val_mae: 0.1845\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1274 - mae: 0.2260\n",
            "Epoch 131: val_loss did not improve from 0.09919\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1274 - mae: 0.2260 - val_loss: 0.0993 - val_mae: 0.1811\n",
            "Epoch 132/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1236 - mae: 0.2218\n",
            "Epoch 132: val_loss did not improve from 0.09919\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1235 - mae: 0.2218 - val_loss: 0.0997 - val_mae: 0.1825\n",
            "Epoch 133/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1241 - mae: 0.2232\n",
            "Epoch 133: val_loss did not improve from 0.09919\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1239 - mae: 0.2231 - val_loss: 0.0993 - val_mae: 0.1813\n",
            "Epoch 134/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1242 - mae: 0.2238\n",
            "Epoch 134: val_loss improved from 0.09919 to 0.09879, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1243 - mae: 0.2238 - val_loss: 0.0988 - val_mae: 0.1796\n",
            "Epoch 135/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1243 - mae: 0.2227\n",
            "Epoch 135: val_loss did not improve from 0.09879\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1242 - mae: 0.2227 - val_loss: 0.0988 - val_mae: 0.1798\n",
            "Epoch 136/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1241 - mae: 0.2219\n",
            "Epoch 136: val_loss improved from 0.09879 to 0.09866, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1241 - mae: 0.2218 - val_loss: 0.0987 - val_mae: 0.1794\n",
            "Epoch 137/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1243 - mae: 0.2254\n",
            "Epoch 137: val_loss did not improve from 0.09866\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1242 - mae: 0.2250 - val_loss: 0.0990 - val_mae: 0.1802\n",
            "Epoch 138/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1230 - mae: 0.2235\n",
            "Epoch 138: val_loss improved from 0.09866 to 0.09848, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1230 - mae: 0.2234 - val_loss: 0.0985 - val_mae: 0.1791\n",
            "Epoch 139/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1234 - mae: 0.2203\n",
            "Epoch 139: val_loss did not improve from 0.09848\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1236 - mae: 0.2205 - val_loss: 0.0996 - val_mae: 0.1831\n",
            "Epoch 140/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1242 - mae: 0.2237\n",
            "Epoch 140: val_loss did not improve from 0.09848\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1238 - mae: 0.2234 - val_loss: 0.0987 - val_mae: 0.1803\n",
            "Epoch 141/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1241 - mae: 0.2222\n",
            "Epoch 141: val_loss did not improve from 0.09848\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1242 - mae: 0.2222 - val_loss: 0.0993 - val_mae: 0.1817\n",
            "Epoch 142/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1250 - mae: 0.2252\n",
            "Epoch 142: val_loss did not improve from 0.09848\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1250 - mae: 0.2251 - val_loss: 0.0988 - val_mae: 0.1807\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1226 - mae: 0.2198\n",
            "Epoch 143: val_loss did not improve from 0.09848\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1226 - mae: 0.2198 - val_loss: 0.0994 - val_mae: 0.1816\n",
            "Epoch 144/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1234 - mae: 0.2233\n",
            "Epoch 144: val_loss did not improve from 0.09848\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1239 - mae: 0.2234 - val_loss: 0.0990 - val_mae: 0.1815\n",
            "Epoch 144: early stopping\n",
            "testing on the testing datasets.....\n",
            "modelpath: SG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.11966605484485626, RMSE:0.34592780470848083, MAE:0.1913253515958786, R2:0.866782333186304\n",
            "0.866782333186304\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing on the filtered testing datasets.....\n",
            "[0.866782333186304]\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量，初始化嵌入层...\n",
            "embedding layers trainalble True\n",
            "Model: \"model_44\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_32 (Embedding)       (None, 17, 900)      649800      ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 17, 100)      270100      ['embedding_32[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 17, 100)      270100      ['embedding_32[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 17, 100)      270100      ['embedding_32[0][0]']           \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_96 (Globa  (None, 100)         0           ['conv1d_42[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_97 (Globa  (None, 100)         0           ['conv1d_43[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_98 (Globa  (None, 100)         0           ['conv1d_44[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 3)            33          ['input_30[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_96[0][0]',\n",
            "                                                                  'global_max_pooling1d_97[0][0]',\n",
            "                                                                  'global_max_pooling1d_98[0][0]',\n",
            "                                                                  'dense_55[0][0]']               \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 1000)         304000      ['concatenate_14[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 1000)         0           ['dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 500)          500500      ['dropout_49[0][0]']             \n",
            "                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dropout_50 (Dropout)           (None, 500)          0           ['dense_57[0][0]']               \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 100)          50100       ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 100)          0           ['dense_58[0][0]']               \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 1)            101         ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,314,834\n",
            "Trainable params: 2,314,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 5.5979 - mae: 1.5411\n",
            "Epoch 1: val_loss improved from inf to 0.82016, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 14ms/step - loss: 5.4458 - mae: 1.5114 - val_loss: 0.8202 - val_mae: 0.6129\n",
            "Epoch 2/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8824 - mae: 0.6256\n",
            "Epoch 2: val_loss did not improve from 0.82016\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8835 - mae: 0.6256 - val_loss: 0.8228 - val_mae: 0.5436\n",
            "Epoch 3/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8778 - mae: 0.6190\n",
            "Epoch 3: val_loss improved from 0.82016 to 0.81534, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8769 - mae: 0.6187 - val_loss: 0.8153 - val_mae: 0.5755\n",
            "Epoch 4/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.8717 - mae: 0.6178\n",
            "Epoch 4: val_loss improved from 0.81534 to 0.81236, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8700 - mae: 0.6183 - val_loss: 0.8124 - val_mae: 0.5967\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8751 - mae: 0.6205\n",
            "Epoch 5: val_loss improved from 0.81236 to 0.81057, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8751 - mae: 0.6205 - val_loss: 0.8106 - val_mae: 0.5954\n",
            "Epoch 6/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.8686 - mae: 0.6169\n",
            "Epoch 6: val_loss improved from 0.81057 to 0.80927, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8710 - mae: 0.6183 - val_loss: 0.8093 - val_mae: 0.6086\n",
            "Epoch 7/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.8675 - mae: 0.6194\n",
            "Epoch 7: val_loss improved from 0.80927 to 0.80769, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8674 - mae: 0.6189 - val_loss: 0.8077 - val_mae: 0.5690\n",
            "Epoch 8/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8603 - mae: 0.6134\n",
            "Epoch 8: val_loss improved from 0.80769 to 0.80633, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8617 - mae: 0.6139 - val_loss: 0.8063 - val_mae: 0.6294\n",
            "Epoch 9/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8580 - mae: 0.6137\n",
            "Epoch 9: val_loss improved from 0.80633 to 0.79947, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8580 - mae: 0.6141 - val_loss: 0.7995 - val_mae: 0.5669\n",
            "Epoch 10/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8587 - mae: 0.6131\n",
            "Epoch 10: val_loss improved from 0.79947 to 0.79171, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8569 - mae: 0.6123 - val_loss: 0.7917 - val_mae: 0.6012\n",
            "Epoch 11/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8470 - mae: 0.6078\n",
            "Epoch 11: val_loss did not improve from 0.79171\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8497 - mae: 0.6092 - val_loss: 0.7929 - val_mae: 0.6294\n",
            "Epoch 12/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8490 - mae: 0.6107\n",
            "Epoch 12: val_loss improved from 0.79171 to 0.78632, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8483 - mae: 0.6103 - val_loss: 0.7863 - val_mae: 0.5832\n",
            "Epoch 13/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8448 - mae: 0.6072\n",
            "Epoch 13: val_loss did not improve from 0.78632\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8421 - mae: 0.6063 - val_loss: 0.7864 - val_mae: 0.5491\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8438 - mae: 0.6055\n",
            "Epoch 14: val_loss improved from 0.78632 to 0.78179, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8438 - mae: 0.6055 - val_loss: 0.7818 - val_mae: 0.5709\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8384 - mae: 0.6039\n",
            "Epoch 15: val_loss improved from 0.78179 to 0.78007, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8384 - mae: 0.6039 - val_loss: 0.7801 - val_mae: 0.5991\n",
            "Epoch 16/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8359 - mae: 0.6019\n",
            "Epoch 16: val_loss did not improve from 0.78007\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8388 - mae: 0.6036 - val_loss: 0.7838 - val_mae: 0.6324\n",
            "Epoch 17/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.8340 - mae: 0.6059\n",
            "Epoch 17: val_loss improved from 0.78007 to 0.77495, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8335 - mae: 0.6056 - val_loss: 0.7749 - val_mae: 0.5769\n",
            "Epoch 18/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8305 - mae: 0.5991\n",
            "Epoch 18: val_loss did not improve from 0.77495\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8323 - mae: 0.6002 - val_loss: 0.7791 - val_mae: 0.6300\n",
            "Epoch 19/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8287 - mae: 0.6014\n",
            "Epoch 19: val_loss improved from 0.77495 to 0.77136, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8284 - mae: 0.6013 - val_loss: 0.7714 - val_mae: 0.5903\n",
            "Epoch 20/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8300 - mae: 0.5988\n",
            "Epoch 20: val_loss improved from 0.77136 to 0.77048, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8301 - mae: 0.5989 - val_loss: 0.7705 - val_mae: 0.5465\n",
            "Epoch 21/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8290 - mae: 0.5987\n",
            "Epoch 21: val_loss did not improve from 0.77048\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8279 - mae: 0.5982 - val_loss: 0.7730 - val_mae: 0.5192\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8230 - mae: 0.5947\n",
            "Epoch 22: val_loss improved from 0.77048 to 0.76743, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8230 - mae: 0.5947 - val_loss: 0.7674 - val_mae: 0.5822\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8214 - mae: 0.5956\n",
            "Epoch 23: val_loss improved from 0.76743 to 0.76417, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8214 - mae: 0.5956 - val_loss: 0.7642 - val_mae: 0.5825\n",
            "Epoch 24/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8180 - mae: 0.5945\n",
            "Epoch 24: val_loss improved from 0.76417 to 0.76179, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8181 - mae: 0.5946 - val_loss: 0.7618 - val_mae: 0.5818\n",
            "Epoch 25/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.8164 - mae: 0.5944\n",
            "Epoch 25: val_loss did not improve from 0.76179\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8135 - mae: 0.5934 - val_loss: 0.7620 - val_mae: 0.5378\n",
            "Epoch 26/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8152 - mae: 0.5918\n",
            "Epoch 26: val_loss improved from 0.76179 to 0.75832, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8139 - mae: 0.5911 - val_loss: 0.7583 - val_mae: 0.5552\n",
            "Epoch 27/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.8122 - mae: 0.5879\n",
            "Epoch 27: val_loss did not improve from 0.75832\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8147 - mae: 0.5889 - val_loss: 0.7688 - val_mae: 0.6430\n",
            "Epoch 28/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8182 - mae: 0.5950\n",
            "Epoch 28: val_loss improved from 0.75832 to 0.75631, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8164 - mae: 0.5944 - val_loss: 0.7563 - val_mae: 0.5376\n",
            "Epoch 29/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8112 - mae: 0.5897\n",
            "Epoch 29: val_loss improved from 0.75631 to 0.75369, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8088 - mae: 0.5880 - val_loss: 0.7537 - val_mae: 0.5540\n",
            "Epoch 30/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8116 - mae: 0.5889\n",
            "Epoch 30: val_loss improved from 0.75369 to 0.75349, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8112 - mae: 0.5886 - val_loss: 0.7535 - val_mae: 0.5405\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8082 - mae: 0.5861\n",
            "Epoch 31: val_loss improved from 0.75349 to 0.75008, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8082 - mae: 0.5861 - val_loss: 0.7501 - val_mae: 0.5757\n",
            "Epoch 32/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.8069 - mae: 0.5874\n",
            "Epoch 32: val_loss improved from 0.75008 to 0.74820, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8071 - mae: 0.5876 - val_loss: 0.7482 - val_mae: 0.5652\n",
            "Epoch 33/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8057 - mae: 0.5850\n",
            "Epoch 33: val_loss improved from 0.74820 to 0.74746, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8055 - mae: 0.5854 - val_loss: 0.7475 - val_mae: 0.5393\n",
            "Epoch 34/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8000 - mae: 0.5827\n",
            "Epoch 34: val_loss improved from 0.74746 to 0.74665, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.8006 - mae: 0.5830 - val_loss: 0.7466 - val_mae: 0.5810\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8029 - mae: 0.5856\n",
            "Epoch 35: val_loss did not improve from 0.74665\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8029 - mae: 0.5856 - val_loss: 0.7541 - val_mae: 0.5317\n",
            "Epoch 36/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8006 - mae: 0.5794\n",
            "Epoch 36: val_loss improved from 0.74665 to 0.74413, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.8012 - mae: 0.5796 - val_loss: 0.7441 - val_mae: 0.5884\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7967 - mae: 0.5826\n",
            "Epoch 37: val_loss did not improve from 0.74413\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7967 - mae: 0.5826 - val_loss: 0.7485 - val_mae: 0.5570\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.8004 - mae: 0.5828\n",
            "Epoch 38: val_loss did not improve from 0.74413\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8004 - mae: 0.5828 - val_loss: 0.7595 - val_mae: 0.5168\n",
            "Epoch 39/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.8052 - mae: 0.5798\n",
            "Epoch 39: val_loss did not improve from 0.74413\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.8037 - mae: 0.5802 - val_loss: 0.7445 - val_mae: 0.5258\n",
            "Epoch 40/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.7962 - mae: 0.5785\n",
            "Epoch 40: val_loss improved from 0.74413 to 0.74373, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7953 - mae: 0.5772 - val_loss: 0.7437 - val_mae: 0.5579\n",
            "Epoch 41/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7912 - mae: 0.5764\n",
            "Epoch 41: val_loss improved from 0.74373 to 0.73572, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7926 - mae: 0.5769 - val_loss: 0.7357 - val_mae: 0.5338\n",
            "Epoch 42/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7919 - mae: 0.5776\n",
            "Epoch 42: val_loss did not improve from 0.73572\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7893 - mae: 0.5761 - val_loss: 0.7378 - val_mae: 0.5067\n",
            "Epoch 43/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7833 - mae: 0.5722\n",
            "Epoch 43: val_loss improved from 0.73572 to 0.73316, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7831 - mae: 0.5720 - val_loss: 0.7332 - val_mae: 0.5275\n",
            "Epoch 44/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7891 - mae: 0.5729\n",
            "Epoch 44: val_loss did not improve from 0.73316\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7866 - mae: 0.5716 - val_loss: 0.7333 - val_mae: 0.5326\n",
            "Epoch 45/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7831 - mae: 0.5697\n",
            "Epoch 45: val_loss improved from 0.73316 to 0.72954, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7858 - mae: 0.5715 - val_loss: 0.7295 - val_mae: 0.5492\n",
            "Epoch 46/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.7872 - mae: 0.5742\n",
            "Epoch 46: val_loss did not improve from 0.72954\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7843 - mae: 0.5717 - val_loss: 0.7351 - val_mae: 0.5489\n",
            "Epoch 47/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7793 - mae: 0.5689\n",
            "Epoch 47: val_loss improved from 0.72954 to 0.72923, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7817 - mae: 0.5702 - val_loss: 0.7292 - val_mae: 0.5091\n",
            "Epoch 48/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7790 - mae: 0.5682\n",
            "Epoch 48: val_loss did not improve from 0.72923\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7800 - mae: 0.5682 - val_loss: 0.7375 - val_mae: 0.5800\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7834 - mae: 0.5714\n",
            "Epoch 49: val_loss improved from 0.72923 to 0.72582, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7834 - mae: 0.5714 - val_loss: 0.7258 - val_mae: 0.5672\n",
            "Epoch 50/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7747 - mae: 0.5673\n",
            "Epoch 50: val_loss improved from 0.72582 to 0.72462, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7751 - mae: 0.5668 - val_loss: 0.7246 - val_mae: 0.5454\n",
            "Epoch 51/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7786 - mae: 0.5649\n",
            "Epoch 51: val_loss improved from 0.72462 to 0.72169, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7808 - mae: 0.5659 - val_loss: 0.7217 - val_mae: 0.5476\n",
            "Epoch 52/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.7837 - mae: 0.5679\n",
            "Epoch 52: val_loss did not improve from 0.72169\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7761 - mae: 0.5658 - val_loss: 0.7344 - val_mae: 0.4961\n",
            "Epoch 53/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.7751 - mae: 0.5631\n",
            "Epoch 53: val_loss did not improve from 0.72169\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7752 - mae: 0.5636 - val_loss: 0.7240 - val_mae: 0.5399\n",
            "Epoch 54/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7721 - mae: 0.5618\n",
            "Epoch 54: val_loss improved from 0.72169 to 0.72095, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7782 - mae: 0.5641 - val_loss: 0.7209 - val_mae: 0.5720\n",
            "Epoch 55/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7743 - mae: 0.5628\n",
            "Epoch 55: val_loss did not improve from 0.72095\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7753 - mae: 0.5637 - val_loss: 0.7300 - val_mae: 0.4767\n",
            "Epoch 56/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7671 - mae: 0.5567\n",
            "Epoch 56: val_loss improved from 0.72095 to 0.71579, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7705 - mae: 0.5590 - val_loss: 0.7158 - val_mae: 0.5315\n",
            "Epoch 57/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7711 - mae: 0.5602\n",
            "Epoch 57: val_loss did not improve from 0.71579\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7674 - mae: 0.5587 - val_loss: 0.7240 - val_mae: 0.4950\n",
            "Epoch 58/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7744 - mae: 0.5624\n",
            "Epoch 58: val_loss did not improve from 0.71579\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7703 - mae: 0.5608 - val_loss: 0.7298 - val_mae: 0.4767\n",
            "Epoch 59/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7703 - mae: 0.5556\n",
            "Epoch 59: val_loss improved from 0.71579 to 0.71474, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7720 - mae: 0.5568 - val_loss: 0.7147 - val_mae: 0.5284\n",
            "Epoch 60/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7606 - mae: 0.5554\n",
            "Epoch 60: val_loss did not improve from 0.71474\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7622 - mae: 0.5559 - val_loss: 0.7229 - val_mae: 0.6037\n",
            "Epoch 61/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.7611 - mae: 0.5568\n",
            "Epoch 61: val_loss did not improve from 0.71474\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7606 - mae: 0.5570 - val_loss: 0.7215 - val_mae: 0.4888\n",
            "Epoch 62/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7622 - mae: 0.5546\n",
            "Epoch 62: val_loss did not improve from 0.71474\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7612 - mae: 0.5542 - val_loss: 0.7188 - val_mae: 0.5105\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7638 - mae: 0.5569\n",
            "Epoch 63: val_loss did not improve from 0.71474\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7638 - mae: 0.5569 - val_loss: 0.7201 - val_mae: 0.4950\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7586 - mae: 0.5501\n",
            "Epoch 64: val_loss improved from 0.71474 to 0.70763, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7586 - mae: 0.5501 - val_loss: 0.7076 - val_mae: 0.5486\n",
            "Epoch 65/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7635 - mae: 0.5541\n",
            "Epoch 65: val_loss did not improve from 0.70763\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7593 - mae: 0.5519 - val_loss: 0.7187 - val_mae: 0.4868\n",
            "Epoch 66/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7580 - mae: 0.5509\n",
            "Epoch 66: val_loss did not improve from 0.70763\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.7561 - mae: 0.5503 - val_loss: 0.7089 - val_mae: 0.4880\n",
            "Epoch 67/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.7679 - mae: 0.5523\n",
            "Epoch 67: val_loss improved from 0.70763 to 0.70624, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7632 - mae: 0.5488 - val_loss: 0.7062 - val_mae: 0.4911\n",
            "Epoch 68/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7530 - mae: 0.5460\n",
            "Epoch 68: val_loss improved from 0.70624 to 0.70310, saving model to /content/gdrive/MyDrive/cnn_model/SG_splitweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7530 - mae: 0.5462 - val_loss: 0.7031 - val_mae: 0.5369\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7510 - mae: 0.5463\n",
            "Epoch 69: val_loss did not improve from 0.70310\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7510 - mae: 0.5463 - val_loss: 0.7152 - val_mae: 0.5608\n",
            "Epoch 70/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7569 - mae: 0.5486\n",
            "Epoch 70: val_loss did not improve from 0.70310\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7572 - mae: 0.5486 - val_loss: 0.7099 - val_mae: 0.5139\n",
            "Epoch 71/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7526 - mae: 0.5481\n",
            "Epoch 71: val_loss did not improve from 0.70310\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7503 - mae: 0.5463 - val_loss: 0.7032 - val_mae: 0.4744\n",
            "Epoch 72/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7521 - mae: 0.5416\n",
            "Epoch 72: val_loss did not improve from 0.70310\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.7530 - mae: 0.5418 - val_loss: 0.7079 - val_mae: 0.5623\n",
            "Epoch 73/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.7490 - mae: 0.5457\n",
            "Epoch 73: val_loss did not improve from 0.70310\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7544 - mae: 0.5472 - val_loss: 0.7033 - val_mae: 0.5709\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7501 - mae: 0.5426\n",
            "Epoch 74: val_loss did not improve from 0.70310\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7501 - mae: 0.5426 - val_loss: 0.7217 - val_mae: 0.6249\n",
            "Epoch 74: early stopping\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.7706365585327148, RMSE:0.8778590559959412, MAE:0.561241090297699, R2:0.142092508980323\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing on the filtered testing datasets.....\n",
            "[0.142092508980323]\n",
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "Model: \"model_45\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_31 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_33 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_34 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_35 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_36 (Embedding)       (None, 17, 900)      649800      ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 17, 100)      270100      ['embedding_33[0][0]',           \n",
            "                                                                  'embedding_34[0][0]',           \n",
            "                                                                  'embedding_35[0][0]',           \n",
            "                                                                  'embedding_36[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 17, 100)      270100      ['embedding_33[0][0]',           \n",
            "                                                                  'embedding_34[0][0]',           \n",
            "                                                                  'embedding_35[0][0]',           \n",
            "                                                                  'embedding_36[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 17, 100)      270100      ['embedding_33[0][0]',           \n",
            "                                                                  'embedding_34[0][0]',           \n",
            "                                                                  'embedding_35[0][0]',           \n",
            "                                                                  'embedding_36[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_99 (Globa  (None, 100)         0           ['conv1d_45[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_100 (Glob  (None, 100)         0           ['conv1d_45[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_101 (Glob  (None, 100)         0           ['conv1d_45[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_102 (Glob  (None, 100)         0           ['conv1d_45[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_103 (Glob  (None, 100)         0           ['conv1d_46[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_104 (Glob  (None, 100)         0           ['conv1d_46[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_105 (Glob  (None, 100)         0           ['conv1d_46[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_106 (Glob  (None, 100)         0           ['conv1d_46[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_107 (Glob  (None, 100)         0           ['conv1d_47[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_108 (Glob  (None, 100)         0           ['conv1d_47[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_109 (Glob  (None, 100)         0           ['conv1d_47[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_110 (Glob  (None, 100)         0           ['conv1d_47[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 100)          0           ['global_max_pooling1d_99[0][0]',\n",
            "                                                                  'global_max_pooling1d_100[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_101[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_102[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 100)          0           ['global_max_pooling1d_103[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_104[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_105[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_106[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 100)          0           ['global_max_pooling1d_107[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_108[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_109[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_110[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 1000)         11000       ['input_32[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 1300)         0           ['add_18[0][0]',                 \n",
            "                                                                  'add_19[0][0]',                 \n",
            "                                                                  'add_20[0][0]',                 \n",
            "                                                                  'dense_60[0][0]']               \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['concatenate_15[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_55[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (None, 1)            3           ['dropout_56[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,277,255\n",
            "Trainable params: 5,277,255\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.0968 - mae: 0.8645\n",
            "Epoch 1: val_loss improved from inf to 0.66916, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 5s 35ms/step - loss: 2.0939 - mae: 0.8643 - val_loss: 0.6692 - val_mae: 0.7182\n",
            "Epoch 2/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.9356 - mae: 0.5546\n",
            "Epoch 2: val_loss improved from 0.66916 to 0.43464, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.9452 - mae: 0.5570 - val_loss: 0.4346 - val_mae: 0.3450\n",
            "Epoch 3/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8616 - mae: 0.5175\n",
            "Epoch 3: val_loss did not improve from 0.43464\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.8602 - mae: 0.5173 - val_loss: 0.6055 - val_mae: 0.5638\n",
            "Epoch 4/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8127 - mae: 0.4886\n",
            "Epoch 4: val_loss improved from 0.43464 to 0.38879, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.8129 - mae: 0.4889 - val_loss: 0.3888 - val_mae: 0.3947\n",
            "Epoch 5/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8505 - mae: 0.5169\n",
            "Epoch 5: val_loss did not improve from 0.38879\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.8494 - mae: 0.5164 - val_loss: 0.6305 - val_mae: 0.6110\n",
            "Epoch 6/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8663 - mae: 0.5134\n",
            "Epoch 6: val_loss did not improve from 0.38879\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.8737 - mae: 0.5143 - val_loss: 0.4108 - val_mae: 0.4215\n",
            "Epoch 7/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8438 - mae: 0.5269\n",
            "Epoch 7: val_loss did not improve from 0.38879\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.8437 - mae: 0.5266 - val_loss: 0.7824 - val_mae: 0.6808\n",
            "Epoch 8/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.7786 - mae: 0.4689\n",
            "Epoch 8: val_loss improved from 0.38879 to 0.34516, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 0.7783 - mae: 0.4695 - val_loss: 0.3452 - val_mae: 0.2901\n",
            "Epoch 9/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8067 - mae: 0.4677\n",
            "Epoch 9: val_loss did not improve from 0.34516\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.8053 - mae: 0.4673 - val_loss: 0.3710 - val_mae: 0.3216\n",
            "Epoch 10/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7847 - mae: 0.4743\n",
            "Epoch 10: val_loss improved from 0.34516 to 0.31622, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.7890 - mae: 0.4751 - val_loss: 0.3162 - val_mae: 0.2908\n",
            "Epoch 11/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7358 - mae: 0.4628\n",
            "Epoch 11: val_loss did not improve from 0.31622\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.7391 - mae: 0.4634 - val_loss: 0.4006 - val_mae: 0.3629\n",
            "Epoch 12/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.6909 - mae: 0.4293\n",
            "Epoch 12: val_loss did not improve from 0.31622\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.6942 - mae: 0.4298 - val_loss: 0.4540 - val_mae: 0.5103\n",
            "Epoch 13/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7040 - mae: 0.4380\n",
            "Epoch 13: val_loss improved from 0.31622 to 0.23739, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.7033 - mae: 0.4380 - val_loss: 0.2374 - val_mae: 0.2666\n",
            "Epoch 14/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.6528 - mae: 0.4044\n",
            "Epoch 14: val_loss did not improve from 0.23739\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.6517 - mae: 0.4043 - val_loss: 0.3618 - val_mae: 0.4612\n",
            "Epoch 15/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5721 - mae: 0.3941\n",
            "Epoch 15: val_loss improved from 0.23739 to 0.18476, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 0.5733 - mae: 0.3942 - val_loss: 0.1848 - val_mae: 0.2985\n",
            "Epoch 16/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5192 - mae: 0.3595\n",
            "Epoch 16: val_loss did not improve from 0.18476\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.5217 - mae: 0.3604 - val_loss: 0.6446 - val_mae: 0.7124\n",
            "Epoch 17/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5831 - mae: 0.4068\n",
            "Epoch 17: val_loss did not improve from 0.18476\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.5827 - mae: 0.4069 - val_loss: 0.2226 - val_mae: 0.2982\n",
            "Epoch 18/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4623 - mae: 0.3363\n",
            "Epoch 18: val_loss improved from 0.18476 to 0.11915, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.4615 - mae: 0.3362 - val_loss: 0.1191 - val_mae: 0.1914\n",
            "Epoch 19/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5631 - mae: 0.3640\n",
            "Epoch 19: val_loss improved from 0.11915 to 0.09579, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.5623 - mae: 0.3640 - val_loss: 0.0958 - val_mae: 0.1629\n",
            "Epoch 20/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.5142 - mae: 0.3784\n",
            "Epoch 20: val_loss did not improve from 0.09579\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.5134 - mae: 0.3774 - val_loss: 0.1426 - val_mae: 0.2677\n",
            "Epoch 21/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4657 - mae: 0.3314\n",
            "Epoch 21: val_loss did not improve from 0.09579\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.4668 - mae: 0.3314 - val_loss: 0.1089 - val_mae: 0.1638\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4140 - mae: 0.2991\n",
            "Epoch 22: val_loss improved from 0.09579 to 0.08499, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 0.4140 - mae: 0.2991 - val_loss: 0.0850 - val_mae: 0.1378\n",
            "Epoch 23/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4441 - mae: 0.3117\n",
            "Epoch 23: val_loss did not improve from 0.08499\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.4434 - mae: 0.3116 - val_loss: 0.2897 - val_mae: 0.3220\n",
            "Epoch 24/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4336 - mae: 0.3256\n",
            "Epoch 24: val_loss did not improve from 0.08499\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.4328 - mae: 0.3253 - val_loss: 0.1052 - val_mae: 0.2238\n",
            "Epoch 25/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4193 - mae: 0.3006\n",
            "Epoch 25: val_loss did not improve from 0.08499\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.4225 - mae: 0.3012 - val_loss: 0.0920 - val_mae: 0.1667\n",
            "Epoch 26/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4239 - mae: 0.2854\n",
            "Epoch 26: val_loss improved from 0.08499 to 0.07853, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.4230 - mae: 0.2851 - val_loss: 0.0785 - val_mae: 0.1204\n",
            "Epoch 27/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3888 - mae: 0.2912\n",
            "Epoch 27: val_loss did not improve from 0.07853\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.3897 - mae: 0.2914 - val_loss: 0.0848 - val_mae: 0.1537\n",
            "Epoch 28/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.5579 - mae: 0.3809\n",
            "Epoch 28: val_loss did not improve from 0.07853\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.5608 - mae: 0.3810 - val_loss: 0.1160 - val_mae: 0.1854\n",
            "Epoch 29/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4094 - mae: 0.2973\n",
            "Epoch 29: val_loss did not improve from 0.07853\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.4093 - mae: 0.2974 - val_loss: 0.1863 - val_mae: 0.3269\n",
            "Epoch 30/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3899 - mae: 0.2994\n",
            "Epoch 30: val_loss did not improve from 0.07853\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.3909 - mae: 0.2995 - val_loss: 0.2153 - val_mae: 0.3794\n",
            "Epoch 31/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.4468 - mae: 0.2963\n",
            "Epoch 31: val_loss improved from 0.07853 to 0.07768, saving model to /content/gdrive/MyDrive/cnn_model/MG_mergeweighted_53:47_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.4425 - mae: 0.2953 - val_loss: 0.0777 - val_mae: 0.1252\n",
            "Epoch 32/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4255 - mae: 0.2953\n",
            "Epoch 32: val_loss did not improve from 0.07768\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.4246 - mae: 0.2949 - val_loss: 0.0788 - val_mae: 0.1704\n",
            "Epoch 33/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3910 - mae: 0.3013\n",
            "Epoch 33: val_loss did not improve from 0.07768\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.3924 - mae: 0.3027 - val_loss: 0.5686 - val_mae: 0.3457\n",
            "Epoch 34/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.8354 - mae: 0.5062\n",
            "Epoch 34: val_loss did not improve from 0.07768\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.8373 - mae: 0.5066 - val_loss: 0.4372 - val_mae: 0.3037\n",
            "Epoch 35/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7746 - mae: 0.4654\n",
            "Epoch 35: val_loss did not improve from 0.07768\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.7731 - mae: 0.4649 - val_loss: 0.3497 - val_mae: 0.3140\n",
            "Epoch 36/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5486 - mae: 0.3891\n",
            "Epoch 36: val_loss did not improve from 0.07768\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 0.5476 - mae: 0.3887 - val_loss: 0.1548 - val_mae: 0.2568\n",
            "Epoch 37/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3727 - mae: 0.2886\n",
            "Epoch 37: val_loss did not improve from 0.07768\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.3736 - mae: 0.2888 - val_loss: 0.2294 - val_mae: 0.4054\n",
            "Epoch 37: early stopping\n",
            "491/491 [==============================] - 2s 4ms/step\n",
            "(15684, 100)\n",
            "491/491 [==============================] - 2s 4ms/step\n",
            "(15684, 50)\n",
            "491/491 [==============================] - 2s 4ms/step\n",
            "(15684, 2)\n",
            "/content/gdrive/MyDrive/15684_new_2.csv testing on the testing datasets.....\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 7ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.09039788693189621, RMSE:0.30066242814064026, MAE:0.13810378313064575, R2:0.8993649768118412\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8993649768118412]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_SG_merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLxL4HGSnph9",
        "outputId": "92645696-9d46-4d26-f271-ac8b46bb20be"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.866782333186304]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_SG_split"
      ],
      "metadata": {
        "id": "n5ymUj51nwbw",
        "outputId": "3043438d-1e2f-4f71-8db4-2c25ebb7c4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.142092508980323]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_MG_merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxeO7uVT4n8F",
        "outputId": "ee449368-9e4e-4b09-f0bf-ed35af125fd8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8993649768118412]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1):\n",
        "  modelpath = 'SG_merge' + path\n",
        "  model = single_channel_merge_model(demgras_dim)\n",
        "  if not args.test:\n",
        "      train_single_channel_merge_model(model, modelpath, X_train, demographics_train, y_train)\n",
        "  print('testing on the testing datasets.....')\n",
        "  print(\"modelpath:\",modelpath)\n",
        "  test_single_channel_merge_model(model, modelpath, X_test, demographics_test, y_test, 3)\n",
        "  print('testing on the filtered testing datasets.....')\n",
        "  print(r2_SG_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRyFC93doftg",
        "outputId": "54da86d9-33c8-451c-b45b-7272719126ee"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始模型1:单尺度融合模型\n",
            "initialize embedding layer with pre-training vectors\n",
            "依据预训练向量 初始化嵌入层\n",
            "embedding layers trainalble True\n",
            "添加人口统计信息，融合\n",
            "Model: \"model_36\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_23 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_26 (Embedding)       (None, 17, 900)      649800      ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 17, 100)      270100      ['embedding_26[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 17, 100)      270100      ['embedding_26[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 17, 100)      270100      ['embedding_26[0][0]']           \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_78 (Globa  (None, 100)         0           ['conv1d_33[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_79 (Globa  (None, 100)         0           ['conv1d_34[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_80 (Globa  (None, 100)         0           ['conv1d_35[0][0]']              \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 3)            33          ['input_24[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 303)          0           ['global_max_pooling1d_78[0][0]',\n",
            "                                                                  'global_max_pooling1d_79[0][0]',\n",
            "                                                                  'global_max_pooling1d_80[0][0]',\n",
            "                                                                  'dense_43[0][0]']               \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 500)          152000      ['concatenate_11[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 500)          0           ['dense_44[0][0]']               \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (None, 100)          50100       ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 100)          0           ['dense_45[0][0]']               \n",
            "                                                                                                  \n",
            " dense_46 (Dense)               (None, 1)            101         ['dropout_41[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,662,334\n",
            "Trainable params: 1,662,334\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training the merge model....\n",
            "Epoch 1/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.0304 - mae: 1.4754\n",
            "Epoch 1: val_loss improved from inf to 0.90908, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 5s 21ms/step - loss: 5.0304 - mae: 1.4754 - val_loss: 0.9091 - val_mae: 0.6343\n",
            "Epoch 2/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.8991 - mae: 0.6286\n",
            "Epoch 2: val_loss improved from 0.90908 to 0.81544, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.8980 - mae: 0.6282 - val_loss: 0.8154 - val_mae: 0.5878\n",
            "Epoch 3/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.8164 - mae: 0.5989\n",
            "Epoch 3: val_loss improved from 0.81544 to 0.74470, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.8159 - mae: 0.5996 - val_loss: 0.7447 - val_mae: 0.5921\n",
            "Epoch 4/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.7518 - mae: 0.5799\n",
            "Epoch 4: val_loss improved from 0.74470 to 0.68342, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.7516 - mae: 0.5798 - val_loss: 0.6834 - val_mae: 0.5491\n",
            "Epoch 5/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.6966 - mae: 0.5575\n",
            "Epoch 5: val_loss improved from 0.68342 to 0.63447, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.6948 - mae: 0.5570 - val_loss: 0.6345 - val_mae: 0.5534\n",
            "Epoch 6/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.6439 - mae: 0.5418\n",
            "Epoch 6: val_loss improved from 0.63447 to 0.58568, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.6457 - mae: 0.5415 - val_loss: 0.5857 - val_mae: 0.4938\n",
            "Epoch 7/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.6049 - mae: 0.5215\n",
            "Epoch 7: val_loss improved from 0.58568 to 0.54849, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.6049 - mae: 0.5215 - val_loss: 0.5485 - val_mae: 0.5052\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.5631 - mae: 0.5087\n",
            "Epoch 8: val_loss improved from 0.54849 to 0.51308, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.5631 - mae: 0.5087 - val_loss: 0.5131 - val_mae: 0.4660\n",
            "Epoch 9/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.5216 - mae: 0.4896\n",
            "Epoch 9: val_loss improved from 0.51308 to 0.48554, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.5248 - mae: 0.4903 - val_loss: 0.4855 - val_mae: 0.4762\n",
            "Epoch 10/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.5002 - mae: 0.4828\n",
            "Epoch 10: val_loss improved from 0.48554 to 0.45651, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.5002 - mae: 0.4829 - val_loss: 0.4565 - val_mae: 0.4534\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4713 - mae: 0.4692\n",
            "Epoch 11: val_loss improved from 0.45651 to 0.43395, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.4713 - mae: 0.4692 - val_loss: 0.4340 - val_mae: 0.4341\n",
            "Epoch 12/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.4490 - mae: 0.4565\n",
            "Epoch 12: val_loss improved from 0.43395 to 0.40890, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.4494 - mae: 0.4566 - val_loss: 0.4089 - val_mae: 0.4269\n",
            "Epoch 13/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.4243 - mae: 0.4461\n",
            "Epoch 13: val_loss improved from 0.40890 to 0.39193, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.4231 - mae: 0.4455 - val_loss: 0.3919 - val_mae: 0.4135\n",
            "Epoch 14/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.4045 - mae: 0.4332\n",
            "Epoch 14: val_loss improved from 0.39193 to 0.36606, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.4044 - mae: 0.4335 - val_loss: 0.3661 - val_mae: 0.4052\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3832 - mae: 0.4214\n",
            "Epoch 15: val_loss improved from 0.36606 to 0.34692, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.3832 - mae: 0.4214 - val_loss: 0.3469 - val_mae: 0.3933\n",
            "Epoch 16/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3615 - mae: 0.4089\n",
            "Epoch 16: val_loss improved from 0.34692 to 0.32926, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3612 - mae: 0.4087 - val_loss: 0.3293 - val_mae: 0.3815\n",
            "Epoch 17/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3436 - mae: 0.3988\n",
            "Epoch 17: val_loss improved from 0.32926 to 0.31052, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3431 - mae: 0.3985 - val_loss: 0.3105 - val_mae: 0.3717\n",
            "Epoch 18/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.3280 - mae: 0.3895\n",
            "Epoch 18: val_loss improved from 0.31052 to 0.29375, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3280 - mae: 0.3896 - val_loss: 0.2937 - val_mae: 0.3621\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3166 - mae: 0.3807\n",
            "Epoch 19: val_loss improved from 0.29375 to 0.27907, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3166 - mae: 0.3807 - val_loss: 0.2791 - val_mae: 0.3480\n",
            "Epoch 20/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2970 - mae: 0.3674\n",
            "Epoch 20: val_loss improved from 0.27907 to 0.26299, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2975 - mae: 0.3678 - val_loss: 0.2630 - val_mae: 0.3408\n",
            "Epoch 21/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2840 - mae: 0.3604\n",
            "Epoch 21: val_loss improved from 0.26299 to 0.24897, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2838 - mae: 0.3604 - val_loss: 0.2490 - val_mae: 0.3308\n",
            "Epoch 22/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2734 - mae: 0.3517\n",
            "Epoch 22: val_loss improved from 0.24897 to 0.23646, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2726 - mae: 0.3516 - val_loss: 0.2365 - val_mae: 0.3239\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.2602 - mae: 0.3414\n",
            "Epoch 23: val_loss improved from 0.23646 to 0.22390, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2602 - mae: 0.3414 - val_loss: 0.2239 - val_mae: 0.3144\n",
            "Epoch 24/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.2473 - mae: 0.3341\n",
            "Epoch 24: val_loss improved from 0.22390 to 0.21144, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2478 - mae: 0.3338 - val_loss: 0.2114 - val_mae: 0.3010\n",
            "Epoch 25/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.2364 - mae: 0.3261\n",
            "Epoch 25: val_loss improved from 0.21144 to 0.20083, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2374 - mae: 0.3263 - val_loss: 0.2008 - val_mae: 0.2910\n",
            "Epoch 26/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2271 - mae: 0.3176\n",
            "Epoch 26: val_loss improved from 0.20083 to 0.19149, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2272 - mae: 0.3177 - val_loss: 0.1915 - val_mae: 0.2814\n",
            "Epoch 27/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.2192 - mae: 0.3124\n",
            "Epoch 27: val_loss improved from 0.19149 to 0.18449, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.2190 - mae: 0.3118 - val_loss: 0.1845 - val_mae: 0.2747\n",
            "Epoch 28/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.2113 - mae: 0.3056\n",
            "Epoch 28: val_loss improved from 0.18449 to 0.17831, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2104 - mae: 0.3051 - val_loss: 0.1783 - val_mae: 0.2696\n",
            "Epoch 29/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.2028 - mae: 0.2995\n",
            "Epoch 29: val_loss improved from 0.17831 to 0.16765, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2031 - mae: 0.2996 - val_loss: 0.1677 - val_mae: 0.2595\n",
            "Epoch 30/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1975 - mae: 0.2931\n",
            "Epoch 30: val_loss improved from 0.16765 to 0.16195, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1976 - mae: 0.2936 - val_loss: 0.1619 - val_mae: 0.2546\n",
            "Epoch 31/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1877 - mae: 0.2864\n",
            "Epoch 31: val_loss improved from 0.16195 to 0.15568, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1875 - mae: 0.2863 - val_loss: 0.1557 - val_mae: 0.2485\n",
            "Epoch 32/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1884 - mae: 0.2858\n",
            "Epoch 32: val_loss improved from 0.15568 to 0.15081, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1880 - mae: 0.2856 - val_loss: 0.1508 - val_mae: 0.2445\n",
            "Epoch 33/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1830 - mae: 0.2803\n",
            "Epoch 33: val_loss improved from 0.15081 to 0.14698, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1822 - mae: 0.2799 - val_loss: 0.1470 - val_mae: 0.2399\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1790 - mae: 0.2776\n",
            "Epoch 34: val_loss improved from 0.14698 to 0.14356, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1790 - mae: 0.2776 - val_loss: 0.1436 - val_mae: 0.2357\n",
            "Epoch 35/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.2741\n",
            "Epoch 35: val_loss improved from 0.14356 to 0.13985, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1734 - mae: 0.2739 - val_loss: 0.1399 - val_mae: 0.2314\n",
            "Epoch 36/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.2708\n",
            "Epoch 36: val_loss improved from 0.13985 to 0.13712, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1718 - mae: 0.2708 - val_loss: 0.1371 - val_mae: 0.2281\n",
            "Epoch 37/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.2700\n",
            "Epoch 37: val_loss improved from 0.13712 to 0.13485, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1711 - mae: 0.2700 - val_loss: 0.1349 - val_mae: 0.2274\n",
            "Epoch 38/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1635 - mae: 0.2647\n",
            "Epoch 38: val_loss improved from 0.13485 to 0.13256, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1655 - mae: 0.2661 - val_loss: 0.1326 - val_mae: 0.2232\n",
            "Epoch 39/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1656 - mae: 0.2679\n",
            "Epoch 39: val_loss improved from 0.13256 to 0.13142, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1646 - mae: 0.2672 - val_loss: 0.1314 - val_mae: 0.2223\n",
            "Epoch 40/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1612 - mae: 0.2620\n",
            "Epoch 40: val_loss improved from 0.13142 to 0.13019, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1616 - mae: 0.2625 - val_loss: 0.1302 - val_mae: 0.2209\n",
            "Epoch 41/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1582 - mae: 0.2594\n",
            "Epoch 41: val_loss improved from 0.13019 to 0.12721, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1587 - mae: 0.2600 - val_loss: 0.1272 - val_mae: 0.2172\n",
            "Epoch 42/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1600 - mae: 0.2623\n",
            "Epoch 42: val_loss improved from 0.12721 to 0.12601, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1599 - mae: 0.2622 - val_loss: 0.1260 - val_mae: 0.2160\n",
            "Epoch 43/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1600 - mae: 0.2607\n",
            "Epoch 43: val_loss improved from 0.12601 to 0.12462, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1594 - mae: 0.2605 - val_loss: 0.1246 - val_mae: 0.2135\n",
            "Epoch 44/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1548 - mae: 0.2551\n",
            "Epoch 44: val_loss improved from 0.12462 to 0.12395, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1563 - mae: 0.2559 - val_loss: 0.1240 - val_mae: 0.2140\n",
            "Epoch 45/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1541 - mae: 0.2552\n",
            "Epoch 45: val_loss improved from 0.12395 to 0.12232, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1546 - mae: 0.2555 - val_loss: 0.1223 - val_mae: 0.2107\n",
            "Epoch 46/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1530 - mae: 0.2529\n",
            "Epoch 46: val_loss improved from 0.12232 to 0.12138, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1529 - mae: 0.2529 - val_loss: 0.1214 - val_mae: 0.2089\n",
            "Epoch 47/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1538 - mae: 0.2541\n",
            "Epoch 47: val_loss did not improve from 0.12138\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1537 - mae: 0.2542 - val_loss: 0.1229 - val_mae: 0.2121\n",
            "Epoch 48/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1509 - mae: 0.2534\n",
            "Epoch 48: val_loss improved from 0.12138 to 0.12031, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1505 - mae: 0.2533 - val_loss: 0.1203 - val_mae: 0.2089\n",
            "Epoch 49/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1523 - mae: 0.2529\n",
            "Epoch 49: val_loss improved from 0.12031 to 0.12012, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1516 - mae: 0.2527 - val_loss: 0.1201 - val_mae: 0.2070\n",
            "Epoch 50/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1493 - mae: 0.2506\n",
            "Epoch 50: val_loss improved from 0.12012 to 0.11826, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1491 - mae: 0.2505 - val_loss: 0.1183 - val_mae: 0.2045\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1488 - mae: 0.2502\n",
            "Epoch 51: val_loss did not improve from 0.11826\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1488 - mae: 0.2502 - val_loss: 0.1185 - val_mae: 0.2046\n",
            "Epoch 52/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1487 - mae: 0.2499\n",
            "Epoch 52: val_loss improved from 0.11826 to 0.11700, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1483 - mae: 0.2503 - val_loss: 0.1170 - val_mae: 0.2029\n",
            "Epoch 53/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1455 - mae: 0.2467\n",
            "Epoch 53: val_loss improved from 0.11700 to 0.11652, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1454 - mae: 0.2467 - val_loss: 0.1165 - val_mae: 0.2028\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1453 - mae: 0.2468\n",
            "Epoch 54: val_loss improved from 0.11652 to 0.11584, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1453 - mae: 0.2468 - val_loss: 0.1158 - val_mae: 0.2013\n",
            "Epoch 55/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1473 - mae: 0.2475\n",
            "Epoch 55: val_loss improved from 0.11584 to 0.11523, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1471 - mae: 0.2472 - val_loss: 0.1152 - val_mae: 0.2004\n",
            "Epoch 56/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1441 - mae: 0.2447\n",
            "Epoch 56: val_loss improved from 0.11523 to 0.11467, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1440 - mae: 0.2444 - val_loss: 0.1147 - val_mae: 0.1996\n",
            "Epoch 57/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1442 - mae: 0.2463\n",
            "Epoch 57: val_loss improved from 0.11467 to 0.11446, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1436 - mae: 0.2459 - val_loss: 0.1145 - val_mae: 0.1987\n",
            "Epoch 58/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1431 - mae: 0.2435\n",
            "Epoch 58: val_loss did not improve from 0.11446\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1442 - mae: 0.2437 - val_loss: 0.1153 - val_mae: 0.2039\n",
            "Epoch 59/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1438 - mae: 0.2467\n",
            "Epoch 59: val_loss improved from 0.11446 to 0.11359, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1436 - mae: 0.2465 - val_loss: 0.1136 - val_mae: 0.1972\n",
            "Epoch 60/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1420 - mae: 0.2423\n",
            "Epoch 60: val_loss improved from 0.11359 to 0.11353, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1418 - mae: 0.2422 - val_loss: 0.1135 - val_mae: 0.1970\n",
            "Epoch 61/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1408 - mae: 0.2416\n",
            "Epoch 61: val_loss improved from 0.11353 to 0.11260, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1413 - mae: 0.2422 - val_loss: 0.1126 - val_mae: 0.1970\n",
            "Epoch 62/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1418 - mae: 0.2414\n",
            "Epoch 62: val_loss improved from 0.11260 to 0.11228, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1417 - mae: 0.2414 - val_loss: 0.1123 - val_mae: 0.1967\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1396 - mae: 0.2406\n",
            "Epoch 63: val_loss did not improve from 0.11228\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1396 - mae: 0.2406 - val_loss: 0.1124 - val_mae: 0.1952\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1416 - mae: 0.2428\n",
            "Epoch 64: val_loss improved from 0.11228 to 0.11145, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1416 - mae: 0.2428 - val_loss: 0.1114 - val_mae: 0.1937\n",
            "Epoch 65/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1417 - mae: 0.2432\n",
            "Epoch 65: val_loss improved from 0.11145 to 0.11083, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1410 - mae: 0.2426 - val_loss: 0.1108 - val_mae: 0.1932\n",
            "Epoch 66/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1378 - mae: 0.2407\n",
            "Epoch 66: val_loss did not improve from 0.11083\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1379 - mae: 0.2405 - val_loss: 0.1131 - val_mae: 0.1976\n",
            "Epoch 67/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1390 - mae: 0.2393\n",
            "Epoch 67: val_loss did not improve from 0.11083\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1395 - mae: 0.2398 - val_loss: 0.1110 - val_mae: 0.1928\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1393 - mae: 0.2403\n",
            "Epoch 68: val_loss improved from 0.11083 to 0.11041, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1393 - mae: 0.2403 - val_loss: 0.1104 - val_mae: 0.1919\n",
            "Epoch 69/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1362 - mae: 0.2360\n",
            "Epoch 69: val_loss improved from 0.11041 to 0.10954, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1364 - mae: 0.2363 - val_loss: 0.1095 - val_mae: 0.1915\n",
            "Epoch 70/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1350 - mae: 0.2357\n",
            "Epoch 70: val_loss improved from 0.10954 to 0.10915, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1356 - mae: 0.2362 - val_loss: 0.1091 - val_mae: 0.1902\n",
            "Epoch 71/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1375 - mae: 0.2382\n",
            "Epoch 71: val_loss improved from 0.10915 to 0.10899, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1368 - mae: 0.2379 - val_loss: 0.1090 - val_mae: 0.1897\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1358 - mae: 0.2356\n",
            "Epoch 72: val_loss improved from 0.10899 to 0.10859, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1358 - mae: 0.2356 - val_loss: 0.1086 - val_mae: 0.1898\n",
            "Epoch 73/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1349 - mae: 0.2349\n",
            "Epoch 73: val_loss did not improve from 0.10859\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1349 - mae: 0.2350 - val_loss: 0.1089 - val_mae: 0.1893\n",
            "Epoch 74/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1358 - mae: 0.2355\n",
            "Epoch 74: val_loss improved from 0.10859 to 0.10804, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1367 - mae: 0.2359 - val_loss: 0.1080 - val_mae: 0.1890\n",
            "Epoch 75/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1350 - mae: 0.2353\n",
            "Epoch 75: val_loss did not improve from 0.10804\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1348 - mae: 0.2352 - val_loss: 0.1083 - val_mae: 0.1883\n",
            "Epoch 76/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1372 - mae: 0.2354\n",
            "Epoch 76: val_loss improved from 0.10804 to 0.10795, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1371 - mae: 0.2359 - val_loss: 0.1079 - val_mae: 0.1899\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1345 - mae: 0.2347\n",
            "Epoch 77: val_loss did not improve from 0.10795\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1345 - mae: 0.2347 - val_loss: 0.1088 - val_mae: 0.1892\n",
            "Epoch 78/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1332 - mae: 0.2330\n",
            "Epoch 78: val_loss improved from 0.10795 to 0.10712, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1331 - mae: 0.2330 - val_loss: 0.1071 - val_mae: 0.1865\n",
            "Epoch 79/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1336 - mae: 0.2335\n",
            "Epoch 79: val_loss improved from 0.10712 to 0.10695, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1334 - mae: 0.2335 - val_loss: 0.1070 - val_mae: 0.1872\n",
            "Epoch 80/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1341 - mae: 0.2342\n",
            "Epoch 80: val_loss did not improve from 0.10695\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1339 - mae: 0.2341 - val_loss: 0.1081 - val_mae: 0.1881\n",
            "Epoch 81/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1331 - mae: 0.2330\n",
            "Epoch 81: val_loss did not improve from 0.10695\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1327 - mae: 0.2330 - val_loss: 0.1074 - val_mae: 0.1869\n",
            "Epoch 82/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1334 - mae: 0.2338\n",
            "Epoch 82: val_loss improved from 0.10695 to 0.10627, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1332 - mae: 0.2338 - val_loss: 0.1063 - val_mae: 0.1861\n",
            "Epoch 83/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1353 - mae: 0.2336\n",
            "Epoch 83: val_loss did not improve from 0.10627\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1354 - mae: 0.2337 - val_loss: 0.1070 - val_mae: 0.1862\n",
            "Epoch 84/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1305 - mae: 0.2312\n",
            "Epoch 84: val_loss did not improve from 0.10627\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1317 - mae: 0.2316 - val_loss: 0.1069 - val_mae: 0.1894\n",
            "Epoch 85/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1318 - mae: 0.2319\n",
            "Epoch 85: val_loss improved from 0.10627 to 0.10564, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1317 - mae: 0.2319 - val_loss: 0.1056 - val_mae: 0.1839\n",
            "Epoch 86/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1294 - mae: 0.2287\n",
            "Epoch 86: val_loss improved from 0.10564 to 0.10562, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1294 - mae: 0.2286 - val_loss: 0.1056 - val_mae: 0.1854\n",
            "Epoch 87/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1287 - mae: 0.2296\n",
            "Epoch 87: val_loss improved from 0.10562 to 0.10519, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1288 - mae: 0.2298 - val_loss: 0.1052 - val_mae: 0.1834\n",
            "Epoch 88/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1297 - mae: 0.2311\n",
            "Epoch 88: val_loss did not improve from 0.10519\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1301 - mae: 0.2308 - val_loss: 0.1059 - val_mae: 0.1845\n",
            "Epoch 89/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1287 - mae: 0.2284\n",
            "Epoch 89: val_loss improved from 0.10519 to 0.10480, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1291 - mae: 0.2285 - val_loss: 0.1048 - val_mae: 0.1829\n",
            "Epoch 90/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1283 - mae: 0.2283\n",
            "Epoch 90: val_loss did not improve from 0.10480\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1289 - mae: 0.2288 - val_loss: 0.1054 - val_mae: 0.1834\n",
            "Epoch 91/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1284 - mae: 0.2272\n",
            "Epoch 91: val_loss improved from 0.10480 to 0.10449, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1286 - mae: 0.2273 - val_loss: 0.1045 - val_mae: 0.1821\n",
            "Epoch 92/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1293 - mae: 0.2291\n",
            "Epoch 92: val_loss did not improve from 0.10449\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1283 - mae: 0.2286 - val_loss: 0.1045 - val_mae: 0.1831\n",
            "Epoch 93/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1283 - mae: 0.2285\n",
            "Epoch 93: val_loss improved from 0.10449 to 0.10413, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1279 - mae: 0.2281 - val_loss: 0.1041 - val_mae: 0.1820\n",
            "Epoch 94/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1267 - mae: 0.2266\n",
            "Epoch 94: val_loss improved from 0.10413 to 0.10408, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1269 - mae: 0.2267 - val_loss: 0.1041 - val_mae: 0.1814\n",
            "Epoch 95/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1291 - mae: 0.2286\n",
            "Epoch 95: val_loss did not improve from 0.10408\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1288 - mae: 0.2288 - val_loss: 0.1048 - val_mae: 0.1829\n",
            "Epoch 96/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1278 - mae: 0.2283\n",
            "Epoch 96: val_loss did not improve from 0.10408\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1279 - mae: 0.2285 - val_loss: 0.1053 - val_mae: 0.1842\n",
            "Epoch 97/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1287 - mae: 0.2260\n",
            "Epoch 97: val_loss did not improve from 0.10408\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1289 - mae: 0.2263 - val_loss: 0.1041 - val_mae: 0.1815\n",
            "Epoch 98/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1265 - mae: 0.2264\n",
            "Epoch 98: val_loss did not improve from 0.10408\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1269 - mae: 0.2265 - val_loss: 0.1044 - val_mae: 0.1819\n",
            "Epoch 99/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1270 - mae: 0.2257\n",
            "Epoch 99: val_loss improved from 0.10408 to 0.10352, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1269 - mae: 0.2256 - val_loss: 0.1035 - val_mae: 0.1803\n",
            "Epoch 100/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1261 - mae: 0.2251\n",
            "Epoch 100: val_loss did not improve from 0.10352\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1260 - mae: 0.2252 - val_loss: 0.1036 - val_mae: 0.1805\n",
            "Epoch 101/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1242 - mae: 0.2245\n",
            "Epoch 101: val_loss did not improve from 0.10352\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1245 - mae: 0.2249 - val_loss: 0.1038 - val_mae: 0.1808\n",
            "Epoch 102/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1240 - mae: 0.2238\n",
            "Epoch 102: val_loss improved from 0.10352 to 0.10307, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1244 - mae: 0.2242 - val_loss: 0.1031 - val_mae: 0.1795\n",
            "Epoch 103/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1258 - mae: 0.2264\n",
            "Epoch 103: val_loss did not improve from 0.10307\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1256 - mae: 0.2264 - val_loss: 0.1033 - val_mae: 0.1799\n",
            "Epoch 104/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1246 - mae: 0.2240\n",
            "Epoch 104: val_loss did not improve from 0.10307\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1249 - mae: 0.2243 - val_loss: 0.1035 - val_mae: 0.1827\n",
            "Epoch 105/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1232 - mae: 0.2244\n",
            "Epoch 105: val_loss improved from 0.10307 to 0.10258, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1238 - mae: 0.2249 - val_loss: 0.1026 - val_mae: 0.1787\n",
            "Epoch 106/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1226 - mae: 0.2233\n",
            "Epoch 106: val_loss did not improve from 0.10258\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1236 - mae: 0.2235 - val_loss: 0.1035 - val_mae: 0.1804\n",
            "Epoch 107/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1247 - mae: 0.2241\n",
            "Epoch 107: val_loss did not improve from 0.10258\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1238 - mae: 0.2237 - val_loss: 0.1036 - val_mae: 0.1806\n",
            "Epoch 108/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1243 - mae: 0.2231\n",
            "Epoch 108: val_loss improved from 0.10258 to 0.10234, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1241 - mae: 0.2233 - val_loss: 0.1023 - val_mae: 0.1787\n",
            "Epoch 109/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1234 - mae: 0.2227\n",
            "Epoch 109: val_loss improved from 0.10234 to 0.10221, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1237 - mae: 0.2230 - val_loss: 0.1022 - val_mae: 0.1779\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1240 - mae: 0.2245\n",
            "Epoch 110: val_loss improved from 0.10221 to 0.10208, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1240 - mae: 0.2245 - val_loss: 0.1021 - val_mae: 0.1776\n",
            "Epoch 111/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1216 - mae: 0.2215\n",
            "Epoch 111: val_loss did not improve from 0.10208\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1213 - mae: 0.2207 - val_loss: 0.1022 - val_mae: 0.1787\n",
            "Epoch 112/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1238 - mae: 0.2232\n",
            "Epoch 112: val_loss improved from 0.10208 to 0.10198, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1234 - mae: 0.2229 - val_loss: 0.1020 - val_mae: 0.1781\n",
            "Epoch 113/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1213 - mae: 0.2209\n",
            "Epoch 113: val_loss improved from 0.10198 to 0.10197, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1213 - mae: 0.2208 - val_loss: 0.1020 - val_mae: 0.1773\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1204 - mae: 0.2205\n",
            "Epoch 114: val_loss improved from 0.10197 to 0.10176, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1204 - mae: 0.2205 - val_loss: 0.1018 - val_mae: 0.1776\n",
            "Epoch 115/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1203 - mae: 0.2208\n",
            "Epoch 115: val_loss improved from 0.10176 to 0.10169, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1207 - mae: 0.2213 - val_loss: 0.1017 - val_mae: 0.1771\n",
            "Epoch 116/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1214 - mae: 0.2229\n",
            "Epoch 116: val_loss did not improve from 0.10169\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1218 - mae: 0.2230 - val_loss: 0.1024 - val_mae: 0.1782\n",
            "Epoch 117/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1203 - mae: 0.2206\n",
            "Epoch 117: val_loss improved from 0.10169 to 0.10168, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1205 - mae: 0.2207 - val_loss: 0.1017 - val_mae: 0.1778\n",
            "Epoch 118/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1215 - mae: 0.2214\n",
            "Epoch 118: val_loss improved from 0.10168 to 0.10145, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1214 - mae: 0.2215 - val_loss: 0.1015 - val_mae: 0.1769\n",
            "Epoch 119/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1209 - mae: 0.2193\n",
            "Epoch 119: val_loss did not improve from 0.10145\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1213 - mae: 0.2199 - val_loss: 0.1018 - val_mae: 0.1789\n",
            "Epoch 120/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1204 - mae: 0.2216\n",
            "Epoch 120: val_loss improved from 0.10145 to 0.10135, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1208 - mae: 0.2216 - val_loss: 0.1014 - val_mae: 0.1768\n",
            "Epoch 121/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1229 - mae: 0.2226\n",
            "Epoch 121: val_loss did not improve from 0.10135\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1222 - mae: 0.2223 - val_loss: 0.1028 - val_mae: 0.1791\n",
            "Epoch 122/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1192 - mae: 0.2188\n",
            "Epoch 122: val_loss improved from 0.10135 to 0.10127, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1200 - mae: 0.2194 - val_loss: 0.1013 - val_mae: 0.1768\n",
            "Epoch 123/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1207 - mae: 0.2201\n",
            "Epoch 123: val_loss did not improve from 0.10127\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1215 - mae: 0.2200 - val_loss: 0.1032 - val_mae: 0.1809\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1178 - mae: 0.2165\n",
            "Epoch 124: val_loss did not improve from 0.10127\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1178 - mae: 0.2165 - val_loss: 0.1024 - val_mae: 0.1789\n",
            "Epoch 125/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1183 - mae: 0.2187\n",
            "Epoch 125: val_loss improved from 0.10127 to 0.10093, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1196 - mae: 0.2194 - val_loss: 0.1009 - val_mae: 0.1757\n",
            "Epoch 126/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1182 - mae: 0.2173\n",
            "Epoch 126: val_loss did not improve from 0.10093\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1180 - mae: 0.2175 - val_loss: 0.1011 - val_mae: 0.1757\n",
            "Epoch 127/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1174 - mae: 0.2166\n",
            "Epoch 127: val_loss did not improve from 0.10093\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1177 - mae: 0.2170 - val_loss: 0.1014 - val_mae: 0.1763\n",
            "Epoch 128/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1198 - mae: 0.2185\n",
            "Epoch 128: val_loss improved from 0.10093 to 0.10063, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1196 - mae: 0.2184 - val_loss: 0.1006 - val_mae: 0.1750\n",
            "Epoch 129/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1156 - mae: 0.2164\n",
            "Epoch 129: val_loss did not improve from 0.10063\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1172 - mae: 0.2172 - val_loss: 0.1012 - val_mae: 0.1761\n",
            "Epoch 130/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1194 - mae: 0.2186\n",
            "Epoch 130: val_loss did not improve from 0.10063\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1188 - mae: 0.2182 - val_loss: 0.1010 - val_mae: 0.1758\n",
            "Epoch 131/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1175 - mae: 0.2163\n",
            "Epoch 131: val_loss did not improve from 0.10063\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1175 - mae: 0.2163 - val_loss: 0.1023 - val_mae: 0.1831\n",
            "Epoch 132/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1162 - mae: 0.2169\n",
            "Epoch 132: val_loss did not improve from 0.10063\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1175 - mae: 0.2174 - val_loss: 0.1011 - val_mae: 0.1759\n",
            "Epoch 133/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1173 - mae: 0.2172\n",
            "Epoch 133: val_loss improved from 0.10063 to 0.10045, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1171 - mae: 0.2170 - val_loss: 0.1005 - val_mae: 0.1759\n",
            "Epoch 134/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1170 - mae: 0.2159\n",
            "Epoch 134: val_loss did not improve from 0.10045\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1177 - mae: 0.2164 - val_loss: 0.1007 - val_mae: 0.1774\n",
            "Epoch 135/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1175 - mae: 0.2177\n",
            "Epoch 135: val_loss improved from 0.10045 to 0.10013, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1170 - mae: 0.2174 - val_loss: 0.1001 - val_mae: 0.1748\n",
            "Epoch 136/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1185 - mae: 0.2164\n",
            "Epoch 136: val_loss improved from 0.10013 to 0.10007, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1180 - mae: 0.2160 - val_loss: 0.1001 - val_mae: 0.1743\n",
            "Epoch 137/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1170 - mae: 0.2153\n",
            "Epoch 137: val_loss improved from 0.10007 to 0.09999, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1172 - mae: 0.2155 - val_loss: 0.1000 - val_mae: 0.1741\n",
            "Epoch 138/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1186 - mae: 0.2173\n",
            "Epoch 138: val_loss did not improve from 0.09999\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1173 - mae: 0.2163 - val_loss: 0.1005 - val_mae: 0.1749\n",
            "Epoch 139/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1154 - mae: 0.2138\n",
            "Epoch 139: val_loss improved from 0.09999 to 0.09981, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1152 - mae: 0.2138 - val_loss: 0.0998 - val_mae: 0.1738\n",
            "Epoch 140/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1148 - mae: 0.2143\n",
            "Epoch 140: val_loss did not improve from 0.09981\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1153 - mae: 0.2146 - val_loss: 0.1032 - val_mae: 0.1820\n",
            "Epoch 141/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1160 - mae: 0.2157\n",
            "Epoch 141: val_loss did not improve from 0.09981\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1157 - mae: 0.2154 - val_loss: 0.1002 - val_mae: 0.1742\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1161 - mae: 0.2154\n",
            "Epoch 142: val_loss did not improve from 0.09981\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1161 - mae: 0.2154 - val_loss: 0.1006 - val_mae: 0.1752\n",
            "Epoch 143/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1150 - mae: 0.2159\n",
            "Epoch 143: val_loss did not improve from 0.09981\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1157 - mae: 0.2162 - val_loss: 0.1011 - val_mae: 0.1766\n",
            "Epoch 144/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1160 - mae: 0.2142\n",
            "Epoch 144: val_loss improved from 0.09981 to 0.09977, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1156 - mae: 0.2139 - val_loss: 0.0998 - val_mae: 0.1734\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1145 - mae: 0.2141\n",
            "Epoch 145: val_loss did not improve from 0.09977\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1145 - mae: 0.2141 - val_loss: 0.1019 - val_mae: 0.1787\n",
            "Epoch 146/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1172 - mae: 0.2157\n",
            "Epoch 146: val_loss improved from 0.09977 to 0.09968, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1162 - mae: 0.2149 - val_loss: 0.0997 - val_mae: 0.1733\n",
            "Epoch 147/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1148 - mae: 0.2123\n",
            "Epoch 147: val_loss improved from 0.09968 to 0.09963, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1140 - mae: 0.2117 - val_loss: 0.0996 - val_mae: 0.1732\n",
            "Epoch 148/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1149 - mae: 0.2123\n",
            "Epoch 148: val_loss improved from 0.09963 to 0.09947, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1151 - mae: 0.2125 - val_loss: 0.0995 - val_mae: 0.1732\n",
            "Epoch 149/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1142 - mae: 0.2125\n",
            "Epoch 149: val_loss did not improve from 0.09947\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1142 - mae: 0.2122 - val_loss: 0.0999 - val_mae: 0.1737\n",
            "Epoch 150/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1144 - mae: 0.2121\n",
            "Epoch 150: val_loss did not improve from 0.09947\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1141 - mae: 0.2121 - val_loss: 0.0996 - val_mae: 0.1731\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1145 - mae: 0.2133\n",
            "Epoch 151: val_loss improved from 0.09947 to 0.09938, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1145 - mae: 0.2133 - val_loss: 0.0994 - val_mae: 0.1736\n",
            "Epoch 152/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1146 - mae: 0.2129\n",
            "Epoch 152: val_loss did not improve from 0.09938\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1155 - mae: 0.2133 - val_loss: 0.1007 - val_mae: 0.1795\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1146 - mae: 0.2131\n",
            "Epoch 153: val_loss did not improve from 0.09938\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1146 - mae: 0.2131 - val_loss: 0.0995 - val_mae: 0.1729\n",
            "Epoch 154/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1132 - mae: 0.2126\n",
            "Epoch 154: val_loss did not improve from 0.09938\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1131 - mae: 0.2127 - val_loss: 0.0995 - val_mae: 0.1729\n",
            "Epoch 155/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1133 - mae: 0.2127\n",
            "Epoch 155: val_loss did not improve from 0.09938\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1128 - mae: 0.2124 - val_loss: 0.1007 - val_mae: 0.1757\n",
            "Epoch 156/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1121 - mae: 0.2115\n",
            "Epoch 156: val_loss improved from 0.09938 to 0.09912, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1131 - mae: 0.2119 - val_loss: 0.0991 - val_mae: 0.1723\n",
            "Epoch 157/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1117 - mae: 0.2112\n",
            "Epoch 157: val_loss improved from 0.09912 to 0.09898, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1124 - mae: 0.2113 - val_loss: 0.0990 - val_mae: 0.1720\n",
            "Epoch 158/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1122 - mae: 0.2104\n",
            "Epoch 158: val_loss improved from 0.09898 to 0.09888, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1121 - mae: 0.2103 - val_loss: 0.0989 - val_mae: 0.1719\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1129 - mae: 0.2116\n",
            "Epoch 159: val_loss did not improve from 0.09888\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1129 - mae: 0.2116 - val_loss: 0.0996 - val_mae: 0.1757\n",
            "Epoch 160/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1134 - mae: 0.2123\n",
            "Epoch 160: val_loss did not improve from 0.09888\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1127 - mae: 0.2119 - val_loss: 0.0997 - val_mae: 0.1763\n",
            "Epoch 161/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1130 - mae: 0.2117\n",
            "Epoch 161: val_loss did not improve from 0.09888\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1127 - mae: 0.2117 - val_loss: 0.0994 - val_mae: 0.1729\n",
            "Epoch 162/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1138 - mae: 0.2133\n",
            "Epoch 162: val_loss improved from 0.09888 to 0.09869, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1132 - mae: 0.2129 - val_loss: 0.0987 - val_mae: 0.1719\n",
            "Epoch 163/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1138 - mae: 0.2125\n",
            "Epoch 163: val_loss improved from 0.09869 to 0.09863, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1134 - mae: 0.2121 - val_loss: 0.0986 - val_mae: 0.1716\n",
            "Epoch 164/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1114 - mae: 0.2101\n",
            "Epoch 164: val_loss did not improve from 0.09863\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1113 - mae: 0.2100 - val_loss: 0.0988 - val_mae: 0.1717\n",
            "Epoch 165/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1117 - mae: 0.2107\n",
            "Epoch 165: val_loss did not improve from 0.09863\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1121 - mae: 0.2110 - val_loss: 0.1008 - val_mae: 0.1767\n",
            "Epoch 166/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1107 - mae: 0.2098\n",
            "Epoch 166: val_loss improved from 0.09863 to 0.09850, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1117 - mae: 0.2103 - val_loss: 0.0985 - val_mae: 0.1713\n",
            "Epoch 167/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1117 - mae: 0.2115\n",
            "Epoch 167: val_loss did not improve from 0.09850\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1115 - mae: 0.2113 - val_loss: 0.0985 - val_mae: 0.1712\n",
            "Epoch 168/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1113 - mae: 0.2101\n",
            "Epoch 168: val_loss did not improve from 0.09850\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1111 - mae: 0.2100 - val_loss: 0.0989 - val_mae: 0.1719\n",
            "Epoch 169/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1111 - mae: 0.2092\n",
            "Epoch 169: val_loss did not improve from 0.09850\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1112 - mae: 0.2095 - val_loss: 0.0996 - val_mae: 0.1736\n",
            "Epoch 170/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1098 - mae: 0.2081\n",
            "Epoch 170: val_loss did not improve from 0.09850\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1102 - mae: 0.2085 - val_loss: 0.0996 - val_mae: 0.1735\n",
            "Epoch 171/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1111 - mae: 0.2100\n",
            "Epoch 171: val_loss did not improve from 0.09850\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1110 - mae: 0.2099 - val_loss: 0.0986 - val_mae: 0.1712\n",
            "Epoch 172/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1124 - mae: 0.2121\n",
            "Epoch 172: val_loss improved from 0.09850 to 0.09836, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.1120 - mae: 0.2118 - val_loss: 0.0984 - val_mae: 0.1716\n",
            "Epoch 173/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1101 - mae: 0.2101\n",
            "Epoch 173: val_loss did not improve from 0.09836\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1108 - mae: 0.2103 - val_loss: 0.0984 - val_mae: 0.1721\n",
            "Epoch 174/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1123 - mae: 0.2100\n",
            "Epoch 174: val_loss did not improve from 0.09836\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1120 - mae: 0.2098 - val_loss: 0.0994 - val_mae: 0.1735\n",
            "Epoch 175/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1128 - mae: 0.2111\n",
            "Epoch 175: val_loss did not improve from 0.09836\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1119 - mae: 0.2108 - val_loss: 0.0991 - val_mae: 0.1725\n",
            "Epoch 176/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1094 - mae: 0.2092\n",
            "Epoch 176: val_loss improved from 0.09836 to 0.09810, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1097 - mae: 0.2094 - val_loss: 0.0981 - val_mae: 0.1711\n",
            "Epoch 177/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1089 - mae: 0.2076\n",
            "Epoch 177: val_loss did not improve from 0.09810\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1090 - mae: 0.2077 - val_loss: 0.0992 - val_mae: 0.1729\n",
            "Epoch 178/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1101 - mae: 0.2101\n",
            "Epoch 178: val_loss did not improve from 0.09810\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1099 - mae: 0.2099 - val_loss: 0.0986 - val_mae: 0.1715\n",
            "Epoch 179/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1099 - mae: 0.2088\n",
            "Epoch 179: val_loss did not improve from 0.09810\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1101 - mae: 0.2089 - val_loss: 0.0981 - val_mae: 0.1716\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1098 - mae: 0.2084\n",
            "Epoch 180: val_loss improved from 0.09810 to 0.09808, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1098 - mae: 0.2084 - val_loss: 0.0981 - val_mae: 0.1713\n",
            "Epoch 181/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1102 - mae: 0.2101\n",
            "Epoch 181: val_loss did not improve from 0.09808\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1103 - mae: 0.2100 - val_loss: 0.0988 - val_mae: 0.1747\n",
            "Epoch 182/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1066 - mae: 0.2064\n",
            "Epoch 182: val_loss did not improve from 0.09808\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1073 - mae: 0.2066 - val_loss: 0.0992 - val_mae: 0.1729\n",
            "Epoch 183/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1086 - mae: 0.2079\n",
            "Epoch 183: val_loss did not improve from 0.09808\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1094 - mae: 0.2081 - val_loss: 0.1015 - val_mae: 0.1796\n",
            "Epoch 184/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1087 - mae: 0.2074\n",
            "Epoch 184: val_loss improved from 0.09808 to 0.09781, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1093 - mae: 0.2079 - val_loss: 0.0978 - val_mae: 0.1704\n",
            "Epoch 185/200\n",
            " 95/100 [===========================>..] - ETA: 0s - loss: 0.1093 - mae: 0.2074\n",
            "Epoch 185: val_loss did not improve from 0.09781\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1099 - mae: 0.2083 - val_loss: 0.0980 - val_mae: 0.1718\n",
            "Epoch 186/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1082 - mae: 0.2074\n",
            "Epoch 186: val_loss did not improve from 0.09781\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1086 - mae: 0.2076 - val_loss: 0.0979 - val_mae: 0.1703\n",
            "Epoch 187/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1089 - mae: 0.2084\n",
            "Epoch 187: val_loss improved from 0.09781 to 0.09768, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1086 - mae: 0.2084 - val_loss: 0.0977 - val_mae: 0.1701\n",
            "Epoch 188/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1079 - mae: 0.2074\n",
            "Epoch 188: val_loss did not improve from 0.09768\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1090 - mae: 0.2077 - val_loss: 0.0981 - val_mae: 0.1705\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1086 - mae: 0.2082\n",
            "Epoch 189: val_loss did not improve from 0.09768\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1086 - mae: 0.2082 - val_loss: 0.0977 - val_mae: 0.1700\n",
            "Epoch 190/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1077 - mae: 0.2062\n",
            "Epoch 190: val_loss did not improve from 0.09768\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1079 - mae: 0.2063 - val_loss: 0.0979 - val_mae: 0.1703\n",
            "Epoch 191/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1099 - mae: 0.2078\n",
            "Epoch 191: val_loss did not improve from 0.09768\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1088 - mae: 0.2071 - val_loss: 0.0978 - val_mae: 0.1702\n",
            "Epoch 192/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1062 - mae: 0.2042\n",
            "Epoch 192: val_loss improved from 0.09768 to 0.09764, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1064 - mae: 0.2045 - val_loss: 0.0976 - val_mae: 0.1699\n",
            "Epoch 193/200\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.1073 - mae: 0.2067\n",
            "Epoch 193: val_loss improved from 0.09764 to 0.09749, saving model to /content/gdrive/MyDrive/cnn_model/SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window.hdf5\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1080 - mae: 0.2069 - val_loss: 0.0975 - val_mae: 0.1698\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1063 - mae: 0.2049\n",
            "Epoch 194: val_loss did not improve from 0.09749\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1063 - mae: 0.2049 - val_loss: 0.0975 - val_mae: 0.1706\n",
            "Epoch 195/200\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1066 - mae: 0.2052\n",
            "Epoch 195: val_loss did not improve from 0.09749\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1066 - mae: 0.2052 - val_loss: 0.0990 - val_mae: 0.1728\n",
            "Epoch 196/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1069 - mae: 0.2063\n",
            "Epoch 196: val_loss did not improve from 0.09749\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1071 - mae: 0.2065 - val_loss: 0.0981 - val_mae: 0.1706\n",
            "Epoch 197/200\n",
            " 96/100 [===========================>..] - ETA: 0s - loss: 0.1055 - mae: 0.2049\n",
            "Epoch 197: val_loss did not improve from 0.09749\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1063 - mae: 0.2054 - val_loss: 0.0981 - val_mae: 0.1732\n",
            "Epoch 198/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1073 - mae: 0.2064\n",
            "Epoch 198: val_loss did not improve from 0.09749\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1073 - mae: 0.2066 - val_loss: 0.0976 - val_mae: 0.1697\n",
            "Epoch 199/200\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1075 - mae: 0.2066\n",
            "Epoch 199: val_loss did not improve from 0.09749\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1080 - mae: 0.2066 - val_loss: 0.0977 - val_mae: 0.1716\n",
            "Epoch 199: early stopping\n",
            "testing on the testing datasets.....\n",
            "modelpath: SG_mergeweighted_7:35_Falsedays_Trueinit_Truetrainable_0.010000dpt_0.001000lr_3fz_100fn_17maxlen_900dim_Truefilter_Truetransform 128batch_10window\n",
            "Testing model...\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.11760633438825607, RMSE:0.34293779730796814, MAE:0.1901424080133438, R2:0.8627770132052555\n",
            "0.8627770132052555\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing on the filtered testing datasets.....\n",
            "[0.8562258751044397]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3 机器学习预测——独热编码、多尺度编码**"
      ],
      "metadata": {
        "id": "YWSOXmm7FQB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3.1 导入模型**"
      ],
      "metadata": {
        "id": "6MDCAf8vhaif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================第一部分 独热编码 ============================================\n",
        "from sklearn import tree\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn import neighbors\n",
        "from sklearn import ensemble\n",
        "from sklearn import ensemble\n",
        "from sklearn import ensemble\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import ExtraTreeRegressor\n",
        "import xgboost as xgb\n",
        "def onehot_RF(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    #one_hot_encoder\n",
        "    print('RandomForestRegressor with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = ensemble.RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_SVR(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('SVR with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = svm.SVR()\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...') \n",
        "\n",
        "def onehot_AdaBoost(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('AdaBoost with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = ensemble.AdaBoostRegressor(n_estimators=10,random_state=90)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_GradientBoosting(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('GradientBoosting with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = ensemble.GradientBoostingRegressor(n_estimators=10,random_state=90)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_Bagging(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('Bagging with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = BaggingRegressor(n_estimators=10,random_state=90)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_ExtraTree(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('ExtraTree with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = ExtraTreeRegressor(random_state=90)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_DecisionTree(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('DecisionTree with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = tree.DecisionTreeRegressor(random_state=90)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_KNeighbors(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('KNeighbors with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = neighbors.KNeighborsRegressor()\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_XGBoost(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    print('XGBoost with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    rf = xgb.XGBRegressor(n_estimators=10,random_state=90)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "def onehot_LR(X_train, X_test, y_train,y_test, demographics_train, demographics_test):\n",
        "    #one_hot_encoder\n",
        "    print('LR with one_hot encoding...') \n",
        "    onehot_train = one_hot_encoder(X_train, nb_words)\n",
        "    #train_mat = onehot_train\n",
        "    train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "    #rf = ensemble.RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
        "    #rf.fit(train_mat, y_train.ravel())\n",
        "    rf = LinearRegression(normalize=True, n_jobs=-1)\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = onehot_test\n",
        "    onehot_test = one_hot_encoder(X_test, nb_words)\n",
        "    test_mat = np.hstack((onehot_test, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse =  evaluation(y_test, y_pred)\n",
        "    print('testing on filter samples...') \n",
        "\n",
        "#============================== 第二部分 预测函数 单尺度、多尺度======================================================\n",
        "\n",
        "\n",
        "\n",
        "def embedding_RF(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #word2vec encoder\n",
        "    print(f,'RandomForestRegressor with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "\n",
        "    #rf = ensemble.RandomForestRegressor(n_estimators=10,random_state=90)\n",
        "    #rf.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = rf.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    \n",
        "    #multichannel word2vec embedding encoder\n",
        "    print(f,'RandomForestRegressor with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    rf = ensemble.RandomForestRegressor(n_estimators=10,random_state=90) #n_jobs=-1便是使用全部的CPU\n",
        "    rf.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = rf.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "    index = 1\n",
        "    name = 'MG_RF%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "    print('testing on filter testing samples...')\n",
        "\n",
        "    index = 2\n",
        "    name = 'MG_RF%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "    print('testing on filter samples...')\n",
        "\n",
        "    index = 3\n",
        "    name = 'MG_RF%d'%(index)\n",
        "    if args.isdays:\n",
        "        daysplots(y_test, y_pred, r2, rmse, name)\n",
        "    else:\n",
        "        costplots(y_test, y_pred, r2, rmse, name)\n",
        "\n",
        "\n",
        "def embedding_LR(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #word2vec encoder\n",
        "    print(f,'Linear regression with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #print('LR_train_mat')\n",
        "    #print(train_mat.shape)\n",
        "    #lr = LinearRegression(normalize=True)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "    #multichannel word2vec embedding encoder\n",
        "    print(f,'Linear regression with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = svm.SVR()\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "    \n",
        "\n",
        "def embedding_SVR(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'SVR with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #print('SVR_train_mat')\n",
        "    #lr = svm.SVR()\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'SVR with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = svm.SVR()\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "def embedding_AdaBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'AdaBoost with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = ensemble.AdaBoostRegressor(n_estimators=10,random_state=90)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'AdaBoost with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = ensemble.AdaBoostRegressor(n_estimators=10,random_state=90)\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "def embedding_GradientBoosting(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    #print(f,'GradientBoosting with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = ensemble.GradientBoostingRegressor(n_estimators=10,random_state=90)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'GradientBoosting with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = ensemble.GradientBoostingRegressor(n_estimators=10,random_state=90)\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "\n",
        "def embedding_Bagging(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'Bagging with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = BaggingRegressor(n_estimators=10,random_state=90)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'Bagging with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = BaggingRegressor(n_estimators=10,random_state=90)\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "\n",
        "def embedding_ExtraTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'ExtraTree with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = ExtraTreeRegressor(random_state=90)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'ExtraTree with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = ExtraTreeRegressor(random_state=90)\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "\n",
        "def embedding_DecisionTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'DecisionTree with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = tree.DecisionTreeRegressor(random_state=90)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'DecisionTree with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = tree.DecisionTreeRegressor(random_state=90)\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "\n",
        "def embedding_KNeighbors(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'KNeighbors with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = neighbors.KNeighborsRegressor()\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'KNeighbors with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = neighbors.KNeighborsRegressor()\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n",
        "\n",
        "def embedding_XGBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test):\n",
        "    #1、word2vec encoder\n",
        "    print(f,'XGBoost with embedding encoding...')\n",
        "    #train_mat = embedding_encoder(X_train, index_embedding)\n",
        "    #test_mat = embedding_encoder(X_test, index_embedding)\n",
        "    #train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    #lr = xgb.XGBRegressor(n_estimators=10,random_state=90)\n",
        "    #lr.fit(train_mat, y_train.ravel())\n",
        "    #test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    #y_pred = lr.predict(test_mat)\n",
        "    #r2, rmse = evaluation(y_test, y_pred)\n",
        "    #2、multichannel word2vec embedding encoder\n",
        "    print(f,'XGBoost with multichannel embedding encoding...')\n",
        "    train_mat = multichannel_embedding_encoder(X_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    test_mat = multichannel_embedding_encoder(X_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "    train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "    lr = xgb.XGBRegressor(n_estimators=10,random_state=90)\n",
        "    lr.fit(train_mat, y_train.ravel())\n",
        "    test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "    y_pred = lr.predict(test_mat)\n",
        "    r2, rmse = evaluation(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "OV7PN1avFQab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3.2 机器学习：X_train**"
      ],
      "metadata": {
        "id": "o2TO8Cjvm_PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#onehot_SVR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_AdaBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_GradientBoosting(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_RF(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_Bagging(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_ExtraTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_DecisionTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_LR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_KNeighbors(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_XGBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_RF(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "\n",
        "embedding_SVR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_AdaBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_GradientBoosting(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_Bagging(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_ExtraTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_DecisionTree(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_LR(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_KNeighbors(X_train, X_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_XGBoost(X_train, X_test,y_train,y_test,demographics_train,demographics_test)"
      ],
      "metadata": {
        "id": "vqRHTyfUfvRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3.2 机器学习：X1_train**"
      ],
      "metadata": {
        "id": "FSVeSGSRmc7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_RF(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yap1KX53iYAL",
        "outputId": "45f2bfaf-081e-4f05-efe8-b471109f15c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/15684_new_2.csv RandomForestRegressor with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv RandomForestRegressor with multichannel embedding encoding...\n",
            "开始步骤52/3:开始多尺度编码...\n",
            "开始步骤52/3:开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.06514871970351838, RMSE:0.2552424723738555, MAE:0.1028683127363143, R2:0.9239845174521355\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:409: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing on filter testing samples...\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:409: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing on filter samples...\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:409: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#onehot_SVR(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_AdaBoost(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_GradientBoosting(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_RF(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_Bagging(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_ExtraTree(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_DecisionTree(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_LR(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_KNeighbors(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "#onehot_XGBoost(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "\n",
        "embedding_SVR(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_AdaBoost(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_GradientBoosting(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_Bagging(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_ExtraTree(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_DecisionTree(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_LR(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_KNeighbors(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_XGBoost(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)\n",
        "embedding_RF(X1_train, X1_test,y_train,y_test,demographics_train,demographics_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7hSvpL_f_Mg",
        "outputId": "fe6a72d3-d812-496f-9b59-3894d309aff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/15684_new_2.csv SVR with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv SVR with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.8320842966076956, RMSE:0.9121865470438026, MAE:0.42257155299910787, R2:0.0291245996086944\n",
            "/content/gdrive/MyDrive/15684_new_2.csv AdaBoost with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv AdaBoost with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.0915339382750846, RMSE:0.3025457622824762, MAE:0.16853713599393558, R2:0.8931982620817147\n",
            "/content/gdrive/MyDrive/15684_new_2.csv GradientBoosting with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.16220130679550598, RMSE:0.40274223368738715, MAE:0.25416818042611694, R2:0.8107436237877639\n",
            "/content/gdrive/MyDrive/15684_new_2.csv Bagging with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv Bagging with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.06940223087252957, RMSE:0.26344303155052246, MAE:0.10544534289712787, R2:0.9190215234668884\n",
            "/content/gdrive/MyDrive/15684_new_2.csv ExtraTree with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv ExtraTree with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.1187610337033677, RMSE:0.3446172278098814, MAE:0.1266083335430728, R2:0.8614297053583212\n",
            "/content/gdrive/MyDrive/15684_new_2.csv DecisionTree with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv DecisionTree with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.11865731502157353, RMSE:0.3444667110499555, MAE:0.1257844642896877, R2:0.8615507242468226\n",
            "/content/gdrive/MyDrive/15684_new_2.csv Linear regression with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv Linear regression with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:5.3015374352163227e+23, RMSE:728116572755.7863, MAE:55331298742.74734, R2:-6.185830331240913e+23\n",
            "/content/gdrive/MyDrive/15684_new_2.csv KNeighbors with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv KNeighbors with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float32\n",
            "MSE:0.4764047861099243, RMSE:0.690220832824707, MAE:0.29000937938690186, R2:0.4441311894182388\n",
            "/content/gdrive/MyDrive/15684_new_2.csv XGBoost with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv XGBoost with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "[11:23:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float32\n",
            "MSE:3.898815393447876, RMSE:1.9745417833328247, MAE:1.9325222969055176, R2:-3.549135038679654\n",
            "/content/gdrive/MyDrive/15684_new_2.csv RandomForestRegressor with embedding encoding...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv RandomForestRegressor with multichannel embedding encoding...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.06965487422732682, RMSE:0.2639220987854689, MAE:0.10638085194041276, R2:0.918726739369598\n",
            "testing on filter testing samples...\n",
            "testing on filter samples...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2、RF.py**"
      ],
      "metadata": {
        "id": "siBGvNSeQNke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X1_train_1,X1_val_1, demographics_train_1,demographics_val_1,y_train_1,y_val_1\n",
        "\n",
        "=\n",
        "train_test_split(X1_train, demographics_train, y_train, test_size=0.1, random_state=1) \n"
      ],
      "metadata": {
        "id": "c4ZJhzOdRmIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 RF1: 特征数据**"
      ],
      "metadata": {
        "id": "KxVcXvtz-d2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.1 数据整理**"
      ],
      "metadata": {
        "id": "SJ5ISnZO9ye9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demographics_train_1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltLOZyHVQs1I",
        "outputId": "8c0b0285-2610-4482-fc5e-b2b6635ed260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12703, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pgm9DpM5Zx8",
        "outputId": "e31c145a-a238-4049-8d9b-b01dfa61ef3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.71278  ],\n",
              "       [5.70374  ],\n",
              "       [6.3433576],\n",
              "       ...,\n",
              "       [5.70374  ],\n",
              "       [5.70374  ],\n",
              "       [5.70374  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_val_1=np.log(y_val_1)\n",
        "y_val_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOn4E5hsRagO",
        "outputId": "cac7e1b3-6f36-4b76-807b-f1ca2510c8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.70374 ],\n",
              "       [5.70374 ],\n",
              "       [5.70374 ],\n",
              "       ...,\n",
              "       [8.166428],\n",
              "       [5.70374 ],\n",
              "       [5.70374 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 **数据预测：验证集**"
      ],
      "metadata": {
        "id": "9D9UO2PS976R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import ensemble\n",
        "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=90)\n",
        "model_RandomForestRegressor.fit(demographics_train_1,y_train_1)\n",
        "y_RF_hat = model_RandomForestRegressor.predict(demographics_val_1)\n",
        "R2_RF_1=r2_score(y_val_1, y_RF_hat)\n",
        "print(R2_RF_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhEeyxcqQM30",
        "outputId": "7f69c84c-079f-4962-f402-cfb01aa6fbab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8888829531653479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 数据预测：测试集**"
      ],
      "metadata": {
        "id": "vQZsIcWp-Bm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=90)\n",
        "model_RandomForestRegressor.fit(demographics_train,y_train)\n",
        "y_RF_pred_1 = model_RandomForestRegressor.predict(demographics_test)\n",
        "R2=r2_score(y_test, y_RF_pred_1)\n",
        "print(R2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dxQyN1ZSYYi",
        "outputId": "8a7fb92a-1697-471b-ff7e-a39ebb6aeca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8661664754472297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 RF2: 疾病数据+特征数据**"
      ],
      "metadata": {
        "id": "EP21g9j5V_FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#可以试试这个数据集\n",
        "\n",
        "train_mat = multichannel_embedding_encoder(X1_train_1, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "val_mat= multichannel_embedding_encoder(X1_val_1, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "test_mat = multichannel_embedding_encoder(X1_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "\n",
        "train_mat = np.hstack((train_mat, demographics_train_1))#训练集\n",
        "val_mat = np.hstack((val_mat, demographics_val_1))#验证集\n",
        "test_mat = np.hstack((test_mat, demographics_test))#测试集\n",
        "\n",
        "#测试集预测 \n",
        "train_mat_all=multichannel_embedding_encoder(X1_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "train_all = np.hstack((train_mat_all, demographics_train))#训练集+验证集"
      ],
      "metadata": {
        "id": "yq4PRc7jqoe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587b09fb-dee7-446e-f1d9-ec204f7841d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1 数据预测：验证集** "
      ],
      "metadata": {
        "id": "bno5nrgo-kV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import ensemble\n",
        "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=90)\n",
        "\n",
        "data_train_1=np.concatenate((X1_train_1, demographics_train_1),axis=1)\n",
        "#data_train_1=train_mat\n",
        "model_RandomForestRegressor.fit(data_train_1,y_train_1)\n",
        "\n",
        "\n",
        "data_val_1=np.concatenate((X1_val_1, demographics_val_1),axis=1)\n",
        "#data_val_1=val_mat\n",
        "y_RF_hat = model_RandomForestRegressor.predict(data_val_1)\n",
        "\n",
        "R2_RF_2=r2_score(y_val_1, y_RF_hat)\n",
        "print(R2_RF_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0wdcT3zV-A_",
        "outputId": "0d9eab4c-5981-40be-ca32-847b9ca4a771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9392796414986722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2 数据预测：测试集**"
      ],
      "metadata": {
        "id": "RZ3jjvEZ-qjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgPmtWz0QvCc",
        "outputId": "452104d1-0b27-49d0-986d-04038cdae384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.021788 ],\n",
              "       [5.297479 ],\n",
              "       [6.9925523],\n",
              "       ...,\n",
              "       [5.70374  ],\n",
              "       [7.8099203],\n",
              "       [5.297479 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=90)\n",
        "data_train=np.concatenate((X1_train, demographics_train),axis=1)\n",
        "#data_train=train_mat_all\n",
        "model_RandomForestRegressor.fit(data_train,y_train) #数据拟合\n",
        "data_test=np.concatenate((X1_test, demographics_test),axis=1)\n",
        "#data_test=test_mat\n",
        "y_RF_pred_2 = model_RandomForestRegressor.predict(data_test) #模型预测\n",
        "R2=r2_score(y_test, y_RF_pred_2) #拟合优度\n",
        "print(R2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeqnDHvYUs3V",
        "outputId": "efdd9088-ab63-4b0b-a6f5-959189ddfac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9128287590325465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 RF3（原代码）：X1_train**"
      ],
      "metadata": {
        "id": "Gi35W1xrciI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.1 独热编码、单尺度——随机森林模型**"
      ],
      "metadata": {
        "id": "I7pzk1P-1Vp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comm_X1111=pd.DataFrame(comm_X1) #\n",
        "comm_X1111.to_csv('/content/gdrive/MyDrive/comm_X1.csv')"
      ],
      "metadata": {
        "id": "EgZdsUBThU9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one_hot_encoder 独热编码\n",
        "print('RandomForestRegressor with one_hot encoding...') \n",
        "onehot_train = one_hot_encoder(X1_train, nb_words)\n",
        "#train_mat = onehot_train\n",
        "train_mat = np.hstack((onehot_train, demographics_train))#merge the demographics info\n",
        "rf = ensemble.RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
        "rf.fit(train_mat, y_train.ravel())\n",
        "#test_mat = onehot_test\n",
        "onehot_test = one_hot_encoder(X1_test, nb_words)\n",
        "test_mat = np.hstack((onehot_test, demographics_test))  #merge the demographics info\n",
        "y_pred = rf.predict(test_mat)\n",
        "r2_RF_1, rmse =  evaluation(y_test, y_pred)\n",
        "print('testing on filter samples...')\n",
        "\n",
        "#word2vec encoder 单尺度编码\n",
        "print(f,'RandomForestRegressor with embedding encoding...')\n",
        "train_mat = embedding_encoder(X1_train, index_embedding)\n",
        "test_mat = embedding_encoder(X1_test, index_embedding)\n",
        "train_mat = np.hstack((train_mat, demographics_train))#merge the demographics info\n",
        "\n",
        "rf = ensemble.RandomForestRegressor(n_estimators=10,random_state=90)\n",
        "rf.fit(train_mat, y_train.ravel())                 #ravel()方法将数组维度拉成一维数组\n",
        "test_mat = np.hstack((test_mat, demographics_test))#merge the demographics info\n",
        "y_pred = rf.predict(test_mat)\n",
        "r2_RF_2, rmse = evaluation(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zaq80e_vX2-I",
        "outputId": "3ffe2a92-c459-446b-8730-07ad319cf122"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor with one_hot encoding...\n",
            "开始步骤53/3:开始独热编码one_hot encoding...\n",
            "开始步骤53/3:开始独热编码one_hot encoding...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.0703171263228731, RMSE:0.2651737662795343, MAE:0.10516539739453161, R2:0.9217197944208495\n",
            "testing on filter samples...\n",
            "/content/gdrive/MyDrive/15684_new_2.csv RandomForestRegressor with embedding encoding...\n",
            "开始步骤51/3:单尺度编码\n",
            "开始步骤51/3:单尺度编码\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.1330823894033981, RMSE:0.36480459071042143, MAE:0.17538567136396185, R2:0.8518466645859811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.2 多尺度随机森林模型**"
      ],
      "metadata": {
        "id": "tyrUDnsT1fVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#multichannel word2vec embedding encoder\n",
        "print(f,'RandomForestRegressor with multichannel embedding encoding...')\n",
        "train_mat = multichannel_embedding_encoder(X1_train_1, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "val_mat= multichannel_embedding_encoder(X1_val_1, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "test_mat = multichannel_embedding_encoder(X1_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "\n",
        "train_mat = np.hstack((train_mat, demographics_train_1))#训练集\n",
        "val_mat = np.hstack((val_mat, demographics_val_1))#验证集\n",
        "test_mat = np.hstack((test_mat, demographics_test))#测试集\n",
        "\n",
        "rf = ensemble.RandomForestRegressor(n_estimators=10) #n_jobs=-1便是使用全部的CPU\n",
        "rf.fit(train_mat, y_train_1.ravel())\n",
        "#验证集预测 \n",
        "y_RF_pred= rf.predict(val_mat)\n",
        "R2_RF_3, rmse = evaluation(y_val_1, y_RF_pred) #验证集的拟合优度\n",
        "#测试集预测 \n",
        "train_mat=multichannel_embedding_encoder(X1_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "train_all = np.hstack((train_mat, demographics_train))#训练集+验证集\n",
        "rf.fit(train_all, y_train.ravel()) \n",
        "y_RF_pred_3= rf.predict(test_mat) #测试集的预测值\n",
        "R2_RF, rmse = evaluation(y_test, y_RF_pred_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0IyGqVBcjPe",
        "outputId": "0fc2f4e8-b197-4021-f3bd-64dc29cac22f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/15684_new_2.csv RandomForestRegressor with multichannel embedding encoding...\n",
            "开始步骤52/3:开始多尺度编码...\n",
            "开始步骤52/3:开始多尺度编码...\n",
            "开始步骤52/3:开始多尺度编码...\n",
            "y_test:(1412, 1)\n",
            "y_pred:(1412,)\n",
            "float64\n",
            "MSE:0.0681524394138873, RMSE:0.26106022181459837, MAE:0.10606485127333316, R2:0.9254703754580028\n",
            "开始步骤52/3:开始多尺度编码...\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569,)\n",
            "float64\n",
            "MSE:0.07210274586443305, RMSE:0.26851954466003597, MAE:0.10451736832364035, R2:0.919731961980746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#可视化\n",
        "font2 = {'family': 'Times New Roman',\n",
        "     'weight': 'normal',\n",
        "     'size': 22,\n",
        "     }\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(y_test,'rs',label='true value')\n",
        "plt.plot(y_RF_pred_3,'go',label='predict value')\n",
        "plt.title('Random Forest_'+'modelscore: %f'%R2_RF,font2)\n",
        "plt.xticks(size=22)\n",
        "plt.yticks(size=22)\n",
        "plt.xlabel('Time Series',font2)\n",
        "plt.ylabel('Medical Cost',font2)\n",
        "plt.legend(loc=1,fontsize=14,frameon=True)\n",
        "plt.savefig('/content/gdrive/MyDrive/RF_0.9197.png',dpi=500,bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOwZfBzrxTqh",
        "outputId": "e58b1860-ea8d-4b7d-897c-b2e2ee527898"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  \n",
            "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 新方法：Stacking**"
      ],
      "metadata": {
        "id": "sD4mgP2NkzT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#不要运行\n",
        "X_train, X_test, X1_train, X1_test, y_train, y_test, demographics_train, demographics_test = train_test_split(comm_X, comm_X1,comm_y, comm_demographics, test_size=0.1, random_state=seed)"
      ],
      "metadata": {
        "id": "muWCrWYtXxoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#不要运行\n",
        "X1_train_1,X1_val_1, demographics_train_1,demographics_val_1,y_train_1,y_val_1=train_test_split( X1_train, demographics_train, y_train, test_size=0.1, random_state=90) "
      ],
      "metadata": {
        "id": "lsrJhQ4yYG2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 blending**"
      ],
      "metadata": {
        "id": "N3WD7NUbZQ2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_mat = multichannel_embedding_encoder(X1_train_1, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "val_mat= multichannel_embedding_encoder(X1_val_1, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "test_mat = multichannel_embedding_encoder(X1_test, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "\n",
        "train_mat = np.hstack((train_mat, demographics_train_1))#训练集\n",
        "val_mat = np.hstack((val_mat, demographics_val_1))#验证集\n",
        "test_mat = np.hstack((test_mat, demographics_test))#测试集\n",
        "\n",
        "#测试集预测 \n",
        "train_mat_all=multichannel_embedding_encoder(X1_train, index_embedding, index_embedding1, index_embedding2, index_embedding3)\n",
        "train_all = np.hstack((train_mat_all, demographics_train))#训练集+验证集"
      ],
      "metadata": {
        "id": "D7CJJ_QURtCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c396f3-9a8a-4f5e-ed2d-f37263fddc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n",
            "开始多尺度编码...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn import neighbors\n",
        "from sklearn import ensemble\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "#from sklearn.tree import ExtraTreeRegressor\n",
        "import xgboost as xgb\n",
        "#模型融合中使用到的各个单模型\n",
        "clfs = [ensemble.AdaBoostRegressor(random_state=90),\n",
        "        tree.DecisionTreeRegressor(random_state=90),\n",
        "        tree.ExtraTreeRegressor(random_state=90),\n",
        "        ensemble.GradientBoostingRegressor(random_state=90)]\n",
        "\n",
        "dataset_d1 = np.zeros((val_mat.shape[0], len(clfs)))\n",
        "dataset_d2 = np.zeros((X1_test.shape[0], len(clfs)))\n",
        " \n",
        "for j, clf in enumerate(clfs):\n",
        "    \n",
        "    clf.fit(train_mat, y_train_1) #依次训练各个单模型\n",
        "    y_submission = clf.predict(val_mat)[:, 1]\n",
        "    dataset_d1[:, j] = y_submission\n",
        "    dataset_d2[:, j] = clf.predict(X1_test)[:, 1]  #对于测试集，直接用这k个模型的预测值作为新的特征。\n",
        "    #print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n",
        "\n",
        "#融合使用的模型\n",
        "clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=-1)\n",
        "clf.fit(dataset_d1, y_val_1)\n",
        "R2=r2_score(y_test, y_submission)\n",
        "print(R2)"
      ],
      "metadata": {
        "id": "hi2VCrlXRtaT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "25e907d5-e91a-4cf8-c0dc-bd2e62c3342d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-01f8c10c1c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#依次训练各个单模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0my_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdataset_d1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdataset_d2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#对于测试集，直接用这k个模型的预测值作为新的特征。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 Stacking**"
      ],
      "metadata": {
        "id": "AI4Sfr39ZgbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend "
      ],
      "metadata": {
        "id": "rS9W-Ao5i_ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3148465-2b3c-4a99-abc5-38ddcee902f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->mlxtend) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.1->mlxtend) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2.1 尝试1**"
      ],
      "metadata": {
        "id": "Q_WlcxAUkcgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import itertools\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn import neighbors\n",
        "from sklearn import ensemble\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import ExtraTreeRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "#from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from mlxtend.plotting import plot_learning_curves\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "X =train_all\n",
        "y=y_train\n",
        "\n",
        "clf1 = ensemble.GradientBoostingRegressor(random_state=90) \n",
        "clf2 = ensemble.RandomForestRegressor(random_state=90)\n",
        "clf3 = ensemble.AdaBoostRegressor(random_state=90)\n",
        "#lr = BaggingRegressor(n_estimators=10,random_state=90)\n",
        "#sclf = StackingRegressor(Regressors=[clf1, clf2, clf3], meta_Regressor=lr)\n",
        "\n",
        "\n",
        "label = ['GradientBoosting', 'Random Forest', 'AdaBoost', 'Stacking Regressor']\n",
        "clf_list = [clf1, clf2, clf3]\n",
        "    \n",
        "fig = plt.figure(figsize=(10,8))\n",
        "gs = gridspec.GridSpec(2, 2)\n",
        "grid = itertools.product([0,1],repeat=2)\n",
        "\n",
        "\n",
        "clf_cv_mean = []\n",
        "clf_cv_std = []\n",
        "for clf, label, grd in zip(clf_list, label, grid):\n",
        "        \n",
        "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
        "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
        "    clf_cv_mean.append(scores.mean())\n",
        "    clf_cv_std.append(scores.std())\n",
        "        \n",
        "    clf.fit(X, y)\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
        "    plt.title(label)\n",
        "    vclf=clf.predict(X1_test)\n",
        "    R2=r2_score(y_test,vclf)\n",
        "    print(R2)\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N59yiy6HZkxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2.2 尝试2**"
      ],
      "metadata": {
        "id": "-L3imz21kjWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = np.hstack((comm_X1, comm_demographics))#训练集+验证集\n",
        "y=comm_y"
      ],
      "metadata": {
        "id": "9wIBjrexkzv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf8\n",
        " \n",
        " \n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.cross_validation import StratifiedKFold\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets.samples_generator import make_blobs #聚类数据生成器其参数设置详见：https://blog.csdn.net/kevinelstri/article/details/52622960\n",
        " \n",
        "'''创建训练的数据集'''\n",
        "data, target = X_all,y\n",
        " \n",
        "'''模型融合中使用到的各个单模型'''\n",
        "clfs = [ensemble.GradientBoostingRegressor(random_state=90) ,\n",
        "        ensemble.RandomForestRegressor(random_state=90),\n",
        "        ensemble.AdaBoostRegressor(random_state=90),\n",
        "        BaggingRegressor(n_estimators=10,random_state=90)]\n",
        " \n",
        "'''切分一部分数据作为测试集'''\n",
        "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.1, random_state=90)\n",
        " \n",
        " \n",
        "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
        "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
        " \n",
        "'''5折stacking'''\n",
        "n_folds = 5\n",
        "skf = list(StratifiedKFold(y, n_folds))\n",
        "for j, clf in enumerate(clfs):\n",
        "    '''依次训练各个单模型'''\n",
        "    # print(j, clf)\n",
        "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n",
        "    for i, (train, test) in enumerate(skf):\n",
        "        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
        "        # print(\"Fold\", i)\n",
        "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
        "        dataset_blend_train[test, j] = y_submission\n",
        "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
        "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
        "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
        "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
        "# clf = LogisticRegression()\n",
        "clf = BaggingRegressor(n_estimators=10,random_state=90)\n",
        "clf.fit(dataset_blend_train, y)\n",
        "y_submission = clf.predict(dataset_blend_test)[:, 1]\n",
        " \n",
        "print(\"Linear stretch of predictions to [0,1]\")\n",
        "#y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
        "print(\"blend result\")\n",
        "print(\"val r2 Score: %f\" % (r2_score(y_predict, y_submission)))\n",
        "\n"
      ],
      "metadata": {
        "id": "EO1GhoDXmFJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2.3 尝试3**"
      ],
      "metadata": {
        "id": "46Gpg7JHktGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, ShuffleSplit, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Loading Train & Test dataset\n",
        "#df_train = pd.read_csv('001_train.csv')\n",
        "#df_test = pd.read_csv('001_test.csv')\n",
        "\n",
        "\n",
        "# Predicators & Target columns\n",
        "#predicators = ['Item_Weight', 'Item_Fat_Content',\n",
        "       'Item_MRP', 'Item_Visibility', 'Items_ID',\n",
        "        'Outlet_Size','Outlet_Age', 'Outlet_Location_Type', 'Outlet_Type' ]\n",
        "#target = 'Item_Outlet_Sales'\n",
        "\n",
        "# Choosing base models\n",
        "base_models = [DecisionTreeRegressor, RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, XGBRegressor]\n",
        "\n",
        "# Setting up train dataset for ensemble model\n",
        "row_train = len(train_all) #数据量（长度）\n",
        "col_train = len(base_models) #所用模型数量\n",
        "stacking_train_dataset1 = np.zeros(shape = (row_train, col_train))\n",
        "stacking_train_dataset = pd.DataFrame(data= stacking_train_dataset1)\n",
        "# Setting up test dataset for ensemble model\n",
        "row_test = len(X1_test)\n",
        "col_test = len(base_models)\n",
        "stacking_test_dataset1 = np.zeros(shape = (row_test, col_test))\n",
        "stacking_test_dataset = pd.DataFrame(data = stacking_test_dataset1)\n",
        "\n",
        "\n",
        "# Setting up KFold splitter\n",
        "number_of_split = 10\n",
        "split = KFold(n_splits = number_of_split, random_state = 42, shuffle = True)\n",
        "\n",
        "#Training Model and Filling up Train & Test dataset for stacking ensemble\n",
        "for i, base_alg in enumerate(base_models):\n",
        "\n",
        "    counter = randint(0, number_of_split)\n",
        "    inner_counter = 0\n",
        "\n",
        "    for trainix , testix in split.split(stacking_train_dataset):\n",
        "        if inner_counter == counter:\n",
        "            x_train = train_all\n",
        "            y_train = y_train \n",
        "\n",
        "            model = base_alg()\n",
        "            print('training : %s'%(type(model)))\n",
        "            model.fit(x_train, y_train)\n",
        "\n",
        "            stacking_train_dataset[i] = model.predict(X1_val_1)\n",
        "            stacking_test_dataset[i] = model.predict(df_test[predicators])\n",
        "\n",
        "        inner_counter += 1\n",
        "\n",
        "\n",
        "# You can change the ensemble model here\n",
        "ensemble_model = LinearRegression()\n",
        "\n",
        "# Ensemble Model Prediction\n",
        "stacking_train_dataset['Ensemble'] = ensemble_model.fit(stacking_train_dataset, df_train[target]).predict(stacking_train_dataset)\n",
        "\n",
        "# Preparing for sumbission\n",
        "IDcol = ['Item_Identifier', 'Outlet_Identifier']\n",
        "submission = df_test[IDcol]\n",
        "submission[target] = ensemble_model.predict(stacking_test_dataset)\n",
        "submission.to_csv('Submit01.csv', index = False)\n",
        "\n",
        "stacking_test_dataset['Ensemble_Output'] = ensemble_model.predict(stacking_test_dataset)\n",
        "\n",
        "# Saving Created Datasets\n",
        "stacking_test_dataset.to_csv('stacking_test.csv', index = False)\n",
        "stacking_train_dataset.to_csv('stacking_train.csv', index = False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "005vzM18dN9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4、组合预测.py**"
      ],
      "metadata": {
        "id": "9Z8_x0GaRsbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 预测1（特征）**"
      ],
      "metadata": {
        "id": "NlUBEsnUA1xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1.1 数据整理**"
      ],
      "metadata": {
        "id": "LT2vKtN__LjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_CNN_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEaPpqfJ_NK",
        "outputId": "feb3a201-0bdf-4e5c-96c3-45c94a8616a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.7471347],\n",
              "       [5.737805 ],\n",
              "       [5.7776055],\n",
              "       ...,\n",
              "       [5.7335706],\n",
              "       [5.7313743],\n",
              "       [5.750322 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "vRLsKTMWX_ps",
        "outputId": "bd4f0b77-ae2e-4252-fce7-59e6a2f80fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-01d90b76d2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_RF_pred_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_RF_pred_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_new= np.expand_dims(y_RF_pred_1,axis=1)\n",
        "y_RF_pred_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUSuTXcWZUCN",
        "outputId": "1d220bf0-3806-4673-f3fd-74dfd754b096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1569, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWzkshVRW6o",
        "outputId": "1ff5b597-d49c-4b51-ada0-4bbed621ac82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.50467526],\n",
              "       [5.70948706],\n",
              "       [5.90536427],\n",
              "       ...,\n",
              "       [5.72725911],\n",
              "       [5.73694811],\n",
              "       [5.67266374]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1.2 数据预测**"
      ],
      "metadata": {
        "id": "wPREypjO_QqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2)\n",
        "print(R2_RF_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4o8s65yVwlI",
        "outputId": "c8b0f4e1-fa76-457e-80d0-1b4eb4e9ba5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8291481184004648\n",
            "0.9051198378220215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=r2+R2_RF_1\n",
        "Y_predict = (y_CNN_pred *r2+y_RF_pred_new* R2_RF_1)/t\n",
        "print(Y_predict)\n",
        "print(Y_predict.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J48qSzO9Rcly",
        "outputId": "f98a34eb-c3af-427e-d7f2-0b3389b53b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.87565263]\n",
            " [5.62744704]\n",
            " [5.8227331 ]\n",
            " ...\n",
            " [5.64986356]\n",
            " [5.62981377]\n",
            " [5.62561448]]\n",
            "(1569, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = r2_score(y_test, Y_predict)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdSW6d6MR5XM",
        "outputId": "fb0874a7-fd26-46be-f27a-cd11cc7443af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8835185924298106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 预测2（特征+疾病）**"
      ],
      "metadata": {
        "id": "8OTzKIAM9ZUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2.1 数据整理**"
      ],
      "metadata": {
        "id": "SlJH3fRpAaYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_new= np.expand_dims(y_RF_pred_2,axis=1)\n",
        "y_RF_pred_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "KDVHQWNHAfM6",
        "outputId": "a8ca1bd4-18f7-46e5-b897-3e7425244aa3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a7cc64c3b62f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_RF_pred_new\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_RF_pred_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_RF_pred_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_RF_pred_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2.2 数据预测**"
      ],
      "metadata": {
        "id": "jfmrLjw1AgIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2)\n",
        "print(R2_RF_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wm1pL9GWYYd",
        "outputId": "481d24fe-733e-4b99-a336-ff5bff5fabff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8291481184004648\n",
            "0.9391008234392352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=r2+R2_RF_2\n",
        "Y_predict = (y_CNN_pred *r2 + y_RF_pred_new* R2_RF_2)/t\n",
        "print(Y_predict)\n",
        "print(Y_predict.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77M8589ACxr",
        "outputId": "51fee59a-ff82-4608-c6dc-c461d6e73bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6.11147823]\n",
            " [5.62597148]\n",
            " [5.7174304 ]\n",
            " ...\n",
            " [5.64337355]\n",
            " [5.61457943]\n",
            " [5.66451593]]\n",
            "(1569, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = r2_score(y_test, Y_predict)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf7k74QwAFL0",
        "outputId": "632f2bd7-1b2f-4a7c-df07-75ec2a79675a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9037265637633737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Weighted_method(y_CNN_pred, y_RF_pred_new , w=[1/3, 1/3]):\n",
        "  t=w[0]+w[1]\n",
        "  Weighted_result = (y_CNN_pred *w[0] + y_RF_pred_new*w[1])/t \n",
        "  return Weighted_result\n",
        "w=[]\n",
        "w.append(r2)\n",
        "w.append(R2_RF_2)\n",
        "Weighted_pre = Weighted_method(y_CNN_pred, y_RF_pred_new ,w) \n",
        "print('Weighted_pre MAE:',mean_absolute_error(y_test, Weighted_pre))   # 会发现这个效果会提高一些\n",
        "print('Weighted_pre r2:',r2_score(y_test, Weighted_pre))   # 会发现这个效果会提高一些"
      ],
      "metadata": {
        "id": "Oo-Jfp5Jy1XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3 RF3组合预测**"
      ],
      "metadata": {
        "id": "iSf7qeD8kRSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3.1 拟合优度权重**"
      ],
      "metadata": {
        "id": "buvm39zIYwNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_new= np.expand_dims(y_RF_pred_3,axis=1)\n",
        "y_RF_pred_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69xm6APtuiOK",
        "outputId": "e3103589-c05e-449e-99e0-0a16641fdc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1569, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2)\n",
        "print(R2_RF_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8iGBSRnke_P",
        "outputId": "27e20ddb-a11a-4314-88ce-65f92893e12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9370573616906956\n",
            "0.9416302371979749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "t=r2+R2_RF_3\n",
        "Y_predict = (y_CNN_pred *(r2) + y_RF_pred_new* (R2_RF_3))/t\n",
        "print(Y_predict)\n",
        "print(Y_predict.shape)\n",
        "\n",
        "score = r2_score(y_test, Y_predict)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_MZCNRZ5GoA",
        "outputId": "ab275b57-7cb2-4948-cf8a-4738204ee05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.87321832]\n",
            " [5.72073101]\n",
            " [5.74058307]\n",
            " ...\n",
            " [5.71861902]\n",
            " [5.71752356]\n",
            " [5.76541073]]\n",
            "(1569, 1)\n",
            "0.922890864492953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2=0.904542947043012\n",
        "t=r2+R2_RF_3\n",
        "Y_predict = (y_CNN_pred *(r2) + y_RF_pred_new* (R2_RF_3))/t\n",
        "print(Y_predict)\n",
        "print(Y_predict.shape)\n",
        "\n",
        "score = r2_score(y_test, Y_predict)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm2YzRDm5Bw1",
        "outputId": "203cdb0b-8907-42a8-906e-307170cc72fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.90304553]\n",
            " [5.71632628]\n",
            " [5.85498241]\n",
            " ...\n",
            " [5.7052852 ]\n",
            " [5.69435484]\n",
            " [5.72257327]]\n",
            "(1569, 1)\n",
            "0.9173941478197482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3.2 以 1/MAE 为权重**"
      ],
      "metadata": {
        "id": "16rzvbhbY37H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = '有时间间隔_5月16日(下午)_'+'第7次训练_'+'MG_merge' + path # %i\n",
        "#定义模型\n",
        "model= ATTENTION_multi_channel_split_model(demgras_dim)\n",
        "test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
        "model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "y_CNN_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "r22,rmse = evaluation(y_test, y_CNN_pred)\n",
        "print(\"r2:\",r22,\"rmse:\",rmse)\n",
        "r2_CNN_test.append(r22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0ITT_mGJXiC",
        "outputId": "39534eb3-20a5-485e-f07e-eb243e9fce56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_23/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_23'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_133\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_92 (Embedding)       (None, 17, 900)      649800      ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_93 (Embedding)       (None, 17, 900)      649800      ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_94 (Embedding)       (None, 17, 900)      649800      ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_95 (Embedding)       (None, 17, 900)      649800      ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_69 (Conv1D)             (None, 17, 100)      270100      ['embedding_92[0][0]',           \n",
            "                                                                  'embedding_93[0][0]',           \n",
            "                                                                  'embedding_94[0][0]',           \n",
            "                                                                  'embedding_95[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_70 (Conv1D)             (None, 17, 100)      270100      ['embedding_92[0][0]',           \n",
            "                                                                  'embedding_93[0][0]',           \n",
            "                                                                  'embedding_94[0][0]',           \n",
            "                                                                  'embedding_95[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_71 (Conv1D)             (None, 17, 100)      270100      ['embedding_92[0][0]',           \n",
            "                                                                  'embedding_93[0][0]',           \n",
            "                                                                  'embedding_94[0][0]',           \n",
            "                                                                  'embedding_95[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_276 (Glob  (None, 100)         0           ['conv1d_69[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_277 (Glob  (None, 100)         0           ['conv1d_69[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_278 (Glob  (None, 100)         0           ['conv1d_69[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_279 (Glob  (None, 100)         0           ['conv1d_69[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_280 (Glob  (None, 100)         0           ['conv1d_70[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_281 (Glob  (None, 100)         0           ['conv1d_70[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_282 (Glob  (None, 100)         0           ['conv1d_70[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_283 (Glob  (None, 100)         0           ['conv1d_70[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_284 (Glob  (None, 100)         0           ['conv1d_71[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_285 (Glob  (None, 100)         0           ['conv1d_71[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_286 (Glob  (None, 100)         0           ['conv1d_71[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_287 (Glob  (None, 100)         0           ['conv1d_71[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_48 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 100)          0           ['global_max_pooling1d_276[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_277[0][0]\n",
            "                                                                 ',                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                  'global_max_pooling1d_278[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_279[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_70 (Add)                   (None, 100)          0           ['global_max_pooling1d_280[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_281[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_282[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_283[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 100)          0           ['global_max_pooling1d_284[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_285[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_286[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_287[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 1000)         11000       ['input_48[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 1300)         0           ['add_69[0][0]',                 \n",
            "                                                                  'add_70[0][0]',                 \n",
            "                                                                  'add_71[0][0]',                 \n",
            "                                                                  'dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_23 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_23[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_23 (G  (1, 1300)           0           ['tf.expand_dims_23[0][0]']      \n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_23 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_23[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (1, 1, 1, 650)       845650      ['reshape_23[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_23 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_23[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_23 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_94[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_23 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_23[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_23[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_23 (TFOpL  (None, 1300)        0           ['multiply_23[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_23[0][0]']\n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_115[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_116[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_117 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_117[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_118 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_118[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_119 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 1)            3           ['dropout_119[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Testing model...\n",
            "13/13 [==============================] - 1s 16ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07077091932296753, RMSE:0.2660280466079712, MAE:0.10452725738286972, R2:0.9212146105790974\n",
            "r2: 0.9212146105790974 rmse: 0.26602805\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 18ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07077091932296753, RMSE:0.2660280466079712, MAE:0.10452725738286972, R2:0.9212146105790974\n",
            "r2: 0.9212146105790974 rmse: 0.26602805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_new= np.expand_dims(y_RF_pred_3,axis=1)\n",
        "y_RF_pred_new.shape\n",
        "a=1/0.104527\n",
        "b=1/0.104517\n",
        "t=a+b\n",
        "Y_predict = (y_CNN_pred *(a) + y_RF_pred_new* (b))/t\n",
        "print(Y_predict)\n",
        "print(Y_predict.shape)\n",
        "\n",
        "score = r2_score(y_test, Y_predict)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSKs87D6kXCD",
        "outputId": "1b575e24-32a3-41bb-84c9-53da11a2d82d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.14885529]\n",
            " [8.1756007 ]\n",
            " [5.75853155]\n",
            " ...\n",
            " [8.25161563]\n",
            " [5.79005329]\n",
            " [5.70253697]]\n",
            "(1569, 1)\n",
            "0.9270155410507026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R2_组合, rmse = evaluation(y_test, Y_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZdSkRvm60OG",
        "outputId": "ca54066a-7246-429c-efa2-ff520c3e7cf8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float64\n",
            "MSE:0.06556009123347507, RMSE:0.25604704886695157, MAE:0.10117292599279153, R2:0.9270155410507026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **没有时间间隔**"
      ],
      "metadata": {
        "id": "B7uUrFjzLMaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = '没有时间间隔_5月16日(下午)_'+'第2次训练_'+'MG_merge' + path # %i\n",
        "#定义模型\n",
        "model= ATTENTION_multi_channel_split_model(demgras_dim)\n",
        "test_ATTENTION_multi_channel_split_model(model, modelpath, X1_test, demographics_test,y_test,3)\n",
        "model.load_weights(\"/content/gdrive/MyDrive/cnn_model/%s.hdf5\"%(modelpath))\n",
        "y_CNN_pred = model.predict([X1_test, demographics_test], batch_size=args.batchsize, verbose=1)\n",
        "r22,rmse = evaluation(y_test, y_CNN_pred)\n",
        "print(\"r2:\",r22,\"rmse:\",rmse)\n",
        "r2_CNN_test.append(r22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxSi9V1KLXAm",
        "outputId": "e5b058c2-58f1-46c9-b8e5-1749ffaa3934"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demgras_dim:10\n",
            "demgras_in.shape:(None, 10)\n",
            "通道数:1300\n",
            "sigmoid激活，权重归一化: KerasTensor(type_spec=TensorSpec(shape=(1, 1, 1, 1300), dtype=tf.float32, name=None), name='tf.math.sigmoid_24/Sigmoid:0', description=\"created by layer 'tf.math.sigmoid_24'\")\n",
            "(1, 1, None, 1300)\n",
            "Model: \"model_134\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_49 (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_96 (Embedding)       (None, 17, 900)      649800      ['input_49[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_97 (Embedding)       (None, 17, 900)      649800      ['input_49[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_98 (Embedding)       (None, 17, 900)      649800      ['input_49[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_99 (Embedding)       (None, 17, 900)      649800      ['input_49[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_72 (Conv1D)             (None, 17, 100)      270100      ['embedding_96[0][0]',           \n",
            "                                                                  'embedding_97[0][0]',           \n",
            "                                                                  'embedding_98[0][0]',           \n",
            "                                                                  'embedding_99[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_73 (Conv1D)             (None, 17, 100)      270100      ['embedding_96[0][0]',           \n",
            "                                                                  'embedding_97[0][0]',           \n",
            "                                                                  'embedding_98[0][0]',           \n",
            "                                                                  'embedding_99[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_74 (Conv1D)             (None, 17, 100)      270100      ['embedding_96[0][0]',           \n",
            "                                                                  'embedding_97[0][0]',           \n",
            "                                                                  'embedding_98[0][0]',           \n",
            "                                                                  'embedding_99[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_288 (Glob  (None, 100)         0           ['conv1d_72[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_289 (Glob  (None, 100)         0           ['conv1d_72[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_290 (Glob  (None, 100)         0           ['conv1d_72[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_291 (Glob  (None, 100)         0           ['conv1d_72[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_292 (Glob  (None, 100)         0           ['conv1d_73[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_293 (Glob  (None, 100)         0           ['conv1d_73[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_294 (Glob  (None, 100)         0           ['conv1d_73[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_295 (Glob  (None, 100)         0           ['conv1d_73[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_296 (Glob  (None, 100)         0           ['conv1d_74[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_297 (Glob  (None, 100)         0           ['conv1d_74[1][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_298 (Glob  (None, 100)         0           ['conv1d_74[2][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_299 (Glob  (None, 100)         0           ['conv1d_74[3][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " input_50 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " add_72 (Add)                   (None, 100)          0           ['global_max_pooling1d_288[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_289[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_290[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_291[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_73 (Add)                   (None, 100)          0           ['global_max_pooling1d_292[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_293[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_294[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_295[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " add_74 (Add)                   (None, 100)          0           ['global_max_pooling1d_296[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_297[0][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_max_pooling1d_298[0][0]'\n",
            "                                                                 , 'global_max_pooling1d_299[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 1000)         11000       ['input_50[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 1300)         0           ['add_72[0][0]',                 \n",
            "                                                                  'add_73[0][0]',                 \n",
            "                                                                  'add_74[0][0]',                 \n",
            "                                                                  'dense_96[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_24 (TFOpLambda)  (1, None, 1300)     0           ['concatenate_24[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling1d_24 (G  (1, 1300)           0           ['tf.expand_dims_24[0][0]']      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " reshape_24 (Reshape)           (1, 1, 1, 1300)      0           ['global_average_pooling1d_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (1, 1, 1, 650)       845650      ['reshape_24[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_24 (TFOpLambda)     (1, 1, 1, 650)       0           ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (1, 1, 1, 1300)      846300      ['tf.nn.relu_24[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_24 (TFOpLambda  (1, 1, 1, 1300)     0           ['dense_98[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " multiply_24 (Multiply)         (1, 1, None, 1300)   0           ['tf.expand_dims_24[0][0]',      \n",
            "                                                                  'tf.math.sigmoid_24[0][0]']     \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_24 (TFOpL  (None, 1300)        0           ['multiply_24[0][0]']            \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " F1 (Dense)                     (None, 1000)         1301000     ['tf.compat.v1.squeeze_24[0][0]']\n",
            "                                                                                                  \n",
            " dropout_120 (Dropout)          (None, 1000)         0           ['F1[0][0]']                     \n",
            "                                                                                                  \n",
            " F2 (Dense)                     (None, 500)          500500      ['dropout_120[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_121 (Dropout)          (None, 500)          0           ['F2[0][0]']                     \n",
            "                                                                                                  \n",
            " F3 (Dense)                     (None, 100)          50100       ['dropout_121[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_122 (Dropout)          (None, 100)          0           ['F3[0][0]']                     \n",
            "                                                                                                  \n",
            " F4 (Dense)                     (None, 50)           5050        ['dropout_122[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_123 (Dropout)          (None, 50)           0           ['F4[0][0]']                     \n",
            "                                                                                                  \n",
            " F5 (Dense)                     (None, 2)            102         ['dropout_123[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_124 (Dropout)          (None, 2)            0           ['F5[0][0]']                     \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 1)            3           ['dropout_124[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,969,205\n",
            "Trainable params: 6,969,205\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Testing model...\n",
            "13/13 [==============================] - 1s 17ms/step\n",
            "y_pred:(1569, 1)\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07188301533460617, RMSE:0.268110066652298, MAE:0.1207733228802681, R2:0.9199765740763938\n",
            "r2: 0.9199765740763938 rmse: 0.26811007\n",
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:446: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始步骤92/2:预测可视化-医疗费用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:457: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!开始画图：预测值与真实值\n",
            "y_test: (1569, 1)\n",
            "y_pred: (1569, 1)\n",
            "y_pred.ndim: 2\n",
            "y_test.ndim: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 17ms/step\n",
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float32\n",
            "MSE:0.07188301533460617, RMSE:0.268110066652298, MAE:0.1207733228802681, R2:0.9199765740763938\n",
            "r2: 0.9199765740763938 rmse: 0.26811007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_new= np.expand_dims(y_RF_pred_3,axis=1)\n",
        "y_RF_pred_new.shape\n",
        "a=1/0.120773\n",
        "b=1/0.104517\n",
        "t=a+b\n",
        "Y_predict = (y_CNN_pred *(a) + y_RF_pred_new* (b))/t\n",
        "print(Y_predict)\n",
        "print(Y_predict.shape)\n",
        "\n",
        "score = r2_score(y_test, Y_predict)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DpS5QZ1MPg2",
        "outputId": "72ac436b-dd16-42aa-8a27-f2c34b52e9e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.16601276]\n",
            " [8.20135302]\n",
            " [5.77544829]\n",
            " ...\n",
            " [8.28682786]\n",
            " [5.80673686]\n",
            " [5.71162716]]\n",
            "(1569, 1)\n",
            "0.9257909201932832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R2_组合, rmse = evaluation(y_test, Y_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgXnHgQRMUai",
        "outputId": "ad19ba87-bda8-4439-ae61-1bf261b79324"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test:(1569, 1)\n",
            "y_pred:(1569, 1)\n",
            "float64\n",
            "MSE:0.06666013713769428, RMSE:0.25818624505905474, MAE:0.10891758684677759, R2:0.9257909201932832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3.3 其他权重确定方法**"
      ],
      "metadata": {
        "id": "64ZosbnFZJi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_CNN_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGX03uENZOhY",
        "outputId": "b27842ae-df57-4ea7-bf5c-e7bfc9545cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.732129],\n",
              "       [5.752484],\n",
              "       [5.740054],\n",
              "       ...,\n",
              "       [8.680058],\n",
              "       [5.695688],\n",
              "       [5.757047]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_CNN_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2sVw-iBfng-",
        "outputId": "27216675-7f72-4328-d61e-bc9d58fbcea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.7357826],\n",
              "       [5.7293897],\n",
              "       [6.011958 ],\n",
              "       ...,\n",
              "       [5.706889 ],\n",
              "       [5.6846137],\n",
              "       [5.6890583]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_CNN_hat_1=pd.DataFrame(y_CNN_hat)\n",
        "y_CNN_hat_1.to_csv('/content/gdrive/MyDrive/y_CNN_hat.csv')\n",
        "y_CNN_pred_1=pd.DataFrame(y_CNN_pred)\n",
        "y_CNN_pred_1.to_csv('/content/gdrive/MyDrive/y_CNN_pred.csv')"
      ],
      "metadata": {
        "id": "-qIs4daPfswJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3.4 遗传算法**"
      ],
      "metadata": {
        "id": "bfTZ0vT1ki7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#coding=utf-8\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        " \n",
        "generations = 20   # 繁殖代数 100\n",
        "pop_size = 20      # 种群数量  500\n",
        "max_value = 10      # 基因中允许出现的最大值  \n",
        "chrom_length = 8    # 染色体长度  \n",
        "pc = 0.6            # 交配概率  \n",
        "pm = 0.01           # 变异概率  \n",
        "results = [[]]      # 存储每一代的最优解，N个三元组（auc最高值, n_estimators, max_depth）  \n",
        "fit_value = []      # 个体适应度  \n",
        "fit_mean = []       # 平均适应度 \n",
        "pop = [[0, 1, 0, 1, 0, 1, 0, 1] for i in range(pop_size)] # 初始化种群中所有个体的基因初始序列\n",
        "\n",
        "#可行解在遗传算法中均被称为染色体 \n",
        " \n",
        " \n",
        " \n",
        "'''\n",
        "n_estimators 取 {10、20、30、40、50、60、70、80、90、100、110、120、130、140、150、160}\n",
        "max_depth 取 {1、2、3、4、5、6、7、8、9、10、11、12、13、14、15、16} \n",
        "（1111，1111）基因组8位长\n",
        "'''\n",
        "def aimFunction(a,b):\n",
        "  \n",
        "    rf = y_val_1 - a*y_train_1 - b*y_RF_pred\n",
        "   \n",
        "    return rf\n",
        " \n",
        "# Step 1 : 对参数进行编码（用于初始化基因序列，可以选择初始化基因序列，本函数省略）\n",
        "def geneEncoding(pop_size, chrom_length):  \n",
        "    pop = [[]]\n",
        "    for i in range(pop_size):\n",
        "        temp = []\n",
        "        for j in range(chrom_length):\n",
        "            temp.append(random.randint(0, 1))\n",
        "        pop.append(temp)\n",
        "    return pop[1:]\n",
        " \n",
        " \n",
        "# Step 2 : 计算个体的目标函数值\n",
        "def cal_obj_value(pop):\n",
        "    objvalue = []\n",
        "    variable = decodechrom(pop)\n",
        "    for i in range(len(variable)):\n",
        "        tempVar = variable[i]\n",
        "        n_estimators_value = (tempVar[0] + 1) * 10\n",
        "        max_depth_value = tempVar[1] + 1\n",
        "        aucValue = aimFunction(a,b)\n",
        "        objvalue.append(aucValue)\n",
        "    return objvalue #目标函数值objvalue[m] 与个体基因 pop[m] 对应 \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# 对每个个体进行解码，并拆分成单个变量，返回 n_estimators 和 max_depth\n",
        "def decodechrom(pop):\n",
        "    variable = []\n",
        "    n_estimators_value = []\n",
        "    max_depth_value = []\n",
        "    for i in range(len(pop)):\n",
        "        res = []\n",
        "        \n",
        "        # 计算第一个变量值，即 0101->10(逆转)\n",
        "        temp1 = pop[i][0:4]\n",
        "        preValue = 0\n",
        "        for pre in range(4):\n",
        "            preValue += temp1[pre] * (math.pow(2, pre))\n",
        "        res.append(int(preValue))\n",
        "        \n",
        "        # 计算第二个变量值\n",
        "        temp2 = pop[i][4:8]\n",
        "        aftValue = 0\n",
        "        for aft in range(4):\n",
        "            aftValue += temp2[aft] * (math.pow(2, aft))\n",
        "        res.append(int(aftValue))\n",
        "        variable.append(res)\n",
        "    return variable\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# Step 3: 计算个体的适应值（计算最大值，于是就淘汰负值就好了）\n",
        "def calfitvalue(obj_value):\n",
        "    fit_value = []\n",
        "    temp = 0.0\n",
        "    Cmin = 0\n",
        "    for i in range(len(obj_value)):\n",
        "        if(obj_value[i] + Cmin > 0):\n",
        "            temp = Cmin + obj_value[i]\n",
        "        else:\n",
        "            temp = 0.0\n",
        "        fit_value.append(temp)\n",
        "    return fit_value\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# Step 4: 找出适应函数值中最大值，和对应的个体\n",
        "def best(pop, fit_value):\n",
        "    best_individual = pop[0]\n",
        "    best_fit = fit_value[0]\n",
        "    for i in range(1, len(pop)):\n",
        "        if(fit_value[i] > best_fit):\n",
        "            best_fit = fit_value[i]\n",
        "            best_individual = pop[i]\n",
        "    return [best_individual, best_fit]\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# Step 5: 每次繁殖，将最好的结果记录下来(将二进制转化为十进制)\n",
        "def b2d(best_individual):\n",
        "    temp1 = best_individual[0:4]\n",
        "    preValue = 0\n",
        "    for pre in range(4):\n",
        "        preValue += temp1[pre] * (math.pow(2, pre))\n",
        "    preValue = preValue + 1\n",
        "    preValue = preValue * 10\n",
        "    \n",
        "    # 计算第二个变量值\n",
        "    temp2 = best_individual[4:8]\n",
        "    aftValue = 0\n",
        "    for aft in range(4):\n",
        "        aftValue += temp2[aft] * (math.pow(2, aft))\n",
        "    aftValue = aftValue + 1\n",
        "    return int(preValue), int(aftValue)\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# Step 6: 自然选择（轮盘赌算法）\n",
        "def selection(pop, fit_value):\n",
        "    # 计算每个适应值的概率\n",
        "    new_fit_value = []\n",
        "    total_fit = sum(fit_value)\n",
        "    for i in range(len(fit_value)):\n",
        "        new_fit_value.append(fit_value[i] / total_fit)\n",
        "    # 计算每个适应值的累积概率\n",
        "    cumsum(new_fit_value)\n",
        "    # 生成随机浮点数序列\n",
        "    ms = []\n",
        "    pop_len = len(pop)\n",
        "    for i in range(pop_len):\n",
        "        ms.append(random.random())\n",
        "    # 对生成的随机浮点数序列进行排序\n",
        "    ms.sort()\n",
        "    # 轮盘赌算法（选中的个体成为下一轮，没有被选中的直接淘汰，被选中的个体代替）\n",
        "    fitin = 0\n",
        "    newin = 0\n",
        "    newpop = pop\n",
        "    while newin < pop_len:\n",
        "        if(ms[newin] < new_fit_value[fitin]):\n",
        "            newpop[newin] = pop[fitin]\n",
        "            newin = newin + 1\n",
        "        else:\n",
        "            fitin = fitin + 1\n",
        "    pop = newpop\n",
        " \n",
        " \n",
        "# 求适应值的总和\n",
        "def sum(fit_value):\n",
        "    total = 0\n",
        "    for i in range(len(fit_value)):\n",
        "        total += fit_value[i]\n",
        "    return total\n",
        " \n",
        " \n",
        "# 计算累积概率\n",
        "def cumsum(fit_value):\n",
        "    temp=[]\n",
        "    for i in range(len(fit_value)):\n",
        "        t = 0\n",
        "        j = 0\n",
        "        while(j <= i):\n",
        "            t += fit_value[j]\n",
        "            j = j + 1\n",
        "        temp.append(t)\n",
        "    for i in range(len(fit_value)):\n",
        "        fit_value[i]=temp[i]\n",
        " \n",
        " \n",
        "# Step 7: 交叉繁殖\n",
        "def crossover(pop, pc): #个体间交叉，实现基因交换\n",
        "    poplen = len(pop)\n",
        "    for i in range(poplen - 1):\n",
        "        if(random.random() < pc):\n",
        "            cpoint = random.randint(0,len(pop[0]))\n",
        "            temp1 = []\n",
        "            temp2 = []\n",
        "            temp1.extend(pop[i][0 : cpoint])\n",
        "            temp1.extend(pop[i+1][cpoint : len(pop[i])])\n",
        "            temp2.extend(pop[i+1][0 : cpoint])\n",
        "            temp2.extend(pop[i][cpoint : len(pop[i])])\n",
        "            pop[i] = temp1\n",
        "            pop[i+1] = temp2\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# Step 8: 基因突变\n",
        "def mutation(pop, pm):\n",
        "    px = len(pop)\n",
        "    py = len(pop[0])\n",
        "    for i in range(px):\n",
        "        if(random.random() < pm):\n",
        "            mpoint = random.randint(0,py-1)\n",
        "            if(pop[i][mpoint] == 1):\n",
        "                pop[i][mpoint] = 0\n",
        "            else:\n",
        "                pop[i][mpoint] = 1\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    # pop = geneEncoding(pop_size, chrom_length)\n",
        "    for i in range(generations):\n",
        "        print(\"第 \" + str(i) + \" 代开始繁殖......\")\n",
        "        obj_value = cal_obj_value(pop) # 计算目标函数值\n",
        "        #print(obj_value)\n",
        "        fit_value = calfitvalue(obj_value) #计算个体的适应值\n",
        "        # print(fit_value)\n",
        "        [best_individual, best_fit] = best(pop, fit_value) #选出最好的个体和最好的函数值\n",
        "        # print(\"best_individual: \"+ str(best_individual))\n",
        "        temp_n_estimator, temp_max_depth = b2d(best_individual)\n",
        "        results.append([best_fit, temp_n_estimator, temp_max_depth]) #每次繁殖，将最好的结果记录下来\n",
        "        print(str(best_individual) + \" \" + str(best_fit))\n",
        "        selection(pop, fit_value) #自然选择，淘汰掉一部分适应性低的个体\n",
        "        crossover(pop, pc) #交叉繁殖\n",
        "        mutation(pop, pc) #基因突变\n",
        "    # print(results)\n",
        "    results.sort()\n",
        "    print(results[-1])"
      ],
      "metadata": {
        "id": "9YkpEPprPC-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ATTENTION_CNN_MODEL-66.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}